title,description,tags,answer_count,accepted_answers,creation_date,accepted_answer_creation_date,view_count,link,combined_text_desc,fully_processed_combined_desc,fully_processed_answers,combine_answer_with_desc_and_title,predicted_category
NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models,"I am trying to set up hugging face locally and im running into this issue. NameError: name 'init_empty_weights' is not defined Here is the code I have tested my installation with from transformers import pipeline classifier = pipeline(""sentiment-analysis"") text = ""I love using Hugging Face Transformers!"" result = classifier(text) print(result) transformers: 4.51.0 tokenizers: 0.21.1 accelerate: 1.6.0 sentence-transformers: 4.0.2 huggingface_hub: 0.30.1 I am currently using pytorch-metal mac M3 pro. What causes this, and how can I fix it?","['nlp', 'huggingface-transformers', 'huggingface']",2,"Try using this version, it should resolve the issue. transformers==4.50.3",2025-04-07 11:02:41,2025-04-16 10:57:35,638,https://stackoverflow.com/questions/79559702/nameerror-name-init-empty-weights-is-not-defined-while-using-hugging-face-mod,"NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models I am trying to set up hugging face locally and im running into this issue. NameError: name 'init_empty_weights' is not defined Here is the code I have tested my installation with from transformers import pipeline classifier = pipeline(""sentiment-analysis"") text = ""I love using Hugging Face Transformers!"" result = classifier(text) print(result) transformers: 4.51.0 tokenizers: 0.21.1 accelerate: 1.6.0 sentence-transformers: 4.0.2 huggingface_hub: 0.30.1 I am currently using pytorch-metal mac M3 pro. What causes this, and how can I fix it?",nameerror  name   39  initemptyweight   39  define use hug face model try set hug face locally i m run issue  nameerror  name  initemptyweight  define code test installation transformer import pipeline classifier  pipeline    sentiment  analysis   text    love use hugging face transformers   result  classifier  text  print  result  transformer  4510 tokenizer  0211 accelerate  160 sentence  transformer  402 huggingfacehub  0301 currently use pytorch  metal mac m3 pro  cause  fix ,try use version  resolve issue  transformers4503,nameerror  name   39  initemptyweight   39  define use hug face model try set hug face locally i m run issue  nameerror  name  initemptyweight  define code test installation transformer import pipeline classifier  pipeline    sentiment  analysis   text    love use hugging face transformers   result  classifier  text  print  result  transformer  4510 tokenizer  0211 accelerate  160 sentence  transformer  402 huggingfacehub  0301 currently use pytorch  metal mac m3 pro  cause  fix  try use version  resolve issue  transformers4503,Implementation Issues
Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?,"I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER): import spacy nlp = spacy.load(""pl_core_news_lg"") text = ""Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google."" doc = nlp(text) entities = [(ent.text, ent.label_) for ent in doc.ents] print(entities) Output: [('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')] However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities. from presidio_analyzer import AnalyzerEngine, RecognizerRegistry from presidio_analyzer.nlp_engine import NlpEngineProvider provider = NlpEngineProvider(conf_file=""path_to_my_file/nlp_config.yaml"") nlp_engine = provider.create_engine() print(f""Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}"") supported_languages = list(nlp_engine.get_supported_languages()) registry = RecognizerRegistry(supported_languages=[""pl""]) registry.load_predefined_recognizers([""pl""]) print(f""Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}"") analyzer = AnalyzerEngine( registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine ) results = analyzer.analyze(text, ""pl"") for entity in results: print(f""Found entity: {entity.entity_type} with score {entity.score}"") Output: Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION'] Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS'] Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text. My config file: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?","['python', 'nlp', 'spacy', 'presidio']",1,"The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine : labels_to_ignore: - O On your configuration it would look like this: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: labels_to_ignore: - O model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Edit: This was fixed to be the default if 'labels_to_ignore' is not specified Will be part of version 2.2.359 release",2025-04-02 05:56:11,2025-04-03 07:02:54,97,https://stackoverflow.com/questions/79549787/why-does-presidio-with-spacy-nlp-engine-not-recognize-organizations-and-pesel-wh,"Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does? I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER): import spacy nlp = spacy.load(""pl_core_news_lg"") text = ""Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google."" doc = nlp(text) entities = [(ent.text, ent.label_) for ent in doc.ents] print(entities) Output: [('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')] However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities. from presidio_analyzer import AnalyzerEngine, RecognizerRegistry from presidio_analyzer.nlp_engine import NlpEngineProvider provider = NlpEngineProvider(conf_file=""path_to_my_file/nlp_config.yaml"") nlp_engine = provider.create_engine() print(f""Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}"") supported_languages = list(nlp_engine.get_supported_languages()) registry = RecognizerRegistry(supported_languages=[""pl""]) registry.load_predefined_recognizers([""pl""]) print(f""Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}"") analyzer = AnalyzerEngine( registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine ) results = analyzer.analyze(text, ""pl"") for entity in results: print(f""Found entity: {entity.entity_type} with score {entity.score}"") Output: Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION'] Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS'] Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text. My config file: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?",presidio spacy nlp engine recognize organization pesel spacy   m use spacy plcorenewslg model extract name entity polish text  correctly detect organization  org  people s name  per   import spacy nlp  spacyload    plcorenewslg   text    jan kowalski pracuje w ibm wsppracuje z microsoft oraz google   doc  nlp  text  entity    enttext  entlabel   ent docent  print  entity  output     jan kowalski    persname      ibm    orgname      microsoft    orgname      google    orgname    however  use presidio plcorenewslg model configuration file  recognizer correctly detect organization  org  pesel number  even though appear list support entity  presidioanalyzer import analyzerengine  recognizerregistry presidioanalyzernlpengine import nlpengineprovider provider  nlpengineprovider  conffile  pathtomyfile  nlpconfigyaml   nlpengine  providercreateengine   print  f  supported recognizer  nlp engine    nlpenginegetsupportedentitie      supportedlanguage  list  nlpenginegetsupportedlanguage    registry  recognizerregistry  supportedlanguages    pl    registryloadpredefinedrecognizer     pl    print  f  supported recognizer  registry    registrygetsupportedentitie    pl       analyzer  analyzerengine  registry  registry  supportedlanguage  supportedlanguage  nlpengine  nlpengine  result  analyzeranalyze  text    pl   entity result  print  f  find entity   entityentitytype  score  entityscore    output  support recognizer  nlp engine     id    nrp    datetime    person    location   support recognizer  registry     invoter    url    ibancode    creditcard    datetime    nrp    phonenumber    medicallicense    person    ipaddress    organization    crypto    location    plpesel    emailaddress   even though  organization   plpesel  list  org list nlp engine  support recognizer  presidio detect correctly text  config file  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id presidio fail detect organization  org  pesel number  plpesel   spacy correctly detect ,configuration file miss  labelstoignore  field  state entity ignore nlp engine  labelstoignore   configuration would look like  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  labelstoignore   modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id edit  fix default  labelstoignore  specify part version 22359 release,presidio spacy nlp engine recognize organization pesel spacy   m use spacy plcorenewslg model extract name entity polish text  correctly detect organization  org  people s name  per   import spacy nlp  spacyload    plcorenewslg   text    jan kowalski pracuje w ibm wsppracuje z microsoft oraz google   doc  nlp  text  entity    enttext  entlabel   ent docent  print  entity  output     jan kowalski    persname      ibm    orgname      microsoft    orgname      google    orgname    however  use presidio plcorenewslg model configuration file  recognizer correctly detect organization  org  pesel number  even though appear list support entity  presidioanalyzer import analyzerengine  recognizerregistry presidioanalyzernlpengine import nlpengineprovider provider  nlpengineprovider  conffile  pathtomyfile  nlpconfigyaml   nlpengine  providercreateengine   print  f  supported recognizer  nlp engine    nlpenginegetsupportedentitie      supportedlanguage  list  nlpenginegetsupportedlanguage    registry  recognizerregistry  supportedlanguages    pl    registryloadpredefinedrecognizer     pl    print  f  supported recognizer  registry    registrygetsupportedentitie    pl       analyzer  analyzerengine  registry  registry  supportedlanguage  supportedlanguage  nlpengine  nlpengine  result  analyzeranalyze  text    pl   entity result  print  f  find entity   entityentitytype  score  entityscore    output  support recognizer  nlp engine     id    nrp    datetime    person    location   support recognizer  registry     invoter    url    ibancode    creditcard    datetime    nrp    phonenumber    medicallicense    person    ipaddress    organization    crypto    location    plpesel    emailaddress   even though  organization   plpesel  list  org list nlp engine  support recognizer  presidio detect correctly text  config file  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id presidio fail detect organization  org  pesel number  plpesel   spacy correctly detect  configuration file miss  labelstoignore  field  state entity ignore nlp engine  labelstoignore   configuration would look like  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  labelstoignore   modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id edit  fix default  labelstoignore  specify part version 22359 release,Implementation Issues
"GPT-2 and other models from huggingface -100 label index for training, instead of pad token","I understand the -100 label id is used so that the predictions for these are not included when calculating the loss. However on huggingface , they state ""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument ""ignore_index"". Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same? The way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.","['nlp', 'huggingface-transformers', 'pre-trained-model']",1,The author of the tutorial you mentioned sets it to -100 and uses ignore_index to save a few lines of code. You don't see the line where the author pass something to ignore_index because it has a default value. The default value of ignore_index for nn.CrossEntropyLoss is -100 . Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.,2025-04-01 09:21:17,2025-04-02 17:11:43,54,https://stackoverflow.com/questions/79548202/gpt-2-and-other-models-from-huggingface-100-label-index-for-training-instead-o,"GPT-2 and other models from huggingface -100 label index for training, instead of pad token I understand the -100 label id is used so that the predictions for these are not included when calculating the loss. However on huggingface , they state ""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument ""ignore_index"". Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same? The way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.",gpt2 model huggingface 100 label index training  instead pad token understand 100 label i d use prediction include calculate loss  however huggingface  state   complicated list comprehension padtokenid alone good enough know whether label exclude   replace pad token  implementation  use nn  crossentropyloss    argument   ignoreindex   benefit change i d 100 oppose add argument ignoreindex loss set pad token i d  result  way write make think benefit  description   ignoreindex  appear achieve want ,author tutorial mention set 100 use ignoreindex save line code  not see line author pass something ignoreindex default value  default value ignoreindex nn  crossentropyloss 100  use value instead respective pad token i d allow write model indepent training code not pass pad token i d tokenizer loss function ,gpt2 model huggingface 100 label index training  instead pad token understand 100 label i d use prediction include calculate loss  however huggingface  state   complicated list comprehension padtokenid alone good enough know whether label exclude   replace pad token  implementation  use nn  crossentropyloss    argument   ignoreindex   benefit change i d 100 oppose add argument ignoreindex loss set pad token i d  result  way write make think benefit  description   ignoreindex  appear achieve want  author tutorial mention set 100 use ignoreindex save line code  not see line author pass something ignoreindex default value  default value ignoreindex nn  crossentropyloss 100  use value instead respective pad token i d allow write model indepent training code not pass pad token i d tokenizer loss function ,Implementation Issues
Trouble getting importing gensim to work in colab,"I am trying to import gensim into colab. !pip install gensim I get the following error: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr) 365 raise AssertionError() 366 except AssertionError: --> 367 msg = (""The current Numpy installation ({!r}) fails to "" 368 ""pass simple sanity checks. This can be caused for example "" 369 ""by incorrect BLAS library being linked in, or by mixing "" ModuleNotFoundError: No module named 'numpy.char' my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks","['numpy', 'nlp', 'dependencies', 'google-colaboratory', 'gensim']",1,"You have to restart the session for the underlying runtime to notice the package changes. See: https://stackoverflow.com/a/79518359/130288 I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded numpy /etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade. But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two. Explicitly restarting after the Gensim-install & numpy / scipy downgrades resolves the errors.",2025-03-20 14:36:02,2025-03-20 18:07:50,213,https://stackoverflow.com/questions/79523269/trouble-getting-importing-gensim-to-work-in-colab,"Trouble getting importing gensim to work in colab I am trying to import gensim into colab. !pip install gensim I get the following error: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr) 365 raise AssertionError() 366 except AssertionError: --> 367 msg = (""The current Numpy installation ({!r}) fails to "" 368 ""pass simple sanity checks. This can be caused for example "" 369 ""by incorrect BLAS library being linked in, or by mixing "" ModuleNotFoundError: No module named 'numpy.char' my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks",trouble get import gensim work colab try import gensim colab   pip install gensim get follow error  usr  local  lib  python311  dist  package  numpyinitpy   getattr    attr  365 raise assertionerror   366 except assertionerror    367 msg     current numpy installation    r   fail   368   pass simple sanity check  cause example   369   incorrect blas library link  mix   modulenotfounderror  module name  numpychar  numpy version 202  downgrade numpy another version like say 1264 get different error always numpy string relate issue  thank,restart session underlie runtime notice package change  see    stackoverflowcom  a79518359130288 recall past colab offering warning  possibly also  past  colab not yet load numpy etc fresh environment  ok downgrade behind scene without problem  1st import downgrade  something change colab recently  maybe fast  start optimization   bunch report problem like last day two  explicitly restart gensim  install  numpy  scipy downgrade resolve error ,trouble get import gensim work colab try import gensim colab   pip install gensim get follow error  usr  local  lib  python311  dist  package  numpyinitpy   getattr    attr  365 raise assertionerror   366 except assertionerror    367 msg     current numpy installation    r   fail   368   pass simple sanity check  cause example   369   incorrect blas library link  mix   modulenotfounderror  module name  numpychar  numpy version 202  downgrade numpy another version like say 1264 get different error always numpy string relate issue  thank restart session underlie runtime notice package change  see    stackoverflowcom  a79518359130288 recall past colab offering warning  possibly also  past  colab not yet load numpy etc fresh environment  ok downgrade behind scene without problem  1st import downgrade  something change colab recently  maybe fast  start optimization   bunch report problem like last day two  explicitly restart gensim  install  numpy  scipy downgrade resolve error ,Library/Tool-Based Queries
Store images instead of showing in a server,"I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my server via an SSH connection. The code is for instance this one: skip_tokens = [1] # skip the special token for the start of the text <s> inp = TextTokenInput( eval_prompt, tokenizer, skip_tokens=skip_tokens, ) target = ""playing guitar, hiking, and spending time with his family."" attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens) attr_res.plot_token_attr(show=True) How to store the files locally instead of showing them?","['python', 'nlp', 'large-language-model']",1,"I can't test it but ... I checked source code and it uses matplotlib for this. If you remove show=True then it shouldn't show it but it should only get fig, ax . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. import matplotlib.pyplot as plt # ... code ... attr_res.plot_token_attr() # without `show=True plt.savefig(""output.png"") #plt.show() # eventually show it after saving Probably you can also use fig for this fig, ax = attr_res.plot_token_attr() # without `show=True fig.savefig(""output.png"")",2025-03-11 14:50:31,2025-03-11 15:44:10,39,https://stackoverflow.com/questions/79501178/store-images-instead-of-showing-in-a-server,"Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my server via an SSH connection. The code is for instance this one: skip_tokens = [1] # skip the special token for the start of the text <s> inp = TextTokenInput( eval_prompt, tokenizer, skip_tokens=skip_tokens, ) target = ""playing guitar, hiking, and spending time with his family."" attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens) attr_res.plot_token_attr(show=True) How to store the files locally instead of showing them?",store image instead show server run code find site server would like store image instead show since connect remotely ssh connection server via ssh connection  code instance one  skiptokens   1   skip special token start text   inp  texttokeninput  evalprompt  tokenizer  skiptoken  skiptokens   target    play guitar  hike  spend time family   attrre  llmattrattribute  inp  target  target  skiptoken  skiptokens  attrresplottokenattr  show  true  store file locally instead show ,can not test  check source code use matplotlib  remove show  true not show get fig  ax  think could use matplotlibpyplotsavefig  filename  save file  import matplotlibpyplot plt   code  attrresplottokenattr    without  show  true pltsavefig    outputpng    pltshow    eventually show save probably also use fig fig  ax  attrresplottokenattr    without  show  true figsavefig    outputpng  ,store image instead show server run code find site server would like store image instead show since connect remotely ssh connection server via ssh connection  code instance one  skiptokens   1   skip special token start text   inp  texttokeninput  evalprompt  tokenizer  skiptoken  skiptokens   target    play guitar  hike  spend time family   attrre  llmattrattribute  inp  target  target  skiptoken  skiptokens  attrresplottokenattr  show  true  store file locally instead show  can not test  check source code use matplotlib  remove show  true not show get fig  ax  think could use matplotlibpyplotsavefig  filename  save file  import matplotlibpyplot plt   code  attrresplottokenattr    without  show  true pltsavefig    outputpng    pltshow    eventually show save probably also use fig fig  ax  attrresplottokenattr    without  show  true figsavefig    outputpng  ,Task-Specific Queries
Presidio with Langchain Experimental does not detect Polish names,"I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., ""Jan Kowalski""). Here is my code: from presidio_anonymizer import PresidioAnonymizer from presidio_reversible_anonymizer import PresidioReversibleAnonymizer config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": ""pl"", ""model_name"": ""pl_core_news_lg""}], } anonymizer = PresidioAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) text = ""Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com."" anonymized_result = anonymizer_tool.anonymize(text) anon_result = anonymizer.anonymize(text) deanonymized_result = anonymizer_tool.deanonymize(anonymized_result) print(""Anonymized text:"", anonymized_result) print(""Deanonymized text:"", deanonymized_result) print(""Map:"", anonymizer_tool.deanonymizer_mapping) print(""Anonymized text:"", anon_result) Output: Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {} Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. I expected the name ""Jan Kowalski"" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: python -m spacy download pl_core_news_lg Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only anonymizer_tool = PresidioReversibleAnonymizer() Then the output look like this: Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}} As mentioned below if I use only spaCy: nlp = spacy.load(""pl_core_news_lg"") doc = nlp(text) Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: Jan Kowalski persName Warszawie placeName So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.","['python', 'nlp', 'spacy', 'langchain', 'presidio']",2,"After some test I was able to find the solution: config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": 'pl', ""model_name"": ""pl_core_news_lg""}], } spacy_recognizer = SpacyRecognizer( supported_language=""pl"", supported_entities=[""persName""] ) anonymizer.add_recognizer(spacy_recognizer) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS"", ""CREDIT_CARD""], languages_config=config) The output look like this: Anonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}} You need to directly add SpacyRecognizer with supported_entities formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",2025-03-03 22:27:07,2025-03-09 13:15:41,241,https://stackoverflow.com/questions/79482283/presidio-with-langchain-experimental-does-not-detect-polish-names,"Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., ""Jan Kowalski""). Here is my code: from presidio_anonymizer import PresidioAnonymizer from presidio_reversible_anonymizer import PresidioReversibleAnonymizer config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": ""pl"", ""model_name"": ""pl_core_news_lg""}], } anonymizer = PresidioAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) text = ""Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com."" anonymized_result = anonymizer_tool.anonymize(text) anon_result = anonymizer.anonymize(text) deanonymized_result = anonymizer_tool.deanonymize(anonymized_result) print(""Anonymized text:"", anonymized_result) print(""Deanonymized text:"", deanonymized_result) print(""Map:"", anonymizer_tool.deanonymizer_mapping) print(""Anonymized text:"", anon_result) Output: Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {} Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. I expected the name ""Jan Kowalski"" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: python -m spacy download pl_core_news_lg Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only anonymizer_tool = PresidioReversibleAnonymizer() Then the output look like this: Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}} As mentioned below if I use only spaCy: nlp = spacy.load(""pl_core_news_lg"") doc = nlp(text) Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: Jan Kowalski persName Warszawie placeName So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.",presidio langchain experimental detect polish name use presidio  langchainexperimental anonymize text polish  detect name  eg    jan kowalski    code  presidioanonymizer import presidioanonymizer presidioreversibleanonymizer import presidioreversibleanonymizer config     nlpenginename     spacy     model       langcode     pl     modelname     plcorenewslg      anonymizer  presidioanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  text    jan kowalski mieszka w warszawie e  mail jankowalski  examplecom   anonymizedresult  anonymizertoolanonymize  text  anonresult  anonymizeranonymize  text  deanonymizedresult  anonymizertooldeanonymize  anonymizedresult  print    anonymized text    anonymizedresult  print    deanonymized text    deanonymizedresult  print    map    anonymizertooldeanonymizermappe  print    anonymized text    anonresult  output  anonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    anonymize text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  expect name   jan kowalski  email address anonymize  output remain unchanged  instal plcorenewslg model use  python m spacy download plcorenewslg miss something configuration  presidio support polish entity recognition properly  suggestion make detect name polish  interesting thing use anonymizertool  presidioreversibleanonymizer   output look like  anonymized text  elizabeth tate mieszka w warszawie e  mail christinemurray  examplenet  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    person     elizabeth tate    jan kowalski     emailaddress     christinemurray  examplenet    jankowalski  examplecom    mention use spacy  nlp  spacyload    plcorenewslg   doc  nlp  text  output correct guess s problem presidio  output spacy  jan kowalski persname warszawie placename would like create custom analyzer use spacy presidio work expect ,test able find solution  config     nlpenginename     spacy     model       langcode    pl     modelname     plcorenewslg      spacyrecognizer  spacyrecognizer  supportedlanguage  pl   supportedentities    persname    anonymizeraddrecognizer  spacyrecognizer  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress     creditcard    languagesconfig  config  output look like  anonymized text   persname  mieszka w warszawie e  mail glenn58  exampleorg  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    persname      persname     jan kowalski     persname2     jana kowalskiego     emailaddress     glenn58  exampleorg    jankowalski  examplecom    need directly add spacyrecognizer supportedentities format accord spacy s requirement  believe s something miss unclear documentation  cause misunderstanding ,presidio langchain experimental detect polish name use presidio  langchainexperimental anonymize text polish  detect name  eg    jan kowalski    code  presidioanonymizer import presidioanonymizer presidioreversibleanonymizer import presidioreversibleanonymizer config     nlpenginename     spacy     model       langcode     pl     modelname     plcorenewslg      anonymizer  presidioanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  text    jan kowalski mieszka w warszawie e  mail jankowalski  examplecom   anonymizedresult  anonymizertoolanonymize  text  anonresult  anonymizeranonymize  text  deanonymizedresult  anonymizertooldeanonymize  anonymizedresult  print    anonymized text    anonymizedresult  print    deanonymized text    deanonymizedresult  print    map    anonymizertooldeanonymizermappe  print    anonymized text    anonresult  output  anonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    anonymize text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  expect name   jan kowalski  email address anonymize  output remain unchanged  instal plcorenewslg model use  python m spacy download plcorenewslg miss something configuration  presidio support polish entity recognition properly  suggestion make detect name polish  interesting thing use anonymizertool  presidioreversibleanonymizer   output look like  anonymized text  elizabeth tate mieszka w warszawie e  mail christinemurray  examplenet  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    person     elizabeth tate    jan kowalski     emailaddress     christinemurray  examplenet    jankowalski  examplecom    mention use spacy  nlp  spacyload    plcorenewslg   doc  nlp  text  output correct guess s problem presidio  output spacy  jan kowalski persname warszawie placename would like create custom analyzer use spacy presidio work expect  test able find solution  config     nlpenginename     spacy     model       langcode    pl     modelname     plcorenewslg      spacyrecognizer  spacyrecognizer  supportedlanguage  pl   supportedentities    persname    anonymizeraddrecognizer  spacyrecognizer  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress     creditcard    languagesconfig  config  output look like  anonymized text   persname  mieszka w warszawie e  mail glenn58  exampleorg  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    persname      persname     jan kowalski     persname2     jana kowalskiego     emailaddress     glenn58  exampleorg    jankowalski  examplecom    need directly add spacyrecognizer supportedentities format accord spacy s requirement  believe s something miss unclear documentation  cause misunderstanding ,Task-Specific Queries
OpenNLP POSTaggerME and ChunkerME synergy,"I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from ""arvore deitada"" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.","['nlp', 'opennlp']",1,"Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the ""classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of ""mapping"" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",2025-02-22 16:06:11,2025-02-28 11:55:50,42,https://stackoverflow.com/questions/79459888/opennlp-postaggerme-and-chunkerme-synergy,"OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from ""arvore deitada"" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.",opennlp postaggerme chunkerme synergy  m try use opennlp chunk api chunk portuguese sentence   first tokenized sentence use tokenizerme  tag postaggerme  use ready  make model provide project  sentence  ivo viu uva   postaggerme return tag  propn  verb  det  noun   model seem use ud pos tags  ready  make model chunkerme portuguese  follow instruction training first use chunkerconverter tool  convert   arvore deitada  conll2000  generating model chunkertrainerme tool  everything work well  sentence  chunker produce correct tag   b  np  b  vp  b  np  i  np     complex sentence  not produce good result  try identify could improve chunker training  one thing notice difference type tag  portuguese corpus  bosque 80  seem use portuguese tag  example  instead propn  corpus use prop instead det  use art  seem could lead problem  especially since one parameter chunker receive array ud tag  train another type tag  write code create routine convert portuguese notation ud  penn  want ask  indeed impact  tool already translation suggestion improve chunker precision  recall ,q1 yes  choose tag set  ud  penn  custom  impact  conversion possible bi  directional manner  penn   ud work well  ud   penn good idea lossy conversion  ud tag set less detailed compare   classic  penn tag set  use custom  language specific tag  set work  matter   mapping  from  to ud correctly  might work tag set language  other might complicated  lossy  q2  not  opennlp project take code donation upcoming release  want provide mapping  translation pt lang  q3 need detail  discussion apache opennlp user andor dev mailing list  alternatively  feel free open jira issue drill topic clear idea propose code addition ,opennlp postaggerme chunkerme synergy  m try use opennlp chunk api chunk portuguese sentence   first tokenized sentence use tokenizerme  tag postaggerme  use ready  make model provide project  sentence  ivo viu uva   postaggerme return tag  propn  verb  det  noun   model seem use ud pos tags  ready  make model chunkerme portuguese  follow instruction training first use chunkerconverter tool  convert   arvore deitada  conll2000  generating model chunkertrainerme tool  everything work well  sentence  chunker produce correct tag   b  np  b  vp  b  np  i  np     complex sentence  not produce good result  try identify could improve chunker training  one thing notice difference type tag  portuguese corpus  bosque 80  seem use portuguese tag  example  instead propn  corpus use prop instead det  use art  seem could lead problem  especially since one parameter chunker receive array ud tag  train another type tag  write code create routine convert portuguese notation ud  penn  want ask  indeed impact  tool already translation suggestion improve chunker precision  recall  q1 yes  choose tag set  ud  penn  custom  impact  conversion possible bi  directional manner  penn   ud work well  ud   penn good idea lossy conversion  ud tag set less detailed compare   classic  penn tag set  use custom  language specific tag  set work  matter   mapping  from  to ud correctly  might work tag set language  other might complicated  lossy  q2  not  opennlp project take code donation upcoming release  want provide mapping  translation pt lang  q3 need detail  discussion apache opennlp user andor dev mailing list  alternatively  feel free open jira issue drill topic clear idea propose code addition ,Library/Tool-Based Queries
Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged,"I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: lstm_model = Sequential() lstm_model.add(Input(shape=(max_len,))) lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)) lstm_model.add(LSTM(256, return_sequences=True)) lstm_model.add(LSTM(128, return_sequences=True)) lstm_model.add(LSTM(64)) lstm_model.add(Dense(128, activation='relu')) lstm_model.add(Dense(units=3, activation='softmax')) lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy']) lstm_model.summary()","['keras', 'deep-learning', 'nlp', 'lstm', 'sentiment-analysis']",1,"Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So ""queen"" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)",2025-02-07 02:48:25,2025-02-09 16:00:51,50,https://stackoverflow.com/questions/79419884/underfitting-pre-trained-glove-lstm-model-accurcacy-unchanged,"Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: lstm_model = Sequential() lstm_model.add(Input(shape=(max_len,))) lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)) lstm_model.add(LSTM(256, return_sequences=True)) lstm_model.add(LSTM(128, return_sequences=True)) lstm_model.add(LSTM(64)) lstm_model.add(Dense(128, activation='relu')) lstm_model.add(Dense(units=3, activation='softmax')) lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy']) lstm_model.summary()",underfitting pre  train glove  lstm model  accurcacy unchanged sentiment classification use pre  trained glove lstm model  use google play review scrap  result 50k text  implement random sampling minority class  however  train lstm model  training accuracy remain unchanged several epoch  need insight fix issue  several information dataset  embed size   41151  100  maximum sequence length  731 label distribution random sampling    positive   58749   negative   26643   neutral   9106  label distribution random sampling    positive   58749   negative   26643   neutral   9106  total x training set  pad    140997  200  total x validation set  pad    17625  200  total x testing set  pad    17625  200  total training set  one hot    140997  3  total validation set  one hot    17625  3  total testing set  one hot    17625  2003 full code  enter link description highlight code issue  lstmmodel  sequential   lstmmodeladd  input  shape  maxlen     lstmmodeladd  embed  inputdim  totalvocab  outputdim  embeddingdim  weights  embeddingmatrix   trainable  false   lstmmodeladd  lstm  256  returnsequence  true   lstmmodeladd  lstm  128  returnsequence  true   lstmmodeladd  lstm  64   lstmmodeladd  dense  128  activationrelu    lstmmodeladd  dense  units3  activationsoftmax    lstmmodelcompile  losscategoricalcrossentropy   optimizer  adam  learningrate0001   metrics   accuracy    lstmmodelsummary  ,base extra information comment   m go say reason lstm model hit wall  unspecified  low accuracy 85  try reach good type model problem  case tweak parameter likely waste effort   m fairly sure encoder transformer  eg  bert  surpass sentiment analysis benchmark number year back  sorry  quick search could not find killer reference insert   transformer got big well since  extra think  build top glove embedding present problem not handle multiple meaning word    queen  might female king  embed s party trick  king  male  female  queen  might pop group  might gay man  might chess piece  going put limit accuracy model build  whereas transformer not limitation look whole string see word context   possible argue  course  bring context lstm come  transformer still scale strongly 20  layer  whereas lstm tend choke two layer  ,underfitting pre  train glove  lstm model  accurcacy unchanged sentiment classification use pre  trained glove lstm model  use google play review scrap  result 50k text  implement random sampling minority class  however  train lstm model  training accuracy remain unchanged several epoch  need insight fix issue  several information dataset  embed size   41151  100  maximum sequence length  731 label distribution random sampling    positive   58749   negative   26643   neutral   9106  label distribution random sampling    positive   58749   negative   26643   neutral   9106  total x training set  pad    140997  200  total x validation set  pad    17625  200  total x testing set  pad    17625  200  total training set  one hot    140997  3  total validation set  one hot    17625  3  total testing set  one hot    17625  2003 full code  enter link description highlight code issue  lstmmodel  sequential   lstmmodeladd  input  shape  maxlen     lstmmodeladd  embed  inputdim  totalvocab  outputdim  embeddingdim  weights  embeddingmatrix   trainable  false   lstmmodeladd  lstm  256  returnsequence  true   lstmmodeladd  lstm  128  returnsequence  true   lstmmodeladd  lstm  64   lstmmodeladd  dense  128  activationrelu    lstmmodeladd  dense  units3  activationsoftmax    lstmmodelcompile  losscategoricalcrossentropy   optimizer  adam  learningrate0001   metrics   accuracy    lstmmodelsummary   base extra information comment   m go say reason lstm model hit wall  unspecified  low accuracy 85  try reach good type model problem  case tweak parameter likely waste effort   m fairly sure encoder transformer  eg  bert  surpass sentiment analysis benchmark number year back  sorry  quick search could not find killer reference insert   transformer got big well since  extra think  build top glove embedding present problem not handle multiple meaning word    queen  might female king  embed s party trick  king  male  female  queen  might pop group  might gay man  might chess piece  going put limit accuracy model build  whereas transformer not limitation look whole string see word context   possible argue  course  bring context lstm come  transformer still scale strongly 20  layer  whereas lstm tend choke two layer  ,Implementation Issues
Can&#39;t compile Marian NMT,"I'm using endeavouros. I'm trying to compile Marian with these instructions: https://marian-nmt.github.io/docs/#installation . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the CMakeLists.txt files of the repo, there is the line set (CMAKE_CXX_STANDARD 11) . These are the steps that I followed: git clone https://github.com/marian-nmt/marian mkdir marian/build cd marian/build cmake .. make -j4 This is the result I had: ➜ make -j4 [ 1%] Built target 3rd_party_installs [ 1%] Built target marian_version [ 6%] Built target sentencepiece_train-static [ 19%] Built target libyaml-cpp [ 25%] Built target SQLiteCpp [ 25%] Built target pathie-cpp [ 32%] Built target zlib [ 35%] Built target intgemm [ 35%] Built target faiss [ 53%] Built target sentencepiece-static [ 55%] Built target spm_decode [ 55%] Built target spm_normalize [ 55%] Built target spm_encode [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o [ 56%] Built target spm_train [ 57%] Built target spm_export_vocab [ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o [ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/fastopt.h:3, from /data/tools/marian/src/common/fastopt.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/utils.cpp:2: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/logging.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/cli_wrapper.h:6, from /data/tools/marian/src/common/config_parser.h:4, from /data/tools/marian/src/common/aliases.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Please help.","['gcc', 'cmake', 'nlp', 'g++']",1,"The diagnostic that your build is tripping, Wtemplate-id-cdtor , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies -std=c++11 in src/3rd_party/spdlog/CMakeLists.txt , which generates the failure, g++-14 emits Wtemplate-id-cdtor to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option -Werror . This option is included in the list of compile options ALL_WARNINGS , which is created in the top-level marian/CMakeLists.txt at line 227 et seq : # These are used in src/CMakeLists.txt on a per-target basis list(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated; -Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function; -Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare; -Wno-missing-field-initializers;) and then applied as compile options for the marian library target in src/CMakeLists.txt at line 133: target_compile_options(marian PRIVATE ${ALL_WARNINGS}) whence the options are operative for the failing compilation of src/CMakeFiles/marian.dir/common/logging.cpp . This failure is a bug in the marian repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ e.g. make it registry_t(const registry_t<Mutex>&) = delete; in this occurrence. Or: Locally disable -Wtemplate-id-cdtor at each occurrence, e.g: #pragma GCC diagnostic push #pragma GCC diagnostic ignored ""-Wtemplate-id-cdtor"" registry_t<Mutex>(const registry_t<Mutex>&) = delete; #pragma GCC diagnostic pop Or: Remove -Werror from the ALL_WARNINGS list in marian/CMakeLists.txt so that Wtemplate-id-cdtor remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",2025-01-05 06:04:59,2025-01-06 10:09:03,76,https://stackoverflow.com/questions/79330283/cant-compile-marian-nmt,"Can&#39;t compile Marian NMT I'm using endeavouros. I'm trying to compile Marian with these instructions: https://marian-nmt.github.io/docs/#installation . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the CMakeLists.txt files of the repo, there is the line set (CMAKE_CXX_STANDARD 11) . These are the steps that I followed: git clone https://github.com/marian-nmt/marian mkdir marian/build cd marian/build cmake .. make -j4 This is the result I had: ➜ make -j4 [ 1%] Built target 3rd_party_installs [ 1%] Built target marian_version [ 6%] Built target sentencepiece_train-static [ 19%] Built target libyaml-cpp [ 25%] Built target SQLiteCpp [ 25%] Built target pathie-cpp [ 32%] Built target zlib [ 35%] Built target intgemm [ 35%] Built target faiss [ 53%] Built target sentencepiece-static [ 55%] Built target spm_decode [ 55%] Built target spm_normalize [ 55%] Built target spm_encode [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o [ 56%] Built target spm_train [ 57%] Built target spm_export_vocab [ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o [ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/fastopt.h:3, from /data/tools/marian/src/common/fastopt.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/utils.cpp:2: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/logging.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/cli_wrapper.h:6, from /data/tools/marian/src/common/config_parser.h:4, from /data/tools/marian/src/common/aliases.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Please help.",  39  compile marian nmt  m use endeavouro   m try compile marian instruction    marian  nmtgithubio  docs  installation  fail  error message seemingly indicate conflict code c20  cmakeliststxt file repo  line set  cmakecxxstandard 11   step follow  git clone   githubcom  marian  nmt  marian mkdir marian  build cd marian  build cmake  make j4 result   make j4  1   build target 3rdpartyinstall  1   build target marianversion  6   build target sentencepiecetrain  static  19   build target libyaml  cpp  25   build target sqlitecpp  25   build target pathie  cpp  32   build target zlib  35   build target intgemm  35   build target faiss  53   build target sentencepiece  static  55   build target spmdecode  55   build target spmnormalize  55   build target spmencode  55   building cxx object src  cmakefile  mariandir  common  aliasescppo  55   building cxx object src  cmakefile  mariandir  common  fastoptcppo  56   build target spmtrain  57   build target spmexportvocab  57   building cxx object src  cmakefile  mariandir  common  utilscppo  58   building cxx object src  cmakefile  mariandir  common  loggingcppo file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  fastopth3  data  tool  marian  src  common  fastoptcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  utilscpp2  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  loggingcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  cliwrapperh6  data  tool  marian  src  common  configparserh4  data  tool  marian  src  common  aliasescpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     cc1plu  warning treat error make  2       src  cmakefile  mariandir  buildmake93  src  cmakefile  mariandir  common  fastoptcppo  error 1 make  2      wait unfinished job  cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake121  src  cmakefile  mariandir  common  utilscppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake79  src  cmakefile  mariandir  common  aliasescppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake135  src  cmakefile  mariandir  common  loggingcppo  error 1 make  1       cmakefile  makefile2374  src  cmakefile  mariandir  all  error 2 make      makefile156   error 2 please help ,diagnostic build tripping  wtemplate  id  cdtor  introduce gcc 141  warning  error  build promote warning error  break build  although build specifie std  c11 src3rdparty  spdlog  cmakeliststxt  generate failure  g14 emit wtemplate  id  cdtor warn code would illegal recent standard c20  later   warn make error  warning make error compile option werror  option include list compile option allwarning  create top  level marian  cmakeliststxt line 227 et seq   use src  cmakeliststxt per  target basis list  append allwarning wall  werror  wextra  wno  unuse  result  wno  deprecate  wno  pragma  wno  unuse  parameter  wno  unuse  function  wno  unuse  value  wno  unknown  pragma  wno  sign  compare  wno  miss  field  initializer   apply compile option marian library target src  cmakeliststxt line 133  targetcompileoptions  marian private   allwarning   whence option operative failing compilation src  cmakefile  mariandir  common  loggingcpp  failure bug marian repo report maintainer  seem report already  head revision v1120 year old gcc 14  pende fix  seem three interim option get build do  either  make code legal c11 c20 diagnostic advice say occurrence  data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     eg  make registryt  const registryt  mutex     delete  occurrence   locally disable wtemplate  id  cdtor occurrence  eg   pragma gcc diagnostic push  pragma gcc diagnostic ignore   wtemplate  id  cdtor  registryt  mutex   const registryt  mutex     delete   pragma gcc diagnostic pop  remove werror allwarning list marian  cmakeliststxt wtemplate  id  cdtor remains warn  may result diagnostic demote error warning  default status   not test option would need go trouble instal cuda ,  39  compile marian nmt  m use endeavouro   m try compile marian instruction    marian  nmtgithubio  docs  installation  fail  error message seemingly indicate conflict code c20  cmakeliststxt file repo  line set  cmakecxxstandard 11   step follow  git clone   githubcom  marian  nmt  marian mkdir marian  build cd marian  build cmake  make j4 result   make j4  1   build target 3rdpartyinstall  1   build target marianversion  6   build target sentencepiecetrain  static  19   build target libyaml  cpp  25   build target sqlitecpp  25   build target pathie  cpp  32   build target zlib  35   build target intgemm  35   build target faiss  53   build target sentencepiece  static  55   build target spmdecode  55   build target spmnormalize  55   build target spmencode  55   building cxx object src  cmakefile  mariandir  common  aliasescppo  55   building cxx object src  cmakefile  mariandir  common  fastoptcppo  56   build target spmtrain  57   build target spmexportvocab  57   building cxx object src  cmakefile  mariandir  common  utilscppo  58   building cxx object src  cmakefile  mariandir  common  loggingcppo file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  fastopth3  data  tool  marian  src  common  fastoptcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  utilscpp2  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  loggingcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  cliwrapperh6  data  tool  marian  src  common  configparserh4  data  tool  marian  src  common  aliasescpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     cc1plu  warning treat error make  2       src  cmakefile  mariandir  buildmake93  src  cmakefile  mariandir  common  fastoptcppo  error 1 make  2      wait unfinished job  cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake121  src  cmakefile  mariandir  common  utilscppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake79  src  cmakefile  mariandir  common  aliasescppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake135  src  cmakefile  mariandir  common  loggingcppo  error 1 make  1       cmakefile  makefile2374  src  cmakefile  mariandir  all  error 2 make      makefile156   error 2 please help  diagnostic build tripping  wtemplate  id  cdtor  introduce gcc 141  warning  error  build promote warning error  break build  although build specifie std  c11 src3rdparty  spdlog  cmakeliststxt  generate failure  g14 emit wtemplate  id  cdtor warn code would illegal recent standard c20  later   warn make error  warning make error compile option werror  option include list compile option allwarning  create top  level marian  cmakeliststxt line 227 et seq   use src  cmakeliststxt per  target basis list  append allwarning wall  werror  wextra  wno  unuse  result  wno  deprecate  wno  pragma  wno  unuse  parameter  wno  unuse  function  wno  unuse  value  wno  unknown  pragma  wno  sign  compare  wno  miss  field  initializer   apply compile option marian library target src  cmakeliststxt line 133  targetcompileoptions  marian private   allwarning   whence option operative failing compilation src  cmakefile  mariandir  common  loggingcpp  failure bug marian repo report maintainer  seem report already  head revision v1120 year old gcc 14  pende fix  seem three interim option get build do  either  make code legal c11 c20 diagnostic advice say occurrence  data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     eg  make registryt  const registryt  mutex     delete  occurrence   locally disable wtemplate  id  cdtor occurrence  eg   pragma gcc diagnostic push  pragma gcc diagnostic ignore   wtemplate  id  cdtor  registryt  mutex   const registryt  mutex     delete   pragma gcc diagnostic pop  remove werror allwarning list marian  cmakeliststxt wtemplate  id  cdtor remains warn  may result diagnostic demote error warning  default status   not test option would need go trouble instal cuda ,Implementation Issues
how to get custom column in the model&#39;s forward() function when training with Huggingface Trainer?,"I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' input_ids ', ' labels ' and so on, and I additionally add 2 custom colunms ' interact_ids ' and ' candidate_ids '. But i can't get these custom fields in the forward() function of my Model ' class LLMWithCustomLayer(LlamaForCausalLM) '. def forward( self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, interact_ids = None, candidate_ids = None, ): print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none interact_embs = [] candidate_embs = [] for i in range(interact_ids.shape(0)): # O_i = F_i (e_i) interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids))) # O_i = F_i (e_i) candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids))) # replace [CandidateEmb] and [HistoryEmb] inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs) return super().forward( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels = labels ) I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.","['pytorch', 'nlp', 'large-language-model', 'huggingface-trainer']",1,"You need to modify the data collator to pass interact_ids and candidate_ids to your model, as Trainer ignores extra columns by default. To modify the data collator class CustomDataCollator(DataCollatorWithPadding): def __call__(self, features): batch = super().__call__(features) batch[""interact_ids""] = torch.tensor([f[""interact_ids""] for f in features]) batch[""candidate_ids""] = torch.tensor([f[""candidate_ids""] for f in features]) return batch then pass it to Trainer trainer = Trainer( model=LLMWithCustomLayer.from_pretrained(""your-llama-model""), args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer, data_collator=CustomDataCollator(tokenizer) ) Now, your forward() method will receive interact_ids and candidate_ids . Hope, it will work!",2025-01-04 08:57:44,2025-01-04 10:45:55,44,https://stackoverflow.com/questions/79328514/how-to-get-custom-column-in-the-models-forward-function-when-training-with-hu,"how to get custom column in the model&#39;s forward() function when training with Huggingface Trainer? I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' input_ids ', ' labels ' and so on, and I additionally add 2 custom colunms ' interact_ids ' and ' candidate_ids '. But i can't get these custom fields in the forward() function of my Model ' class LLMWithCustomLayer(LlamaForCausalLM) '. def forward( self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, interact_ids = None, candidate_ids = None, ): print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none interact_embs = [] candidate_embs = [] for i in range(interact_ids.shape(0)): # O_i = F_i (e_i) interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids))) # O_i = F_i (e_i) candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids))) # replace [CandidateEmb] and [HistoryEmb] inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs) return super().forward( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels = labels ) I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.",get custom column model   39  forward   function train huggingface trainer  use huggingface trainer train cumstom model subclasse llama llm  tokenized tokenizer  dataset field  inputids    label   additionally add 2 custom colunms  interactids   candidateids   can not get custom field forward   function model  class llmwithcustomlayer  llamaforcausallm    def forward  self  inputid  torch  longtensor  none  attentionmask  optional  torch  tensor   none  positionid  optional  torch  longtensor   none  pastkeyvalues  optional  list  torch  floattensor    none  inputsembed  optional  torch  floattensor   none  label  optional  torch  longtensor   none  usecache  optional  bool   none  outputattention  optional  bool   none  outputhiddenstates  optional  bool   none  returndict  optional  bool   none  interactid  none  candidateid  none    print   interactids  candidateids   interactids  candidateids   none interactemb    candidateembs    range  interactidsshape  0     oi  fi  ei  interactembsappend  selfitemembproj  selfgetitememb  interactids     oi  fi  ei  candidateembsappend  selfitemembproj  selfgetitememb  candidateids     replace  candidateemb   historyemb  inputsembed  selfreplacehistcanditoken  inputids  inputsembeds  interactembs  candidateembs  return super   forward  inputid  inputids  attentionmask  attentionmask  positionid  positionid  pastkeyvalue  pastkeyvalues  inputsembed  inputsembeds  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict  label  label  new llm fine tuning  anyone help  would grateful much ,need modify datum collator pass interactids candidateids model  trainer ignore extra column default  modify datum collator class customdatacollator  datacollatorwithpadding   def   call    self  feature   batch  super   call    feature  batch    interactids    torchtensor   f    interactids   f feature   batch    candidateids    torchtensor   f    candidateids   f feature   return batch pass trainer trainer  trainer  model  llmwithcustomlayerfrompretraine    your  llama  model    arg  trainingargs  traindataset  traindataset  evaldataset  evaldataset  tokenizer  tokenizer  datacollator  customdatacollator  tokenizer    forward   method receive interactids candidateids  hope  work ,get custom column model   39  forward   function train huggingface trainer  use huggingface trainer train cumstom model subclasse llama llm  tokenized tokenizer  dataset field  inputids    label   additionally add 2 custom colunms  interactids   candidateids   can not get custom field forward   function model  class llmwithcustomlayer  llamaforcausallm    def forward  self  inputid  torch  longtensor  none  attentionmask  optional  torch  tensor   none  positionid  optional  torch  longtensor   none  pastkeyvalues  optional  list  torch  floattensor    none  inputsembed  optional  torch  floattensor   none  label  optional  torch  longtensor   none  usecache  optional  bool   none  outputattention  optional  bool   none  outputhiddenstates  optional  bool   none  returndict  optional  bool   none  interactid  none  candidateid  none    print   interactids  candidateids   interactids  candidateids   none interactemb    candidateembs    range  interactidsshape  0     oi  fi  ei  interactembsappend  selfitemembproj  selfgetitememb  interactids     oi  fi  ei  candidateembsappend  selfitemembproj  selfgetitememb  candidateids     replace  candidateemb   historyemb  inputsembed  selfreplacehistcanditoken  inputids  inputsembeds  interactembs  candidateembs  return super   forward  inputid  inputids  attentionmask  attentionmask  positionid  positionid  pastkeyvalue  pastkeyvalues  inputsembed  inputsembeds  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict  label  label  new llm fine tuning  anyone help  would grateful much  need modify datum collator pass interactids candidateids model  trainer ignore extra column default  modify datum collator class customdatacollator  datacollatorwithpadding   def   call    self  feature   batch  super   call    feature  batch    interactids    torchtensor   f    interactids   f feature   batch    candidateids    torchtensor   f    candidateids   f feature   return batch pass trainer trainer  trainer  model  llmwithcustomlayerfrompretraine    your  llama  model    arg  trainingargs  traindataset  traindataset  evaldataset  evaldataset  tokenizer  tokenizer  datacollator  customdatacollator  tokenizer    forward   method receive interactids candidateids  hope  work ,Implementation Issues
Getting all leaf words (reverse stemming) into one Python List,"On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package get_word_forms Imagine I have a shorter sample word list as follows: my_list = [' jail', ' belief',' board',' target', ' challenge', ' command'] If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words): get_word_forms(""command"") and get the following output: {'n': {'command', 'commandant', 'commandants', 'commander', 'commanders', 'commandership', 'commanderships', 'commandment', 'commandments', 'commands'}, 'a': set(), 'v': {'command', 'commanded', 'commanding', 'commands'}, 'r': set()} 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: [get_word_forms(word) for word in sample] I fail at getting any output: [{'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}] I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: ['command','commandant','commandants', 'commander', 'commanders', 'commandership', 'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.","['python', 'nlp', 'nltk']",1,"One solution using nested list comprehensions after stripping forgotten spaces: all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)] # Flatten the list of sets all_words = [word for setx in all_words for word in setx] # Remove the repetitions and sort the set all_words = sorted(set(all_words)) print(all_words) ['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",2024-12-27 15:04:05,2024-12-27 23:39:51,52,https://stackoverflow.com/questions/79312133/getting-all-leaf-words-reverse-stemming-into-one-python-list,"Getting all leaf words (reverse stemming) into one Python List On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package get_word_forms Imagine I have a shorter sample word list as follows: my_list = [' jail', ' belief',' board',' target', ' challenge', ' command'] If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words): get_word_forms(""command"") and get the following output: {'n': {'command', 'commandant', 'commandants', 'commander', 'commanders', 'commandership', 'commanderships', 'commandment', 'commandments', 'commands'}, 'a': set(), 'v': {'command', 'commanded', 'commanding', 'commands'}, 'r': set()} 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: [get_word_forms(word) for word in sample] I fail at getting any output: [{'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}] I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: ['command','commandant','commandants', 'commander', 'commanders', 'commandership', 'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",get leaf word  reverse stem  one python list line solution provide link  try get leaf word one stem word  use community  contribute   divyanshu srivastava  package getwordform imagine short sample word list follow  mylist    jail    belief    board    target    challenge    command   work manually  follow  go word  by  word  time  consume list 200 word   getwordform    command   get follow output    n     command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command       set     v     command    command    command    command     r   set     n  noun    adjective   v  verb   r  adverb  try reverse  stem entire list one go   getwordform  word  word sample  fail get output     n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set     think fail save output dictionary  eventually  would like output list without break noun  adjective  adverb  verb  something like    command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command    command    command    command    jail    jailer    jailer    jailor    jailor    jail    jail    jail     ,one solution use nested list comprehension strip forget space  allword   setx word mylist setx getwordform  wordstrip    values   len  setx    flatten list set allword   word setx allword word setx   remove repetition sort set allwords  sort  set  allwords   print  allword    belief    belief    believability    believability    believable    believably    believe    believe    believer    believer    believe    believe    board    board    boarder    boarder    boarding    board    challenge    challengeable    challenge    challenger    challenger    challenge    challenge    command    commandant    commandant    command    commander    commander    commandership    commandership    command    commandment    commandment    command    jail    jail    jailer    jailer    jail    jailor    jailor    jail    target    target    target    target  ,get leaf word  reverse stem  one python list line solution provide link  try get leaf word one stem word  use community  contribute   divyanshu srivastava  package getwordform imagine short sample word list follow  mylist    jail    belief    board    target    challenge    command   work manually  follow  go word  by  word  time  consume list 200 word   getwordform    command   get follow output    n     command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command       set     v     command    command    command    command     r   set     n  noun    adjective   v  verb   r  adverb  try reverse  stem entire list one go   getwordform  word  word sample  fail get output     n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set     think fail save output dictionary  eventually  would like output list without break noun  adjective  adverb  verb  something like    command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command    command    command    command    jail    jailer    jailer    jailor    jailor    jail    jail    jail      one solution use nested list comprehension strip forget space  allword   setx word mylist setx getwordform  wordstrip    values   len  setx    flatten list set allword   word setx allword word setx   remove repetition sort set allwords  sort  set  allwords   print  allword    belief    belief    believability    believability    believable    believably    believe    believe    believer    believer    believe    believe    board    board    boarder    boarder    boarding    board    challenge    challengeable    challenge    challenger    challenger    challenge    challenge    command    commandant    commandant    command    commander    commander    commandership    commandership    command    commandment    commandment    command    jail    jail    jailer    jailer    jail    jailor    jailor    jail    target    target    target    target  ,Basic Understanding
Inspect all probabilities of BERTopic model,"Say I build a BERTopic model using from bertopic import BERTopic topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20) topics, probs = topic_model.fit_transform(docs) Inspecting probs gives me just a single value for each item in docs . probs array([0.51914467, 0. , 0. , ..., 1. , 1. , 1. ]) I would like the entire probability vector across all topics (so in this case, where nr_topics=20 , I want a vector of 20 probabilities for each item in docs ). In other words, if I have N items in docs and K topics, I would like an NxK output.","['python', 'nlp', 'topic-modeling']",1,"For individual topic probability across each document you need to add one more argument. topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True) Note: This calculate_probabilities = True will only work if you are using HDBSCAN clustering embedding model. And Bertopic by default uses all-MiniLM-L6-v2 . Official documentation: https://maartengr.github.io/BERTopic/api/bertopic.html They have mentioned the same in document as well.",2024-12-20 20:49:34,2024-12-21 16:03:22,58,https://stackoverflow.com/questions/79298368/inspect-all-probabilities-of-bertopic-model,"Inspect all probabilities of BERTopic model Say I build a BERTopic model using from bertopic import BERTopic topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20) topics, probs = topic_model.fit_transform(docs) Inspecting probs gives me just a single value for each item in docs . probs array([0.51914467, 0. , 0. , ..., 1. , 1. , 1. ]) I would like the entire probability vector across all topics (so in this case, where nr_topics=20 , I want a vector of 20 probabilities for each item in docs ). In other words, if I have N items in docs and K topics, I would like an NxK output.",inspect probability bertopic model say build bertopic model use bertopic import bertopic topicmodel  bertopic  ngramrange  1  1   nrtopics20  topic  prob  topicmodelfittransform  doc  inspect prob give single value item doc  prob array   051914467  0   0     1   1   1    would like entire probability vector across topic  case  nrtopics20  want vector 20 probability item doc   word  n item doc k topic  would like nxk output ,individual topic probability across document need add one argument  topicmodel  bertopic  ngramrange  1  1   nrtopics20  calculateprobabilitie  true  note  calculateprobabilitie  true work use hdbscan cluster embed model  bertopic default use all  minilm  l6  v2  official documentation    maartengrgithubio  bertopic  api  bertopichtml mention document well ,inspect probability bertopic model say build bertopic model use bertopic import bertopic topicmodel  bertopic  ngramrange  1  1   nrtopics20  topic  prob  topicmodelfittransform  doc  inspect prob give single value item doc  prob array   051914467  0   0     1   1   1    would like entire probability vector across topic  case  nrtopics20  want vector 20 probability item doc   word  n item doc k topic  would like nxk output  individual topic probability across document need add one argument  topicmodel  bertopic  ngramrange  1  1   nrtopics20  calculateprobabilitie  true  note  calculateprobabilitie  true work use hdbscan cluster embed model  bertopic default use all  minilm  l6  v2  official documentation    maartengrgithubio  bertopic  api  bertopichtml mention document well ,Library/Tool-Based Queries
Determining most popular words in the English dictionary within a dictionary of words,"Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence ""I enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.","['python', 'nlp', 'nltk', 'detection']",2,"You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: ""I enjoy a cold glass of water on a hot day"". Output : ""water"". Split the sentences into words list. Example: [""I"", ""enjoy"", ""a"", ""cold"", ""glass"", ""of"", ""water"", ""on"", ""a"", ""hot"", ""day""] First loop in through all the word of the sentences. so let say you are at first word ""I"". Now you will look the same word ""I"" in external dataset and will look for the frequency of that word. Let say the word ""I"" in external dataset is repeated 5000000 times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. { ""I"": 5000000, ""enjoy"": 50000, ""a"": 10000000, ""cold"": 30000, ""glass"": 100000, ""of"": 8000000, ""water"": 1200000, ""on"": 6000000, ""hot"": 700000, ""day"": 400000 } Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",2024-12-19 10:24:04,2024-12-19 11:17:51,69,https://stackoverflow.com/questions/79293919/determining-most-popular-words-in-the-english-dictionary-within-a-dictionary-of,"Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence ""I enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.",determine popular word english dictionary within dictionary word forgive wording awful   m try figure determine use word english language set word dictionary  ve make   ve do research nltk can not seem find function within  library matter  help need  example  sentence   enjoy cold glass water hot day  would return   water  s use word day day conversation sentence  essentially need return value frequently use word conversation  figure will likely involve ai  time  ve try use ai wind copy paste code not understand   m try avoid go route help welcome appreciate  context  decide start project would essentially guess predetermine word base character user say not computer guess ,need external dataset task  try dataset google n gram dataset  breakdown problem statement  input    enjoy cold glass water hot day   output    water   split sentence word list  example         enjoy         cold     glass         water             hot     day   first loop word sentence  let say first word     look word    external dataset look frequency word  let say word    external dataset repeat 5000000 time repeat task word  dictionary word sentence key value frequency word get external datum  frequency example random value exact value       5000000    enjoy   50000      10000000    cold   30000    glass   100000      8000000    water   1200000      6000000    hot   700000    day   400000  pick word high frequency  note  try big corpus external datum  use big corpus english word use conversation  even frequency mention create,determine popular word english dictionary within dictionary word forgive wording awful   m try figure determine use word english language set word dictionary  ve make   ve do research nltk can not seem find function within  library matter  help need  example  sentence   enjoy cold glass water hot day  would return   water  s use word day day conversation sentence  essentially need return value frequently use word conversation  figure will likely involve ai  time  ve try use ai wind copy paste code not understand   m try avoid go route help welcome appreciate  context  decide start project would essentially guess predetermine word base character user say not computer guess  need external dataset task  try dataset google n gram dataset  breakdown problem statement  input    enjoy cold glass water hot day   output    water   split sentence word list  example         enjoy         cold     glass         water             hot     day   first loop word sentence  let say first word     look word    external dataset look frequency word  let say word    external dataset repeat 5000000 time repeat task word  dictionary word sentence key value frequency word get external datum  frequency example random value exact value       5000000    enjoy   50000      10000000    cold   30000    glass   100000      8000000    water   1200000      6000000    hot   700000    day   400000  pick word high frequency  note  try big corpus external datum  use big corpus english word use conversation  even frequency mention create,Task-Specific Queries
catelog sentences into 5 words that represent them,"I have dataframe with 1000 text rows. df['text'] I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in df[""word1""] , df[""word2""] and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is ""i want to eat"" and I have 2 words : food and house. so in df[""food ""] it would be higher score than in df[""house""]","['python', 'pandas', 'nlp', 'text-mining', 'similarity']",1,"You could use a pre-trained sentence transformer model from sentence_transformers : import pandas as pd from sentence_transformers import SentenceTransformer, util class SemanticSimilarityCalculator: def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None: self.model = SentenceTransformer(model_name) self.word_embeddings = None def encode_words(self, words: list[str]) -> None: self.word_embeddings = self.model.encode(words, convert_to_tensor=True) self.words = words def calculate_similarity(self, text: str) -> list[float]: if self.word_embeddings is None: raise ValueError('Words must be encoded before calculating similarity.') text_embedding = self.model.encode(text, convert_to_tensor=True) similarities = util.cos_sim(text_embedding, self.word_embeddings)[ 0 ].tolist() return similarities def add_similarity_scores_to_df( self, df: pd.DataFrame, text_column: str ) -> pd.DataFrame: if self.words is None: raise ValueError( 'Words must be encoded before adding scores to the DataFrame.' ) similarity_columns = ['word_' + word for word in self.words] df[similarity_columns] = df[text_column].apply( lambda text: pd.Series(self.calculate_similarity(text)) ) return df def main(): data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']} df = pd.DataFrame(data) words = ['food', 'house', 'sleep', 'drink', 'run'] calculator = SemanticSimilarityCalculator() calculator.encode_words(words) df_with_scores = calculator.add_similarity_scores_to_df( df, text_column='text' ) print(df_with_scores) if __name__ == '__main__': main() Output: text word_food word_house word_sleep word_drink word_run 0 I want to eat 0.592410 0.215032 0.254065 0.370329 0.259350 1 The house is big 0.243262 0.672110 0.170785 0.213780 0.119716 2 I need to sleep 0.253703 0.222462 0.725105 0.358372 0.303838",2024-12-19 10:16:47,2024-12-19 11:26:17,57,https://stackoverflow.com/questions/79293889/catelog-sentences-into-5-words-that-represent-them,"catelog sentences into 5 words that represent them I have dataframe with 1000 text rows. df['text'] I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in df[""word1""] , df[""word2""] and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is ""i want to eat"" and I have 2 words : food and house. so in df[""food ""] it would be higher score than in df[""house""]",catelog sentence 5 word represent dataframe 1000 text row  df   text   also 5 word want know one much represnt text  0 1  every score df    word1    df    word2   etc glad recomendation edit represnt  semantic distance word text  example  let say row 1 text   want eat  2 word  food house  df    food    would high score df    house  ,could use pre  train sentence transformer model sentencetransformer  import panda pd sentencetransformer import sentencetransformer  util class semanticsimilaritycalculator  def   init    self  modelname  str   all  minilm  l6  v2     none  selfmodel  sentencetransformer  modelname  selfwordembedding  none def encodeword  self  word  list  str     none  selfwordembedding  selfmodelencode  word  converttotensor  true  selfword  word def calculatesimilarity  self  text  str    list  float   selfwordembedding none  raise valueerror   word must encode calculate similarity    textembedding  selfmodelencode  text  converttotensor  true  similarity  utilcossim  textembedding  selfwordembedding   0  tolist   return similarity def addsimilarityscorestodf  self  df  pd  dataframe  textcolumn  str    pd  dataframe  selfword none  raise valueerror   word must encode add score dataframe    similaritycolumn    word    word word selfword  df  similaritycolumns   df  textcolumn  apply  lambda text  pd  series  selfcalculatesimilarity  text    return df def main    datum    text     want eat    the house big    need sleep    df  pd  dataframe  datum  word    food    house    sleep    drink    run   calculator  semanticsimilaritycalculator   calculatorencodeword  word  dfwithscore  calculatoraddsimilarityscorestodf  df  textcolumntext   print  dfwithscores    name        main     main   output  text wordfood wordhouse wordsleep worddrink wordrun 0 want eat 0592410 0215032 0254065 0370329 0259350 1 house big 0243262 0672110 0170785 0213780 0119716 2 need sleep 0253703 0222462 0725105 0358372 0303838,catelog sentence 5 word represent dataframe 1000 text row  df   text   also 5 word want know one much represnt text  0 1  every score df    word1    df    word2   etc glad recomendation edit represnt  semantic distance word text  example  let say row 1 text   want eat  2 word  food house  df    food    would high score df    house   could use pre  train sentence transformer model sentencetransformer  import panda pd sentencetransformer import sentencetransformer  util class semanticsimilaritycalculator  def   init    self  modelname  str   all  minilm  l6  v2     none  selfmodel  sentencetransformer  modelname  selfwordembedding  none def encodeword  self  word  list  str     none  selfwordembedding  selfmodelencode  word  converttotensor  true  selfword  word def calculatesimilarity  self  text  str    list  float   selfwordembedding none  raise valueerror   word must encode calculate similarity    textembedding  selfmodelencode  text  converttotensor  true  similarity  utilcossim  textembedding  selfwordembedding   0  tolist   return similarity def addsimilarityscorestodf  self  df  pd  dataframe  textcolumn  str    pd  dataframe  selfword none  raise valueerror   word must encode add score dataframe    similaritycolumn    word    word word selfword  df  similaritycolumns   df  textcolumn  apply  lambda text  pd  series  selfcalculatesimilarity  text    return df def main    datum    text     want eat    the house big    need sleep    df  pd  dataframe  datum  word    food    house    sleep    drink    run   calculator  semanticsimilaritycalculator   calculatorencodeword  word  dfwithscore  calculatoraddsimilarityscorestodf  df  textcolumntext   print  dfwithscores    name        main     main   output  text wordfood wordhouse wordsleep worddrink wordrun 0 want eat 0592410 0215032 0254065 0370329 0259350 1 house big 0243262 0672110 0170785 0213780 0119716 2 need sleep 0253703 0222462 0725105 0358372 0303838,Task-Specific Queries
Counting the Frequency of Some Words within some other Key Words in Text,"I have two sets of word lists - first one I called search words and the second one I called key words . My goal is to calculate the frequency of search words within 10 words of key words . For example, assume that the word - acquire - is in key words list, then I will look for the words in search words list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my search word and key word lists - search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'] key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use'] A small Example - text_dict = { 'ITEM7':[""Last year, from AVG we have acquired Alibaba Security. This year we are in the process \ of adopting Symantec. We believe these technologies will improve our access control. \ Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system.""] } df = pd.DataFrame(text_dict) My expected outcome is - ITEM7 Frequency Last year, from AVG we have acquired Alibaba S... 6 We are planning to install end-point security,... 2 For the first row in df , we see the word AVG and Alibaba Security are from search_words list and around the word acquired , the base form of which - acquire - is in the key_words list. Similarly, Symantec , Access Control , data security , diagnostic program are from search_words list and these words are within 10 words of adopting , improve , integrated from key_words list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the Frequency column of df , the value is 6. Please note that the words in key_words are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.","['python', 'pandas', 'nlp']",1,"You need to process each row of text by identifying occurrences of key_words and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique search_word found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique search_words near key_words . import pandas as pd from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords import string import re text_dict = { 'ITEM7': [ ""Last year, from AVG we have acquired Alibaba Security. This year we are in the process "" ""of adopting Symantec. We believe these technologies will improve our access control. "" ""Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system."" ] } df = pd.DataFrame(text_dict) search_words = [ 'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm' ] key_words = [ 'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use' ] def preprocess_text_no_lemmatization(text): tokens = re.findall(r'\b\w+\b', text.lower()) return tokens def calculate_final_frequency(row, search_phrases, key_phrases): text = row.lower() tokens = preprocess_text_no_lemmatization(text) search_phrases = [phrase.lower() for phrase in search_phrases] key_phrases = [phrase.lower() for phrase in key_phrases] all_matches = set() token_len = len(tokens) for idx, token in enumerate(tokens): if any(token.startswith(key) for key in key_phrases): window_start = max(0, idx - 10) window_end = min(token_len, idx + 10 + 1) window_tokens = tokens[window_start:window_end] window_text = "" "".join(window_tokens) for phrase in search_phrases: if phrase in window_text: all_matches.add(phrase) return len(all_matches) df['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words)) print(df) Which returns ITEM7 Frequency 0 Last year, from AVG we have acquired Alibaba S... 6 1 We are planning to install end-point security,... 2",2024-12-05 03:05:06,2024-12-08 17:27:28,83,https://stackoverflow.com/questions/79253283/counting-the-frequency-of-some-words-within-some-other-key-words-in-text,"Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists - first one I called search words and the second one I called key words . My goal is to calculate the frequency of search words within 10 words of key words . For example, assume that the word - acquire - is in key words list, then I will look for the words in search words list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my search word and key word lists - search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'] key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use'] A small Example - text_dict = { 'ITEM7':[""Last year, from AVG we have acquired Alibaba Security. This year we are in the process \ of adopting Symantec. We believe these technologies will improve our access control. \ Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system.""] } df = pd.DataFrame(text_dict) My expected outcome is - ITEM7 Frequency Last year, from AVG we have acquired Alibaba S... 6 We are planning to install end-point security,... 2 For the first row in df , we see the word AVG and Alibaba Security are from search_words list and around the word acquired , the base form of which - acquire - is in the key_words list. Similarly, Symantec , Access Control , data security , diagnostic program are from search_words list and these words are within 10 words of adopting , improve , integrated from key_words list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the Frequency column of df , the value is 6. Please note that the words in key_words are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.",count frequency word within key word text two set word list  first one call search word second one call key word  goal calculate frequency search word within 10 word key word  example  assume word  acquire  key word list  look word search word list within 10 word acquire  within 10 word mean  10 word forward key word 10 word backward key word  mean forward backward movement  search word key word list  searchwords    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   small example  textdict    item7      last year  avg acquire alibaba security  year process  adopt symantec  believe technology improve access control   moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  expect outcome  item7 frequency last year  avg acquire alibaba  6 planning install end  point security   2 first row df  see word avg alibaba security searchwords list around word acquire  base form  acquire  keywords list  similarly  symantec  access control  datum security  diagnostic program searchword list word within 10 word adopt  improve  integrate keywords list   total search word 6  avgalibaba securitysymantecaccess controldata securitydiagnostic program   therefore  frequency column df  value 6  please note word keyword basically base form  variation  like adopt  adopt  count key word also ,need process row text identify occurrence keyword capture 10  word window around  within window  need check multi  word searchword  ensure match phrase  unique searchword find within window need count  avoid double  counting across row  store result frequency count row  accurately reflect number unique searchword near keyword  import panda pd nltktokenize import wordtokenize nltkstem import wordnetlemmatizer nltkcorpus import stopwords import string import textdict    item7      last year  avg acquire alibaba security  year process     adopt symantec  believe technology improve access control      moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  searchword    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   def preprocesstextnolemmatization  text   token  refindall  rbwb   textlower    return tokens def calculatefinalfrequency  row  searchphrase  keyphrase   text  rowlower   token  preprocesstextnolemmatization  text  searchphrase   phraselower   phrase searchphrases  keyphrase   phraselower   phrase keyphrase  allmatche  set   tokenlen  len  tokens  idx  token enumerate  token    tokenstartswith  key  key keyphrase   windowstart  max  0  idx  10  windowend  min  tokenlen  idx  10  1  windowtoken  token  windowstart  windowend  windowtext      join  windowtoken  phrase searchphrases  phrase windowtext  allmatchesadd  phrase  return len  allmatches  df   frequency    df   item7   apply  lambda x  calculatefinalfrequency  x  searchwords  keywords   print  df  return item7 frequency 0 last year  avg acquire alibaba  6 1 planning install end  point security   2,count frequency word within key word text two set word list  first one call search word second one call key word  goal calculate frequency search word within 10 word key word  example  assume word  acquire  key word list  look word search word list within 10 word acquire  within 10 word mean  10 word forward key word 10 word backward key word  mean forward backward movement  search word key word list  searchwords    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   small example  textdict    item7      last year  avg acquire alibaba security  year process  adopt symantec  believe technology improve access control   moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  expect outcome  item7 frequency last year  avg acquire alibaba  6 planning install end  point security   2 first row df  see word avg alibaba security searchwords list around word acquire  base form  acquire  keywords list  similarly  symantec  access control  datum security  diagnostic program searchword list word within 10 word adopt  improve  integrate keywords list   total search word 6  avgalibaba securitysymantecaccess controldata securitydiagnostic program   therefore  frequency column df  value 6  please note word keyword basically base form  variation  like adopt  adopt  count key word also  need process row text identify occurrence keyword capture 10  word window around  within window  need check multi  word searchword  ensure match phrase  unique searchword find within window need count  avoid double  counting across row  store result frequency count row  accurately reflect number unique searchword near keyword  import panda pd nltktokenize import wordtokenize nltkstem import wordnetlemmatizer nltkcorpus import stopwords import string import textdict    item7      last year  avg acquire alibaba security  year process     adopt symantec  believe technology improve access control      moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  searchword    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   def preprocesstextnolemmatization  text   token  refindall  rbwb   textlower    return tokens def calculatefinalfrequency  row  searchphrase  keyphrase   text  rowlower   token  preprocesstextnolemmatization  text  searchphrase   phraselower   phrase searchphrases  keyphrase   phraselower   phrase keyphrase  allmatche  set   tokenlen  len  tokens  idx  token enumerate  token    tokenstartswith  key  key keyphrase   windowstart  max  0  idx  10  windowend  min  tokenlen  idx  10  1  windowtoken  token  windowstart  windowend  windowtext      join  windowtoken  phrase searchphrases  phrase windowtext  allmatchesadd  phrase  return len  allmatches  df   frequency    df   item7   apply  lambda x  calculatefinalfrequency  x  searchwords  keywords   print  df  return item7 frequency 0 last year  avg acquire alibaba  6 1 planning install end  point security   2,Implementation Issues
Error in getting Captum text explanations for text classification,"I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset import pandas as pd import torch from torch.utils.data import DataLoader from transformers import BertTokenizer, BertForSequenceClassification, AdamW from sklearn.metrics import accuracy_score from captum.attr import IntegratedGradients # Loading data train_df = pd.read_csv('train_dataset.csv') test_df = pd.read_csv('test_dataset.csv') # Tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') def preprocess_data(df, tokenizer, max_len=128): inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"") labels = torch.tensor(df['label'].values) return inputs, labels train_inputs, train_labels = preprocess_data(train_df, tokenizer) test_inputs, test_labels = preprocess_data(test_df, tokenizer) # DataLoader train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels) train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels) test_loader = DataLoader(test_dataset, batch_size=16) # Model setup device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device) # Optimizer optimizer = AdamW(model.parameters(), lr=5e-5) # Training Loop model.train() for epoch in range(3): # Train for 3 epochs for batch in train_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] optimizer.zero_grad() outputs = model(input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss loss.backward() optimizer.step() print(f""Epoch {epoch+1} loss: {loss.item()}"") # Evaluation model.eval() correct_predictions = [] with torch.no_grad(): for batch in test_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] outputs = model(input_ids, attention_mask=attention_mask) preds = torch.argmax(outputs.logits, dim=1) correct_predictions.extend( (preds == labels).cpu().numpy().tolist() ) accuracy = accuracy_score(test_labels.numpy(), correct_predictions) print(f""Test Accuracy: {accuracy:.2f}"") # Integrated Gradients ig = IntegratedGradients(model) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device, dtype=torch.long) # Explicitly convert to LongTensor attention_mask = inputs['attention_mask'].to(device, dtype=torch.long) # Explicitly convert to LongTensor print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) # forward function for IG def forward_func(input_ids): outputs = model(input_ids, attention_mask=attention_mask) return outputs.logits # Applying Integrated Gradients attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) # Analysing influential words for correctly predicted texts for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words) But I am getting the following error in running the above. Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( Epoch 1 loss: 0.4719192385673523 Epoch 2 loss: 0.39585667848587036 Epoch 3 loss: 0.14659778773784637 Test Accuracy: 0.70 Input IDs shape: torch.Size([1, 8]) dtype: torch.int64 Attention mask shape: torch.Size([1, 8]) dtype: torch.int64 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-9-f047b509c98d> in <cell line: 90>() 90 for idx, correct in enumerate(correct_predictions): 91 if correct: ---> 92 influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) 93 print(f""Influential words for text: {test_df['text'].iloc[idx]}"") 94 print(influential_words) 18 frames /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 2549 # remove once script supports set_grad_enabled 2550 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -> 2551 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 2552 2553 RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'text-classification']",1,"You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial https://captum.ai/tutorials/Bert_SQUAD_Interpret Below please find snippet that works: from captum.attr import LayerIntegratedGradients def custom_forward(inputs): preds = predict(inputs) return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1) lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device) attention_mask = inputs['attention_mask'].to(device) # print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) # print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) attributions, delta = lig.attribute(input_ids, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) results = [] for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words)",2024-12-03 12:47:45,2024-12-03 16:12:13,89,https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification,"Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset import pandas as pd import torch from torch.utils.data import DataLoader from transformers import BertTokenizer, BertForSequenceClassification, AdamW from sklearn.metrics import accuracy_score from captum.attr import IntegratedGradients # Loading data train_df = pd.read_csv('train_dataset.csv') test_df = pd.read_csv('test_dataset.csv') # Tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') def preprocess_data(df, tokenizer, max_len=128): inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"") labels = torch.tensor(df['label'].values) return inputs, labels train_inputs, train_labels = preprocess_data(train_df, tokenizer) test_inputs, test_labels = preprocess_data(test_df, tokenizer) # DataLoader train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels) train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels) test_loader = DataLoader(test_dataset, batch_size=16) # Model setup device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device) # Optimizer optimizer = AdamW(model.parameters(), lr=5e-5) # Training Loop model.train() for epoch in range(3): # Train for 3 epochs for batch in train_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] optimizer.zero_grad() outputs = model(input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss loss.backward() optimizer.step() print(f""Epoch {epoch+1} loss: {loss.item()}"") # Evaluation model.eval() correct_predictions = [] with torch.no_grad(): for batch in test_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] outputs = model(input_ids, attention_mask=attention_mask) preds = torch.argmax(outputs.logits, dim=1) correct_predictions.extend( (preds == labels).cpu().numpy().tolist() ) accuracy = accuracy_score(test_labels.numpy(), correct_predictions) print(f""Test Accuracy: {accuracy:.2f}"") # Integrated Gradients ig = IntegratedGradients(model) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device, dtype=torch.long) # Explicitly convert to LongTensor attention_mask = inputs['attention_mask'].to(device, dtype=torch.long) # Explicitly convert to LongTensor print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) # forward function for IG def forward_func(input_ids): outputs = model(input_ids, attention_mask=attention_mask) return outputs.logits # Applying Integrated Gradients attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) # Analysing influential words for correctly predicted texts for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words) But I am getting the following error in running the above. Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( Epoch 1 loss: 0.4719192385673523 Epoch 2 loss: 0.39585667848587036 Epoch 3 loss: 0.14659778773784637 Test Accuracy: 0.70 Input IDs shape: torch.Size([1, 8]) dtype: torch.int64 Attention mask shape: torch.Size([1, 8]) dtype: torch.int64 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-9-f047b509c98d> in <cell line: 90>() 90 for idx, correct in enumerate(correct_predictions): 91 if correct: ---> 92 influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) 93 print(f""Influential words for text: {test_df['text'].iloc[idx]}"") 94 print(influential_words) 18 frames /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 2549 # remove once script supports set_grad_enabled 2550 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -> 2551 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 2552 2553 RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset import panda pd import torch torchutilsdata import dataloader transformer import berttokenizer  bertforsequenceclassification  adamw sklearnmetric import accuracyscore captumattr import integratedgradients  loading datum traindf  pdreadcsv   traindatasetcsv   testdf  pdreadcsv   testdatasetcsv    tokenizer tokenizer  berttokenizerfrompretraine   bert  base  uncase   def preprocessdata  df  tokenizer  maxlen128   input  tokenizer  list  df   text     pad  true  truncation  true  maxlength  maxlen  returntensors  pt   label  torchtensor  df   label   values  return input  label traininput  trainlabel  preprocessdata  traindf  tokenizer  testinput  testlabel  preprocessdata  testdf  tokenizer   dataloader traindataset  torchutilsdata  tensordataset  traininput   inputids    traininput   attentionmask    trainlabel  trainloader  dataloader  traindataset  batchsize16  shuffle  true  testdataset  torchutilsdata  tensordataset  testinput   inputids    testinput   attentionmask    testlabel  testloader  dataloader  testdataset  batchsize16   model setup device  torchdevice    cuda  torchcudaisavailable   else   cpu   model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels2  to  device   optimizer optimizer  adamw  modelparameter    lr5e5   training loop modeltrain   epoch range  3    train 3 epoch batch trainloader  inputids  attentionmask  label   xto  device  x batch  optimizerzerograd   output  model  inputids  attentionmask  attentionmask  label  label  loss  outputsloss lossbackward   optimizerstep   print  f  epoch  epoch1  loss   lossitem       evaluation modeleval   correctprediction    torchnograd    batch testloader  inputid  attentionmask  label   xto  device  x batch  output  model  inputids  attentionmask  attentionmask  pred  torchargmax  outputslogit  dim1  correctpredictionsextend   pred   label  cpu   numpy   tolist    accuracy  accuracyscore  testlabelsnumpy    correctprediction  print  f  test accuracy   accuracy  2f     integrated gradients ig  integratedgradients  model  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  dtype  torchlong   explicitly convert longtensor attentionmask  input   attentionmask   to  device  dtype  torchlong   explicitly convert longtensor print    input id shape    inputidsshape    dtype    inputidsdtype  print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype   forward function ig def forwardfunc  inputids   output  model  inputids  attentionmask  attentionmask  return outputslogit  apply integrated gradient attribution  delta  igattribute  inputids  target1  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances    analyse influential word correctly predict text idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword  get follow error run  weight bertforsequenceclassification initialize model checkpoint bert  base  uncase newly initialize    classifierbia    classifierweight   probably train model down  stream task able use prediction inference  usr  local  lib  python310  dist  package  transformer  optimizationpy591  futurewarning  implementation adamw deprecate remove future version  use pytorch implementation torchoptim  adamw instead  set  nodeprecationwarne  true  disable warning warningswarn  epoch 1 loss  04719192385673523 epoch 2 loss  039585667848587036 epoch 3 loss  014659778773784637 test accuracy  070 input id shape  torch  size   1  8   dtype  torchint64 attention mask shape  torch  size   1  8   dtype  torchint64                                       runtimeerror traceback  recent call last   ipython  input9  f047b509c98d   cell line  90    90 idx  correct enumerate  correctprediction   91 correct     92 influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  93 print  f  influential word text   testdf   text   iloc  idx     94 print  influentialword  18 frame usr  local  lib  python310  dist  package  torch  nn  functionalpy embed  input  weight  paddingidx  maxnorm  normtype  scalegradbyfreq  sparse  2549  remove script support setgradenable 2550  nogradembeddingrenorm   weight  input  maxnorm  normtype    2551 return torchembedding  weight  input  paddingidx  scalegradbyfreq  sparse  2552 2553 runtimeerror  expect tensor argument  1  index  one follow scalar type  long  int  get torchcuda  floattensor instead  check argument embed ,need slightly change gradient calculation class  also  not include forwardfunc gradient class constructor  attribute method able launch stuff properly  think use layerintegratedgradient well debug bert  line tutorial   captumai  tutorial  bertsquadinterpret please find snippet work  captumattr import layerintegratedgradients def customforward  input   pred  predict  input  return torchsoftmax  pred  dim  1   0   1  unsqueeze  1  lig  layerintegratedgradients  customforward  modelbertembedding  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  attentionmask  input   attentionmask   to  device   print    input id shape    inputidsshape    dtype    inputidsdtype   print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype  attribution  delta  ligattribute  inputid  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances   result    idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword ,error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset import panda pd import torch torchutilsdata import dataloader transformer import berttokenizer  bertforsequenceclassification  adamw sklearnmetric import accuracyscore captumattr import integratedgradients  loading datum traindf  pdreadcsv   traindatasetcsv   testdf  pdreadcsv   testdatasetcsv    tokenizer tokenizer  berttokenizerfrompretraine   bert  base  uncase   def preprocessdata  df  tokenizer  maxlen128   input  tokenizer  list  df   text     pad  true  truncation  true  maxlength  maxlen  returntensors  pt   label  torchtensor  df   label   values  return input  label traininput  trainlabel  preprocessdata  traindf  tokenizer  testinput  testlabel  preprocessdata  testdf  tokenizer   dataloader traindataset  torchutilsdata  tensordataset  traininput   inputids    traininput   attentionmask    trainlabel  trainloader  dataloader  traindataset  batchsize16  shuffle  true  testdataset  torchutilsdata  tensordataset  testinput   inputids    testinput   attentionmask    testlabel  testloader  dataloader  testdataset  batchsize16   model setup device  torchdevice    cuda  torchcudaisavailable   else   cpu   model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels2  to  device   optimizer optimizer  adamw  modelparameter    lr5e5   training loop modeltrain   epoch range  3    train 3 epoch batch trainloader  inputids  attentionmask  label   xto  device  x batch  optimizerzerograd   output  model  inputids  attentionmask  attentionmask  label  label  loss  outputsloss lossbackward   optimizerstep   print  f  epoch  epoch1  loss   lossitem       evaluation modeleval   correctprediction    torchnograd    batch testloader  inputid  attentionmask  label   xto  device  x batch  output  model  inputids  attentionmask  attentionmask  pred  torchargmax  outputslogit  dim1  correctpredictionsextend   pred   label  cpu   numpy   tolist    accuracy  accuracyscore  testlabelsnumpy    correctprediction  print  f  test accuracy   accuracy  2f     integrated gradients ig  integratedgradients  model  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  dtype  torchlong   explicitly convert longtensor attentionmask  input   attentionmask   to  device  dtype  torchlong   explicitly convert longtensor print    input id shape    inputidsshape    dtype    inputidsdtype  print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype   forward function ig def forwardfunc  inputids   output  model  inputids  attentionmask  attentionmask  return outputslogit  apply integrated gradient attribution  delta  igattribute  inputids  target1  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances    analyse influential word correctly predict text idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword  get follow error run  weight bertforsequenceclassification initialize model checkpoint bert  base  uncase newly initialize    classifierbia    classifierweight   probably train model down  stream task able use prediction inference  usr  local  lib  python310  dist  package  transformer  optimizationpy591  futurewarning  implementation adamw deprecate remove future version  use pytorch implementation torchoptim  adamw instead  set  nodeprecationwarne  true  disable warning warningswarn  epoch 1 loss  04719192385673523 epoch 2 loss  039585667848587036 epoch 3 loss  014659778773784637 test accuracy  070 input id shape  torch  size   1  8   dtype  torchint64 attention mask shape  torch  size   1  8   dtype  torchint64                                       runtimeerror traceback  recent call last   ipython  input9  f047b509c98d   cell line  90    90 idx  correct enumerate  correctprediction   91 correct     92 influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  93 print  f  influential word text   testdf   text   iloc  idx     94 print  influentialword  18 frame usr  local  lib  python310  dist  package  torch  nn  functionalpy embed  input  weight  paddingidx  maxnorm  normtype  scalegradbyfreq  sparse  2549  remove script support setgradenable 2550  nogradembeddingrenorm   weight  input  maxnorm  normtype    2551 return torchembedding  weight  input  paddingidx  scalegradbyfreq  sparse  2552 2553 runtimeerror  expect tensor argument  1  index  one follow scalar type  long  int  get torchcuda  floattensor instead  check argument embed  need slightly change gradient calculation class  also  not include forwardfunc gradient class constructor  attribute method able launch stuff properly  think use layerintegratedgradient well debug bert  line tutorial   captumai  tutorial  bertsquadinterpret please find snippet work  captumattr import layerintegratedgradients def customforward  input   pred  predict  input  return torchsoftmax  pred  dim  1   0   1  unsqueeze  1  lig  layerintegratedgradients  customforward  modelbertembedding  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  attentionmask  input   attentionmask   to  device   print    input id shape    inputidsshape    dtype    inputidsdtype   print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype  attribution  delta  ligattribute  inputid  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances   result    idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword ,Basic Understanding
euclidian distance from word to sentence after doing Vectorizer,"I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word ""king"". df['king'] I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.","['pandas', 'dataframe', 'nlp', 'text-classification', 'tf-idf']",1,"I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity import numpy as np data = { 'text': [ ""The king sat on the throne with wisdom."", ""A queen ruled the kingdom alongside the king."", ""Knights were loyal to their king."", ""The empire prospered under the rule of a wise monarch."" ] } df = pd.DataFrame(data) tfidf = TfidfVectorizer() tfidf_matrix = tfidf.fit_transform(df['text']) try: king_vector = tfidf.transform([""king""]).toarray() except KeyError: print(""The word 'king' is not in the vocabulary."") king_vector = np.zeros((1, tfidf_matrix.shape[1])) similarities = cosine_similarity(tfidf_matrix, king_vector).flatten() feature_names = np.array(tfidf.get_feature_names_out()) def get_top_n_words(row_vector, top_n=5): indices = row_vector.argsort()[::-1][:top_n] return feature_names[indices] averages = [] for i in range(tfidf_matrix.shape[0]): sentence_vector = tfidf_matrix[i].toarray().flatten() top_words = get_top_n_words(sentence_vector) top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words] averages.append(np.mean(top_similarities)) df['king_similarity'] = similarities df['avg_closest_similarity'] = averages print(df) which would give you text king_similarity \ 0 The king sat on the throne with wisdom. 0.240614 1 A queen ruled the kingdom alongside the king. 0.259779 2 Knights were loyal to their king. 0.274487 3 The empire prospered under the rule of a wise ... 0.000000 avg_closest_similarity 0 0.0 1 0.0 2 0.0 3 0.0 That being said, if you absolutely want to focus on Euclidean distance, here is a method: import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer import numpy as np from scipy.spatial.distance import euclidean data = { 'text': [ ""The king sat on the throne with wisdom."", ""A queen ruled the kingdom alongside the king."", ""Knights were loyal to their king."", ""The empire prospered under the rule of a wise monarch."" ] } df = pd.DataFrame(data) tfidf = TfidfVectorizer() tfidf_matrix = tfidf.fit_transform(df['text']).toarray() feature_names = tfidf.get_feature_names_out() if ""king"" in feature_names: king_index = np.where(feature_names == ""king"")[0][0] king_vector = np.zeros_like(tfidf_matrix[0]) king_vector[king_index] = 1 else: print(""The word 'king' is not in the vocabulary."") king_vector = np.zeros_like(tfidf_matrix[0]) df['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix] print(df) which gives text king_distance 0 The king sat on the throne with wisdom. 1.232385 1 A queen ruled the kingdom alongside the king. 1.216734 2 Knights were loyal to their king. 1.204586 3 The empire prospered under the rule of a wise ... 1.414214",2024-12-03 12:25:05,2024-12-03 14:51:12,46,https://stackoverflow.com/questions/79247594/euclidian-distance-from-word-to-sentence-after-doing-vectorizer,"euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word ""king"". df['king'] I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.",euclidian distance word sentence vectorizer dataframe 1000 text row  tfidfvectorizer  want create new field give distance sentence word want  let say word   king   df   king   think take sentence 5 closet word word king make average  glad know hear another method ,convince euclidean distance would optimal measure  would actually look similarity score  import panda pd sklearnfeatureextractiontext import tfidfvectorizer sklearnmetricspairwise import cosinesimilarity import numpy np datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    try  kingvector  tfidftransform     king    toarray   except keyerror  print    word  king  vocabulary    kingvector  npzeros   1  tfidfmatrixshape  1    similarity  cosinesimilarity  tfidfmatrix  kingvector  flatten   featurename  nparray  tfidfgetfeaturenamesout    def gettopnword  rowvector  topn5   index  rowvectorargsort      1    topn  return featurename  index  average    range  tfidfmatrixshape  0    sentencevector  tfidfmatrix   toarray   flatten   topword  gettopnwords  sentencevector  topsimilaritie   cosinesimilarity  tfidftransform   word    kingvector  flatten    0  word topwords  averagesappend  npmean  topsimilaritie   df   kingsimilarity    similaritie df   avgclosestsimilarity    average print  df  would give text kingsimilarity  0 king sit throne wisdom  0240614 1 queen rule kingdom alongside king  0259779 2 knights loyal king  0274487 3 empire prosper rule wise  0000000 avgclosestsimilarity 0 00 1 00 2 00 3 00 say  absolutely want focus euclidean distance  method  import panda pd sklearnfeatureextractiontext import tfidfvectorizer import numpy np scipyspatialdistance import euclidean datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    toarray   featurename  tfidfgetfeaturenamesout     king  featurename  kingindex  npwhere  featurename     king    0   0  kingvector  npzeroslike  tfidfmatrix  0   kingvector  kingindex   1 else  print    word  king  vocabulary    kingvector  npzeroslike  tfidfmatrix  0   df   kingdistance     euclidean  sentencevector  kingvector  sentencevector tfidfmatrix  print  df  give text kingdistance 0 king sit throne wisdom  1232385 1 queen rule kingdom alongside king  1216734 2 knights loyal king  1204586 3 empire prosper rule wise  1414214,euclidian distance word sentence vectorizer dataframe 1000 text row  tfidfvectorizer  want create new field give distance sentence word want  let say word   king   df   king   think take sentence 5 closet word word king make average  glad know hear another method  convince euclidean distance would optimal measure  would actually look similarity score  import panda pd sklearnfeatureextractiontext import tfidfvectorizer sklearnmetricspairwise import cosinesimilarity import numpy np datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    try  kingvector  tfidftransform     king    toarray   except keyerror  print    word  king  vocabulary    kingvector  npzeros   1  tfidfmatrixshape  1    similarity  cosinesimilarity  tfidfmatrix  kingvector  flatten   featurename  nparray  tfidfgetfeaturenamesout    def gettopnword  rowvector  topn5   index  rowvectorargsort      1    topn  return featurename  index  average    range  tfidfmatrixshape  0    sentencevector  tfidfmatrix   toarray   flatten   topword  gettopnwords  sentencevector  topsimilaritie   cosinesimilarity  tfidftransform   word    kingvector  flatten    0  word topwords  averagesappend  npmean  topsimilaritie   df   kingsimilarity    similaritie df   avgclosestsimilarity    average print  df  would give text kingsimilarity  0 king sit throne wisdom  0240614 1 queen rule kingdom alongside king  0259779 2 knights loyal king  0274487 3 empire prosper rule wise  0000000 avgclosestsimilarity 0 00 1 00 2 00 3 00 say  absolutely want focus euclidean distance  method  import panda pd sklearnfeatureextractiontext import tfidfvectorizer import numpy np scipyspatialdistance import euclidean datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    toarray   featurename  tfidfgetfeaturenamesout     king  featurename  kingindex  npwhere  featurename     king    0   0  kingvector  npzeroslike  tfidfmatrix  0   kingvector  kingindex   1 else  print    word  king  vocabulary    kingvector  npzeroslike  tfidfmatrix  0   df   kingdistance     euclidean  sentencevector  kingvector  sentencevector tfidfmatrix  print  df  give text kingdistance 0 king sit throne wisdom  1232385 1 queen rule kingdom alongside king  1216734 2 knights loyal king  1204586 3 empire prosper rule wise  1414214,Task-Specific Queries
Llama-3.2-1B-Instruct generate inconsistent output,"I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0 , it still generates inconsistent output. This is how my pipeline looks like: pipe = pipeline( ""text-generation"", model=model_id, torch_dtype=torch.bfloat16, device_map=""mps"", model_kwargs={""temperature"": 0.0, ""do_sample"":True, ""top_p"":0.0, ""top_k"":0,}, ) Any idea how to solve this issue?","['python', 'nlp', 'huggingface-transformers', 'large-language-model']",2,"The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to very low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",2024-11-28 13:02:37,2024-12-03 07:18:24,689,https://stackoverflow.com/questions/79234004/llama-3-2-1b-instruct-generate-inconsistent-output,"Llama-3.2-1B-Instruct generate inconsistent output I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0 , it still generates inconsistent output. This is how my pipeline looks like: pipe = pipeline( ""text-generation"", model=model_id, torch_dtype=torch.bfloat16, device_map=""mps"", model_kwargs={""temperature"": 0.0, ""do_sample"":True, ""top_p"":0.0, ""top_k"":0,}, ) Any idea how to solve this issue?",llama32  1b  instruct generate inconsistent output want use llama32  1b  instruct model  although set   temperature   00    topp   00   topk  0  still generate inconsistent output  pipeline look like  pipe  pipeline    text  generation   model  modelid  torchdtype  torchbfloat16  devicemap  mp   modelkwargs    temperature   00    dosample   true    topp   00    topk  0     idea solve issue ,model inconsistent output due two main factor  1  temperature  set temperature zero give inconsistent result  refer opeani discussion page detail  good option set temperature low value 000001 instead zero  2  dosample already set false  remain way ,llama32  1b  instruct generate inconsistent output want use llama32  1b  instruct model  although set   temperature   00    topp   00   topk  0  still generate inconsistent output  pipeline look like  pipe  pipeline    text  generation   model  modelid  torchdtype  torchbfloat16  devicemap  mp   modelkwargs    temperature   00    dosample   true    topp   00    topk  0     idea solve issue  model inconsistent output due two main factor  1  temperature  set temperature zero give inconsistent result  refer opeani discussion page detail  good option set temperature low value 000001 instead zero  2  dosample already set false  remain way ,Library/Tool-Based Queries
Using an AWS service to execute a python script that will extract keywords from text using keyBERT?,"I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.","['python', 'amazon-web-services', 'aws-lambda', 'nlp', 'large-language-model']",2,"In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.",2024-11-15 11:13:36,2024-11-15 12:43:31,60,https://stackoverflow.com/questions/79192130/using-an-aws-service-to-execute-a-python-script-that-will-extract-keywords-from,"Using an AWS service to execute a python script that will extract keywords from text using keyBERT? I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.",use aws service execute python script extract keyword text use keybert  simple python script give two block text  extract keyword use keybert  compare list keyword sort two list depend list share keyword  aws service would well fit need  want able esentially spin need  give block text  execute return result  not want integrate project not use python   ve attempt use lambda  m concerned potential cost run  thank ,case  would normally think two resource align good practice aws software engineering  sagemaker lambda  model  m use resource  intensive require gpu acceleration would go sagemaker otherwise lambda good solution  case  be d  package keybert script lambda easily deploy container  invoke whenever need process text block  aws lambda charge execution time   cost  efficient occasional task ,use aws service execute python script extract keyword text use keybert  simple python script give two block text  extract keyword use keybert  compare list keyword sort two list depend list share keyword  aws service would well fit need  want able esentially spin need  give block text  execute return result  not want integrate project not use python   ve attempt use lambda  m concerned potential cost run  thank  case  would normally think two resource align good practice aws software engineering  sagemaker lambda  model  m use resource  intensive require gpu acceleration would go sagemaker otherwise lambda good solution  case  be d  package keybert script lambda easily deploy container  invoke whenever need process text block  aws lambda charge execution time   cost  efficient occasional task ,Task-Specific Queries
Normalization of token embeddings in BERT encoder blocks,"Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?","['nlp', 'normalization', 'bert-language-model', 'attention-model']",2,I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.,2024-11-11 14:30:31,2024-11-29 21:21:04,178,https://stackoverflow.com/questions/79178041/normalization-of-token-embeddings-in-bert-encoder-blocks,"Normalization of token embeddings in BERT encoder blocks Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?",normalization token embedding bert encoder block follow multi  headed attention layer bert encoder block  layer normalization done separately embed token  ie  one mean variance per token embed   concatenate vector token embedding  mean variance embedding  ,track full detail layer normalization  ln  bert  mean variance compute per token  weight bias parameter learn ln per token   per embed dimension ,normalization token embedding bert encoder block follow multi  headed attention layer bert encoder block  layer normalization done separately embed token  ie  one mean variance per token embed   concatenate vector token embedding  mean variance embedding   track full detail layer normalization  ln  bert  mean variance compute per token  weight bias parameter learn ln per token   per embed dimension ,Implementation Issues
How to convert character indices to BERT token indices,"I am working with a question-answer dataset UCLNLP/adversarial_qa . from datasets import load_dataset ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: d0 = ds['train'][0] d0 {'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95', 'title': 'Brain', 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.', 'question': 'What sare the benifts of the blood brain barrir?', 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]}, 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}} After tokenization, the answer indices are 56 and 16: from transformers import BertTokenizerFast bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61]) 'isolated from the bloodstream' I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:","['python', 'nlp', 'dataset', 'large-language-model', 'bert-language-model']",1,"You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: def get_token_indices(example): # Tokenize with `return_offsets_mapping=True` to get character offsets for each token encoded = tokenizer( example['question'], example['context'], return_offsets_mapping=True ) # Find character start and end from the original answer char_start = example['answers']['answer_start'][0] char_end = char_start + len(example['answers']['text'][0]) # Identify token indices for the answer start_token_idx = None end_token_idx = None for i, (start, end) in enumerate(encoded['offset_mapping']): if start <= char_start < end: start_token_idx = i if start < char_end <= end: end_token_idx = i break example['answer_start_token_idx'] = start_token_idx example['answer_end_token_idx'] = end_token_idx return example Here's how you can use and test this function: ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) tokenized_ds = ds['train'].map(get_token_indices) # Example d0_tokenized = tokenized_ds[0] print(""Tokenized start index:"", d0_tokenized['answer_start_token_idx']) print(""Tokenized end index:"", d0_tokenized['answer_end_token_idx']) answer_tokens = tokenizer.decode( tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1] ) print(""Tokenized answer:"", answer_tokens) Output: Tokenized start index: 56 Tokenized end index: 60 Tokenized answer: isolated from the bloodstream",2024-11-09 15:15:33,2024-11-10 15:18:14,37,https://stackoverflow.com/questions/79173053/how-to-convert-character-indices-to-bert-token-indices,"How to convert character indices to BERT token indices I am working with a question-answer dataset UCLNLP/adversarial_qa . from datasets import load_dataset ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: d0 = ds['train'][0] d0 {'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95', 'title': 'Brain', 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.', 'question': 'What sare the benifts of the blood brain barrir?', 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]}, 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}} After tokenization, the answer indices are 56 and 16: from transformers import BertTokenizerFast bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61]) 'isolated from the bloodstream' I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:",convert character index bert token index work question  answer dataset uclnlp  adversarialqa  dataset import loaddataset ds  loaddataset    uclnlp  adversarialqa     adversarialqa   map character  base answer indice token  base index tokenize context question together use tokenizer like bert  s example row dataset  d0  ds   train    0  d0   i d    7ba1e8f4261d3170fcf42e84a81dd749116fae95    title    brain    context    another approach brain function examine consequence damage specific brain area  even though protect skull meninge  surround cerebrospinal fluid  isolated bloodstream blood  brain barrier  delicate nature brain make vulnerable numerous disease several type damage  human  effect stroke type brain damage key source information brain function  ability experimentally control nature damage  however  information often difficult interpret  animal study  commonly involve rat  possible use electrode locally inject chemical produce precise pattern damage examine consequence behavior     question    what sare benift blood brain barrir     answer     text     isolated bloodstream     answerstart    195     metadata     split    train    modelintheloop    combined    tokenization  answer indice 56 16  transformer import berttokenizerfast berttokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  berttokenizerdecode  berttokenizerencode  d0   question    d0   context     5661    isolated bloodstream  want create new dataset answer s token index  eg  56 ad 60  linkedin learn class  instructor conversion create csv file share code  expect result ,encode question context  locate token span answer within tokenized context  update dataset token  level index  follow function  def gettokenindice  example    tokenize  returnoffsetsmapping  true  get character offset token encode  tokenizer  example   question    example   context    returnoffsetsmappe  true   find character start end original answer charstart  example   answer     answerstart    0  charend  charstart  len  example   answer     text    0    identify token index answer starttokenidx  none endtokenidx  none   start  end  enumerate  encode   offsetmappe     start   charstart  end  starttokenidx  start  charend   end  endtokenidx  break example   answerstarttokenidx    starttokenidx example   answerendtokenidx    endtokenidx return example s use test function  ds  loaddataset    uclnlp  adversarialqa     adversarialqa   tokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  tokenizedds  ds   train   map  gettokenindice   example d0tokenized  tokenizedds  0  print    tokenized start index    d0tokenize   answerstarttokenidx    print    tokenized end index    d0tokenize   answerendtokenidx    answertokens  tokenizerdecode  tokenizerencode  d0tokenized   question    d0tokenize   context     d0tokenize   answerstarttokenidx    d0tokenize   answerendtokenidx   1   print    tokenized answer    answertokens  output  tokenized start index  56 tokenized end index  60 tokenized answer  isolate bloodstream,convert character index bert token index work question  answer dataset uclnlp  adversarialqa  dataset import loaddataset ds  loaddataset    uclnlp  adversarialqa     adversarialqa   map character  base answer indice token  base index tokenize context question together use tokenizer like bert  s example row dataset  d0  ds   train    0  d0   i d    7ba1e8f4261d3170fcf42e84a81dd749116fae95    title    brain    context    another approach brain function examine consequence damage specific brain area  even though protect skull meninge  surround cerebrospinal fluid  isolated bloodstream blood  brain barrier  delicate nature brain make vulnerable numerous disease several type damage  human  effect stroke type brain damage key source information brain function  ability experimentally control nature damage  however  information often difficult interpret  animal study  commonly involve rat  possible use electrode locally inject chemical produce precise pattern damage examine consequence behavior     question    what sare benift blood brain barrir     answer     text     isolated bloodstream     answerstart    195     metadata     split    train    modelintheloop    combined    tokenization  answer indice 56 16  transformer import berttokenizerfast berttokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  berttokenizerdecode  berttokenizerencode  d0   question    d0   context     5661    isolated bloodstream  want create new dataset answer s token index  eg  56 ad 60  linkedin learn class  instructor conversion create csv file share code  expect result  encode question context  locate token span answer within tokenized context  update dataset token  level index  follow function  def gettokenindice  example    tokenize  returnoffsetsmapping  true  get character offset token encode  tokenizer  example   question    example   context    returnoffsetsmappe  true   find character start end original answer charstart  example   answer     answerstart    0  charend  charstart  len  example   answer     text    0    identify token index answer starttokenidx  none endtokenidx  none   start  end  enumerate  encode   offsetmappe     start   charstart  end  starttokenidx  start  charend   end  endtokenidx  break example   answerstarttokenidx    starttokenidx example   answerendtokenidx    endtokenidx return example s use test function  ds  loaddataset    uclnlp  adversarialqa     adversarialqa   tokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  tokenizedds  ds   train   map  gettokenindice   example d0tokenized  tokenizedds  0  print    tokenized start index    d0tokenize   answerstarttokenidx    print    tokenized end index    d0tokenize   answerendtokenidx    answertokens  tokenizerdecode  tokenizerencode  d0tokenized   question    d0tokenize   context     d0tokenize   answerstarttokenidx    d0tokenize   answerendtokenidx   1   print    tokenized answer    answertokens  output  tokenized start index  56 tokenized end index  60 tokenized answer  isolate bloodstream,Basic Understanding
How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?,"I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage. I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing. Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process? Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes? If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects? Any suggestions or examples would be greatly appreciated! Thank you!","['nlp', 'multiprocessing', 'python-multiprocessing', 'spacy']",2,"I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application. Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved. An example of implementing such a microservice using FastAPI + Docker could look like this: # main.py: FastAPI service with spaCy model from fastapi import FastAPI import spacy app = FastAPI() nlp = spacy.load(""en_core_web_lg"") # Load model once @app.post(""/process/"") async def process_text(text: str): doc = nlp(text) return {""tokens"": [(token.text, token.pos_) for token in doc]} To containerize above FastAPI service: # Dockerfile for the NLP model microservice FROM python:3.9-slim COPY requirements.txt . RUN pip install -r requirements.txt && python -m spacy download en_core_web_lg COPY . /app WORKDIR /app CMD [""gunicorn"", ""-w"", ""4"", ""-k"", ""uvicorn.workers.UvicornWorker"", ""main:app""]",2024-11-05 15:49:33,2024-11-06 10:41:19,96,https://stackoverflow.com/questions/79159805/how-can-i-share-a-complex-spacy-nlp-model-across-multiple-python-processes-to-mi,"How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage? I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage. I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing. Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process? Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes? If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects? Any suggestions or examples would be greatly appreciated! Thank you!",share complex spacy nlp model across multiple python process minimize memory usage   m work multiprocesse python application multiple process need access large  pre  loaded spacy nlp model  eg  encoreweblg   since model memory  intensive  want avoid load separately process  since quickly run main memory object read  only  instead   like load share location process read without duplicate memory usage  look multiprocesse  manager multiprocessingsharedmemory  approach seem well suited numpy array  raw datum buffer simple object  complex object internal reference like nlp model  also look mpi s mpiwin  allocateshared   run issue  use redis server make rank 0 processing work mpi  since process do single rank  defeat propose use multiprocessing  efficient way share spacy model instance across multiple process python avoid reloading process  library technique specifically suited sharing complex  read  only object like nlp model memory across process  multiprocesse  manager sharedmemory viable  way improve performance reduce memory overhead work complex object  suggestion example would greatly appreciate  thank ,would strongly advise treat nlp model like python object  would always prefer load nlp model use microservice approach  align ml  software engineering good practice separate model logic main application  instead load model process  memory  intensive   model load dedicated service  setup allow model use multiple part application without duplicate memory usage  make efficient  modular  scalable  concern memory efficiency address  scalability modularity also improve  example implement microservice use fastapi  docker could look like   mainpy  fastapi service spacy model fastapi import fastapi import spacy app  fastapi   nlp  spacyload    encoreweblg    load model  apppost    process   async def processtext  text  str   doc  nlp  text  return    token     tokentext  tokenpos   token doc   containerize fastapi service   dockerfile nlp model microservice python39  slim copy requirementstxt  run pip install r requirementstxt   python m spacy download encoreweblg copy  app workdir app cmd    gunicorn     w     4     k     uvicornworker  uvicornworker     main  app  ,share complex spacy nlp model across multiple python process minimize memory usage   m work multiprocesse python application multiple process need access large  pre  loaded spacy nlp model  eg  encoreweblg   since model memory  intensive  want avoid load separately process  since quickly run main memory object read  only  instead   like load share location process read without duplicate memory usage  look multiprocesse  manager multiprocessingsharedmemory  approach seem well suited numpy array  raw datum buffer simple object  complex object internal reference like nlp model  also look mpi s mpiwin  allocateshared   run issue  use redis server make rank 0 processing work mpi  since process do single rank  defeat propose use multiprocessing  efficient way share spacy model instance across multiple process python avoid reloading process  library technique specifically suited sharing complex  read  only object like nlp model memory across process  multiprocesse  manager sharedmemory viable  way improve performance reduce memory overhead work complex object  suggestion example would greatly appreciate  thank  would strongly advise treat nlp model like python object  would always prefer load nlp model use microservice approach  align ml  software engineering good practice separate model logic main application  instead load model process  memory  intensive   model load dedicated service  setup allow model use multiple part application without duplicate memory usage  make efficient  modular  scalable  concern memory efficiency address  scalability modularity also improve  example implement microservice use fastapi  docker could look like   mainpy  fastapi service spacy model fastapi import fastapi import spacy app  fastapi   nlp  spacyload    encoreweblg    load model  apppost    process   async def processtext  text  str   doc  nlp  text  return    token     tokentext  tokenpos   token doc   containerize fastapi service   dockerfile nlp model microservice python39  slim copy requirementstxt  run pip install r requirementstxt   python m spacy download encoreweblg copy  app workdir app cmd    gunicorn     w     4     k     uvicornworker  uvicornworker     main  app  ,Implementation Issues
"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing","When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data. https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative. Is there a way to get neutral labels for such examples in RobBERTje? from transformers import RobertaTokenizer, RobertaForSequenceClassification from transformers import pipeline import torch model_name = ""DTAI-KULeuven/robbert-v2-dutch-sentiment"" model = RobertaForSequenceClassification.from_pretrained(model_name) tokenizer = RobertaTokenizer.from_pretrained(model_name) classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer) result1 = classifier('Fhdf') result2 = classifier('Als gisteren inclusief blauw') print(result1) print(result2) Output: [{'label': 'Positive', 'score': 0.7520257234573364}] [{'label': 'Negative', 'score': 0.7538396120071411}]","['python', 'nlp', 'bert-language-model', 'roberta-language-model']",1,"This model was trained only on negative and positive labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral. what you can do is to: 1- Find other models that was trained to include neutral label. 2- Fine-tune this model on a dataset that includes neutral label. 3- Empirically define a threshold based on the confidence outputs and interpret it as neutral . The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as neutral . Here's a sample: def classify_with_neutral(text, threshold=0.5): result = classifier(text)[0] # Get the classification result if result['score'] < threshold: result['label'] = 'Neutral' # Override label to 'Neutral' return result",2024-11-04 11:36:35,2024-11-04 12:04:43,57,https://stackoverflow.com/questions/79155290/dutch-sentiment-analysis-robbertje-outputs-just-positive-negative-labels-netura,"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data. https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative. Is there a way to get neutral labels for such examples in RobBERTje? from transformers import RobertaTokenizer, RobertaForSequenceClassification from transformers import pipeline import torch model_name = ""DTAI-KULeuven/robbert-v2-dutch-sentiment"" model = RobertaForSequenceClassification.from_pretrained(model_name) tokenizer = RobertaTokenizer.from_pretrained(model_name) classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer) result1 = classifier('Fhdf') result2 = classifier('Als gisteren inclusief blauw') print(result1) print(result2) Output: [{'label': 'Positive', 'score': 0.7520257234573364}] [{'label': 'Negative', 'score': 0.7538396120071411}]",dutch sentiment analysis robbertje outputs positive  negative label  netural label miss run dutch sentiment analysis robbertje  output positive  negative label  netural label miss datum    huggingfaceco  dtai  kuleuven  robbert  v2  dutch  sentiment obvious neutral sentence  word eg   fhdf   nonsense   als gisteren inclusief blauw   neutral   evaluate positive negative  way get neutral label example robbertje  transformer import robertatokenizer  robertaforsequenceclassification transformer import pipeline import torch modelname    dtai  kuleuven  robbert  v2  dutch  sentiment  model  robertaforsequenceclassificationfrompretrained  modelname  tokenizer  robertatokenizerfrompretraine  modelname  classifier  pipeline   sentiment  analysis   model  model  tokenizer  tokenizer  result1  classifi   fhdf   result2  classifi   als gisteren inclusief blauw   print  result1  print  result2  output     label    positive    score   07520257234573364      label    negative    score   07538396120071411  ,model train negative positive label  therefore  try categorize every input positive negative  even nonsensical neutral   1 find model train include neutral label  2 fine  tune model dataset include neutral label  3 empirically define threshold base confidence output interpret neutral  first 2 choice extensive effort  would suggest go third option quick workaround  try feeding model neutral input observe range confidence score output  use threshold classify neutral  s sample  def classifywithneutral  text  threshold05   result  classifier  text   0   get classification result result   score    threshold  result   label     neutral   override label  neutral  return result,dutch sentiment analysis robbertje outputs positive  negative label  netural label miss run dutch sentiment analysis robbertje  output positive  negative label  netural label miss datum    huggingfaceco  dtai  kuleuven  robbert  v2  dutch  sentiment obvious neutral sentence  word eg   fhdf   nonsense   als gisteren inclusief blauw   neutral   evaluate positive negative  way get neutral label example robbertje  transformer import robertatokenizer  robertaforsequenceclassification transformer import pipeline import torch modelname    dtai  kuleuven  robbert  v2  dutch  sentiment  model  robertaforsequenceclassificationfrompretrained  modelname  tokenizer  robertatokenizerfrompretraine  modelname  classifier  pipeline   sentiment  analysis   model  model  tokenizer  tokenizer  result1  classifi   fhdf   result2  classifi   als gisteren inclusief blauw   print  result1  print  result2  output     label    positive    score   07520257234573364      label    negative    score   07538396120071411   model train negative positive label  therefore  try categorize every input positive negative  even nonsensical neutral   1 find model train include neutral label  2 fine  tune model dataset include neutral label  3 empirically define threshold base confidence output interpret neutral  first 2 choice extensive effort  would suggest go third option quick workaround  try feeding model neutral input observe range confidence score output  use threshold classify neutral  s sample  def classifywithneutral  text  threshold05   result  classifier  text   0   get classification result result   score    threshold  result   label     neutral   override label  neutral  return result,Implementation Issues
Finding Root Form of Verbs using Curiosity-AI/Catalyst,"I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match PartOfSpeech.VERB but I don't know how to continue from there. This is what I have so far: const string text = ""The disastrous cat runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var sentence in doc.TokensData) { foreach (var token in sentence) { if(token.Tag == PartOfSpeech.VERB) { // so here I'd like to the root form of the verb } } } Any help is greatly appreciated.","['c#', 'nlp']",1,"The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form. (I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.) //// Installed Curiosity.Library v24.10.52882 //// Installed Catalyst v1.0.51118 //// Installed Catalyst.Models.English v1.0.30952 using Catalyst; using Mosaik.Core; const string text = ""The disastrous cat quickly runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var span in doc.Spans) { foreach (var token in span.Tokens) { if (token.POS == PartOfSpeech.VERB) { Console.WriteLine($""Root of the verb '{token.Value}' is '{token.Lemma}'.""); } } } Console.WriteLine(); Console.WriteLine(""Complete; press any key.""); Console.ReadKey(); Note: For this specific sentence, I have added an adverb (""quickly"") before the verb (""runs""). Without this, the library incorrectly interprets ""runs"" as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.",2024-11-01 17:58:01,2024-11-05 17:40:24,143,https://stackoverflow.com/questions/79148979/finding-root-form-of-verbs-using-curiosity-ai-catalyst,"Finding Root Form of Verbs using Curiosity-AI/Catalyst I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match PartOfSpeech.VERB but I don't know how to continue from there. This is what I have so far: const string text = ""The disastrous cat runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var sentence in doc.TokensData) { foreach (var token in sentence) { if(token.Tag == PartOfSpeech.VERB) { // so here I'd like to the root form of the verb } } } Any help is greatly appreciated.",finding root form verbs use curiosity  ai  catalyst  m try find root form verb  run text pipeline identify tokens match partofspeech  verb not know continue  far  const string text    disastrous cat run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var sentence doc  tokensdata   foreach  var token sentence    token  tag   partofspeech  verb    would like root form verb    help greatly appreciate ,follow code  target net 80  illustrate one method obtain root form verb inflect form   annonote  code comment  three nuget package  version  require  code identical original sample    installed curiosity  library v241052882  installed catalyst v1051118  installed catalyst  model  english v1030952 use catalyst  use mosaik  core  const string text    disastrous cat quickly run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var span doc  spans   foreach  var token span  tokens    token  pos   partofspeech  verb   console  writeline    root verb   token  value     token  lemma           console  writeline    console  writeline    complete  press key      console  readkey    note  specific sentence  add adverb    quickly   verb    run    without  library incorrectly interpret   run  noun  depend source text  might issue  believe separate question ask ,finding root form verbs use curiosity  ai  catalyst  m try find root form verb  run text pipeline identify tokens match partofspeech  verb not know continue  far  const string text    disastrous cat run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var sentence doc  tokensdata   foreach  var token sentence    token  tag   partofspeech  verb    would like root form verb    help greatly appreciate  follow code  target net 80  illustrate one method obtain root form verb inflect form   annonote  code comment  three nuget package  version  require  code identical original sample    installed curiosity  library v241052882  installed catalyst v1051118  installed catalyst  model  english v1030952 use catalyst  use mosaik  core  const string text    disastrous cat quickly run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var span doc  spans   foreach  var token span  tokens    token  pos   partofspeech  verb   console  writeline    root verb   token  value     token  lemma           console  writeline    console  writeline    complete  press key      console  readkey    note  specific sentence  add adverb    quickly   verb    run    without  library incorrectly interpret   run  noun  depend source text  might issue  believe separate question ask ,Implementation Issues
Is it possible to get embeddings from NV-Embed using Candle?,"What I want to do is a CLI program that outputs embeddings of an arbitrary input. To do that, I want to do an inference with an embeddings model, and I chose NV-Embed-v2 . My framework of choice is Candle , but I also looked at Mistral-RS . Basically, what I'm trying to do is this code fragment: https://huggingface.co/nvidia/NV-Embed-v2 but with Rust and Candle. What I tried is to start off with Mistral Candle's example because the NV-Embed's HF page says: Model Details / Base Decoder-only LLM: Mistral-7B-v0.1 . I replaced the model id in the original code with nvidia/NV-Embed-v2 , and was able to download the weights from Hugging Face, but upon loading the config, I got this: Error: missing field `vocab_size` at line 101 column 1 Then I hardcoded the values from the JSON config loaded from HF to a newly created candle_transformers::models::mistral::Config instance. And after that, Mistral::new(&config, vb) fails with: Error: cannot find tensor model.embed_tokens.weight Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?","['machine-learning', 'rust', 'nlp']",1,"candle looking for model.embed_tokens.weight whereas the original tensor name is embedding_model.embed_tokens.weight . You just have to change this line of mistral.rs in candle_transformers. // from let vb_m = vb.pp(""model""); //to let vb_m = vb.pp(""embedding_model"");",2024-10-31 15:55:49,2024-11-04 17:45:20,335,https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle,"Is it possible to get embeddings from NV-Embed using Candle? What I want to do is a CLI program that outputs embeddings of an arbitrary input. To do that, I want to do an inference with an embeddings model, and I chose NV-Embed-v2 . My framework of choice is Candle , but I also looked at Mistral-RS . Basically, what I'm trying to do is this code fragment: https://huggingface.co/nvidia/NV-Embed-v2 but with Rust and Candle. What I tried is to start off with Mistral Candle's example because the NV-Embed's HF page says: Model Details / Base Decoder-only LLM: Mistral-7B-v0.1 . I replaced the model id in the original code with nvidia/NV-Embed-v2 , and was able to download the weights from Hugging Face, but upon loading the config, I got this: Error: missing field `vocab_size` at line 101 column 1 Then I hardcoded the values from the JSON config loaded from HF to a newly created candle_transformers::models::mistral::Config instance. And after that, Mistral::new(&config, vb) fails with: Error: cannot find tensor model.embed_tokens.weight Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?",possible get embedding nv  embed use candle  want cli program output embedding arbitrary input   want inference embedding model  choose nv  embed  v2  framework choice candle  also look mistral  rs  basically   m try code fragment    huggingfaceco  nvidia  nv  embed  v2 rust candle  try start mistral candle s example nv  embed s hf page say  model details  base decoder  only llm  mistral7b  v01  replace model i d original code nvidia  nv  embed  v2  able download weight hugging face  upon load config  get  error  miss field  vocabsize  line 101 column 1 hardcode value json config load hf newly create candletransformer   model   mistral   config instance   mistral   new   config  vb  fail  error  find tensor modelembedtokensweight way around  maybe candle  base open source work could use inspiration   maybe s common mistake could easily diagnose ,candle look modelembedtokensweight whereas original tensor name embeddingmodelembedtokensweight  change line mistralrs candletransformer   let vbm  vbpp    model    to let vbm  vbpp    embeddingmodel   ,possible get embedding nv  embed use candle  want cli program output embedding arbitrary input   want inference embedding model  choose nv  embed  v2  framework choice candle  also look mistral  rs  basically   m try code fragment    huggingfaceco  nvidia  nv  embed  v2 rust candle  try start mistral candle s example nv  embed s hf page say  model details  base decoder  only llm  mistral7b  v01  replace model i d original code nvidia  nv  embed  v2  able download weight hugging face  upon load config  get  error  miss field  vocabsize  line 101 column 1 hardcode value json config load hf newly create candletransformer   model   mistral   config instance   mistral   new   config  vb  fail  error  find tensor modelembedtokensweight way around  maybe candle  base open source work could use inspiration   maybe s common mistake could easily diagnose  candle look modelembedtokensweight whereas original tensor name embeddingmodelembedtokensweight  change line mistralrs candletransformer   let vbm  vbpp    model    to let vbm  vbpp    embeddingmodel   ,Implementation Issues
"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) I have short product descriptions that I’d like to transform into structured attributes. Example: Input: “La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml” Output: Year = 2017 Color = Red Weight = 750 Weight Unit = ml If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach: There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed. There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc. Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues. I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about. I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary. I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies. Appreciate any insight into approaches or suggested learning resources. I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.","['nlp', 'artificial-intelligence', 'large-language-model', 'named-entity-recognition']",1,"LLM would work nicely for this. I'v done similar tasks before and it worked nicely with minimal training. Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate, but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions. For you task I would use a framework like Langchain, and the following prompt (note you might need to work on your prompt a bit this just an example). When run with a model it will create an XML output which would be trivial to parse. You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me. You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components: - **Year**: The vintage year of the wine. - **Color**: The color of the wine (e.g., Red, White, Rosé). - **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500). - **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters). - **Brand**: The brand or producer of the wine. - **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot). **Instructions:** - Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components. - Be cautious of potential ambiguities. For example: - A brand name may include words like ""Red"" or ""White"" (e.g., ""Red Head Vineyard"") which should not be confused with the wine color. - Large numbers may represent weight (e.g., ""1500 ml"") rather than a year. - **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it. **Output Format:** Provide the extracted information in XML format, using the following structure: <Wine> <Year>{{Year}}</Year> <Color>{{Color}}</Color> <Weight>{{Weight}}</Weight> <WeightUnit>{{WeightUnit}}</WeightUnit> <Brand>{{Brand}}</Brand> <GrapeVariety>{{GrapeVariety}}</GrapeVariety> </Wine> **Examples:** 1. **Input:** `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml` **Output:** ```xml <Wine> <Year>2017</Year> <Color>Red</Color> <Weight>750</Weight> <WeightUnit>ml</WeightUnit> <Brand>La Lecciaia</Brand> <GrapeVariety>Cabernet Sauvignon</GrapeVariety> </Wine> ``` `Red Head Vineyard Chardonnay 2020 1.5L` **Output:** <Wine> <Year>2020</Year> <Color></Color> <Weight>1.5</Weight> <WeightUnit>L</WeightUnit> <Brand>Red Head Vineyard</Brand> <GrapeVariety>Chardonnay</GrapeVariety> </Wine> **Task:** Given the following wine description, extract the components and provide the output in XML format as specified. {win_description} Keep in mind that LLMs are not cheap to run. But for this tasks given ambiguousness of the domain it is most likely the best choice. For this particular task it would be 1/1000 of a penny per label using OpenAI service. You might find a cheaper model / provider. However when working with LLM it is very important to ensure accuracy first, then optimize for costs. The whole thing will probably take 1-2 hours to build for the intermediate LLM developer. If you are learning it may vary. But this is a perfect project to learn about LLMs",2024-10-21 20:54:56,2024-10-22 11:49:29,183,https://stackoverflow.com/questions/79111733/how-to-derive-attributes-labels-from-short-plain-text-descriptions-ner-llm,"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) I have short product descriptions that I’d like to transform into structured attributes. Example: Input: “La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml” Output: Year = 2017 Color = Red Weight = 750 Weight Unit = ml If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach: There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed. There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc. Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues. I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about. I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary. I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies. Appreciate any insight into approaches or suggested learning resources. I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.",derive attribute  label short plain text description   ner  llm    derive attribute  label short plain text description   ner  llm    short product description  like transform structure attribute  example  input   la lecciaia cabernet sauvignon 2017  red  750ml  output  year  2017 color  red weight  750 weight unit  ml everything format would trivial write regular expression do  many different format nuance  increasingly cumbersome hard  code logic format  try create generic solution immediately run issue  basic  approach  several different data provider  format  example  another provider might use   red  2017 la lecciaia cabernet sauvignon 750 ml   even give provider  may multiple format may change time  format always strictly follow  many way express particular component  example  weight might express one   15l    1 12 liters    1500ml   etc  part description may confused target component  may white wine brand call  red head vineyard   weight  2000 ml  may confused year  etc   use wine example sake simplicity general audience product domain conceptual issue   consider  nice  would useful able parse even detail like algo would smart enough know  la lecciaia  brand  cabernet sauvignon  grape variety  assume would take front work hard get right  straightforward method would good know   like develop general  purpose function accept description format  little experience nlp  artificial intelligence suspect useful tool  algo leverage  1000  example record could potentially use train model  something run locally would prefer absolutely necessary   look specific implementation guidance anyone  work similar problem  open hybrid approach additional logic manual oversight could account initial inaccuracy  appreciate insight approach suggest learn resource   ve look online information many approach involve significant amount front work unclear will work practical sense ,llm would work nicely   v do similar task work nicely minimal training  keep mind statistical method nlp  llm  ner never 100  accurate  practical purpose find llms accurate custom soup regular expression  task would use framework like langchain  follow prompt  note might need work prompt bit example   run model create xml output would trivial parse  modify prompt create different type output   personally find xml work well  ai language model design parse wine bottle description structure datum  give wine bottle description  task extract follow component     year    vintage year wine     color    color wine  eg  red  white  ros      weight    volume wine bottle express number  eg  750  1500      weight unit    unit measurement weight  eg  ml  ml  l  liters      brand    brand producer wine     grape variety    variety grape use  eg  cabernet sauvignon  merlot     instruction     wine description may come various format may include additional confusing information  carefully analyze description accurately extract component   cautious potential ambiguity  example   brand name may include word like   red    white   eg    red head vineyard   confuse wine color   large number may represent weight  eg    1500 ml   rather year     assume information present description    component missing  may leave correspond tag empty omit    output format    provide extract information xml format  use follow structure   wine   year    year    year   color    color    color   weight    weight    weight   weightunit    weightunit    weightunit   brand    brand    brand   grapevariety    grapevariety    grapevariety   wine    example    1    input     la lecciaia cabernet sauvignon 2017  red  750ml    output       xml  wine   year  2017  year   color  red  color   weight  750  weight   weightunit  ml  weightunit   brand  la lecciaia  brand   grapevariety  cabernet sauvignon  grapevariety   wine      red head vineyard chardonnay 2020 15l    output     wine   year  2020  year   color   color   weight  15  weight   weightunit  l  weightunit   brand  red head vineyard  brand   grapevariety  chardonnay  grapevariety   wine    task    give follow wine description  extract component provide output xml format specify   windescription  keep mind llms cheap run  task give ambiguousness domain likely good choice  particular task would 11000 penny per label use openai service  might find cheap model  provider  however work llm important ensure accuracy first  optimize cost  whole thing probably take 1  2 hour build intermediate llm developer  learning may vary  perfect project learn llms,derive attribute  label short plain text description   ner  llm    derive attribute  label short plain text description   ner  llm    short product description  like transform structure attribute  example  input   la lecciaia cabernet sauvignon 2017  red  750ml  output  year  2017 color  red weight  750 weight unit  ml everything format would trivial write regular expression do  many different format nuance  increasingly cumbersome hard  code logic format  try create generic solution immediately run issue  basic  approach  several different data provider  format  example  another provider might use   red  2017 la lecciaia cabernet sauvignon 750 ml   even give provider  may multiple format may change time  format always strictly follow  many way express particular component  example  weight might express one   15l    1 12 liters    1500ml   etc  part description may confused target component  may white wine brand call  red head vineyard   weight  2000 ml  may confused year  etc   use wine example sake simplicity general audience product domain conceptual issue   consider  nice  would useful able parse even detail like algo would smart enough know  la lecciaia  brand  cabernet sauvignon  grape variety  assume would take front work hard get right  straightforward method would good know   like develop general  purpose function accept description format  little experience nlp  artificial intelligence suspect useful tool  algo leverage  1000  example record could potentially use train model  something run locally would prefer absolutely necessary   look specific implementation guidance anyone  work similar problem  open hybrid approach additional logic manual oversight could account initial inaccuracy  appreciate insight approach suggest learn resource   ve look online information many approach involve significant amount front work unclear will work practical sense  llm would work nicely   v do similar task work nicely minimal training  keep mind statistical method nlp  llm  ner never 100  accurate  practical purpose find llms accurate custom soup regular expression  task would use framework like langchain  follow prompt  note might need work prompt bit example   run model create xml output would trivial parse  modify prompt create different type output   personally find xml work well  ai language model design parse wine bottle description structure datum  give wine bottle description  task extract follow component     year    vintage year wine     color    color wine  eg  red  white  ros      weight    volume wine bottle express number  eg  750  1500      weight unit    unit measurement weight  eg  ml  ml  l  liters      brand    brand producer wine     grape variety    variety grape use  eg  cabernet sauvignon  merlot     instruction     wine description may come various format may include additional confusing information  carefully analyze description accurately extract component   cautious potential ambiguity  example   brand name may include word like   red    white   eg    red head vineyard   confuse wine color   large number may represent weight  eg    1500 ml   rather year     assume information present description    component missing  may leave correspond tag empty omit    output format    provide extract information xml format  use follow structure   wine   year    year    year   color    color    color   weight    weight    weight   weightunit    weightunit    weightunit   brand    brand    brand   grapevariety    grapevariety    grapevariety   wine    example    1    input     la lecciaia cabernet sauvignon 2017  red  750ml    output       xml  wine   year  2017  year   color  red  color   weight  750  weight   weightunit  ml  weightunit   brand  la lecciaia  brand   grapevariety  cabernet sauvignon  grapevariety   wine      red head vineyard chardonnay 2020 15l    output     wine   year  2020  year   color   color   weight  15  weight   weightunit  l  weightunit   brand  red head vineyard  brand   grapevariety  chardonnay  grapevariety   wine    task    give follow wine description  extract component provide output xml format specify   windescription  keep mind llms cheap run  task give ambiguousness domain likely good choice  particular task would 11000 penny per label use openai service  might find cheap model  provider  however work llm important ensure accuracy first  optimize cost  whole thing probably take 1  2 hour build intermediate llm developer  learning may vary  perfect project learn llms,Implementation Issues
Varying embedding dim due to changing padding in batch size,"I want to train a simple neural network, which has embedding_dim as a parameter: class BoolQNN(nn.Module): def __init__(self, embedding_dim): super(BoolQNN, self).__init__() self.fc1 = nn.Linear(embedding_dim, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 1) def forward(self, question_emb, passage_emb): combined = torch.cat((question_emb, passage_emb), dim=1) x = self.fc1(combined) x = self.relu(x) x = self.fc2(x) return torch.sigmoid(x) To load the data I used torchs DataLoader with a custom collate_fn. train_dataset = BoolQDataset(train_data, pretrained_embeddings) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd) model = BoolQNN(301) The collate_fn_padd function looks the following: def collate_fn_padd(batch): questions, passages, labels = zip(*batch) questions = [torch.tensor(q) for q in questions] passages = [torch.tensor(p) for p in passages] padded_questions = pad_sequence(questions, batch_first=True, padding_value=0) padded_passages = pad_sequence(passages, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.float32) return padded_questions, padded_passages, labels The problem: For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch). That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch. Due to that, I receive errors like that: mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64) Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?","['python', 'text', 'nlp', 'padding', 'data-preprocessing']",1,"You can add a maximum length argument set to embedding_dim to pad and truncate all the data to a fixed length: padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions] padded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",2024-10-18 15:54:51,2024-10-19 14:05:46,49,https://stackoverflow.com/questions/79102797/varying-embedding-dim-due-to-changing-padding-in-batch-size,"Varying embedding dim due to changing padding in batch size I want to train a simple neural network, which has embedding_dim as a parameter: class BoolQNN(nn.Module): def __init__(self, embedding_dim): super(BoolQNN, self).__init__() self.fc1 = nn.Linear(embedding_dim, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 1) def forward(self, question_emb, passage_emb): combined = torch.cat((question_emb, passage_emb), dim=1) x = self.fc1(combined) x = self.relu(x) x = self.fc2(x) return torch.sigmoid(x) To load the data I used torchs DataLoader with a custom collate_fn. train_dataset = BoolQDataset(train_data, pretrained_embeddings) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd) model = BoolQNN(301) The collate_fn_padd function looks the following: def collate_fn_padd(batch): questions, passages, labels = zip(*batch) questions = [torch.tensor(q) for q in questions] passages = [torch.tensor(p) for p in passages] padded_questions = pad_sequence(questions, batch_first=True, padding_value=0) padded_passages = pad_sequence(passages, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.float32) return padded_questions, padded_passages, labels The problem: For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch). That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch. Due to that, I receive errors like that: mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64) Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?",vary embed dim due change padding batch size want train simple neural network  embeddingdim parameter  class boolqnn  nn  module   def   init    self  embeddingdim   super  boolqnn  self  init     selffc1  nn  linear  embeddingdim  64  selfrelu  nn  relu   selffc2  nn  linear  64  1  def forward  self  questionemb  passageemb   combine  torchcat   questionemb  passageemb   dim1  x  selffc1  combine  x  selfrelu  x  x  selffc2  x  return torchsigmoid  x  load datum use torchs dataloader custom collatefn  traindataset  boolqdataset  traindata  pretrainedembedding  trainloader  torchutilsdata  dataloader  traindataset  batchsize16  shuffle  true  collatefn  collatefnpadd  model  boolqnn  301  collatefnpadd function look follow  def collatefnpadd  batch   question  passage  label  zip   batch  question   torchtensor  q  q question  passage   torchtensor  p  p passage  paddedquestion  padsequence  question  batchfirst  true  paddingvalue0  paddedpassage  padsequence  passage  batchfirst  true  paddingvalue0  label  torchtensor  label  dtype  torchfloat32  return paddedquestion  paddedpassage  label problem  every batch want train model  embed text get pad differently long  take long sequence current batch   mean embed dim  input size linear layer neural network change batch batch  althoug want size every batch  due  receive error like  mat1 mat2 shape multiply  16x182 301x64  possible adjust collatefnpad function padd sequence size  independet batch size ,add maximum length argument set embeddingdim pad truncate datum fix length  paddedquestion   torchnnfunctionalpad  torchtensor  q    0  maxlength  len  q    value0    maxlength  q question  paddedpassage   torchnnfunctionalpad  torchtensor  p    0  maxlength  len  p    value0    maxlength  p passage ,vary embed dim due change padding batch size want train simple neural network  embeddingdim parameter  class boolqnn  nn  module   def   init    self  embeddingdim   super  boolqnn  self  init     selffc1  nn  linear  embeddingdim  64  selfrelu  nn  relu   selffc2  nn  linear  64  1  def forward  self  questionemb  passageemb   combine  torchcat   questionemb  passageemb   dim1  x  selffc1  combine  x  selfrelu  x  x  selffc2  x  return torchsigmoid  x  load datum use torchs dataloader custom collatefn  traindataset  boolqdataset  traindata  pretrainedembedding  trainloader  torchutilsdata  dataloader  traindataset  batchsize16  shuffle  true  collatefn  collatefnpadd  model  boolqnn  301  collatefnpadd function look follow  def collatefnpadd  batch   question  passage  label  zip   batch  question   torchtensor  q  q question  passage   torchtensor  p  p passage  paddedquestion  padsequence  question  batchfirst  true  paddingvalue0  paddedpassage  padsequence  passage  batchfirst  true  paddingvalue0  label  torchtensor  label  dtype  torchfloat32  return paddedquestion  paddedpassage  label problem  every batch want train model  embed text get pad differently long  take long sequence current batch   mean embed dim  input size linear layer neural network change batch batch  althoug want size every batch  due  receive error like  mat1 mat2 shape multiply  16x182 301x64  possible adjust collatefnpad function padd sequence size  independet batch size  add maximum length argument set embeddingdim pad truncate datum fix length  paddedquestion   torchnnfunctionalpad  torchtensor  q    0  maxlength  len  q    value0    maxlength  q question  paddedpassage   torchnnfunctionalpad  torchtensor  p    0  maxlength  len  p    value0    maxlength  p passage ,Implementation Issues
How can I adjust the performance of tokenizer?,"Working with the tokenizer from the transformers library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not. I'm wondering if I can ""adjust"" (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to. To be more specific, the type of tokenizer is transformers.XLMRobertaTokenizerFast , which is a unigram tokenizer, and the model is paraphrase-multilingual-mpnet-base-v2 .","['nlp', 'huggingface-transformers', 'huggingface', 'huggingface-tokenizers']",1,"You can change the tokenizer's vocabulary: tokenizer.add_tokens([""asadaf"", ""sdfsaf""]) model.resize_token_embeddings(len(tokenizer)) # change input embeddings size input_text = ""This is asadaf and sdfsaf"" print(tokenizer(input_text)) As a result, asadaf and sdfsaf would be tokenized as unique words.",2024-10-18 06:45:15,2024-10-20 16:50:59,49,https://stackoverflow.com/questions/79100835/how-can-i-adjust-the-performance-of-tokenizer,"How can I adjust the performance of tokenizer? Working with the tokenizer from the transformers library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not. I'm wondering if I can ""adjust"" (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to. To be more specific, the type of tokenizer is transformers.XLMRobertaTokenizerFast , which is a unigram tokenizer, and the model is paraphrase-multilingual-mpnet-base-v2 .",adjust performance tokenizer  work tokenizer transformer library hugging face  tokenizer work fine case  case    m wonder   adjust   train new tokenizer scratch  performance tokenizer handle bad case still maintain good performance case use  specific  type tokenizer transformer  xlmrobertatokenizerfast  unigram tokenizer  model paraphrase  multilingual  mpnet  base  v2 ,change tokenizer s vocabulary  tokenizeraddtokens     asadaf     sdfsaf    modelresizetokenembedding  len  tokenizer    change input embedding size inputtext    asadaf sdfsaf  print  tokenizer  inputtext   result  asadaf sdfsaf would tokenize unique word ,adjust performance tokenizer  work tokenizer transformer library hugging face  tokenizer work fine case  case    m wonder   adjust   train new tokenizer scratch  performance tokenizer handle bad case still maintain good performance case use  specific  type tokenizer transformer  xlmrobertatokenizerfast  unigram tokenizer  model paraphrase  multilingual  mpnet  base  v2  change tokenizer s vocabulary  tokenizeraddtokens     asadaf     sdfsaf    modelresizetokenembedding  len  tokenizer    change input embedding size inputtext    asadaf sdfsaf  print  tokenizer  inputtext   result  asadaf sdfsaf would tokenize unique word ,Library/Tool-Based Queries
"With spaCy, how can I get all lemmas from a string?","I have a pandas data frame with a column of text values (documents). I want to apply lemmatization on these values with the spaCy library using the pandas apply function. I've defined my to_lemma function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow. Is there a way to extract the lemmatized form of a document in spaCy? def to_lemma(text): tp = nlp(text) line = """" for word in tp: line = line + word.lemma_ + "" "" return line","['python', 'pandas', 'nlp', 'spacy', 'lemmatization']",2,"There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input. The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use nlp.pipe() with an Iterable of strings. This means it is easier to not use apply. Disable components that you do not use. For token level processing where you need the lemmas this would be 'parser' (the dependency parser) and 'ner' (the Named Entity Recognition component). Increase the batch_size (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot. Increase the number of processors used using n_process . This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an if __name__ == '__main__': wrapper. Basic example with 1. and 2.: texts = df[""column_name""] nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner']) lemmas = [] for processed_doc in nlp.pipe(texts): lemmas.append("" "".join([token.lemma_ for token in processed_doc])) df[""column_name_lemmas""] = lemmas Advanced example for all four: if __name__ == '__main__': texts = df[""column_name""] nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner']) lemmas = [] for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4): lemmas.append("" "".join([token.lemma_ for token in processed_doc])) df[""column_name_lemmas""] = lemmas",2024-10-12 21:03:21,2024-10-14 13:09:50,105,https://stackoverflow.com/questions/79081924/with-spacy-how-can-i-get-all-lemmas-from-a-string,"With spaCy, how can I get all lemmas from a string? I have a pandas data frame with a column of text values (documents). I want to apply lemmatization on these values with the spaCy library using the pandas apply function. I've defined my to_lemma function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow. Is there a way to extract the lemmatized form of a document in spaCy? def to_lemma(text): tp = nlp(text) line = """" for word in tp: line = line + word.lemma_ + "" "" return line",spacy  get lemmas string  panda datum frame column text value  document   want apply lemmatization value spacy library use panda apply function   ve define tolemma function iterate word document concatenate correspond lemmas output string  however slow  way extract lemmatize form document spacy  def tolemma  text   tp  nlp  text  line     word tp  line  line  wordlemma       return line,many way speed spacy processing  question make sense depend mostly size input  obvious one individually apply model every single row  rather use batch processing  use nlppipe   iterable string  mean easy use apply  disable component use  token level processing need lemma would  parser   dependency parser   ner   name entity recognition component   increase batchsize  object buffer  pipe    default 1000  obviously make sense touch memory increase lot  increase number processor use use nprocess  increase time take initially load model decrease processing time  experience start make sense 500k text  note also require code run   name        main     wrapper  basic example 1  2   text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemmas advanced example four    name        main     text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text  batchsize10000  nprocess4   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemma,spacy  get lemmas string  panda datum frame column text value  document   want apply lemmatization value spacy library use panda apply function   ve define tolemma function iterate word document concatenate correspond lemmas output string  however slow  way extract lemmatize form document spacy  def tolemma  text   tp  nlp  text  line     word tp  line  line  wordlemma       return line many way speed spacy processing  question make sense depend mostly size input  obvious one individually apply model every single row  rather use batch processing  use nlppipe   iterable string  mean easy use apply  disable component use  token level processing need lemma would  parser   dependency parser   ner   name entity recognition component   increase batchsize  object buffer  pipe    default 1000  obviously make sense touch memory increase lot  increase number processor use use nprocess  increase time take initially load model decrease processing time  experience start make sense 500k text  note also require code run   name        main     wrapper  basic example 1  2   text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemmas advanced example four    name        main     text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text  batchsize10000  nprocess4   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemma,Library/Tool-Based Queries
Avoiding overlap in frequency and document frequency count in Quanteda,"Below is a dummy corpus of 4 documents. The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in. The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive. Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each, but are intended to be counted once, according to the dictionary. The expected overall frequency count is (extracted from the 'pattern"" column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key. Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key. I have three questions: are there any problems/issues that could affect output accuracy using this approach? is there a better/more parsimonius approach to achieve what I am trying to do? what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table? library (quanteda) library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") x <- x %>% tokens_compound(phrase(""dairy farmers""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farms""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farm""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farming""), concatenator = "" "") %>% tokens_compound(phrase(""dairy industry""), concatenator = "" "") %>% tokens_compound(phrase(""indigenous australians""), concatenator = "" "") %>% tokens_compound(phrase(""australia post""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farmer""), concatenator = "" "") x dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign- owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farmers"", ""dairy farms"",""dairy farm"",""dairy farming"",""dairy industry"", ""dairy farmer"",""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) kwicdict <- kwic(x, pattern = dict, window = 4) write.csv (kwicdict, ""D:/Output/TEST.csv"") DF <- read.csv(""D://Output/TEST.csv"",header=T) ## obtaining frequency count of KWIC table 'pattern ' values > x2 <- DF[,8] > > table (x2) x2 auspost business dairy farmers foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2","['r', 'count', 'nlp', 'overlap', 'quanteda']",1,"I don't think that kwic() is what you want here. tokens_lookup() lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.) library(quanteda) #> Package version: 4.1.0 #> Unicode version: 14.0 #> ICU version: 71.1 #> Parallel computing: 10 of 10 threads used. #> See https://quanteda.io for tutorials and examples. library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign-owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farm*"", ""dairy industry"", ""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) x <- tokens(txt) # with overlap tokens_lookup(x, dict) |> dfm() #> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars. #> features #> docs multinat dairy auspost oz farmers foreign business indig peep industry #> doc1 0 3 0 2 5 0 0 0 0 1 #> doc2 0 2 0 1 0 0 0 0 1 1 #> doc3 1 1 0 0 1 0 1 1 1 0 #> doc4 0 0 1 2 1 1 0 0 0 0 # without overlap tokens_lookup(x, dict, nested_scope = ""dictionary"") |> dfm() #> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars. #> features #> docs multinat dairy auspost oz farmers foreign business indig peep industry #> doc1 0 3 0 2 3 0 0 0 0 1 #> doc2 0 2 0 1 0 0 0 0 1 0 #> doc3 1 1 0 0 1 0 1 1 0 0 #> doc4 0 0 1 1 1 1 0 0 0 0 Created on 2024-10-06 with reprex v2.1.1",2024-10-05 12:43:52,2024-10-06 10:02:36,61,https://stackoverflow.com/questions/79057082/avoiding-overlap-in-frequency-and-document-frequency-count-in-quanteda,"Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents. The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in. The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive. Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each, but are intended to be counted once, according to the dictionary. The expected overall frequency count is (extracted from the 'pattern"" column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key. Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key. I have three questions: are there any problems/issues that could affect output accuracy using this approach? is there a better/more parsimonius approach to achieve what I am trying to do? what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table? library (quanteda) library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") x <- x %>% tokens_compound(phrase(""dairy farmers""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farms""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farm""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farming""), concatenator = "" "") %>% tokens_compound(phrase(""dairy industry""), concatenator = "" "") %>% tokens_compound(phrase(""indigenous australians""), concatenator = "" "") %>% tokens_compound(phrase(""australia post""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farmer""), concatenator = "" "") x dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign- owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farmers"", ""dairy farms"",""dairy farm"",""dairy farming"",""dairy industry"", ""dairy farmer"",""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) kwicdict <- kwic(x, pattern = dict, window = 4) write.csv (kwicdict, ""D:/Output/TEST.csv"") DF <- read.csv(""D://Output/TEST.csv"",header=T) ## obtaining frequency count of KWIC table 'pattern ' values > x2 <- DF[,8] > > table (x2) x2 auspost business dairy farmers foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2",avoid overlap frequency document frequency count quanteda dummy corpus 4 document  dictionary develop identify frequency word phrase corpus  well number document word phrase occur  world  australians  occur two dictionary key  peep  indig   key content intend mutually exclusive  similarly  australia   oz australia post   foreign  foreign multinat  farm  farmer  dairy farmer  occur two dictionary key  intend count  accord dictionary  expect overall frequency count  extract  pattern  column kwic table  report x2  note word industry appears allocate industry define din indig key  dairy frequency occur key  occur three document  calculate unique row kwic table  doc name  column key  three question  problem  issue could affect output accuracy use approach  well  more parsimonius approach achieve try  would well way extract equivalent tetxstat frequency count datum kwic table  library  quanteda  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    x   x    tokenscompound  phrase    dairy farmer    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farming    concatenator          tokenscompound  phrase    dairy industry    concatenator          tokenscompound  phrase    indigenous australian    concatenator          tokenscompound  phrase    australia post    concatenator          tokenscompound  phrase    dairy farmer    concatenator       x dict   dictionary  list  multinat  c    offshore petroleum company     foreign own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farmer     dairy farm    dairy farm    dairy farming    dairy industry     dairy farmer    dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     kwicdict   kwic  x  pattern  dict  window  4  writecsv  kwicdict     output  testcsv   df   readcsv     output  testcsv   header  t    obtain frequency count kwic table  pattern  value  x2   df   8    table  x2  x2 auspost business dairy farmer foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2,not think kwic   want  tokenslookup   lets specify nest scope mutually exclusive across key  within key  observe difference   note use wildcarde dairy key   library  quanteda    package version  410   unicode version  140   icu version  711   parallel computing  10 10 thread use    see   quantedaio tutorial example  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    dict   dictionary  list  multinat  c    offshore petroleum company     foreign  own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farm      dairy industry     dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     x   token  txt   overlap tokenslookup  x  dict    dfm     document  feature matrix  4 document  10 feature  5500  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 5 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 1   doc3 1 1 0 0 1 0 1 1 1 0   doc4 0 0 1 2 1 1 0 0 0 0  without overlap tokenslookup  x  dict  nestedscope    dictionary     dfm     document  feature matrix  4 document  10 feature  6000  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 3 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 0   doc3 1 1 0 0 1 0 1 1 0 0   doc4 0 0 1 1 1 1 0 0 0 0 create 2024  10  06 reprex v211,avoid overlap frequency document frequency count quanteda dummy corpus 4 document  dictionary develop identify frequency word phrase corpus  well number document word phrase occur  world  australians  occur two dictionary key  peep  indig   key content intend mutually exclusive  similarly  australia   oz australia post   foreign  foreign multinat  farm  farmer  dairy farmer  occur two dictionary key  intend count  accord dictionary  expect overall frequency count  extract  pattern  column kwic table  report x2  note word industry appears allocate industry define din indig key  dairy frequency occur key  occur three document  calculate unique row kwic table  doc name  column key  three question  problem  issue could affect output accuracy use approach  well  more parsimonius approach achieve try  would well way extract equivalent tetxstat frequency count datum kwic table  library  quanteda  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    x   x    tokenscompound  phrase    dairy farmer    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farming    concatenator          tokenscompound  phrase    dairy industry    concatenator          tokenscompound  phrase    indigenous australian    concatenator          tokenscompound  phrase    australia post    concatenator          tokenscompound  phrase    dairy farmer    concatenator       x dict   dictionary  list  multinat  c    offshore petroleum company     foreign own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farmer     dairy farm    dairy farm    dairy farming    dairy industry     dairy farmer    dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     kwicdict   kwic  x  pattern  dict  window  4  writecsv  kwicdict     output  testcsv   df   readcsv     output  testcsv   header  t    obtain frequency count kwic table  pattern  value  x2   df   8    table  x2  x2 auspost business dairy farmer foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2 not think kwic   want  tokenslookup   lets specify nest scope mutually exclusive across key  within key  observe difference   note use wildcarde dairy key   library  quanteda    package version  410   unicode version  140   icu version  711   parallel computing  10 10 thread use    see   quantedaio tutorial example  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    dict   dictionary  list  multinat  c    offshore petroleum company     foreign  own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farm      dairy industry     dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     x   token  txt   overlap tokenslookup  x  dict    dfm     document  feature matrix  4 document  10 feature  5500  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 5 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 1   doc3 1 1 0 0 1 0 1 1 1 0   doc4 0 0 1 2 1 1 0 0 0 0  without overlap tokenslookup  x  dict  nestedscope    dictionary     dfm     document  feature matrix  4 document  10 feature  6000  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 3 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 0   doc3 1 1 0 0 1 0 1 1 0 0   doc4 0 0 1 1 1 1 0 0 0 0 create 2024  10  06 reprex v211,Basic Understanding
Seq2Seq trainer.train() keeps giving indexing error,"I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error: IndexError: Invalid key: 39463 is out of bounds for size 0. The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also. Detailed error message: Traceback (most recent call last): File ""nllbtrain.py"", line 273, in <module> print(trainer.train()) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train return inner_training_loop( File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__ data = self._next_data() File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch data = self.dataset.__getitems__(possibly_batched_index) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__ batch = self.__getitem__(keys) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__ return self._getitem(key) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem pa_subtable = query_table(self._data, key, indices=self._indices) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table _check_valid_index_key(key, size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key _check_valid_index_key(int(max(key)), size=size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key raise IndexError(f""Invalid key: {key} is out of bounds for size {size}"") IndexError: Invalid key: 39463 is out of bounds for size 0 0%| The code of the preprocessing done for the data: def preprocess_function(examples): inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]] targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]] model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length') # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) with tokenizer.as_target_tokenizer(): # labels = tokenizer(targets, max_length=max_target_length, truncation=True) labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length') model_inputs['labels'] = labels['input_ids'] return model_inputs Data after preprocessing: DatasetDict({ train: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 39729 }) val: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2210 }) test: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2214 }) }) The code of model params and training: model_path = 'facebook/nllb-200-1.3B' model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path) tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=""hin_Deva"", tgt_lang=""san_Deva"", max_length = 500) training_args = Seq2SeqTrainingArguments( evaluation_strategy=""epoch"", save_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, output_dir=""./output_dir"", weight_decay=0.01, save_total_limit=1, num_train_epochs=4, predict_with_generate=True, fp16=False, push_to_hub=False, ) trainer = Seq2SeqTrainer( model=model, tokenizer=tokenizer, args=training_args, train_dataset=dataset['train'], data_collator=data_collator, compute_metrics=compute_metrics, ) print(trainer.train()) Any idea why this error is persisting?","['python', 'nlp', 'huggingface-transformers', 'huggingface-trainer']",1,"size 0 indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this ( https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25 ) and this ( https://github.com/huggingface/datasets/issues/6535 ) thread suggests adding remove_unused_columns = False to your training_args might resolve the issue, so you could give that a try.",2024-09-20 08:43:32,2024-09-20 16:38:59,57,https://stackoverflow.com/questions/79005985/seq2seq-trainer-train-keeps-giving-indexing-error,"Seq2Seq trainer.train() keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error: IndexError: Invalid key: 39463 is out of bounds for size 0. The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also. Detailed error message: Traceback (most recent call last): File ""nllbtrain.py"", line 273, in <module> print(trainer.train()) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train return inner_training_loop( File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__ data = self._next_data() File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch data = self.dataset.__getitems__(possibly_batched_index) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__ batch = self.__getitem__(keys) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__ return self._getitem(key) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem pa_subtable = query_table(self._data, key, indices=self._indices) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table _check_valid_index_key(key, size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key _check_valid_index_key(int(max(key)), size=size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key raise IndexError(f""Invalid key: {key} is out of bounds for size {size}"") IndexError: Invalid key: 39463 is out of bounds for size 0 0%| The code of the preprocessing done for the data: def preprocess_function(examples): inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]] targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]] model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length') # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) with tokenizer.as_target_tokenizer(): # labels = tokenizer(targets, max_length=max_target_length, truncation=True) labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length') model_inputs['labels'] = labels['input_ids'] return model_inputs Data after preprocessing: DatasetDict({ train: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 39729 }) val: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2210 }) test: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2214 }) }) The code of model params and training: model_path = 'facebook/nllb-200-1.3B' model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path) tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=""hin_Deva"", tgt_lang=""san_Deva"", max_length = 500) training_args = Seq2SeqTrainingArguments( evaluation_strategy=""epoch"", save_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, output_dir=""./output_dir"", weight_decay=0.01, save_total_limit=1, num_train_epochs=4, predict_with_generate=True, fp16=False, push_to_hub=False, ) trainer = Seq2SeqTrainer( model=model, tokenizer=tokenizer, args=training_args, train_dataset=dataset['train'], data_collator=data_collator, compute_metrics=compute_metrics, ) print(trainer.train()) Any idea why this error is persisting?",seq2seq trainertrain   keep give indexing error try machine translation hindi sanskrit use nllb model  keep get error  indexerror  invalid key  39463 bound size 0  error come training pretraine nllb model  facebook  nllb200  13b input datum 40k hindi sentence  error arises try train sample datum also  detailed error message  traceback  recent call last   file   nllbtrainpy   line 273   module  print  trainertrain    file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1645  train return innertrainingloop  file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1907   innertrainingloop step  input enumerate  epochiterator   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 631    next   datum  selfnextdata   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 675   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   homeconda  envs  dict  lib  python38  site  package  torch  util  datautils  fetchpy   line 49  fetch datum  selfdatasetgetitem    possiblybatchedindex  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2814    getitem   batch  selfgetitem    key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2810    getitem   return selfgetitem  key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2794   getitem pasubtable  querytable  selfdata  key  index  selfindice  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 583  querytable  checkvalidindexkey  key  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 536   checkvalidindexkey  checkvalidindexkey  int  max  key    size  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 526   checkvalidindexkey raise indexerror  f  invalid key   key  bound size  size    indexerror  invalid key  39463 bound size 0 0   code preprocessing do datum  def preprocessfunction  example   input   example    s    f   2  slang    example example  sourcelang   target   f   2  tlang     example    s   example example  targetlang   modelinput  tokenizerbatchencodeplus  input  maxlength  maxinputlength  truncation  true  paddingmaxlength    modelinput  tokenizer  input  maxlength  maxinputlength  truncation  true  tokenizerastargettokenizer     label  tokenizer  target  maxlength  maxtargetlength  truncation  true  label  tokenizerbatchencodeplus  target  maxlength  maxinputlength  truncation  true  paddingmaxlength   modelinput   label    label   inputids   return modelinput data preprocesse  datasetdict   train  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  39729   val  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  2210   test  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrow  2214     code model param training  modelpath   facebook  nllb200  13b  model  automodelforseq2seqlmfrompretraine  pretrainedmodelnameorpath  modelpath  tokenizer  autotokenizerfrompretraine   facebook  nllb200  13b   dolowercase  false  usefast  false  truncation  true  xkeepaccent  true  srclang  hindeva   tgtlang  sandeva   maxlength  500  trainingarg  seq2seqtrainingargument  evaluationstrategy  epoch   savestrategyepoch   learningrate2e5  perdevicetrainbatchsize16  perdeviceevalbatchsize16  outputdir  outputdir   weightdecay001  savetotallimit1  numtrainepochs4  predictwithgenerate  true  fp16  false  pushtohub  false   trainer  seq2seqtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  traindataset  dataset   train    datacollator  datacollator  computemetric  computemetric   print  trainertrain    idea error persist ,size 0 indicate dataset trainer get fine  tuning start empty  look    discusshuggingfaceco  t  indexerror  invalid  key16  be  out  of  bound  for  size01429825     githubcom  huggingface  dataset  issues6535  thread suggest add removeunusedcolumns  false trainingargs might resolve issue  could give try ,seq2seq trainertrain   keep give indexing error try machine translation hindi sanskrit use nllb model  keep get error  indexerror  invalid key  39463 bound size 0  error come training pretraine nllb model  facebook  nllb200  13b input datum 40k hindi sentence  error arises try train sample datum also  detailed error message  traceback  recent call last   file   nllbtrainpy   line 273   module  print  trainertrain    file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1645  train return innertrainingloop  file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1907   innertrainingloop step  input enumerate  epochiterator   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 631    next   datum  selfnextdata   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 675   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   homeconda  envs  dict  lib  python38  site  package  torch  util  datautils  fetchpy   line 49  fetch datum  selfdatasetgetitem    possiblybatchedindex  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2814    getitem   batch  selfgetitem    key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2810    getitem   return selfgetitem  key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2794   getitem pasubtable  querytable  selfdata  key  index  selfindice  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 583  querytable  checkvalidindexkey  key  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 536   checkvalidindexkey  checkvalidindexkey  int  max  key    size  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 526   checkvalidindexkey raise indexerror  f  invalid key   key  bound size  size    indexerror  invalid key  39463 bound size 0 0   code preprocessing do datum  def preprocessfunction  example   input   example    s    f   2  slang    example example  sourcelang   target   f   2  tlang     example    s   example example  targetlang   modelinput  tokenizerbatchencodeplus  input  maxlength  maxinputlength  truncation  true  paddingmaxlength    modelinput  tokenizer  input  maxlength  maxinputlength  truncation  true  tokenizerastargettokenizer     label  tokenizer  target  maxlength  maxtargetlength  truncation  true  label  tokenizerbatchencodeplus  target  maxlength  maxinputlength  truncation  true  paddingmaxlength   modelinput   label    label   inputids   return modelinput data preprocesse  datasetdict   train  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  39729   val  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  2210   test  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrow  2214     code model param training  modelpath   facebook  nllb200  13b  model  automodelforseq2seqlmfrompretraine  pretrainedmodelnameorpath  modelpath  tokenizer  autotokenizerfrompretraine   facebook  nllb200  13b   dolowercase  false  usefast  false  truncation  true  xkeepaccent  true  srclang  hindeva   tgtlang  sandeva   maxlength  500  trainingarg  seq2seqtrainingargument  evaluationstrategy  epoch   savestrategyepoch   learningrate2e5  perdevicetrainbatchsize16  perdeviceevalbatchsize16  outputdir  outputdir   weightdecay001  savetotallimit1  numtrainepochs4  predictwithgenerate  true  fp16  false  pushtohub  false   trainer  seq2seqtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  traindataset  dataset   train    datacollator  datacollator  computemetric  computemetric   print  trainertrain    idea error persist  size 0 indicate dataset trainer get fine  tuning start empty  look    discusshuggingfaceco  t  indexerror  invalid  key16  be  out  of  bound  for  size01429825     githubcom  huggingface  dataset  issues6535  thread suggest add removeunusedcolumns  false trainingargs might resolve issue  could give try ,Implementation Issues
Alternative to device_map = &quot;auto&quot; in Huggingface Pretrained,"I have a model that I was reading from huggingface using the following code: from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model. Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model. So simply something like tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) model.device_map = ""auto""","['machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",1,"I found out that there are actually several methods in accelerate for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model: https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map The second one is used to match your model with the devices: https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model So basically, in your case, you can use the following code: from accelerate import dispatch_model, infer_auto_device_map model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) *** ... new_model = CustomModel(model) ... *** device_map_dict = infer_auto_device_map(new_model) dispatch_model(new_model, device_map_dict) P.S. This code still needs to be tested on fine-tuning.",2024-09-14 12:42:03,2024-09-20 15:09:15,1081,https://stackoverflow.com/questions/78985137/alternative-to-device-map-auto-in-huggingface-pretrained,"Alternative to device_map = &quot;auto&quot; in Huggingface Pretrained I have a model that I was reading from huggingface using the following code: from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model. Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model. So simply something like tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) model.device_map = ""auto""",alternative devicemap   quot  auto  quot  huggingface pretrained model read huggingface use follow code  transformer import autotokenizer  automodelforcausallm tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  read model modification internal layer add layer  start training  fine  tuning get everything model  investigation  find custom layer not distribute multi gpus original model  need something like devicemap  auto  reading model  simply something like tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  modeldevicemap    auto ,find actually several method accelerate  first one use analyze model calculate total amount available memory occupy model    huggingfaceco  docs  accelerate  en  packagereference  bigmodele  accelerateinferautodevicemap second one use match model device    huggingfaceco  docs  accelerate  en  packagereference  bigmodeling  acceleratedispatchmodel basically  case  use follow code  accelerate import dispatchmodel  inferautodevicemap model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true      newmodel  custommodel  model      devicemapdict  inferautodevicemap  newmodel  dispatchmodel  newmodel  devicemapdict  ps  code still need test fine  tuning ,alternative devicemap   quot  auto  quot  huggingface pretrained model read huggingface use follow code  transformer import autotokenizer  automodelforcausallm tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  read model modification internal layer add layer  start training  fine  tuning get everything model  investigation  find custom layer not distribute multi gpus original model  need something like devicemap  auto  reading model  simply something like tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  modeldevicemap    auto  find actually several method accelerate  first one use analyze model calculate total amount available memory occupy model    huggingfaceco  docs  accelerate  en  packagereference  bigmodele  accelerateinferautodevicemap second one use match model device    huggingfaceco  docs  accelerate  en  packagereference  bigmodeling  acceleratedispatchmodel basically  case  use follow code  accelerate import dispatchmodel  inferautodevicemap model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true      newmodel  custommodel  model      devicemapdict  inferautodevicemap  newmodel  dispatchmodel  newmodel  devicemapdict  ps  code still need test fine  tuning ,Task-Specific Queries
How are the weights of the Mistral models reinitialized in Huggingface?,"From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model? and https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4 there's different suggestions to reinitialize the model. When I tried this, it seems to work. from transformers import AutoModelForCausalLM, AutoConfig m = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.3"", token=""hf_*****"") c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) print(m2.model.layers[0].mlp.down_proj.state_dict()) print(m.model.layers[0].mlp.down_proj.state_dict()) [out]: OrderedDict([('weight', tensor([[ 0.0315, -0.0025, -0.0015, ..., -0.0022, 0.0168, -0.0296], [-0.0013, -0.0190, -0.0103, ..., 0.0037, 0.0021, -0.0374], [-0.0378, -0.0230, 0.0031, ..., -0.0035, 0.0099, -0.0027], ..., [-0.0029, 0.0042, -0.0041, ..., -0.0003, 0.0396, -0.0012], [-0.0487, -0.0050, -0.0068, ..., 0.0170, 0.0135, -0.0006], [ 0.0103, 0.0424, 0.0019, ..., 0.0155, 0.0254, 0.0061]]))]) OrderedDict([('weight', tensor([[-0.0027, -0.0004, -0.0007, ..., -0.0025, 0.0032, -0.0014], [ 0.0012, -0.0047, 0.0026, ..., -0.0017, 0.0015, -0.0044], [ 0.0056, -0.0084, 0.0027, ..., 0.0026, -0.0053, 0.0038], ..., [ 0.0052, 0.0017, -0.0019, ..., -0.0013, 0.0052, -0.0017], [-0.0032, 0.0029, -0.0014, ..., 0.0003, 0.0006, 0.0023], [-0.0023, -0.0045, -0.0013, ..., -0.0036, 0.0002, -0.0008]]))]) How are the layers re-initialized through the from_config function? Is it using Xaiver/He initialization or just random initialization?","['nlp', 'huggingface-transformers', 'large-language-model', 'mistral-7b']",2,"MistralConfig has a default parameter initializer_range which is set to 0.02 and described as The standard deviation of the truncated_normal_initializer for initializing all weight matrices , so one can assume they use a truncated normal distribution with a standard deviation of 0.02. If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me: import numpy as np from matplotlib import pyplot as plt from scipy.stats import truncnorm from transformers import AutoModelForCausalLM, AutoConfig # histogram of actual weights distribution c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) weights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel() plt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights') # what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like lower = -0.1 upper = 0.1 mean = 0 std = 0.02 a, b = (lower - mean) / std, (upper - mean) / std x = np.linspace(lower, upper, 1000) plt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected') plt.legend() plt.show()",2024-09-09 19:25:52,2024-09-10 13:18:59,185,https://stackoverflow.com/questions/78966943/how-are-the-weights-of-the-mistral-models-reinitialized-in-huggingface,"How are the weights of the Mistral models reinitialized in Huggingface? From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model? and https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4 there's different suggestions to reinitialize the model. When I tried this, it seems to work. from transformers import AutoModelForCausalLM, AutoConfig m = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.3"", token=""hf_*****"") c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) print(m2.model.layers[0].mlp.down_proj.state_dict()) print(m.model.layers[0].mlp.down_proj.state_dict()) [out]: OrderedDict([('weight', tensor([[ 0.0315, -0.0025, -0.0015, ..., -0.0022, 0.0168, -0.0296], [-0.0013, -0.0190, -0.0103, ..., 0.0037, 0.0021, -0.0374], [-0.0378, -0.0230, 0.0031, ..., -0.0035, 0.0099, -0.0027], ..., [-0.0029, 0.0042, -0.0041, ..., -0.0003, 0.0396, -0.0012], [-0.0487, -0.0050, -0.0068, ..., 0.0170, 0.0135, -0.0006], [ 0.0103, 0.0424, 0.0019, ..., 0.0155, 0.0254, 0.0061]]))]) OrderedDict([('weight', tensor([[-0.0027, -0.0004, -0.0007, ..., -0.0025, 0.0032, -0.0014], [ 0.0012, -0.0047, 0.0026, ..., -0.0017, 0.0015, -0.0044], [ 0.0056, -0.0084, 0.0027, ..., 0.0026, -0.0053, 0.0038], ..., [ 0.0052, 0.0017, -0.0019, ..., -0.0013, 0.0052, -0.0017], [-0.0032, 0.0029, -0.0014, ..., 0.0003, 0.0006, 0.0023], [-0.0023, -0.0045, -0.0013, ..., -0.0036, 0.0002, -0.0008]]))]) How are the layers re-initialized through the from_config function? Is it using Xaiver/He initialization or just random initialization?",weight mistral model reinitialize huggingface  one reinitialize weight hugging face llama v2 model official way original model    discusshuggingfaceco  t  how  do  one  reinitialize  the  weight  of  a  hug  face  llama  v2  model  the  official  way  as  the  original  model625474 s different suggestion reinitialize model  try  seem work  transformer import automodelforcausallm  autoconfig  automodelforcausallmfrompretraine    mistralai  mistral7b  v03   token  hf         c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  print  m2modellayer  0  mlpdownprojstatedict    print  mmodellayer  0  mlpdownprojstatedict       ordereddict     weight   tensor    00315  00025  00015    00022  00168  00296    00013  00190  00103    00037  00021  00374    00378  00230  00031    00035  00099  00027      00029  00042  00041    00003  00396  00012    00487  00050  00068    00170  00135  00006    00103  00424  00019    00155  00254  00061       ordereddict     weight   tensor    00027  00004  00007    00025  00032  00014    00012  00047  00026    00017  00015  00044    00056  00084  00027    00026  00053  00038      00052  00017  00019    00013  00052  00017    00032  00029  00014    00003  00006  00023    00023  00045  00013    00036  00002  00008       layer re  initialized fromconfig function  use xaiver  he initialization random initialization ,mistralconfig default parameter initializerrange set 002 describe standard deviation truncatednormalinitializer initialize weight matrix  one assume use truncate normal distribution standard deviation 002  plot actual model weight distribution truncate normal distribution standard deviation 002 look like  seem like fit  import numpy np matplotlib import pyplot plt scipystat import truncnorm transformer import automodelforcausallm  autoconfig  histogram actual weight distribution c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  weight  m2modellayer  0  mlpdownprojstatedict     weight   ravel   plthist  weight  bin  nplinspace  01  01  100   histtypestep   density  true  labelmodel weight    truncate normal distribution mean 0 std 002 suppose look like low  01 upper  01 mean  0 std  002  b   lower  mean   std   upper  mean   std x  nplinspace  low  upper  1000  pltplot  x  truncnormpdf  x   b  loc  mean  scale  std   labelexpecte   pltlegend   pltshow  ,weight mistral model reinitialize huggingface  one reinitialize weight hugging face llama v2 model official way original model    discusshuggingfaceco  t  how  do  one  reinitialize  the  weight  of  a  hug  face  llama  v2  model  the  official  way  as  the  original  model625474 s different suggestion reinitialize model  try  seem work  transformer import automodelforcausallm  autoconfig  automodelforcausallmfrompretraine    mistralai  mistral7b  v03   token  hf         c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  print  m2modellayer  0  mlpdownprojstatedict    print  mmodellayer  0  mlpdownprojstatedict       ordereddict     weight   tensor    00315  00025  00015    00022  00168  00296    00013  00190  00103    00037  00021  00374    00378  00230  00031    00035  00099  00027      00029  00042  00041    00003  00396  00012    00487  00050  00068    00170  00135  00006    00103  00424  00019    00155  00254  00061       ordereddict     weight   tensor    00027  00004  00007    00025  00032  00014    00012  00047  00026    00017  00015  00044    00056  00084  00027    00026  00053  00038      00052  00017  00019    00013  00052  00017    00032  00029  00014    00003  00006  00023    00023  00045  00013    00036  00002  00008       layer re  initialized fromconfig function  use xaiver  he initialization random initialization  mistralconfig default parameter initializerrange set 002 describe standard deviation truncatednormalinitializer initialize weight matrix  one assume use truncate normal distribution standard deviation 002  plot actual model weight distribution truncate normal distribution standard deviation 002 look like  seem like fit  import numpy np matplotlib import pyplot plt scipystat import truncnorm transformer import automodelforcausallm  autoconfig  histogram actual weight distribution c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  weight  m2modellayer  0  mlpdownprojstatedict     weight   ravel   plthist  weight  bin  nplinspace  01  01  100   histtypestep   density  true  labelmodel weight    truncate normal distribution mean 0 std 002 suppose look like low  01 upper  01 mean  0 std  002  b   lower  mean   std   upper  mean   std x  nplinspace  low  upper  1000  pltplot  x  truncnormpdf  x   b  loc  mean  scale  std   labelexpecte   pltlegend   pltshow  ,Library/Tool-Based Queries
Break after first PER sequence found with Spacy,"I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all ""PER"" tags, but I want to reduce the overhead and get only the first contiguous sequence of ""PER"" entities. Here’s the example output I get: Detected Names in Text: ['garcía', 'lópez'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama'] Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias'] But I want the result to be: Detected Names in Text: ['garcía'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez'] Detected Names in Text: ['pedro sánchez'] Here is the code I am currently using: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."" ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}] ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc, start_idx=0): per_entities = [] misc_count = 0 for ent in doc[start_idx:].ents: if ent.label_ == ""PER"": per_entities.append(ent.text) elif ent.label_ == ""MISC"" and misc_count < 1: misc_count += 1 per_entities.append(ent.text) else: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position title_start = None title_end = None for match_id, start, end in matches: title_start = start title_end = end break # If a title was found, start searching for PER entities from that position if title_start is not None: names = find_per_sequence(doc, start_idx=title_end) else: names = find_per_sequence(doc) # Output the detected names for each text print(f""Detected Names in Text: {names}"") What I'm looking for: I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of ""PER"" entities in the text, ignoring any subsequent ""PER"" entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?","['python', 'nlp', 'spacy']",1,"The issues is that doc[start_idx:].ents is only the named entities in that slice of the doc. Thus, you will never process ""habló"" for the first entry, you will just go straight from ""García"" to ""López"". To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the .ents part. Then you just wait until you see the first token with ent_type_ PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."", ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}], ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc: spacy.tokens.Doc, start_idx: int): per_entities = [] misc_count = 0 per_started = False for token in doc[start_idx:]: if token.ent_type_ == ""PER"": per_entities.append(token.text) per_started = True elif token.ent_type_ == ""MISC"" and misc_count < 1 and per_started: misc_count += 1 per_entities.append(token.text) elif per_started: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position _, _, title_end = matches[0] if matches else (None, None, None) names = find_per_sequence(doc, title_end if title_end else 0) # Output the detected names for each text print(f""Detected Names in Text: {names}"")",2024-09-06 13:14:32,2024-09-06 15:03:25,41,https://stackoverflow.com/questions/78957322/break-after-first-per-sequence-found-with-spacy,"Break after first PER sequence found with Spacy I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all ""PER"" tags, but I want to reduce the overhead and get only the first contiguous sequence of ""PER"" entities. Here’s the example output I get: Detected Names in Text: ['garcía', 'lópez'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama'] Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias'] But I want the result to be: Detected Names in Text: ['garcía'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez'] Detected Names in Text: ['pedro sánchez'] Here is the code I am currently using: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."" ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}] ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc, start_idx=0): per_entities = [] misc_count = 0 for ent in doc[start_idx:].ents: if ent.label_ == ""PER"": per_entities.append(ent.text) elif ent.label_ == ""MISC"" and misc_count < 1: misc_count += 1 per_entities.append(ent.text) else: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position title_start = None title_end = None for match_id, start, end in matches: title_start = start title_end = end break # If a title was found, start searching for PER entities from that position if title_start is not None: names = find_per_sequence(doc, start_idx=title_end) else: names = find_per_sequence(doc) # Output the detected names for each text print(f""Detected Names in Text: {names}"") What I'm looking for: I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of ""PER"" entities in the text, ignoring any subsequent ""PER"" entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?",break first per sequence find spacy try extract first speaker s name list text use spacy  currently  function return   per  tag  want reduce overhead get first contiguous sequence   per  entity   example output get  detected names text    garca    lpez   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez    jos guadarrama   detect names text    pedro snchez    jos manuel albares    pablo iglesia   want result  detected names text    garca   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez   detect names text    pedro snchez   code currently use  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias    text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra     matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  startidx0   perentitie    misccount  0 ent doc  startidx   ents  entlabel      per   perentitiesappend  enttext  elif entlabel      misc  misccount  1  misccount   1 perentitiesappend  enttext  else  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position titlestart  none titleend  none matchid  start  end match  titlestart  start titleend  end break  title find  start search per entity position titlestart none  name  findpersequence  doc  startidx  titleend  else  name  findpersequence  doc   output detect name text print  f  detect names text   name     m look  want modify findpersequence function return first contiguous sequence   per  entity text  ignore subsequent   per  entity encounter different type entity  provide function return multiple name partial name  need way ensure first name sequence include  achieve ,issue doc  startidx   ents name entity slice doc  thus  never process   habl  first entry  go straight   garca    lpez   actually iterate token see per sequence end  leave ent part  wait see first token enttype  per start append  break one condition meet  end refactore code little debug  s edit version program produce desire output  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias      text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra      matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  spacytoken  doc  startidx  int   perentitie    misccount  0 perstarte  false token doc  startidx    tokenenttype      per   perentitiesappend  tokentext  perstarte  true elif tokenenttype      misc  misccount  1 perstarte  misccount   1 perentitiesappend  tokentext  elif perstarte  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position     titleend  match  0  match else  none  none  none  name  findpersequence  doc  titleend titleend else 0   output detect name text print  f  detect names text   name   ,break first per sequence find spacy try extract first speaker s name list text use spacy  currently  function return   per  tag  want reduce overhead get first contiguous sequence   per  entity   example output get  detected names text    garca    lpez   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez    jos guadarrama   detect names text    pedro snchez    jos manuel albares    pablo iglesia   want result  detected names text    garca   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez   detect names text    pedro snchez   code currently use  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias    text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra     matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  startidx0   perentitie    misccount  0 ent doc  startidx   ents  entlabel      per   perentitiesappend  enttext  elif entlabel      misc  misccount  1  misccount   1 perentitiesappend  enttext  else  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position titlestart  none titleend  none matchid  start  end match  titlestart  start titleend  end break  title find  start search per entity position titlestart none  name  findpersequence  doc  startidx  titleend  else  name  findpersequence  doc   output detect name text print  f  detect names text   name     m look  want modify findpersequence function return first contiguous sequence   per  entity text  ignore subsequent   per  entity encounter different type entity  provide function return multiple name partial name  need way ensure first name sequence include  achieve  issue doc  startidx   ents name entity slice doc  thus  never process   habl  first entry  go straight   garca    lpez   actually iterate token see per sequence end  leave ent part  wait see first token enttype  per start append  break one condition meet  end refactore code little debug  s edit version program produce desire output  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias      text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra      matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  spacytoken  doc  startidx  int   perentitie    misccount  0 perstarte  false token doc  startidx    tokenenttype      per   perentitiesappend  tokentext  perstarte  true elif tokenenttype      misc  misccount  1 perstarte  misccount   1 perentitiesappend  tokentext  elif perstarte  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position     titleend  match  0  match else  none  none  none  name  findpersequence  doc  titleend titleend else 0   output detect name text print  f  detect names text   name   ,Implementation Issues
Trainer huggingface - RuntimeError: cannot pin &#39;torch.cuda.FloatTensor&#39; only dense CPU tensors can be pinned,"I recently got the following error: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned when doing LoRA on a small LLM. I saw on a discord someone saying: The issue likely stems from the fact that you are manually placing your inputs on the GPU (with to(model.device)), but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally. I can't find anything of the sort written in the Trainer documentation of huggingface https://huggingface.co/docs/transformers/en/main_classes/trainer . Is it true? If not, how can I get rid of that error? MRE: import torch from torch.utils.data import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from transformers import TrainingArguments from transformers import Trainer from peft import LoraConfig, get_peft_model model_name = ""croissantllm/CroissantLLMBase"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"") texts = [ ""The first sentence for fine-tuning. </s>"", ""The second sentence for fine-tuning. </s>"" ] inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts] lora_config = LoraConfig( r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""q_proj"", ""v_proj""], ) model = get_peft_model(model, lora_config) class CustomDataset(Dataset): def __init__(self, input_list): self.input_list = input_list def __len__(self): return len(self.input_list) def __getitem__(self, idx): input_ids = self.input_list[idx]['input_ids'].squeeze() labels = input_ids.clone() return {""input_ids"": input_ids, ""labels"": labels} train_dataset = CustomDataset(inputs) training_args = TrainingArguments( output_dir=""./lora_croissantllm"", per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, ) trainer.train() The issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).","['nlp', 'huggingface-transformers']",1,"Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting dataloader_pin_memory to False for TrainingArguments training_args = TrainingArguments( output_dir=""./lora_croissantllm"", dataloader_pin_memory=False, per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, )",2024-09-04 16:09:18,2024-10-22 01:40:51,1512,https://stackoverflow.com/questions/78949607/trainer-huggingface-runtimeerror-cannot-pin-torch-cuda-floattensor-only-den,"Trainer huggingface - RuntimeError: cannot pin &#39;torch.cuda.FloatTensor&#39; only dense CPU tensors can be pinned I recently got the following error: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned when doing LoRA on a small LLM. I saw on a discord someone saying: The issue likely stems from the fact that you are manually placing your inputs on the GPU (with to(model.device)), but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally. I can't find anything of the sort written in the Trainer documentation of huggingface https://huggingface.co/docs/transformers/en/main_classes/trainer . Is it true? If not, how can I get rid of that error? MRE: import torch from torch.utils.data import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from transformers import TrainingArguments from transformers import Trainer from peft import LoraConfig, get_peft_model model_name = ""croissantllm/CroissantLLMBase"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"") texts = [ ""The first sentence for fine-tuning. </s>"", ""The second sentence for fine-tuning. </s>"" ] inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts] lora_config = LoraConfig( r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""q_proj"", ""v_proj""], ) model = get_peft_model(model, lora_config) class CustomDataset(Dataset): def __init__(self, input_list): self.input_list = input_list def __len__(self): return len(self.input_list) def __getitem__(self, idx): input_ids = self.input_list[idx]['input_ids'].squeeze() labels = input_ids.clone() return {""input_ids"": input_ids, ""labels"": labels} train_dataset = CustomDataset(inputs) training_args = TrainingArguments( output_dir=""./lora_croissantllm"", per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, ) trainer.train() The issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).",trainer huggingface  runtimeerror  pin   39  torchcuda  floattensor   39  dense cpu tensor pin recently get follow error  runtimeerror  pin  torchcuda  floattensor  dense cpu tensor pin lora small llm  see discord someone say  issue likely stem fact manually place input gpu   modeldevice    trainer expect datum cpu handle transfer gpu internally  can not find anything sort write trainer documentation huggingface   huggingfaceco  doc  transformer  en  mainclasse  trainer  true   get rid error  mre  import torch torchutilsdata import dataset transformer import automodelforcausallm  autotokenizer transformer import trainingarguments transformer import trainer peft import loraconfig  getpeftmodel modelname    croissantllm  croissantllmbase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforcausallmfrompretraine  modelname  torchdtype  torchfloat16  devicemap  auto   text     first sentence fine  tuning   s      second sentence fine  tuning   s    input   tokenizer  text  returntensors  pt   to  modeldevice  text text  loraconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    qproj     vproj     model  getpeftmodel  model  loraconfig  class customdataset  dataset   def   init    self  inputlist   selfinputlist  inputlist def   len    self   return len  selfinputlist  def   getitem    self  idx   inputids  selfinputlist  idx    inputids   squeeze   label  inputidsclone   return    inputids   inputid    label   label  traindataset  customdataset  input  trainingargs  trainingarguments  outputdir  loracroissantllm   perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10   trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset   trainertrain   issue fairly easy reproduce directly colab  run  pip install  upgrade torch transformer peft first cell  ,since pin memory available cpu gpu  run gpu colab  disable set dataloaderpinmemory false trainingarguments trainingargs  trainingarguments  outputdir  loracroissantllm   dataloaderpinmemory  false  perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10  ,trainer huggingface  runtimeerror  pin   39  torchcuda  floattensor   39  dense cpu tensor pin recently get follow error  runtimeerror  pin  torchcuda  floattensor  dense cpu tensor pin lora small llm  see discord someone say  issue likely stem fact manually place input gpu   modeldevice    trainer expect datum cpu handle transfer gpu internally  can not find anything sort write trainer documentation huggingface   huggingfaceco  doc  transformer  en  mainclasse  trainer  true   get rid error  mre  import torch torchutilsdata import dataset transformer import automodelforcausallm  autotokenizer transformer import trainingarguments transformer import trainer peft import loraconfig  getpeftmodel modelname    croissantllm  croissantllmbase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforcausallmfrompretraine  modelname  torchdtype  torchfloat16  devicemap  auto   text     first sentence fine  tuning   s      second sentence fine  tuning   s    input   tokenizer  text  returntensors  pt   to  modeldevice  text text  loraconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    qproj     vproj     model  getpeftmodel  model  loraconfig  class customdataset  dataset   def   init    self  inputlist   selfinputlist  inputlist def   len    self   return len  selfinputlist  def   getitem    self  idx   inputids  selfinputlist  idx    inputids   squeeze   label  inputidsclone   return    inputids   inputid    label   label  traindataset  customdataset  input  trainingargs  trainingarguments  outputdir  loracroissantllm   perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10   trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset   trainertrain   issue fairly easy reproduce directly colab  run  pip install  upgrade torch transformer peft first cell   since pin memory available cpu gpu  run gpu colab  disable set dataloaderpinmemory false trainingarguments trainingargs  trainingarguments  outputdir  loracroissantllm   dataloaderpinmemory  false  perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10  ,Implementation Issues
Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot;,"I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem? Here is a minimal example: import os from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast import torch from torch.cuda.amp import GradScaler, autocast model_name = ""facebook/opt-1.3b"" cache_dir = './models' os.environ[""CUDA_VISIBLE_DEVICES""] = ""7"" quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.float16 ) pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config) tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name, cache_dir=cache_dir) optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4) scaler = GradScaler() input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0) labels = torch.LongTensor([[1, 2, 3, 4]]).to(0) with torch.autocast(device_type='cuda'): out = pretrained_model(input_ids=input_ids, labels=labels) loss = out.loss scaler.scale(out.loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() print(f'End') At the line scaler.step(optimizer) , an error occurs: Exception has occurred: ValueError: Attempting to unscale FP16 gradients.","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'fine-tuning']",1,"You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32. You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training. What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details here",2024-09-03 08:38:23,2024-09-03 17:22:57,517,https://stackoverflow.com/questions/78943401/fine-tuning-a-pretrained-model-with-quantization-and-amp-scaler-error-attempti,"Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot; I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem? Here is a minimal example: import os from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast import torch from torch.cuda.amp import GradScaler, autocast model_name = ""facebook/opt-1.3b"" cache_dir = './models' os.environ[""CUDA_VISIBLE_DEVICES""] = ""7"" quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.float16 ) pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config) tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name, cache_dir=cache_dir) optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4) scaler = GradScaler() input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0) labels = torch.LongTensor([[1, 2, 3, 4]]).to(0) with torch.autocast(device_type='cuda'): out = pretrained_model(input_ids=input_ids, labels=labels) loss = out.loss scaler.scale(out.loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() print(f'End') At the line scaler.step(optimizer) , an error occurs: Exception has occurred: ValueError: Attempting to unscale FP16 gradients.",fine  tuning pretrained model quantization amp  scaler error  quot  attempt unscale fp16 gradients  quot  try fine  tune pretraine model limited vram  achieve  use quantization automatic mixed precision  amp   however  encounter issue can not seem resolve  could please help identify problem  minimal example  import os transformer import bitsandbytesconfig  optforcausallm  gpt2tokenizerfast import torch torchcudaamp import gradscaler  autocast modelname    facebook  opt13b  cachedir   model  osenviron    cudavisibledevice      7  quantizationconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchfloat16  pretrainedmodel  optforcausallm  optforcausallmfrompretrained  modelname  cachedir  cachedir  quantizationconfig  quantizationconfig  tokenizer  gpt2tokenizerfast  gpt2tokenizerfastfrompretrained  modelname  cachedir  cachedir  optimizer  torchoptim  adamw  pretrainedmodelparameters    lr1e4  scaler  gradscaler   inputids  torch  longtensor    0  1  2  3    to  0  label  torch  longtensor    1  2  3  4    to  0  torchautocast  devicetypecuda     pretrainedmodel  inputid  inputid  label  label  loss  outloss scalerscale  outloss  backward   scalerstep  optimizer  scalerupdate   optimizerzerograd   print  fend   line scalerstep  optimizer   error occur  exception occur  valueerror  attempt unscale fp16 gradient ,can not fine  tune fp16  uint8 model amp  amp use fp32 parameter  param autocast fp16 forward pass  amp expect master set parameter fp32  also not fine  tune quantize model first place  quantization cause sort numerical issue instability training  suppose keep quantize model static train adapter top quantize model  find detail,fine  tuning pretrained model quantization amp  scaler error  quot  attempt unscale fp16 gradients  quot  try fine  tune pretraine model limited vram  achieve  use quantization automatic mixed precision  amp   however  encounter issue can not seem resolve  could please help identify problem  minimal example  import os transformer import bitsandbytesconfig  optforcausallm  gpt2tokenizerfast import torch torchcudaamp import gradscaler  autocast modelname    facebook  opt13b  cachedir   model  osenviron    cudavisibledevice      7  quantizationconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchfloat16  pretrainedmodel  optforcausallm  optforcausallmfrompretrained  modelname  cachedir  cachedir  quantizationconfig  quantizationconfig  tokenizer  gpt2tokenizerfast  gpt2tokenizerfastfrompretrained  modelname  cachedir  cachedir  optimizer  torchoptim  adamw  pretrainedmodelparameters    lr1e4  scaler  gradscaler   inputids  torch  longtensor    0  1  2  3    to  0  label  torch  longtensor    1  2  3  4    to  0  torchautocast  devicetypecuda     pretrainedmodel  inputid  inputid  label  label  loss  outloss scalerscale  outloss  backward   scalerstep  optimizer  scalerupdate   optimizerzerograd   print  fend   line scalerstep  optimizer   error occur  exception occur  valueerror  attempt unscale fp16 gradient  can not fine  tune fp16  uint8 model amp  amp use fp32 parameter  param autocast fp16 forward pass  amp expect master set parameter fp32  also not fine  tune quantize model first place  quantization cause sort numerical issue instability training  suppose keep quantize model static train adapter top quantize model  find detail,Implementation Issues
Keep training pytorch model on new data,"I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps: Load and process the text. Use a TF-IDF Vectorizer. Build the neural network and save the TF-IDF Vectorizer and model to predict new data. However, every day I need to classify new comments and correct any wrong classifications. Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct). Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen. So the mains questions are: How could i check if the propossed way to solve this problem work as i expect? What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer? Here its the full training process: import torch from torch import nn from torch.utils.data import Dataset, DataLoader, random_split from sklearn.preprocessing import LabelEncoder import polars as pl from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer import joblib set1 = ( pl .read_csv( ""set1.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) ) # since the dateset its unbalanced, im going to force to have more balance fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df]) """""" The text its already clean, so im going to change the labels to numeric and then split it on train, test ,val """""" label_mapping = { ""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3 } train_mapped = ( train_df .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) train_set, pre_Test = train_test_split(train_mapped, test_size=0.4, random_state=42, stratify=train_mapped[""label""]) test_set, val_set = train_test_split(pre_Test, test_size=0.5, random_state=42, stratify=pre_Test[""label""]) # Vectorize text data using TF-IDF vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2)) X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray() X_val_tfidf = vectorizer.transform(val_set['text']).toarray() X_test_tfidf = vectorizer.transform(test_set['text']).toarray() y_train = train_set['label'] y_val = val_set['label'] y_test = test_set['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label train_dataset = TextDataset(X_train_tfidf, y_train) val_dataset = TextDataset(X_val_tfidf, y_val) test_dataset = TextDataset(X_test_tfidf, y_test) batch_size = 32 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size) test_loader = DataLoader(test_dataset, batch_size=batch_size) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x input_dim = X_train_tfidf.shape[1] model = TextClassificationModel(input_dim, 4) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) # Training loop num_epochs = 17 best_val_acc = 0.0 best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() # Validation model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in val_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() val_acc = correct / total if val_acc > best_val_acc: best_val_acc = val_acc torch.save(model.state_dict(), best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}') # Load the best model model.load_state_dict(torch.load(best_model_path)) # Load the best model model.load_state_dict(torch.load(best_model_path)) # Test the model model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in test_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() test_acc = correct / total print(f'Test Acc: {test_acc:.3f}') # Save the TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" joblib.dump(vectorizer, vectorizer_path) # Save the PyTorch model model_path = ""text_classification_model.pth"" torch.save(model.state_dict(), model_path) Proposed code: import torch import joblib import polars as pl from sklearn.model_selection import train_test_split from torch import nn from torch.utils.data import Dataset, DataLoader # Load the saved TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" vectorizer = joblib.load(vectorizer_path) input_dim = len(vectorizer.get_feature_names_out()) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x # Load the saved PyTorch model model_path = ""text_classification_model.pth"" model = TextClassificationModel(input_dim, 4) model.load_state_dict(torch.load(model_path)) # Map labels to numeric values label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} sentiments = [""fear"",""joy"",""sadness"",""anger""] new_data = ( pl .read_csv( ""set2.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) .filter(pl.col(""label"").is_in(sentiments)) .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) # Vectorize the new text data using the loaded TF-IDF vectorizer X_new = vectorizer.transform(new_data['text']).toarray() y_new = new_data['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label batch_size = 10 # Create DataLoader for the new training data new_train_dataset = TextDataset(X_new, y_new) new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) num_epochs = 5 new_best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in new_train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() torch.save(model.state_dict(), new_best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # Save the PyTorch model new_best_model_path = ""new_moedl.pth"" torch.save(model.state_dict(), new_best_model_path) The dataset can be found here","['python', 'scikit-learn', 'pytorch', 'nlp', 'python-polars']",2,"use pre-trained word embeddings like BertForSequenceClassification. These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words. Model Training with BERT import torch from torch import nn, optim from torch.utils.data import DataLoader, Dataset from transformers import BertTokenizer, BertModel, BertForSequenceClassification from transformers import Trainer, TrainingArguments from sklearn.model_selection import train_test_split import polars as pl # Load and prepare data set1 = pl.read_csv(""set1.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""]) # Balance dataset fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df, joy_df, sadness_df, anger_df]) label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} train_df = train_df.with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)) # Split dataset train_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[""label""]) test_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[""label""]) # Dataset class class TextDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_length=128): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_length = max_length def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt' ) return { 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.long) } # Initialize tokenizer and datasets tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') train_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer) val_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer) test_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer) # Initialize BERT model for classification model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4) # Training arguments training_args = TrainingArguments( output_dir='./results', num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, evaluation_strategy='epoch', save_strategy='epoch', logging_dir='./logs', learning_rate=2e-5, load_best_model_at_end=True ) # Define Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset ) # Train model trainer.train() # Evaluate model results = trainer.evaluate(test_dataset) print(f""Test Accuracy: {results['eval_accuracy']:.4f}"") # Save the model and tokenizer model.save_pretrained(""saved_model"") tokenizer.save_pretrained(""saved_tokenizer"") Incremental training with least effort # Load the saved model and tokenizer model = BertForSequenceClassification.from_pretrained(""saved_model"") tokenizer = BertTokenizer.from_pretrained(""saved_tokenizer"") # Load new data new_data = ( pl.read_csv(""set2.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""]) .filter(pl.col(""label"").is_in([""fear"", ""joy"", ""sadness"", ""anger""])) .with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)) ) # Create new dataset new_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer) # Update training arguments for incremental training new_training_args = TrainingArguments( output_dir='./results_incremental', num_train_epochs=2, # Fewer epochs since it's incremental per_device_train_batch_size=16, evaluation_strategy='epoch', logging_dir='./logs_incremental', learning_rate=2e-5, load_best_model_at_end=True ) # Define new trainer new_trainer = Trainer( model=model, args=new_training_args, train_dataset=new_dataset, eval_dataset=val_dataset # Validate on previous validation set ) # Train on new data new_trainer.train() # Evaluate after retraining new_results = new_trainer.evaluate(test_dataset) print(f""Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}"") # Save the updated model model.save_pretrained(""saved_model_incremental"")",2024-08-30 17:47:59,2024-08-31 02:50:05,283,https://stackoverflow.com/questions/78933232/keep-training-pytorch-model-on-new-data,"Keep training pytorch model on new data I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps: Load and process the text. Use a TF-IDF Vectorizer. Build the neural network and save the TF-IDF Vectorizer and model to predict new data. However, every day I need to classify new comments and correct any wrong classifications. Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct). Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen. So the mains questions are: How could i check if the propossed way to solve this problem work as i expect? What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer? Here its the full training process: import torch from torch import nn from torch.utils.data import Dataset, DataLoader, random_split from sklearn.preprocessing import LabelEncoder import polars as pl from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer import joblib set1 = ( pl .read_csv( ""set1.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) ) # since the dateset its unbalanced, im going to force to have more balance fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df]) """""" The text its already clean, so im going to change the labels to numeric and then split it on train, test ,val """""" label_mapping = { ""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3 } train_mapped = ( train_df .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) train_set, pre_Test = train_test_split(train_mapped, test_size=0.4, random_state=42, stratify=train_mapped[""label""]) test_set, val_set = train_test_split(pre_Test, test_size=0.5, random_state=42, stratify=pre_Test[""label""]) # Vectorize text data using TF-IDF vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2)) X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray() X_val_tfidf = vectorizer.transform(val_set['text']).toarray() X_test_tfidf = vectorizer.transform(test_set['text']).toarray() y_train = train_set['label'] y_val = val_set['label'] y_test = test_set['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label train_dataset = TextDataset(X_train_tfidf, y_train) val_dataset = TextDataset(X_val_tfidf, y_val) test_dataset = TextDataset(X_test_tfidf, y_test) batch_size = 32 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size) test_loader = DataLoader(test_dataset, batch_size=batch_size) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x input_dim = X_train_tfidf.shape[1] model = TextClassificationModel(input_dim, 4) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) # Training loop num_epochs = 17 best_val_acc = 0.0 best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() # Validation model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in val_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() val_acc = correct / total if val_acc > best_val_acc: best_val_acc = val_acc torch.save(model.state_dict(), best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}') # Load the best model model.load_state_dict(torch.load(best_model_path)) # Load the best model model.load_state_dict(torch.load(best_model_path)) # Test the model model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in test_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() test_acc = correct / total print(f'Test Acc: {test_acc:.3f}') # Save the TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" joblib.dump(vectorizer, vectorizer_path) # Save the PyTorch model model_path = ""text_classification_model.pth"" torch.save(model.state_dict(), model_path) Proposed code: import torch import joblib import polars as pl from sklearn.model_selection import train_test_split from torch import nn from torch.utils.data import Dataset, DataLoader # Load the saved TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" vectorizer = joblib.load(vectorizer_path) input_dim = len(vectorizer.get_feature_names_out()) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x # Load the saved PyTorch model model_path = ""text_classification_model.pth"" model = TextClassificationModel(input_dim, 4) model.load_state_dict(torch.load(model_path)) # Map labels to numeric values label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} sentiments = [""fear"",""joy"",""sadness"",""anger""] new_data = ( pl .read_csv( ""set2.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) .filter(pl.col(""label"").is_in(sentiments)) .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) # Vectorize the new text data using the loaded TF-IDF vectorizer X_new = vectorizer.transform(new_data['text']).toarray() y_new = new_data['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label batch_size = 10 # Create DataLoader for the new training data new_train_dataset = TextDataset(X_new, y_new) new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) num_epochs = 5 new_best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in new_train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() torch.save(model.state_dict(), new_best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # Save the PyTorch model new_best_model_path = ""new_moedl.pth"" torch.save(model.state_dict(), new_best_model_path) The dataset can be found here",keep training pytorch model new datum  m work text classification task decide use pytorch model purpose  process mainly involve follow step  load process text  use tf  idf vectorizer  build neural network save tf  idf vectorizer model predict new datum  however  every day need classify new comment correct wrong classification  currently  approach add new comment correct classification dataset retrain entire model  process time  consume  new comment lose validation  would like create new dataset newly classify text continue train new datum  new comment classify manually  label correct   use gpt online code  write desire process  however  i m sure working expect  i m make silly mistake happen  main question  could check proposse way solve problem work expect  vectorizer face new token  fittransform   would loose original vectorizer  full training process  import torch torch import nn torchutilsdata import dataset  dataloader  randomsplit sklearnpreprocesse import labelencoder import polar pl sklearnmodelselection import traintestsplit sklearnfeatureextractiontext import tfidfvectorizer import joblib set1   pl readcsv    set1txt   separator     hasheader  false  newcolumns    text    label      since dateset unbalanced  i m go force balance feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf       text already clean  i m go change label numeric split train  test  val     labelmappe     anger   0    fear   1    joy   2    sadness   3  trainmapped   traindf withcolumn  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    trainset  pretest  traintestsplit  trainmapped  testsize04  randomstate42  stratify  trainmapped    label    testset  valset  traintestsplit  pretest  testsize05  randomstate42  stratify  pret    label     vectorize text datum use tf  idf vectorizer  tfidfvectorizer  maxfeatures30000  ngramrange  1  2   xtraintfidf  vectorizerfittransform  trainset   text    toarray   xvaltfidf  vectorizertransform  valset   text    toarray   xtesttfidf  vectorizertransform  testset   text    toarray   ytrain  trainset   label   yval  valset   label   yt  testset   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label traindataset  textdataset  xtraintfidf  ytrain  valdataset  textdataset  xvaltfidf  yval  testdataset  textdataset  xtesttfidf  ytest  batchsize  32 trainloader  dataloader  traindataset  batchsize  batchsize  shuffle  true  valloader  dataloader  valdataset  batchsize  batchsize  testloader  dataloader  testdataset  batchsize  batchsize  class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x inputdim  xtraintfidfshape  1  model  textclassificationmodel  inputdim  4   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter     training loop numepochs  17 bestvalacc  00 bestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label trainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep    validation modeleval   correct  total  0  0 torchnograd    text  label valloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   valacc  correct  total valacc  bestvalacc  bestvalacc  valacc torchsave  modelstatedict    bestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f   val acc   valacc  4f     load good model modelloadstatedict  torchload  bestmodelpath    load good model modelloadstatedict  torchload  bestmodelpath    test model modeleval   correct  total  0  0 torchnograd    text  label testloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   testacc  correct  total print  ft acc   testacc  3f     save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  joblibdump  vectorizer  vectorizerpath   save pytorch model modelpath    textclassificationmodelpth  torchsave  modelstatedict    modelpath  propose code  import torch import joblib import polar pl sklearnmodelselection import traintestsplit torch import nn torchutilsdata import dataset  dataloader  load save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  vectorizer  joblibload  vectorizerpath  inputdim  len  vectorizergetfeaturenamesout    class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x  load save pytorch model modelpath    textclassificationmodelpth  model  textclassificationmodel  inputdim  4  modelloadstatedict  torchload  modelpath    map label numeric value labelmapping     anger   0    fear   1    joy   2    sadness   3  sentiment     fear    joy    sadness    anger   newdata   pl readcsv    set2txt   separator     hasheader  false  newcolumns    text    label    filter  plcol    label   isin  sentiment   withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     vectorize new text datum use load tf  idf vectorizer xnew  vectorizertransform  newdata   text    toarray   ynew  newdata   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label batchsize  10  create dataloader new training datum newtraindataset  textdataset  xnew  ynew  newtrainloader  dataloader  newtraindataset  batchsize  batchsize  shuffle  true   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter    numepochs  5 newbestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label newtrainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep   torchsave  modelstatedict    newbestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f     save pytorch model newbestmodelpath    newmoedlpth  torchsave  modelstatedict    newbestmodelpath  dataset find,use pre  train word embedding like bertforsequenceclassification  embedding handle unseen token gracefully since map word continuous vector base semantic meaning  reduce impact unseen word  model training bert import torch torch import nn  optim torchutilsdata import dataloader  dataset transformer import berttokenizer  bertmodel  bertforsequenceclassification transformer import trainer  trainingarguments sklearnmodelselection import traintestsplit import polar pl  load prepare datum set1  plreadcsv    set1txt   separator     hasheader  false  newcolumns    text     label     balance dataset feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf   labelmapping     anger   0    fear   1    joy   2    sadness   3  traindf  traindfwithcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    split dataset trainset  testvalset  traintestsplit  traindf  testsize04  randomstate42  stratify  traindf    label    testset  valset  traintestsplit  testvalset  testsize05  randomstate42  stratify  testvalset    label     dataset class class textdataset  dataset   def   init    self  text  label  tokenizer  maxlength128   selftext  text selflabels  label selftokenizer  tokenizer selfmaxlength  maxlength def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  encode  selftokenizerencodeplus  text  addspecialtoken  true  maxlength  selfmaxlength  paddingmaxlength   truncation  true  returntensorspt   return   inputids   encode   inputids   flatten     attentionmask   encode   attentionmask   flatten     label   torchtensor  label  dtype  torchlong    initialize tokenizer dataset tokenizer  berttokenizerfrompretraine   bert  base  uncase   traindataset  textdataset  trainset   text    trainset   label    tokenizer  valdataset  textdataset  valset   text    valset   label    tokenizer  testdataset  textdataset  testset   text    testset   label    tokenizer   initialize bert model classification model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels4   training argument trainingargs  trainingarguments  outputdirresults   numtrainepochs3  perdevicetrainbatchsize16  perdeviceevalbatchsize16  evaluationstrategyepoch   savestrategyepoch   loggingdirlogs   learningrate2e5  loadbestmodelatend  true   define trainer trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset   train model trainertrain    evaluate model result  trainerevaluate  testdataset  print  f  test accuracy   result   evalaccuracy    4f     save model tokenizer modelsavepretraine    savedmodel   tokenizersavepretraine    savedtokenizer   incremental training least effort  load save model tokenizer model  bertforsequenceclassificationfrompretraine    savedmodel   tokenizer  berttokenizerfrompretraine    savedtokenizer    load new datum newdata   plreadcsv    set2txt   separator     hasheader  false  newcolumns    text     label    filter  plcol    label   isin     fear     joy     sadness     anger     withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     create new dataset newdataset  textdataset  newdata   text    newdata   label    tokenizer   update training argument incremental training newtrainingarg  trainingarguments  outputdirresultsincremental   numtrainepochs2   few epoch since s incremental perdevicetrainbatchsize16  evaluationstrategyepoch   loggingdirlogsincremental   learningrate2e5  loadbestmodelatend  true   define new trainer newtrainer  trainer  model  model  arg  newtrainingargs  traindataset  newdataset  evaldataset  valdataset  validate previous validation set   train new datum newtrainertrain    evaluate retrain newresult  newtrainerevaluate  testdataset  print  f  test accuracy incremental training   newresult   evalaccuracy    4f     save update model modelsavepretraine    savedmodelincremental  ,keep training pytorch model new datum  m work text classification task decide use pytorch model purpose  process mainly involve follow step  load process text  use tf  idf vectorizer  build neural network save tf  idf vectorizer model predict new datum  however  every day need classify new comment correct wrong classification  currently  approach add new comment correct classification dataset retrain entire model  process time  consume  new comment lose validation  would like create new dataset newly classify text continue train new datum  new comment classify manually  label correct   use gpt online code  write desire process  however  i m sure working expect  i m make silly mistake happen  main question  could check proposse way solve problem work expect  vectorizer face new token  fittransform   would loose original vectorizer  full training process  import torch torch import nn torchutilsdata import dataset  dataloader  randomsplit sklearnpreprocesse import labelencoder import polar pl sklearnmodelselection import traintestsplit sklearnfeatureextractiontext import tfidfvectorizer import joblib set1   pl readcsv    set1txt   separator     hasheader  false  newcolumns    text    label      since dateset unbalanced  i m go force balance feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf       text already clean  i m go change label numeric split train  test  val     labelmappe     anger   0    fear   1    joy   2    sadness   3  trainmapped   traindf withcolumn  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    trainset  pretest  traintestsplit  trainmapped  testsize04  randomstate42  stratify  trainmapped    label    testset  valset  traintestsplit  pretest  testsize05  randomstate42  stratify  pret    label     vectorize text datum use tf  idf vectorizer  tfidfvectorizer  maxfeatures30000  ngramrange  1  2   xtraintfidf  vectorizerfittransform  trainset   text    toarray   xvaltfidf  vectorizertransform  valset   text    toarray   xtesttfidf  vectorizertransform  testset   text    toarray   ytrain  trainset   label   yval  valset   label   yt  testset   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label traindataset  textdataset  xtraintfidf  ytrain  valdataset  textdataset  xvaltfidf  yval  testdataset  textdataset  xtesttfidf  ytest  batchsize  32 trainloader  dataloader  traindataset  batchsize  batchsize  shuffle  true  valloader  dataloader  valdataset  batchsize  batchsize  testloader  dataloader  testdataset  batchsize  batchsize  class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x inputdim  xtraintfidfshape  1  model  textclassificationmodel  inputdim  4   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter     training loop numepochs  17 bestvalacc  00 bestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label trainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep    validation modeleval   correct  total  0  0 torchnograd    text  label valloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   valacc  correct  total valacc  bestvalacc  bestvalacc  valacc torchsave  modelstatedict    bestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f   val acc   valacc  4f     load good model modelloadstatedict  torchload  bestmodelpath    load good model modelloadstatedict  torchload  bestmodelpath    test model modeleval   correct  total  0  0 torchnograd    text  label testloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   testacc  correct  total print  ft acc   testacc  3f     save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  joblibdump  vectorizer  vectorizerpath   save pytorch model modelpath    textclassificationmodelpth  torchsave  modelstatedict    modelpath  propose code  import torch import joblib import polar pl sklearnmodelselection import traintestsplit torch import nn torchutilsdata import dataset  dataloader  load save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  vectorizer  joblibload  vectorizerpath  inputdim  len  vectorizergetfeaturenamesout    class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x  load save pytorch model modelpath    textclassificationmodelpth  model  textclassificationmodel  inputdim  4  modelloadstatedict  torchload  modelpath    map label numeric value labelmapping     anger   0    fear   1    joy   2    sadness   3  sentiment     fear    joy    sadness    anger   newdata   pl readcsv    set2txt   separator     hasheader  false  newcolumns    text    label    filter  plcol    label   isin  sentiment   withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     vectorize new text datum use load tf  idf vectorizer xnew  vectorizertransform  newdata   text    toarray   ynew  newdata   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label batchsize  10  create dataloader new training datum newtraindataset  textdataset  xnew  ynew  newtrainloader  dataloader  newtraindataset  batchsize  batchsize  shuffle  true   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter    numepochs  5 newbestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label newtrainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep   torchsave  modelstatedict    newbestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f     save pytorch model newbestmodelpath    newmoedlpth  torchsave  modelstatedict    newbestmodelpath  dataset find use pre  train word embedding like bertforsequenceclassification  embedding handle unseen token gracefully since map word continuous vector base semantic meaning  reduce impact unseen word  model training bert import torch torch import nn  optim torchutilsdata import dataloader  dataset transformer import berttokenizer  bertmodel  bertforsequenceclassification transformer import trainer  trainingarguments sklearnmodelselection import traintestsplit import polar pl  load prepare datum set1  plreadcsv    set1txt   separator     hasheader  false  newcolumns    text     label     balance dataset feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf   labelmapping     anger   0    fear   1    joy   2    sadness   3  traindf  traindfwithcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    split dataset trainset  testvalset  traintestsplit  traindf  testsize04  randomstate42  stratify  traindf    label    testset  valset  traintestsplit  testvalset  testsize05  randomstate42  stratify  testvalset    label     dataset class class textdataset  dataset   def   init    self  text  label  tokenizer  maxlength128   selftext  text selflabels  label selftokenizer  tokenizer selfmaxlength  maxlength def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  encode  selftokenizerencodeplus  text  addspecialtoken  true  maxlength  selfmaxlength  paddingmaxlength   truncation  true  returntensorspt   return   inputids   encode   inputids   flatten     attentionmask   encode   attentionmask   flatten     label   torchtensor  label  dtype  torchlong    initialize tokenizer dataset tokenizer  berttokenizerfrompretraine   bert  base  uncase   traindataset  textdataset  trainset   text    trainset   label    tokenizer  valdataset  textdataset  valset   text    valset   label    tokenizer  testdataset  textdataset  testset   text    testset   label    tokenizer   initialize bert model classification model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels4   training argument trainingargs  trainingarguments  outputdirresults   numtrainepochs3  perdevicetrainbatchsize16  perdeviceevalbatchsize16  evaluationstrategyepoch   savestrategyepoch   loggingdirlogs   learningrate2e5  loadbestmodelatend  true   define trainer trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset   train model trainertrain    evaluate model result  trainerevaluate  testdataset  print  f  test accuracy   result   evalaccuracy    4f     save model tokenizer modelsavepretraine    savedmodel   tokenizersavepretraine    savedtokenizer   incremental training least effort  load save model tokenizer model  bertforsequenceclassificationfrompretraine    savedmodel   tokenizer  berttokenizerfrompretraine    savedtokenizer    load new datum newdata   plreadcsv    set2txt   separator     hasheader  false  newcolumns    text     label    filter  plcol    label   isin     fear     joy     sadness     anger     withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     create new dataset newdataset  textdataset  newdata   text    newdata   label    tokenizer   update training argument incremental training newtrainingarg  trainingarguments  outputdirresultsincremental   numtrainepochs2   few epoch since s incremental perdevicetrainbatchsize16  evaluationstrategyepoch   loggingdirlogsincremental   learningrate2e5  loadbestmodelatend  true   define new trainer newtrainer  trainer  model  model  arg  newtrainingargs  traindataset  newdataset  evaldataset  valdataset  validate previous validation set   train new datum newtrainertrain    evaluate retrain newresult  newtrainerevaluate  testdataset  print  f  test accuracy incremental training   newresult   evalaccuracy    4f     save update model modelsavepretraine    savedmodelincremental  ,Task-Specific Queries
Capitalized words in sentiment analysis,"I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive . A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and distinction between sentiments as good , very good than just positive to include the importance of this upper case words . this is my current code : from itertools import chain def is_upper_case(text): return [word for word in text.split() if word.isupper() and word != 'I'] unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case))) print(unique_upper_words)","['nlp', 'sentiment-analysis', 'bert-language-model', 'data-preprocessing']",1,"If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data. If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis. If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out this repository as an example.",2024-08-30 13:49:56,2024-08-30 17:50:30,134,https://stackoverflow.com/questions/78932356/capitalized-words-in-sentiment-analysis,"Capitalized words in sentiment analysis I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive . A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and distinction between sentiments as good , very good than just positive to include the importance of this upper case words . this is my current code : from itertools import chain def is_upper_case(text): return [word for word in text.split() if word.isupper() and word != 'I'] unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case))) print(unique_upper_words)",capitalize word sentiment analysis  m currently work datum customer review product sephora  task classify sentiment  negative  neutral  positive  common technique text preprocesse low case word  situation upper case word like  amazing  hide significant emotion behind turn word low case cause information loss  would happy opinion subject still low case word  personally think create class distinction sentiment good  good positive include importance upper case word  current code  itertool import chain def isuppercase  text   return  word word textsplit   wordisupper   word      uniqueupperword  set  chainfromiterable  allreviews   reviewtext   apply  isuppercase    print  uniqueupperwords ,use bert  base model  llm  actual classification would recommend use preprocessing  least come capitalization   model pre  train non  preprocesse datum  want kind analysis result label sentence could lowercase everything group n  gram simplify analysis  think multiple class well distinction prediction  think would make sense switch sentiment regression instead classification  predict value continuous range  come somewhat natural fine  tune language model normal classification would take continuous output model map categorical class use something like softmax  need skip last step directly use model output  many python ml framework fine  tuning use language model class regression task  check repository example ,capitalize word sentiment analysis  m currently work datum customer review product sephora  task classify sentiment  negative  neutral  positive  common technique text preprocesse low case word  situation upper case word like  amazing  hide significant emotion behind turn word low case cause information loss  would happy opinion subject still low case word  personally think create class distinction sentiment good  good positive include importance upper case word  current code  itertool import chain def isuppercase  text   return  word word textsplit   wordisupper   word      uniqueupperword  set  chainfromiterable  allreviews   reviewtext   apply  isuppercase    print  uniqueupperwords  use bert  base model  llm  actual classification would recommend use preprocessing  least come capitalization   model pre  train non  preprocesse datum  want kind analysis result label sentence could lowercase everything group n  gram simplify analysis  think multiple class well distinction prediction  think would make sense switch sentiment regression instead classification  predict value continuous range  come somewhat natural fine  tune language model normal classification would take continuous output model map categorical class use something like softmax  need skip last step directly use model output  many python ml framework fine  tuning use language model class regression task  check repository example ,Library/Tool-Based Queries
cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39;,"I've been using LLAMA 2 for research for a few months now and I import as follows: from transformers import AutoModelForCausalLM, AutoTokenizer device = torch.device(""cuda"") tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"") model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=True) It has always worked. However, today it is showing the following error: RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): Failed to import transformers.generation.utils because of the following error (look up to see its traceback): cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/ init .py) Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.","['python', 'nlp', 'huggingface-transformers', 'transformer-model', 'llama']",1,"The error you're encountering is due to the split_torch_state_dict_into_shards function not being available in huggingface-hub version < 0.23.0 . This function is included starting from version 0.23.0 . To resolve this issue, update the huggingface-hub library to version 0.23.0 or later Also please install accelerate: pip install accelerate==0.31.0 here is a git link: https://github.com/run-llama/llama_index/discussions/14605",2024-08-27 17:20:42,2024-08-27 17:24:43,6132,https://stackoverflow.com/questions/78920095/cannot-import-name-split-torch-state-dict-into-shards-from-huggingface-hub,"cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39; I've been using LLAMA 2 for research for a few months now and I import as follows: from transformers import AutoModelForCausalLM, AutoTokenizer device = torch.device(""cuda"") tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"") model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=True) It has always worked. However, today it is showing the following error: RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): Failed to import transformers.generation.utils because of the following error (look up to see its traceback): cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/ init .py) Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.",import name   39  splittorchstatedictintoshard   39    39  huggingfacehub   39   ve use llama 2 research month import follow  transformer import automodelforcausallm  autotokenizer device  torchdevice    cuda   tokenizer  autotokenizerfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   model  automodelforcausallmfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   loadin4bit  true  always work  however  today show follow error  runtimeerror  fail import transformersmodelsllamamodelingllama follow error  look see traceback   fail import transformersgenerationutil follow error  look see traceback   import name  splittorchstatedictintoshards   huggingfacehub   opt  conda  lib  python310  site  package  huggingfacehub init py  recreate hugging face token  not work  use google colab kaggle notebook ,error be encounter due splittorchstatedictintoshards function available huggingface  hub version  0230  function include start version 0230  resolve issue  update huggingface  hub library version 0230 later also please install accelerate  pip install accelerate0310 git link    githubcom  run  llama  llamaindex  discussions14605,import name   39  splittorchstatedictintoshard   39    39  huggingfacehub   39   ve use llama 2 research month import follow  transformer import automodelforcausallm  autotokenizer device  torchdevice    cuda   tokenizer  autotokenizerfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   model  automodelforcausallmfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   loadin4bit  true  always work  however  today show follow error  runtimeerror  fail import transformersmodelsllamamodelingllama follow error  look see traceback   fail import transformersgenerationutil follow error  look see traceback   import name  splittorchstatedictintoshards   huggingfacehub   opt  conda  lib  python310  site  package  huggingfacehub init py  recreate hugging face token  not work  use google colab kaggle notebook  error be encounter due splittorchstatedictintoshards function available huggingface  hub version  0230  function include start version 0230  resolve issue  update huggingface  hub library version 0230 later also please install accelerate  pip install accelerate0310 git link    githubcom  run  llama  llamaindex  discussions14605,Library/Tool-Based Queries
How to Process Data on GPU Instead of RAM for This Python Code?,"I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance. my code : def prepare_dataset(batch): audio = batch[""audio""] batch[""input_features""] = feature_extractor( audio[""array""], sampling_rate=audio[""sampling_rate""] ).input_features[0] batch[""labels""] = tokenizer(batch[""sentence""]).input_ids return batch common_voice = common_voice.map( prepare_dataset, remove_columns=common_voice.column_names[""train""], num_proc=1 ) How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!","['nlp', 'gpu', 'torch', 'openai-whisper']",1,"you can using the following code to process audio data on GPU import torch device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") print(device) def prepare_dataset(batch): audio = batch[""audio""] input_features = feature_extractor(audio[""array""], sampling_rate=audio[""sampling_rate""]).input_features[0] batch[""input_features""] = torch.tensor(input_features).to(device) labels = tokenizer(batch[""sentence""]).input_ids batch[""labels""] = torch.tensor(labels).to(device) return batch common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[""train""])",2024-08-27 08:03:28,2024-08-27 12:21:29,64,https://stackoverflow.com/questions/78917743/how-to-process-data-on-gpu-instead-of-ram-for-this-python-code,"How to Process Data on GPU Instead of RAM for This Python Code? I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance. my code : def prepare_dataset(batch): audio = batch[""audio""] batch[""input_features""] = feature_extractor( audio[""array""], sampling_rate=audio[""sampling_rate""] ).input_features[0] batch[""labels""] = tokenizer(batch[""sentence""]).input_ids return batch common_voice = common_voice.map( prepare_dataset, remove_columns=common_voice.column_names[""train""], num_proc=1 ) How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!",process data gpu instead ram python code   m currently use follow code process audio datum  run ram  want offload processing gpu improve performance  code  def preparedataset  batch   audio  batch    audio   batch    inputfeature    featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    label    tokenizer  batch    sentence    inputids return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train    numproc1  modify code utilize gpu processing instead ram  guidance specific change much appreciate ,use follow code process audio datum gpu import torch device  torchdevice    cuda  torchcudaisavailable   else   cpu   print  device  def preparedataset  batch   audio  batch    audio   inputfeature  featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    inputfeature    torchtensor  inputfeature  to  device  label  tokenizer  batch    sentence    inputids batch    label    torchtensor  label  to  device  return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train   ,process data gpu instead ram python code   m currently use follow code process audio datum  run ram  want offload processing gpu improve performance  code  def preparedataset  batch   audio  batch    audio   batch    inputfeature    featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    label    tokenizer  batch    sentence    inputids return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train    numproc1  modify code utilize gpu processing instead ram  guidance specific change much appreciate  use follow code process audio datum gpu import torch device  torchdevice    cuda  torchcudaisavailable   else   cpu   print  device  def preparedataset  batch   audio  batch    audio   inputfeature  featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    inputfeature    torchtensor  inputfeature  to  device  label  tokenizer  batch    sentence    inputids batch    label    torchtensor  label  to  device  return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train   ,Basic Understanding
How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation,"I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this. What I’ve Tried: Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me. Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation: # VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET) from transformers import MarianMTModel, MarianTokenizer import torch import matplotlib.pyplot as plt from torch.nn import functional as F model_name = ""Helsinki-NLP/opus-mt-en-de"" tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) model.eval() keys = {} queries = {} def get_key(layer): def hook(module, input, output): key, = input keys[layer] = key return hook def get_query(layer): def hook(module, input, output): query, = input queries[layer] = query return hook def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() hooks = [] for i, layer in enumerate(model.model.decoder.layers): hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i))) hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i))) input_text = ""Please translate this to German."" inputs = tokenizer(input_text, return_tensors=""pt"") translated_tokens = model.generate(**inputs, use_cache=False) translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True) input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0]) output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0]) attentions = [] for layer in range(len(keys)): K, Q = keys[layer], queries[layer] M = Q @ K.transpose(-2, -1) attentions.append(F.softmax(M, dim=-1)) attentions = torch.stack(attentions, dim=0) print(""layers, heads, output tokens, input tokens"") print(attentions.shape) plt.figure(figsize=(10, 8)) plt.imshow(attentions[0, 0], cmap='viridis') plt.colorbar() plt.xticks(range(len(input_tokens)), input_tokens, rotation=90) plt.yticks(range(len(output_tokens)), output_tokens) plt.xlabel(""Input Tokens"") plt.ylabel(""Output Tokens"") plt.title(""Cross-Attention Matrix"") plt.show() This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation. My Question Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8? I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.","['python', 'pytorch', 'nlp', 'huggingface-transformers']",1,"Huggingface has built in methods to return attention weights translated_tokens = model.generate(**inputs, output_attentions=True, return_dict_in_generate=True ) print(translated_tokens.keys()) > odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values']) With return_dict_in_generate=True , model.generate returns a dict-like object. With output_attentions=True , the output dict will contain all attention weights. For this model, it will include encoder attentions, decoder attentions and cross attentions.",2024-08-25 20:13:54,2024-08-26 16:38:28,402,https://stackoverflow.com/questions/78912171/how-to-visualize-cross-attention-matrices-in-marianmtmodel-during-output-generat,"How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this. What I’ve Tried: Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me. Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation: # VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET) from transformers import MarianMTModel, MarianTokenizer import torch import matplotlib.pyplot as plt from torch.nn import functional as F model_name = ""Helsinki-NLP/opus-mt-en-de"" tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) model.eval() keys = {} queries = {} def get_key(layer): def hook(module, input, output): key, = input keys[layer] = key return hook def get_query(layer): def hook(module, input, output): query, = input queries[layer] = query return hook def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() hooks = [] for i, layer in enumerate(model.model.decoder.layers): hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i))) hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i))) input_text = ""Please translate this to German."" inputs = tokenizer(input_text, return_tensors=""pt"") translated_tokens = model.generate(**inputs, use_cache=False) translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True) input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0]) output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0]) attentions = [] for layer in range(len(keys)): K, Q = keys[layer], queries[layer] M = Q @ K.transpose(-2, -1) attentions.append(F.softmax(M, dim=-1)) attentions = torch.stack(attentions, dim=0) print(""layers, heads, output tokens, input tokens"") print(attentions.shape) plt.figure(figsize=(10, 8)) plt.imshow(attentions[0, 0], cmap='viridis') plt.colorbar() plt.xticks(range(len(input_tokens)), input_tokens, rotation=90) plt.yticks(range(len(output_tokens)), output_tokens) plt.xlabel(""Input Tokens"") plt.ylabel(""Output Tokens"") plt.title(""Cross-Attention Matrix"") plt.show() This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation. My Question Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8? I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.",visualize cross  attention matrices marianmtmodel output generation work machine translation task use marianmtmodel hugging face transformer library  specifically  want visualize cross  attention matrix model s translation process  however  encounter difficulty achieve   try  initial attempt  notice cross  attention matrix directly return model generate translation  example find involved feeding source text translation model  however  goal access cross  attention matrix model generate output  translation give  use forward hooks  achieve  implement forward hook key query projection attention mechanism  disable key  value cache  usecache  false  capture full matrix last step   implementation   visualize cros attention translation task  working yet  transformer import marianmtmodel  mariantokenizer import torch import matplotlibpyplot plt torchnn import functional f modelname    helsinki  nlp  opus  mt  en  de  tokenizer  mariantokenizerfrompretraine  modelname  model  marianmtmodelfrompretrained  modelname  modeleval   key    query    def getkey  layer   def hook  module  input  output   key   input key  layer   key return hook def getquery  layer   def hook  module  input  output   query   input query  layer   query return hook def  shape  self  tensor  torch  tensor  seqlen  int  bsz  int   return tensorview  bsz  seqlen  selfnumhead  selfheaddim  transpose  1  2  contiguous   hook     layer enumerate  modelmodeldecoderlayer   hooksappend  layerencoderattnkprojregisterforwardhook  getkey     hooksappend  layerencoderattnqprojregisterforwardhook  getquery     inputtext    please translate german   input  tokenizer  inputtext  returntensors  pt   translatedtoken  modelgenerate    input  usecache  false  translatedtext  tokenizerdecode  translatedtokens  0   skipspecialtoken  true  inputtokens  tokenizerconvertidstotoken  input    inputids    0   outputtoken  tokenizerconvertidstotoken  translatedtokens  0   attention    layer range  len  key    k  q  key  layer   query  layer   q  ktranspose  2  1  attentionsappend  fsoftmax   dim1   attention  torchstack  attention  dim0  print    layer  head  output token  input token   print  attentionsshape  pltfigure  figsize  10  8   pltimshow  attention  0  0   cmapviridis   pltcolorbar   pltxtick  range  len  inputtokens    inputtoken  rotation90  pltytick  range  len  outputtokens    outputtokens  pltxlabel    input tokens   pltylabel    output tokens   plttitle    cross  attention matrix   pltshow   approach seem work capture cross  attention matrix  however  observe matrix 4 attention head instead expect 8  make question correctness implementation  question give issue  encounter  reliable method extract visualize cross  attention matrix translation process  additionally  current approach fundamentally okay  resolve issue capture 4 attention head instead 8  suspect issue might relate  m currently reshape key  k  query  q  tensor head dimension multiplication  want ask advice case  easy effective way ,huggingface build method return attention weight translatedtoken  modelgenerate    input  outputattention  true  returndictingenerate  true  print  translatedtokenskeys     odictkey    sequence    encoderattention    decoderattention    crossattention    pastkeyvalue    returndictingenerate  true  modelgenerate return dict  like object  outputattention  true  output dict contain attention weight  model  include encoder attention  decoder attention cross attention ,visualize cross  attention matrices marianmtmodel output generation work machine translation task use marianmtmodel hugging face transformer library  specifically  want visualize cross  attention matrix model s translation process  however  encounter difficulty achieve   try  initial attempt  notice cross  attention matrix directly return model generate translation  example find involved feeding source text translation model  however  goal access cross  attention matrix model generate output  translation give  use forward hooks  achieve  implement forward hook key query projection attention mechanism  disable key  value cache  usecache  false  capture full matrix last step   implementation   visualize cros attention translation task  working yet  transformer import marianmtmodel  mariantokenizer import torch import matplotlibpyplot plt torchnn import functional f modelname    helsinki  nlp  opus  mt  en  de  tokenizer  mariantokenizerfrompretraine  modelname  model  marianmtmodelfrompretrained  modelname  modeleval   key    query    def getkey  layer   def hook  module  input  output   key   input key  layer   key return hook def getquery  layer   def hook  module  input  output   query   input query  layer   query return hook def  shape  self  tensor  torch  tensor  seqlen  int  bsz  int   return tensorview  bsz  seqlen  selfnumhead  selfheaddim  transpose  1  2  contiguous   hook     layer enumerate  modelmodeldecoderlayer   hooksappend  layerencoderattnkprojregisterforwardhook  getkey     hooksappend  layerencoderattnqprojregisterforwardhook  getquery     inputtext    please translate german   input  tokenizer  inputtext  returntensors  pt   translatedtoken  modelgenerate    input  usecache  false  translatedtext  tokenizerdecode  translatedtokens  0   skipspecialtoken  true  inputtokens  tokenizerconvertidstotoken  input    inputids    0   outputtoken  tokenizerconvertidstotoken  translatedtokens  0   attention    layer range  len  key    k  q  key  layer   query  layer   q  ktranspose  2  1  attentionsappend  fsoftmax   dim1   attention  torchstack  attention  dim0  print    layer  head  output token  input token   print  attentionsshape  pltfigure  figsize  10  8   pltimshow  attention  0  0   cmapviridis   pltcolorbar   pltxtick  range  len  inputtokens    inputtoken  rotation90  pltytick  range  len  outputtokens    outputtokens  pltxlabel    input tokens   pltylabel    output tokens   plttitle    cross  attention matrix   pltshow   approach seem work capture cross  attention matrix  however  observe matrix 4 attention head instead expect 8  make question correctness implementation  question give issue  encounter  reliable method extract visualize cross  attention matrix translation process  additionally  current approach fundamentally okay  resolve issue capture 4 attention head instead 8  suspect issue might relate  m currently reshape key  k  query  q  tensor head dimension multiplication  want ask advice case  easy effective way  huggingface build method return attention weight translatedtoken  modelgenerate    input  outputattention  true  returndictingenerate  true  print  translatedtokenskeys     odictkey    sequence    encoderattention    decoderattention    crossattention    pastkeyvalue    returndictingenerate  true  modelgenerate return dict  like object  outputattention  true  output dict contain attention weight  model  include encoder attention  decoder attention cross attention ,Basic Understanding
Why doesn&#39;t permuting positional encodings in BERT affect the output as expected?,"I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another question I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results. What I expected to happen: No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary. Permuting only the input IDs should return distribution B. Permuting only the positional embeddings should return distribution B. Permuting both the input IDs and positional embeddings should return distribution A. What actually happens: Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result. My question is: Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding? For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in masked_prediction . import torch import ipywidgets as widgets from IPython.display import display from transformers import BertForMaskedLM, AutoTokenizer import matplotlib.pyplot as plt import torch.nn.functional as F # surpress renaming warnings logging.getLogger(""transformers.modeling_utils"").setLevel(logging.ERROR) warnings.simplefilter(""ignore"", FutureWarning) tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") input_ids = torch.Tensor([[]]) tokens = [] permutation = [] output = widgets.Output() def permute_columns(matrix, permutation=None): n = len(permutation) first_n_columns = matrix[:, :n] permuted_columns = first_n_columns[:, permutation] remaining_columns = matrix[:, n:] new_matrix = torch.hstack((permuted_columns, remaining_columns)) return new_matrix def update_permutation(ordered_tags): global permutation fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]] permutation = [tokens.index(tag) for tag in fixed_tokens] def tokenize(text): global input_ids, tokens input_ids = tokenizer(text, return_tensors=""pt"").input_ids tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]] if len(tokens) > 2: reorderable_tokens = tokens[1:-1] else: reorderable_tokens = [] with output: output.clear_output(wait=True) tags_input.allowed_tags = reorderable_tokens tags_input.value = reorderable_tokens update_permutation(tags_input.value) def on_tags_change(change): if len(change['new']) != len(tags_input.allowed_tags): tags_input.value = tags_input.allowed_tags # Restore original value def masked_prediction(input_ids, permutation, permute_input, permute_encoding): with output: output.clear_output(wait=True) # Clear previous outputs if input_ids.numel() == 0: print(""You can't use an empty sequence for prediction"") return model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") if permute_encoding: model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T if permute_input: input_ids = permute_columns(input_ids, permutation) decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False) with torch.no_grad(): outputs = model(input_ids) logits = outputs.logits top_k = 5 mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1] print(decoded_text, mask_token_indices, permutation) num_masks = len(mask_token_indices) if num_masks == 0: print(""You need to include a [MASK] token for prediction"") return fig, axs = plt.subplots(1, num_masks, figsize=(15, 6)) if num_masks == 1: axs = [axs] for i, idx in enumerate(mask_token_indices): mask_token_logits = logits[0, idx, :] softmax_probs = F.softmax(mask_token_logits, dim=0) top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0) predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids] predicted_confidences = top_token_probs.tolist() axs[i].bar(predicted_tokens, predicted_confidences, color='blue') axs[i].set_xlabel('Predicted Tokens') axs[i].set_ylabel('Confidence') axs[i].set_title(f'Masked Token at Position {idx.item()}') axs[i].set_ylim(0, 1) plt.show() def on_predict_button_click(b): masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value) text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:') text_input.observe(lambda change: tokenize(change['new']), names='value') tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False) # Observe changes in tags order to update the permutation and prevent deletion tags_input.observe(on_tags_change, names='value') tags_input.observe(lambda change: update_permutation(change['new']), names='value') # Create checkboxes for permute_input and permute_encoding permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs') permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings') # Create a button to trigger the prediction predict_button = widgets.Button(description=""Run Prediction"") predict_button.on_click(on_predict_button_click) # Display the widgets display(text_input) display(tags_input) display(permute_input_checkbox) display(permute_encoding_checkbox) display(predict_button) display(output)","['python', 'pytorch', 'nlp', 'huggingface-transformers']",1,"The model inputs have token ids and position ids. There are four scenarios to consider: Baseline. Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider: # Given: tokens = [0, 1, 2] positions = [0, 1, 2] permutation = [2, 0, 1] # Ex1: Permute tokens but not positions [2, 0, 1] # permuted tokens [0, 1, 2] # standard positions # Ex2: Permute positions but not tokens [0, 1, 2] # standard tokens [2, 0, 1] # permuted positions In Ex1 , the model is told that token 2 occurs at position 0 . In Ex2 , the model is told that token 2 occurs at position 1 . Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs. The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results. It is simple to test this. Huggingface models take a position_ids input parameter. We can use this to test permutations of the input ids without messing with the weight matrices. To test this, we'll create input data, permute as needed, compute logits and compare logits. When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token i in scenario 1 is permuted to token j in scenario 3, we want to compare logits i from scenario 1 to logits j in scenario 3. import torch from transformers import BertForMaskedLM, AutoTokenizer def get_logits(inputs): with torch.no_grad(): outputs = model(**inputs) logits = outputs.logits return logits def permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True): outputs = {} for k,v in inputs.items(): if k=='position_ids' and permute_positions: outputs[k] = v[permutation] elif k!='position_ids' and permute_ids: outputs[k] = v[:,permutation] else: outputs[k] = v return outputs # load tokenizer/model tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") model.eval() # remember to set model to eval # create input ids and position ids inputs = tokenizer('input text test sequence', return_tensors='pt') inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1]))) # create permutation tensor permutation = torch.randperm(inputs['input_ids'].shape[1]) # compute scenario data data = { 's1' : { # scenario 1 - baseline 'inputs' : inputs, 'permuted_ids' : False }, 's2' : { # scenario 2 - permute positions only 'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True), 'permuted_ids' : False }, 's3' : { # scenario 3 - permute token ids only 'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False), 'permuted_ids' : True }, 's4' : { # scenario 4 - permute tokens and positions 'inputs' : permute_inputs(inputs, permutation), 'permuted_ids' : True } } # compute logits for k,v in data.items(): v['logits'] = get_logits(v['inputs']) comparisons = [ ['s1', 's2'], ['s1', 's3'], ['s1', 's4'], ['s2', 's3'], ['s2', 's4'], ['s3', 's4'], ] # compare scenarios for sa, sb in comparisons: data_a = data[sa] data_b = data[sb] logits_a = data_a['logits'] logits_b = data_b['logits'] if data_a['permuted_ids'] == data_b['permuted_ids']: # either both logits are permuted or both logits are unpermuted # so we can compare directly val = (logits_a - logits_b).abs().mean() elif data_a['permuted_ids'] and (not data_b['permuted_ids']): # if `a` is permuted but `b` is not, we permute `b` to make tokens line up val = (logits_a - logits_b[:,permutation]).abs().mean() else: # otherwise we permute `b` to make tokens line up val = (logits_a[:,permutation] - logits_b).abs().mean() print(f""Comparison {sa}, {sb}: {val.item():.6f}"") The code should produce an output like: Comparison s1, s2: 1.407895 Comparison s1, s3: 1.583560 Comparison s1, s4: 0.000003 Comparison s2, s3: 1.750883 Comparison s2, s4: 1.407894 Comparison s3, s4: 1.583560 Run the code a bunch of times. You will find that the S1, S4 comparison always has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues. You will find the S2, S3 comparison generally has a large deviation, but sometimes has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.",2024-08-23 11:12:08,2024-08-23 17:03:05,80,https://stackoverflow.com/questions/78905614/why-doesnt-permuting-positional-encodings-in-bert-affect-the-output-as-expected,"Why doesn&#39;t permuting positional encodings in BERT affect the output as expected? I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another question I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results. What I expected to happen: No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary. Permuting only the input IDs should return distribution B. Permuting only the positional embeddings should return distribution B. Permuting both the input IDs and positional embeddings should return distribution A. What actually happens: Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result. My question is: Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding? For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in masked_prediction . import torch import ipywidgets as widgets from IPython.display import display from transformers import BertForMaskedLM, AutoTokenizer import matplotlib.pyplot as plt import torch.nn.functional as F # surpress renaming warnings logging.getLogger(""transformers.modeling_utils"").setLevel(logging.ERROR) warnings.simplefilter(""ignore"", FutureWarning) tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") input_ids = torch.Tensor([[]]) tokens = [] permutation = [] output = widgets.Output() def permute_columns(matrix, permutation=None): n = len(permutation) first_n_columns = matrix[:, :n] permuted_columns = first_n_columns[:, permutation] remaining_columns = matrix[:, n:] new_matrix = torch.hstack((permuted_columns, remaining_columns)) return new_matrix def update_permutation(ordered_tags): global permutation fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]] permutation = [tokens.index(tag) for tag in fixed_tokens] def tokenize(text): global input_ids, tokens input_ids = tokenizer(text, return_tensors=""pt"").input_ids tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]] if len(tokens) > 2: reorderable_tokens = tokens[1:-1] else: reorderable_tokens = [] with output: output.clear_output(wait=True) tags_input.allowed_tags = reorderable_tokens tags_input.value = reorderable_tokens update_permutation(tags_input.value) def on_tags_change(change): if len(change['new']) != len(tags_input.allowed_tags): tags_input.value = tags_input.allowed_tags # Restore original value def masked_prediction(input_ids, permutation, permute_input, permute_encoding): with output: output.clear_output(wait=True) # Clear previous outputs if input_ids.numel() == 0: print(""You can't use an empty sequence for prediction"") return model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") if permute_encoding: model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T if permute_input: input_ids = permute_columns(input_ids, permutation) decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False) with torch.no_grad(): outputs = model(input_ids) logits = outputs.logits top_k = 5 mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1] print(decoded_text, mask_token_indices, permutation) num_masks = len(mask_token_indices) if num_masks == 0: print(""You need to include a [MASK] token for prediction"") return fig, axs = plt.subplots(1, num_masks, figsize=(15, 6)) if num_masks == 1: axs = [axs] for i, idx in enumerate(mask_token_indices): mask_token_logits = logits[0, idx, :] softmax_probs = F.softmax(mask_token_logits, dim=0) top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0) predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids] predicted_confidences = top_token_probs.tolist() axs[i].bar(predicted_tokens, predicted_confidences, color='blue') axs[i].set_xlabel('Predicted Tokens') axs[i].set_ylabel('Confidence') axs[i].set_title(f'Masked Token at Position {idx.item()}') axs[i].set_ylim(0, 1) plt.show() def on_predict_button_click(b): masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value) text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:') text_input.observe(lambda change: tokenize(change['new']), names='value') tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False) # Observe changes in tags order to update the permutation and prevent deletion tags_input.observe(on_tags_change, names='value') tags_input.observe(lambda change: update_permutation(change['new']), names='value') # Create checkboxes for permute_input and permute_encoding permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs') permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings') # Create a button to trigger the prediction predict_button = widgets.Button(description=""Run Prediction"") predict_button.on_click(on_predict_button_click) # Display the widgets display(text_input) display(tags_input) display(permute_input_checkbox) display(permute_encoding_checkbox) display(predict_button) display(output)",  39  permute positional encoding bert affect output expect  work jupyter notebook transformers  section positional encoding  want demonstrate transformer rely entirely positional encode understand order sequence  previously learn another question post concept apply model not use mask attention  like gpt2  however  attempt approach bert model  use cross  attention  predict  mask  token  encounter unexpected result  expect happen  permutation cause model predict different token  ie  distribution consistent vocabulary  permute input id return distribution b permuting positional embedding return distribution b permuting input id positional embedding return distribution  actually happen  sometimes result align expectation  time  permute one aspect  either input id positional embedding  lead different outcome  even though occasionally  produce result  question  something else hug face s bert model might influence position  beyond positional encoding  completeness  include full code part notebook  try directly  important part happen maskedprediction  import torch import ipywidget widget ipythondisplay import display transformer import bertformaskedlm  autotokenizer import matplotlibpyplot plt import torchnnfunctional f  surpress rename warning logginggetlogger    transformersmodelingutil   setlevel  log  error  warningssimplefilter    ignore   futurewarning  tokenizer  autotokenizerfrompretraine    bert  base  uncased   inputid  torch  tensor       token    permutation    output  widget  output   def permutecolumn  matrix  permutation  none   n  len  permutation  firstncolumns  matrix     n  permutedcolumn  firstncolumns    permutation  remainingcolumn  matrix    n   newmatrix  torchhstack   permutedcolumn  remainingcolumns   return newmatrix def updatepermutation  orderedtags   global permutation fixedtokens   token  0    orderedtag   token  1   permutation   tokensindex  tag  tag fixedtokens  def tokenize  text   global inputid  token inputids  tokenizer  text  returntensors  pt   inputids token   tokenizerdecode   tokenid   strip   tokenid inputids  0   len  tokens   2  reorderabletokens  token  1  1  else  reorderabletokens    output  outputclearoutput  wait  true  tagsinputallowedtag  reorderabletokens tagsinputvalue  reorderabletokens updatepermutation  tagsinputvalue  def ontagschange  change   len  change   new      len  tagsinputallowedtag   tagsinputvalue  tagsinputallowedtag  restore original value def maskedprediction  inputid  permutation  permuteinput  permuteencoding   output  outputclearoutput  wait  true   clear previous output inputidsnumel     0  print    can not use empty sequence prediction   return model  bertformaskedlmfrompretraine    bert  base  uncased   permuteencoding  modelbertembeddingspositionembeddingsweightdata  permutecolumns  modelbertembeddingspositionembeddingsweight  t  permutation  t permuteinput  inputids  permutecolumns  inputids  permutation  decodedtext  tokenizerdecode  inputid  0   skipspecialtoken  false  torchnograd    output  model  inputids  logit  outputslogit topk  5 masktokenindice  torchwhere  inputid   tokenizermasktokenid   1  print  decodedtext  masktokenindice  permutation  nummask  len  masktokenindice  nummask   0  print    need include  mask  token prediction   return fig  axs  pltsubplot  1  nummask  figsize  15  6   nummask   1  axs   axs   idx enumerate  masktokenindice   masktokenlogit  logit  0  idx    softmaxprob  fsoftmax  masktokenlogits  dim0  toptokenprobs  toptokenid  torchtopk  softmaxprob  topk  dim0  predictedtoken   tokenizerdecode   tokenid   strip   tokenid toptokenids  predictedconfidences  toptokenprobstolist   axs   bar  predictedtokens  predictedconfidences  colorblue   axs   setxlabel   predict tokens   axs   setylabel   confidence   axs   settitle  fmaske token position  idxitem      axs   setylim  0  1  pltshow   def onpredictbuttonclick  b   maskedprediction  inputid  permutation  permuteinputcheckboxvalue  permuteencodingcheckboxvalue  textinput  widget  text  placeholderwrite text encode    descriptioninput    textinputobserve  lambda change  tokenize  change   new     namesvalue   tagsinput  widget  tagsinput  value    allowedtags    allowduplicate  false   observe change tag order update permutation prevent deletion tagsinputobserve  ontagschange  namesvalue   tagsinputobserve  lambda change  updatepermutation  change   new     namesvalue    create checkboxe permuteinput permuteencoding permuteinputcheckbox  widget  checkbox  value  false  descriptionpermute inputs   permuteencodingcheckbox  widget  checkbox  value  false  descriptionpermute encodings    create button trigger prediction predictbutton  widget  button  description  run prediction   predictbuttononclick  onpredictbuttonclick   display widget display  textinput  display  tagsinput  display  permuteinputcheckbox  display  permuteencodingcheckbox  display  predictbutton  display  output ,model input token id position id  four scenario consider  baseline  correct order token position permute position id permute token id permute position id token ids correct scenario 1 4 produce result  however incorrect assuming permute tokens position separately give result  consider   give  token   0  1  2  position   0  1  2  permutation   2  0  1   ex1  permute token position  2  0  1   permute token  0  1  2   standard position  ex2  permute position token  0  1  2   standard token  2  0  1   permute position ex1  model tell token 2 occur position 0  ex2  model tell token 2 occur position 1  even though use permutation  mapping token position different  result different model output  reason sometimes see result line  random chance  sample permutation result token  position embedding line way  mostly way  permute one  luck  average case produce different result  simple test  huggingface model take positionids input parameter  use test permutation input id without mess weight matrix  test  will create input datum  permute need  compute logit compare logit  compare logit  permute depermute need compare token token basis  example token scenario 1 permute token j scenario 3  want compare logits scenario 1 logit j scenario 3  import torch transformer import bertformaskedlm  autotokenizer def getlogit  input   torchnograd    output  model    input  logit  outputslogit return logit def permuteinput  input  permutation  permuteid  true  permuteposition  true   output    k  v inputsitem    kpositionid  permuteposition  output  k   v  permutation  elif k    positionid  permuteid  output  k   v    permutation  else  output  k   v return output  load tokenizer  model tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  bertformaskedlmfrompretraine    bert  base  uncased   modeleval    remember set model eval  create input ids position ids input  tokenizer   input text test sequence   returntensorspt   input   positionid    torchtensor  list  range  input   inputids   shape  1      create permutation tensor permutation  torchrandperm  input   inputids   shape  1    compute scenario data datum    s1     scenario 1  baseline  input   input   permutedid   false    s2     scenario 2  permute position  input   permuteinput  input  permutation  permuteid  false  permuteposition  true    permutedid   false    s3     scenario 3  permute token id  input   permuteinput  input  permutation  permuteid  true  permuteposition  false    permutedid   true    s4     scenario 4  permute token position  input   permuteinput  input  permutation    permutedid   true    compute logits k  v dataitem    v   logit    getlogit  v   input    comparison     s1    s2      s1    s3      s1    s4      s2    s3      s2    s4      s3    s4      compare scenario sa  sb comparison  dataa  datum  sa  datab  datum  sb  logitsa  dataa   logit   logitsb  datab   logit   dataa   permutedid     datab   permutedid     either logit permute logit unpermuted  compare directly val   logitsa  logitsb  abs   mean   elif dataa   permutedid    datab   permutedid        permuted  b   permute  b  make token line val   logitsa  logitsb    permutation   abs   mean   else   otherwise permute  b  make token line val   logitsa    permutation   logitsb  abs   mean   print  f  comparison  sa    sb    valitem    6f    code produce output like  comparison s1  s2  1407895 comparison s1  s3  1583560 comparison s1  s4  0000003 comparison s2  s3  1750883 comparison s2  s4  1407894 comparison s3  s4  1583560 run code bunch time  find s1  s4 comparison always small deviation  permute token position together always produce result  ignore small deviation cause numeric issue  find s2  s3 comparison generally large deviation  sometimes small deviation  discuss  due get lucky permutation position id mostly line ,  39  permute positional encoding bert affect output expect  work jupyter notebook transformers  section positional encoding  want demonstrate transformer rely entirely positional encode understand order sequence  previously learn another question post concept apply model not use mask attention  like gpt2  however  attempt approach bert model  use cross  attention  predict  mask  token  encounter unexpected result  expect happen  permutation cause model predict different token  ie  distribution consistent vocabulary  permute input id return distribution b permuting positional embedding return distribution b permuting input id positional embedding return distribution  actually happen  sometimes result align expectation  time  permute one aspect  either input id positional embedding  lead different outcome  even though occasionally  produce result  question  something else hug face s bert model might influence position  beyond positional encoding  completeness  include full code part notebook  try directly  important part happen maskedprediction  import torch import ipywidget widget ipythondisplay import display transformer import bertformaskedlm  autotokenizer import matplotlibpyplot plt import torchnnfunctional f  surpress rename warning logginggetlogger    transformersmodelingutil   setlevel  log  error  warningssimplefilter    ignore   futurewarning  tokenizer  autotokenizerfrompretraine    bert  base  uncased   inputid  torch  tensor       token    permutation    output  widget  output   def permutecolumn  matrix  permutation  none   n  len  permutation  firstncolumns  matrix     n  permutedcolumn  firstncolumns    permutation  remainingcolumn  matrix    n   newmatrix  torchhstack   permutedcolumn  remainingcolumns   return newmatrix def updatepermutation  orderedtags   global permutation fixedtokens   token  0    orderedtag   token  1   permutation   tokensindex  tag  tag fixedtokens  def tokenize  text   global inputid  token inputids  tokenizer  text  returntensors  pt   inputids token   tokenizerdecode   tokenid   strip   tokenid inputids  0   len  tokens   2  reorderabletokens  token  1  1  else  reorderabletokens    output  outputclearoutput  wait  true  tagsinputallowedtag  reorderabletokens tagsinputvalue  reorderabletokens updatepermutation  tagsinputvalue  def ontagschange  change   len  change   new      len  tagsinputallowedtag   tagsinputvalue  tagsinputallowedtag  restore original value def maskedprediction  inputid  permutation  permuteinput  permuteencoding   output  outputclearoutput  wait  true   clear previous output inputidsnumel     0  print    can not use empty sequence prediction   return model  bertformaskedlmfrompretraine    bert  base  uncased   permuteencoding  modelbertembeddingspositionembeddingsweightdata  permutecolumns  modelbertembeddingspositionembeddingsweight  t  permutation  t permuteinput  inputids  permutecolumns  inputids  permutation  decodedtext  tokenizerdecode  inputid  0   skipspecialtoken  false  torchnograd    output  model  inputids  logit  outputslogit topk  5 masktokenindice  torchwhere  inputid   tokenizermasktokenid   1  print  decodedtext  masktokenindice  permutation  nummask  len  masktokenindice  nummask   0  print    need include  mask  token prediction   return fig  axs  pltsubplot  1  nummask  figsize  15  6   nummask   1  axs   axs   idx enumerate  masktokenindice   masktokenlogit  logit  0  idx    softmaxprob  fsoftmax  masktokenlogits  dim0  toptokenprobs  toptokenid  torchtopk  softmaxprob  topk  dim0  predictedtoken   tokenizerdecode   tokenid   strip   tokenid toptokenids  predictedconfidences  toptokenprobstolist   axs   bar  predictedtokens  predictedconfidences  colorblue   axs   setxlabel   predict tokens   axs   setylabel   confidence   axs   settitle  fmaske token position  idxitem      axs   setylim  0  1  pltshow   def onpredictbuttonclick  b   maskedprediction  inputid  permutation  permuteinputcheckboxvalue  permuteencodingcheckboxvalue  textinput  widget  text  placeholderwrite text encode    descriptioninput    textinputobserve  lambda change  tokenize  change   new     namesvalue   tagsinput  widget  tagsinput  value    allowedtags    allowduplicate  false   observe change tag order update permutation prevent deletion tagsinputobserve  ontagschange  namesvalue   tagsinputobserve  lambda change  updatepermutation  change   new     namesvalue    create checkboxe permuteinput permuteencoding permuteinputcheckbox  widget  checkbox  value  false  descriptionpermute inputs   permuteencodingcheckbox  widget  checkbox  value  false  descriptionpermute encodings    create button trigger prediction predictbutton  widget  button  description  run prediction   predictbuttononclick  onpredictbuttonclick   display widget display  textinput  display  tagsinput  display  permuteinputcheckbox  display  permuteencodingcheckbox  display  predictbutton  display  output  model input token id position id  four scenario consider  baseline  correct order token position permute position id permute token id permute position id token ids correct scenario 1 4 produce result  however incorrect assuming permute tokens position separately give result  consider   give  token   0  1  2  position   0  1  2  permutation   2  0  1   ex1  permute token position  2  0  1   permute token  0  1  2   standard position  ex2  permute position token  0  1  2   standard token  2  0  1   permute position ex1  model tell token 2 occur position 0  ex2  model tell token 2 occur position 1  even though use permutation  mapping token position different  result different model output  reason sometimes see result line  random chance  sample permutation result token  position embedding line way  mostly way  permute one  luck  average case produce different result  simple test  huggingface model take positionids input parameter  use test permutation input id without mess weight matrix  test  will create input datum  permute need  compute logit compare logit  compare logit  permute depermute need compare token token basis  example token scenario 1 permute token j scenario 3  want compare logits scenario 1 logit j scenario 3  import torch transformer import bertformaskedlm  autotokenizer def getlogit  input   torchnograd    output  model    input  logit  outputslogit return logit def permuteinput  input  permutation  permuteid  true  permuteposition  true   output    k  v inputsitem    kpositionid  permuteposition  output  k   v  permutation  elif k    positionid  permuteid  output  k   v    permutation  else  output  k   v return output  load tokenizer  model tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  bertformaskedlmfrompretraine    bert  base  uncased   modeleval    remember set model eval  create input ids position ids input  tokenizer   input text test sequence   returntensorspt   input   positionid    torchtensor  list  range  input   inputids   shape  1      create permutation tensor permutation  torchrandperm  input   inputids   shape  1    compute scenario data datum    s1     scenario 1  baseline  input   input   permutedid   false    s2     scenario 2  permute position  input   permuteinput  input  permutation  permuteid  false  permuteposition  true    permutedid   false    s3     scenario 3  permute token id  input   permuteinput  input  permutation  permuteid  true  permuteposition  false    permutedid   true    s4     scenario 4  permute token position  input   permuteinput  input  permutation    permutedid   true    compute logits k  v dataitem    v   logit    getlogit  v   input    comparison     s1    s2      s1    s3      s1    s4      s2    s3      s2    s4      s3    s4      compare scenario sa  sb comparison  dataa  datum  sa  datab  datum  sb  logitsa  dataa   logit   logitsb  datab   logit   dataa   permutedid     datab   permutedid     either logit permute logit unpermuted  compare directly val   logitsa  logitsb  abs   mean   elif dataa   permutedid    datab   permutedid        permuted  b   permute  b  make token line val   logitsa  logitsb    permutation   abs   mean   else   otherwise permute  b  make token line val   logitsa    permutation   logitsb  abs   mean   print  f  comparison  sa    sb    valitem    6f    code produce output like  comparison s1  s2  1407895 comparison s1  s3  1583560 comparison s1  s4  0000003 comparison s2  s3  1750883 comparison s2  s4  1407894 comparison s3  s4  1583560 run code bunch time  find s1  s4 comparison always small deviation  permute token position together always produce result  ignore small deviation cause numeric issue  find s2  s3 comparison generally large deviation  sometimes small deviation  discuss  due get lucky permutation position id mostly line ,Basic Understanding
How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?,"I'm working with the OpenAIEmbeddings() class from OpenAI , which uses the text-embedding-3-small model. According to the documentation , it generates a 1536-dimensional vector for any input text. However, I'm a bit confused about how this works: Is the 1536-dimensional vector generated for the entire input text? If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? I was expecting this: If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536. But the output is a single vector of size 1536 for the whole input text. Why I expected this? Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings? I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input. Any insights or examples would be greatly appreciated!","['deep-learning', 'nlp', 'openai-api', 'openaiembeddings']",1,"Everything you described is 100% expected. Q: Is the 1536-dimensional vector generated for the entire input text? A: Yes. Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., ""a""), but it doesn't make sense to calculate an embedding vector out of it since ""a"" doesn't semantically mean anything to us humans. Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you use them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes! This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences). There's an excellent Stack Overflow blog post about this topic you should read (pay attention to the bolded text because this is the best explanation): With RAG, you create text embeddings of the pieces of data that you want to draw from and retrieve. That allows you to place a piece of the source text within the semantic space that LLMs use to create responses. /.../ When it comes to RAG systems, you’ll need to pay special attention to how big the individual pieces of data are. How you divide your data up is called chunking, and it’s more complex than embedding whole documents. /.../ The size of the chunked data is going to make a huge difference in what information comes up in a search. When you embed a piece of data, the whole thing is converted into a vector. Include too much in a chunk and the vector loses the ability to be specific to anything it discusses. Include too little and you lose the context of the data.",2024-08-22 14:09:25,2024-08-22 14:37:28,619,https://stackoverflow.com/questions/78901998/how-does-openaiembeddings-work-is-it-creating-a-single-vector-of-size-1536-fo,"How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus? I'm working with the OpenAIEmbeddings() class from OpenAI , which uses the text-embedding-3-small model. According to the documentation , it generates a 1536-dimensional vector for any input text. However, I'm a bit confused about how this works: Is the 1536-dimensional vector generated for the entire input text? If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? I was expecting this: If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536. But the output is a single vector of size 1536 for the whole input text. Why I expected this? Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings? I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input. Any insights or examples would be greatly appreciated!",openaiembedding   work  create single vector size 1536 whole text corpus   m work openaiembeddings   class openai  use text  embedding3  small model  accord documentation  generate 1536  dimensional vector input text  however   m bit confused work  1536  dimensional vector generate entire input text  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph  expect  100 word input text  expect openaiembeddings   would output 100 vector  size 1536  output single vector size 1536 whole input text  expect  learn   ve understand embedding like word2vec glove provide vector word corpus  differ approach take openaiembeddings   m try understand whether s way extract embedding individual word use model output always single vector represent whole input  insight example would greatly appreciate ,everything describe 100  expect  q  1536  dimensional vector generate entire input text   yes  q  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph   first  openai embeddings model not handle single word different long text  model  s input  input even single character  eg       not make sense calculate embed vector since    not semantically mean anything we human  second  probably mean question happen similarity search embedding  word  happen use  happen use embedding word  sentence  paragraph  whole text  matter  yes  call chunk  decision chunk text depend use case  good thing probably simply try see  get meaningful result similarity search  mean chunk appropriate  even mean chunk whole text   not get meaningful result similarity search  mean chunk not appropriate  eg  instead chunk paragraph  try chunk sentence   s excellent stack overflow blog post topic read  pay attention bolde text good explanation   rag  create text embedding piece datum want draw retrieve  allow place piece source text within semantic space llm use create response     come rag system   need pay special attention big individual piece datum  divide datum call chunk   complex embed whole document     size chunk datum go make huge difference information come search  embed piece datum  whole thing convert vector  include much chunk vector lose ability specific anything discuss  include little lose context datum ,openaiembedding   work  create single vector size 1536 whole text corpus   m work openaiembeddings   class openai  use text  embedding3  small model  accord documentation  generate 1536  dimensional vector input text  however   m bit confused work  1536  dimensional vector generate entire input text  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph  expect  100 word input text  expect openaiembeddings   would output 100 vector  size 1536  output single vector size 1536 whole input text  expect  learn   ve understand embedding like word2vec glove provide vector word corpus  differ approach take openaiembeddings   m try understand whether s way extract embedding individual word use model output always single vector represent whole input  insight example would greatly appreciate  everything describe 100  expect  q  1536  dimensional vector generate entire input text   yes  q  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph   first  openai embeddings model not handle single word different long text  model  s input  input even single character  eg       not make sense calculate embed vector since    not semantically mean anything we human  second  probably mean question happen similarity search embedding  word  happen use  happen use embedding word  sentence  paragraph  whole text  matter  yes  call chunk  decision chunk text depend use case  good thing probably simply try see  get meaningful result similarity search  mean chunk appropriate  even mean chunk whole text   not get meaningful result similarity search  mean chunk not appropriate  eg  instead chunk paragraph  try chunk sentence   s excellent stack overflow blog post topic read  pay attention bolde text good explanation   rag  create text embedding piece datum want draw retrieve  allow place piece source text within semantic space llm use create response     come rag system   need pay special attention big individual piece datum  divide datum call chunk   complex embed whole document     size chunk datum go make huge difference information come search  embed piece datum  whole thing convert vector  include much chunk vector lose ability specific anything discuss  include little lose context datum ,Implementation Issues
"NER versus LLM to extract name, gender, role and company from text","I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons. I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them. Is there another, smaller LLM that might be good at this while using fewer processing resources? Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.) Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass? Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction. Thanks in advance!","['nlp', 'large-language-model', 'named-entity-recognition']",2,"Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. NER is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use NLTK or SpaCy for this matter. My personal choice is SpaCy , however a gender as you defined is not a known named entity. you can see a list of named entities in this doc . I guess what you mean by gender is the possible gender associated with the names of a PERSON mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use gender-guesser package . A possible solution would be like this: import spacy import gender_guesser.detector as gender nlp = spacy.load(""en_core_web_sm"") def extract_info(text): doc = nlp(text) gender_detector = gender.Detector() for ent in doc.ents: if ent.label_ == ""PERSON"": name = ent.text name_gender = gender_detector.get_gender(name) return doc.ents, name_gender Note that en_core_web_sm is the small model available via spaCy, you can use the large model by specifying en_core_web_lg , just make sure that the model is downloaded before running your code. here's how you can download the model: python -m spacy download en_core_web_sm",2024-08-21 07:39:13,2024-08-21 09:13:54,1628,https://stackoverflow.com/questions/78895710/ner-versus-llm-to-extract-name-gender-role-and-company-from-text,"NER versus LLM to extract name, gender, role and company from text I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons. I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them. Is there another, smaller LLM that might be good at this while using fewer processing resources? Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.) Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass? Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction. Thanks in advance!",ner versus llm extract name  gender  role company text need extract name  gender  job title employer  company name newspaper article  running process local hardware  cloud allow  due copyright reason   ve play around llama 31  m finding not get useable result model small 70b parameter  size model run much slowly good hardware throw  another  small llm might good use few processing resource  ner use extract datum  ner  ve look extract name gender   not know extract datum gender showstopper   alternatively  approach take first pass ner  pass name llm together original newspaper article extract datum  get well result  fast single llm pass  answer training model  good model use starting point   m much begin machine learn journey would love point right direction  thank advance ,apart limitation  would not recommend use llms like llamma 31 task  ner one classic task nlp small language model tool incorporate achieve goal  use nltk spacy matter  personal choice spacy  however gender define know name entity  see list name entity doc  guess mean gender possible gender associate name person mention article  python package use lookup gender  however  note ambiguous substantial tolerance error  use gender  guesser package  possible solution would like  import spacy import genderguesserdetector gender nlp  spacyload    encorewebsm   def extractinfo  text   doc  nlp  text  genderdetector  gender  detector   ent docent  entlabel      person   name  enttext namegender  genderdetectorgetgender  name  return docent  namegender note encorewebsm small model available via spacy  use large model specify encoreweblg  make sure model download run code  s download model  python m spacy download encorewebsm,ner versus llm extract name  gender  role company text need extract name  gender  job title employer  company name newspaper article  running process local hardware  cloud allow  due copyright reason   ve play around llama 31  m finding not get useable result model small 70b parameter  size model run much slowly good hardware throw  another  small llm might good use few processing resource  ner use extract datum  ner  ve look extract name gender   not know extract datum gender showstopper   alternatively  approach take first pass ner  pass name llm together original newspaper article extract datum  get well result  fast single llm pass  answer training model  good model use starting point   m much begin machine learn journey would love point right direction  thank advance  apart limitation  would not recommend use llms like llamma 31 task  ner one classic task nlp small language model tool incorporate achieve goal  use nltk spacy matter  personal choice spacy  however gender define know name entity  see list name entity doc  guess mean gender possible gender associate name person mention article  python package use lookup gender  however  note ambiguous substantial tolerance error  use gender  guesser package  possible solution would like  import spacy import genderguesserdetector gender nlp  spacyload    encorewebsm   def extractinfo  text   doc  nlp  text  genderdetector  gender  detector   ent docent  entlabel      person   name  enttext namegender  genderdetectorgetgender  name  return docent  namegender note encorewebsm small model available via spacy  use large model specify encoreweblg  make sure model download run code  s download model  python m spacy download encorewebsm,Library/Tool-Based Queries
Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?,"In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%). I am curious about the following: When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively? Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact? Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%? Thank you for your insights!","['pytorch', 'nlp', 'huggingface-transformers', 'transformer-model']",1,"Attention is computed on a tensor of shape (batch_size, sequence_length, embedding_dimension) . The compute and memory requirements scale with the size of those dimensions. For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size. With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length 8 and 10 sequences of length 128 . Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of 128 . If you sort by length before batching, you get one batch with sequence length of 8 and another with length 128 . The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128). That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to ""skip over"" padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.",2024-08-19 11:49:06,2024-08-20 02:25:26,544,https://stackoverflow.com/questions/78887743/does-padding-in-a-batch-of-sequences-affect-performance-how-effective-is-the-at,"Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask? In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%). I am curious about the following: When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively? Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact? Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%? Thank you for your insights!",padding batch sequences affect performance  effective attention mask  transformer model  sequence variable length typically pad maximum length batch  however  sequence length vary significantly  batch may contain substantial amount padding  potentially 50    curious following  pytorch compute transformer  pad token impact calculation speed negatively  presence attention mask allow model effectively skip padding token  result minimal performance impact  overall  effective attention mask  sparse attention mask 10  non  zero value  computation effectively reduce approximately 10   thank insight ,attention compute tensor shape  batchsize  sequencelength  embeddingdimension   compute memory requirement scale size dimension  input fix size  percent padding impact performance  minor overhead apply padding mask  ie padding mask save one mask fill operation   x  padding  padding be go see difference  overall compute requirement set tensor size  respect batching sequence  add inefficiency batch together sequence wildly different length  say 10 sequence length 8 10 sequence length 128  pad batch sequence two batch  mix length evenly  get two batch sequence length 128  sort length batching  get one batch sequence length 8 another length 128  first case  two batch sequence length 128  require overall compute compare second case  one batch 8  one 128   say  fix input size  not go see performance change percent padding  way attention operation   skip  pad token  conditional control flow require sort approach not work well way gpu execute operation parallel  effect padding mask assign 0 attention weight padding token ,padding batch sequences affect performance  effective attention mask  transformer model  sequence variable length typically pad maximum length batch  however  sequence length vary significantly  batch may contain substantial amount padding  potentially 50    curious following  pytorch compute transformer  pad token impact calculation speed negatively  presence attention mask allow model effectively skip padding token  result minimal performance impact  overall  effective attention mask  sparse attention mask 10  non  zero value  computation effectively reduce approximately 10   thank insight  attention compute tensor shape  batchsize  sequencelength  embeddingdimension   compute memory requirement scale size dimension  input fix size  percent padding impact performance  minor overhead apply padding mask  ie padding mask save one mask fill operation   x  padding  padding be go see difference  overall compute requirement set tensor size  respect batching sequence  add inefficiency batch together sequence wildly different length  say 10 sequence length 8 10 sequence length 128  pad batch sequence two batch  mix length evenly  get two batch sequence length 128  sort length batching  get one batch sequence length 8 another length 128  first case  two batch sequence length 128  require overall compute compare second case  one batch 8  one 128   say  fix input size  not go see performance change percent padding  way attention operation   skip  pad token  conditional control flow require sort approach not work well way gpu execute operation parallel  effect padding mask assign 0 attention weight padding token ,Implementation Issues
SpaCy Matcher with optional suffix in pattern reports multiple matches on same text,"Using the following Matcher rule: {'label': 'R-1', 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}], 'greedy': 'LONGEST', } on the text: 'MyLabel: Some Value' I get two matches: 'MyLabel' and 'MyLabel:' For me, that was quite surprising - I was expecting a single match on 'MyLabel:'. Adding the new greedy flag didn't make any difference. Is this the intended behavior or is it a bug? How should I determine that the second match really is just a subset of the first match? Will the shorter match always be reported before the longer match? SpaCy version 3.7.5","['nlp', 'spacy', 'matcher']",1,"i will say that the behavior you're observing with the SpaCy Matcher is expected, and it is not a bug. When you use the {'TEXT': ':', 'OP': '?'} pattern, the OP: '?' operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen. Explanation: Pattern : {'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'} . Text : 'MyLabel: Some Value' . So for this pattern, SpaCy will try to match: 'MyLabel' alone (because the colon is optional). 'MyLabel:' (because the colon can be included). Therefore, you will get two matches: 'MyLabel' and 'MyLabel:' . Now to Answer Your Questions: Is this the intended behavior or is it a bug? This is intended behavior. The OP: '?' operator allows the colon to be optionally matched, leading to multiple matches. How should I determine that the second match really is just a subset of the first match? To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below pip show spacy Name: spacy Version: 3.7.5 Summary: Industrial-strength Natural Language Processing (NLP) in Python Home-page: https://spacy.io Author: Explosion Author-email: contact@explosion.ai License: MIT Location: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel Required-by: en-core-web-sm Now Example in code: import spacy from spacy.matcher import Matcher nlp = spacy.load(""en_core_web_sm"") doc = nlp(""MyLabel: Some Value"") matcher = Matcher(nlp.vocab) pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}] matcher.add(""R-1"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: span = doc[start:end] print(f""Match: {span.text}, Start: {start}, End: {end}"") # Now, we Determine if one match is a subset of another matches.sort(key=lambda x: (x[1], -x[2])) # Sort by start index, then by end index descending filtered_matches = [] last_end = -1 for match_id, start, end in matches: if start >= last_end: # This is for Avoiding adding subsets filtered_matches.append((match_id, start, end)) last_end = end for match_id, start, end in filtered_matches: span = doc[start:end] print(f""Filtered Match: {span.text}"") Now, This code will filter out the shorter match and your output will be Match: MyLabel, Start: 0, End: 1 Match: MyLabel:, Start: 0, End: 2 Filtered Match: MyLabel: , you can see MYLabel: with the colon symbol there Now Will the shorter match always be reported before the longer match? I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches. Another Alternative Solution: If you want to ensure that only the longest match is returned, you can change the way you define the pattern: pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}] note that the greedy flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings. Now back to the Summary of what i explained: The behavior you're seeing is by design, due to the optional OP: '?' operator. in addition, you can filter out the shorter match by comparing start and end indices of the matches. furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.",2024-08-13 09:37:23,2024-08-14 12:27:48,37,https://stackoverflow.com/questions/78865486/spacy-matcher-with-optional-suffix-in-pattern-reports-multiple-matches-on-same-t,"SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule: {'label': 'R-1', 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}], 'greedy': 'LONGEST', } on the text: 'MyLabel: Some Value' I get two matches: 'MyLabel' and 'MyLabel:' For me, that was quite surprising - I was expecting a single match on 'MyLabel:'. Adding the new greedy flag didn't make any difference. Is this the intended behavior or is it a bug? How should I determine that the second match really is just a subset of the first match? Will the shorter match always be reported before the longer match? SpaCy version 3.7.5",spacy matcher optional suffix pattern report multiple match text use follow matcher rule    label    r1    pattern      text    mylabel      text        op          greedy    longest    text   mylabel  value  get two match   mylabel   mylabel    quite surprising  expect single match  mylabel    add new greedy flag not make difference  intend behavior bug  determine second match really subset first match  short match always report long match  spacy version 375,say behavior be observe spacy matcher expect  bug  use   text        op       pattern  op     operator mean colon optional  matcher generate shorter long match   ve see  explanation  pattern    text    mylabel      text        op        text   mylabel  value   pattern  spacy try match   mylabel  alone  colon optional    mylabel    colon include   therefore  get two match   mylabel   mylabel    answer questions  intend behavior bug  intend behavior  op     operator allow colon optionally match  lead multiple match  determine second match really subset first match  determine one match subset another  compare start end index match  long match start index different end index  write code even use spacy version 375  see detail pip show spacy name  spacy version  375 summary  industrial  strength natural language processing  nlp  python home  page    spacyio author  explosion author  email  contact  explosionai license  mit location  home  adesoji  download  visis  backend  assessment  adesoji  visisenv  lib  python311  site  package require  catalogue  cymem  jinja2  langcode  murmurhash  numpy  packaging  preshed  pydantic  request  setuptool  spacy  legacy  spacy  logger  srsly  thinc  tqdm  typer  wasabi  weasel required  by  en  core  web  sm example code  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   doc  nlp    mylabel  value   matcher  matcher  nlpvocab  pattern     text    mylabel      text        op        matcheradd    r1    pattern   match  matcher  doc  matchid  start  end match  span  doc  start  end  print  f  match   spantext   start   start   end   end      determine one match subset another matchessort  key  lambda x   x  1   x  2     sort start index  end index descend filteredmatche    lastend  1 matchid  start  end match  start   lastend   avoid add subset filteredmatchesappend   matchid  start  end   lastend  end matchid  start  end filteredmatches  span  doc  start  end  print  f  filtered match   spantext     code filter short match output match  mylabel  start  0  end  1 match  mylabel   start  0  end  2 filter match  mylabel   see mylabel  colon symbol short match always report long match  not think match guarantee report specific order  handle  sort match start end index show code example above  now  sort  filter match subset long match  another alternative solution  want ensure long match return  change way define pattern  pattern     text    mylabel      text        op        greedy    longest    note greedy flag not change behavior match rather influence overlaps handle certain custom setting  back summary explain  behavior be see design  due optional op     operator  addition  filter short match compare start end index match  furthermore  sort match start end index allow keep long  non  overlapping match ,spacy matcher optional suffix pattern report multiple match text use follow matcher rule    label    r1    pattern      text    mylabel      text        op          greedy    longest    text   mylabel  value  get two match   mylabel   mylabel    quite surprising  expect single match  mylabel    add new greedy flag not make difference  intend behavior bug  determine second match really subset first match  short match always report long match  spacy version 375 say behavior be observe spacy matcher expect  bug  use   text        op       pattern  op     operator mean colon optional  matcher generate shorter long match   ve see  explanation  pattern    text    mylabel      text        op        text   mylabel  value   pattern  spacy try match   mylabel  alone  colon optional    mylabel    colon include   therefore  get two match   mylabel   mylabel    answer questions  intend behavior bug  intend behavior  op     operator allow colon optionally match  lead multiple match  determine second match really subset first match  determine one match subset another  compare start end index match  long match start index different end index  write code even use spacy version 375  see detail pip show spacy name  spacy version  375 summary  industrial  strength natural language processing  nlp  python home  page    spacyio author  explosion author  email  contact  explosionai license  mit location  home  adesoji  download  visis  backend  assessment  adesoji  visisenv  lib  python311  site  package require  catalogue  cymem  jinja2  langcode  murmurhash  numpy  packaging  preshed  pydantic  request  setuptool  spacy  legacy  spacy  logger  srsly  thinc  tqdm  typer  wasabi  weasel required  by  en  core  web  sm example code  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   doc  nlp    mylabel  value   matcher  matcher  nlpvocab  pattern     text    mylabel      text        op        matcheradd    r1    pattern   match  matcher  doc  matchid  start  end match  span  doc  start  end  print  f  match   spantext   start   start   end   end      determine one match subset another matchessort  key  lambda x   x  1   x  2     sort start index  end index descend filteredmatche    lastend  1 matchid  start  end match  start   lastend   avoid add subset filteredmatchesappend   matchid  start  end   lastend  end matchid  start  end filteredmatches  span  doc  start  end  print  f  filtered match   spantext     code filter short match output match  mylabel  start  0  end  1 match  mylabel   start  0  end  2 filter match  mylabel   see mylabel  colon symbol short match always report long match  not think match guarantee report specific order  handle  sort match start end index show code example above  now  sort  filter match subset long match  another alternative solution  want ensure long match return  change way define pattern  pattern     text    mylabel      text        op        greedy    longest    note greedy flag not change behavior match rather influence overlaps handle certain custom setting  back summary explain  behavior be see design  due optional op     operator  addition  filter short match compare start end index match  furthermore  sort match start end index allow keep long  non  overlapping match ,Task-Specific Queries
`mlflow.transformers.log_model()` does not finish,"Problem I want to use mlflow.transformers.log_model() to log a finetuned huggingface model. However, when the mlflow.transformers.log_model method is running, it simply does not finish - runs forever - throws no errors. I suspect my configuration is not right, the model is too big? The output says Skipping saving pretrained model weights to disk so that should not be the problem. Any ideas how to do this properly? Example This is more or less how my setup looks like, you cannot run this, it includes some pseudocode... I am on python 3.11.9 with transformers = ""^4.41.2"" & mlflow = ""^2.15.1"" . import mlflow import torch from peft import LoraConfig from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, ) from trl import SFTTrainer, setup_chat_format train_dataset = ... eval_dataset = ... model_id = ""LeoLM/leo-hessianai-7b-chat-bilingual"" # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( model_id, device_map=""auto"", torch_dtype=torch.bfloat16, quantization_config=bnb_config, ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True) model, tokenizer = setup_chat_format(model, tokenizer) peft_config = LoraConfig(...) args = TrainingArguments(...) # Define Trainer trainer = SFTTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, peft_config=peft_config, tokenizer=tokenizer, packing=True, ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! This function all does not finish... !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", ) The last output I get in the console is: INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead. Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'} /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning. warnings.warn( 2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead. /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils. warnings.warn(""Setuptools is replacing distutils."")","['python', 'nlp', 'huggingface-transformers', 'mlflow', 'mlops']",1,"Before defining the trainer, the model has be turned into a Peft model object via get_peft_model , then the mlflow.transformers.log_model works: from peft import LoraConfig, get_peft_model model = ... peft_config = LoraConfig(...) args = TrainingArguments(...) peft_model = get_peft_model(model, peft_config) trainer = SFTTrainer( model=peft_model, args=args, ... ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! Now the logginig of the model works, we can find it in the artifacts !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", )",2024-08-12 16:27:32,2024-08-16 07:14:17,353,https://stackoverflow.com/questions/78862691/mlflow-transformers-log-model-does-not-finish,"`mlflow.transformers.log_model()` does not finish Problem I want to use mlflow.transformers.log_model() to log a finetuned huggingface model. However, when the mlflow.transformers.log_model method is running, it simply does not finish - runs forever - throws no errors. I suspect my configuration is not right, the model is too big? The output says Skipping saving pretrained model weights to disk so that should not be the problem. Any ideas how to do this properly? Example This is more or less how my setup looks like, you cannot run this, it includes some pseudocode... I am on python 3.11.9 with transformers = ""^4.41.2"" & mlflow = ""^2.15.1"" . import mlflow import torch from peft import LoraConfig from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, ) from trl import SFTTrainer, setup_chat_format train_dataset = ... eval_dataset = ... model_id = ""LeoLM/leo-hessianai-7b-chat-bilingual"" # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( model_id, device_map=""auto"", torch_dtype=torch.bfloat16, quantization_config=bnb_config, ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True) model, tokenizer = setup_chat_format(model, tokenizer) peft_config = LoraConfig(...) args = TrainingArguments(...) # Define Trainer trainer = SFTTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, peft_config=peft_config, tokenizer=tokenizer, packing=True, ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! This function all does not finish... !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", ) The last output I get in the console is: INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead. Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'} /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning. warnings.warn( 2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead. /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils. warnings.warn(""Setuptools is replacing distutils."")", mlflowtransformerslogmodel    finish problem want use mlflowtransformerslogmodel   log finetune huggingface model  however  mlflowtransformerslogmodel method run  simply finish  run forever  throw error  suspect configuration right  model big  output say skipping save pretraine model weights disk problem  idea properly  example less setup look like  run  include pseudocode  python 3119 transformer    4412   mlflow    2151   import mlflow import torch peft import loraconfig transformer import  automodelforcausallm  autotokenizer  bitsandbytesconfig  trainingarguments   trl import sfttrainer  setupchatformat traindataset   evaldataset   modelid    leolm  leo  hessianai7b  chat  bilingual   load model tokenizer model  automodelforcausallmfrompretraine  modelid  devicemap  auto   torchdtype  torchbfloat16  quantizationconfig  bnbconfig   tokenizer  autotokenizerfrompretraine  modelid  tokenizernopad  autotokenizerfrompretraine  modelid  addbostoken  true  model  tokenizer  setupchatformat  model  tokenizer  peftconfig  loraconfig    args  trainingarguments     define trainer trainer  sfttrainer  model  model  arg  args  traindataset  traindataset  evaldataset  evaldataset  peftconfig  peftconfig  tokenizer  tokenizer  pack  true    mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       function finish     mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model    last output get console  info mlflowtransformer  overriding savepretrained false peft model  follow transformers behavior  peft adaptor config save  base model weights reference huggingface hub repository log instead  unrecognized key  ropescaling   ropetypelinear     type   mypath  llm4pa  open  sourcevenv  lib  python311  site  package  peft  util  saveandloadpy209  userwarning  set  saveembeddinglayer   true  embed layer resize finetune  warningswarn  20240812 182114 info mlflowtransformer  skip save pretraine model weights disk savepretrained set false  reference huggingface hub repository leolm  leo  hessianai7b  chat  bilingual log instead  mypath  llm4pa  open  sourcevenv  lib  python311  site  packagesdistutilshackinitpy26  userwarning  setuptool replace distutil  warningswarn    setuptool replace distutil   ,define trainer  model turn peft model object via getpeftmodel  mlflowtransformerslogmodel work  peft import loraconfig  getpeftmodel model   peftconfig  loraconfig    args  trainingarguments    peftmodel  getpeftmodel  model  peftconfig  trainer  sfttrainer  model  peftmodel  arg  args     mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       logginig model work  find artifact    mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model   , mlflowtransformerslogmodel    finish problem want use mlflowtransformerslogmodel   log finetune huggingface model  however  mlflowtransformerslogmodel method run  simply finish  run forever  throw error  suspect configuration right  model big  output say skipping save pretraine model weights disk problem  idea properly  example less setup look like  run  include pseudocode  python 3119 transformer    4412   mlflow    2151   import mlflow import torch peft import loraconfig transformer import  automodelforcausallm  autotokenizer  bitsandbytesconfig  trainingarguments   trl import sfttrainer  setupchatformat traindataset   evaldataset   modelid    leolm  leo  hessianai7b  chat  bilingual   load model tokenizer model  automodelforcausallmfrompretraine  modelid  devicemap  auto   torchdtype  torchbfloat16  quantizationconfig  bnbconfig   tokenizer  autotokenizerfrompretraine  modelid  tokenizernopad  autotokenizerfrompretraine  modelid  addbostoken  true  model  tokenizer  setupchatformat  model  tokenizer  peftconfig  loraconfig    args  trainingarguments     define trainer trainer  sfttrainer  model  model  arg  args  traindataset  traindataset  evaldataset  evaldataset  peftconfig  peftconfig  tokenizer  tokenizer  pack  true    mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       function finish     mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model    last output get console  info mlflowtransformer  overriding savepretrained false peft model  follow transformers behavior  peft adaptor config save  base model weights reference huggingface hub repository log instead  unrecognized key  ropescaling   ropetypelinear     type   mypath  llm4pa  open  sourcevenv  lib  python311  site  package  peft  util  saveandloadpy209  userwarning  set  saveembeddinglayer   true  embed layer resize finetune  warningswarn  20240812 182114 info mlflowtransformer  skip save pretraine model weights disk savepretrained set false  reference huggingface hub repository leolm  leo  hessianai7b  chat  bilingual log instead  mypath  llm4pa  open  sourcevenv  lib  python311  site  packagesdistutilshackinitpy26  userwarning  setuptool replace distutil  warningswarn    setuptool replace distutil    define trainer  model turn peft model object via getpeftmodel  mlflowtransformerslogmodel work  peft import loraconfig  getpeftmodel model   peftconfig  loraconfig    args  trainingarguments    peftmodel  getpeftmodel  model  peftconfig  trainer  sfttrainer  model  peftmodel  arg  args     mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       logginig model work  find artifact    mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model   ,Library/Tool-Based Queries
NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation),"I'm attempting to fine-tune the NLLB model ""facebook/nllb-200-distilled-600M"" for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb. Documentation: link This is the code block which is giving error: DATA_CONFIG = ""/content/sample_data/data_config.json"" OUTPUT_DIR = ""/content/outputs"" MODEL_FOLDER = ""/content/drive/MyDrive/Thesis/nllb-checkpoints"" DROP = 0.1 SRC = ""eng_Latn"" TGT = ""deu_Latn"" !python /content/fairseq/examples/nllb/modeling/train/train_script.py \ cfg=nllb200_dense3.3B_finetune_on_fbseed \ cfg/dataset=default \ cfg.dataset.lang_pairs=""$SRC-$TGT"" \ cfg.fairseq_root=$(pwd) \ cfg.output_dir=$OUTPUT_DIR \ cfg.dropout=$DROP \ cfg.warmup=10 \ cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt This is the error: /content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: The version_base parameter is not specified. Please specify a compatability version level, or None. Will assume defaults for version 1.1 @hydra.main(config_path=""conf"", config_name=""base_config"") /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default. See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information. ret = run_job( TRAINING DIR: /content/outputs Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt'] Traceback (most recent call last): File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main train_module = TrainModule(config) File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__ assert cluster_name in cfg.dataset.data_prefix omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct full_key: cfg.dataset.data_prefix object_type=dict Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace. So far, I understand there is a Missing data_prefix configuration . I created a demo custom data_config.json. Which looks like this: { ""data_prefix"": ""/content/sample_data"", ""train_data"": ""train_demo.json"", ""test_data"": ""test_demo.json"", ""lang_pairs"": ""eng_Latn-deu_Latn"" } While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?","['python', 'nlp', 'machine-translation', 'fine-tuning', 'fairseq']",1,"While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the transformers library, specifically using the Seq2SeqTrainer . I did this before for multiple models, including NLLB, check out this repository: https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/ This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so: tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=""eng_Latn"", tgt_lang=""deu_Latn"") and generate translations like this: model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(""deu_Latn"")[1], max_length=512)",2024-08-09 14:46:54,2024-08-09 21:05:47,159,https://stackoverflow.com/questions/78853409/nllb-fine-tuning-error-missing-data-prefix-configuration-english-german-transl,"NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation) I'm attempting to fine-tune the NLLB model ""facebook/nllb-200-distilled-600M"" for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb. Documentation: link This is the code block which is giving error: DATA_CONFIG = ""/content/sample_data/data_config.json"" OUTPUT_DIR = ""/content/outputs"" MODEL_FOLDER = ""/content/drive/MyDrive/Thesis/nllb-checkpoints"" DROP = 0.1 SRC = ""eng_Latn"" TGT = ""deu_Latn"" !python /content/fairseq/examples/nllb/modeling/train/train_script.py \ cfg=nllb200_dense3.3B_finetune_on_fbseed \ cfg/dataset=default \ cfg.dataset.lang_pairs=""$SRC-$TGT"" \ cfg.fairseq_root=$(pwd) \ cfg.output_dir=$OUTPUT_DIR \ cfg.dropout=$DROP \ cfg.warmup=10 \ cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt This is the error: /content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: The version_base parameter is not specified. Please specify a compatability version level, or None. Will assume defaults for version 1.1 @hydra.main(config_path=""conf"", config_name=""base_config"") /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default. See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information. ret = run_job( TRAINING DIR: /content/outputs Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt'] Traceback (most recent call last): File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main train_module = TrainModule(config) File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__ assert cluster_name in cfg.dataset.data_prefix omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct full_key: cfg.dataset.data_prefix object_type=dict Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace. So far, I understand there is a Missing data_prefix configuration . I created a demo custom data_config.json. Which looks like this: { ""data_prefix"": ""/content/sample_data"", ""train_data"": ""train_demo.json"", ""test_data"": ""test_demo.json"", ""lang_pairs"": ""eng_Latn-deu_Latn"" } While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?",nllb fine  tuning error  miss dataprefix configuration  english  german translation   m attempt fine  tune nllb model   facebook  nllb200  distilled600 m  scientific translation task english  englatn  german  deulatn   follow official guideline fine  tune author nllb  documentation  link code block give error  dataconfig    content  sampledata  dataconfigjson  outputdir    content  output  modelfolder    content  drive  mydrive  thesis  nllb  checkpoint  drop  01 src    englatn  tgt    deulatn   python content  fairseq  example  nllb  modeling  train  trainscriptpy  cfg  nllb200dense33bfinetuneonfbseed  cfg  dataset  default  cfgdatasetlangpairs   src  tgt   cfgfairseqroot   pwd   cfgoutputdir  outputdir  cfgdropout  drop  cfgwarmup10  cfgfinetunefrommodel  modelfolder  checkpointpt error  content  fairseq  example  nllb  modeling  train  trainscriptpy287  userwarning  versionbase parameter specify  please specify compatability version level  none  assume default version 11  hydramain  configpath  conf   configname  baseconfig   usr  local  lib  python310  dist  package  hydrainternal  hydrapy119  userwarning  future hydra version long change work directory job runtime default  see   hydracc  docs12  upgrades11to12  changestojobworkingdir information  ret  runjob  training dir  content  output error executing job override    cfg  nllb200dense33bfinetuneonfbseed    cfg  dataset  default    cfgdatasetlangpair  englatn  deulatn    cfgfairseqrootcontent    cfgoutputdircontent  output    cfgdropout01    cfgwarmup10    cfgfinetunefrommodelcontent  drive  mydrive  lasskgdata  thesis  nllb  checkpoint  checkpointpt   traceback  recent call last   file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 289  main trainmodule  trainmodule  config  file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 122    init   assert clustername cfgdatasetdataprefix omegaconferror  configattributeerror  key  dataprefix  struct fullkey  cfgdatasetdataprefix objecttype  dict set environment variable hydrafullerror1 complete stack trace  far  understand miss dataprefix configuration  create demo custom dataconfigjson  look like     dataprefix     content  sampledata     traindata     traindemojson     testdata     testdemojson     langpairs     englatn  deulatn   official documentation provide information   m encounter difficulty apply specific use case  someone share detailed guide point helpful resource fine  tune nllb ,can not help concrete error message get  guess would issue structure provide json file   personal recommendation would fine  tune nllb transformer library  specifically use seq2seqtrainer  multiple model  include nllb  check repository    githubcom  eliask93  transformer  model  for  domain  specific  machine  translation way fine  tune inference process nllb model bilingual model  find guide easiely   exception load tokenizer like  tokenizer  nllbtokenizerfrompretraine  modelpath  srclang  englatn   tgtlang  deulatn   generate translation like  modelgenerate  tokenizedchunkinputid  forcedbostokenid  tokenizerencode    deulatn    1   maxlength512 ,nllb fine  tuning error  miss dataprefix configuration  english  german translation   m attempt fine  tune nllb model   facebook  nllb200  distilled600 m  scientific translation task english  englatn  german  deulatn   follow official guideline fine  tune author nllb  documentation  link code block give error  dataconfig    content  sampledata  dataconfigjson  outputdir    content  output  modelfolder    content  drive  mydrive  thesis  nllb  checkpoint  drop  01 src    englatn  tgt    deulatn   python content  fairseq  example  nllb  modeling  train  trainscriptpy  cfg  nllb200dense33bfinetuneonfbseed  cfg  dataset  default  cfgdatasetlangpairs   src  tgt   cfgfairseqroot   pwd   cfgoutputdir  outputdir  cfgdropout  drop  cfgwarmup10  cfgfinetunefrommodel  modelfolder  checkpointpt error  content  fairseq  example  nllb  modeling  train  trainscriptpy287  userwarning  versionbase parameter specify  please specify compatability version level  none  assume default version 11  hydramain  configpath  conf   configname  baseconfig   usr  local  lib  python310  dist  package  hydrainternal  hydrapy119  userwarning  future hydra version long change work directory job runtime default  see   hydracc  docs12  upgrades11to12  changestojobworkingdir information  ret  runjob  training dir  content  output error executing job override    cfg  nllb200dense33bfinetuneonfbseed    cfg  dataset  default    cfgdatasetlangpair  englatn  deulatn    cfgfairseqrootcontent    cfgoutputdircontent  output    cfgdropout01    cfgwarmup10    cfgfinetunefrommodelcontent  drive  mydrive  lasskgdata  thesis  nllb  checkpoint  checkpointpt   traceback  recent call last   file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 289  main trainmodule  trainmodule  config  file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 122    init   assert clustername cfgdatasetdataprefix omegaconferror  configattributeerror  key  dataprefix  struct fullkey  cfgdatasetdataprefix objecttype  dict set environment variable hydrafullerror1 complete stack trace  far  understand miss dataprefix configuration  create demo custom dataconfigjson  look like     dataprefix     content  sampledata     traindata     traindemojson     testdata     testdemojson     langpairs     englatn  deulatn   official documentation provide information   m encounter difficulty apply specific use case  someone share detailed guide point helpful resource fine  tune nllb  can not help concrete error message get  guess would issue structure provide json file   personal recommendation would fine  tune nllb transformer library  specifically use seq2seqtrainer  multiple model  include nllb  check repository    githubcom  eliask93  transformer  model  for  domain  specific  machine  translation way fine  tune inference process nllb model bilingual model  find guide easiely   exception load tokenizer like  tokenizer  nllbtokenizerfrompretraine  modelpath  srclang  englatn   tgtlang  deulatn   generate translation like  modelgenerate  tokenizedchunkinputid  forcedbostokenid  tokenizerencode    deulatn    1   maxlength512 ,Library/Tool-Based Queries
How can I use structured_output with Azure OpenAI with the openai Python library?,"I want to use structured output with Azure OpenAI. I tried the following code, based on the code given in https://openai.com/index/introducing-structured-outputs-in-the-api/ : from pydantic import BaseModel from openai import AzureOpenAI class Step(BaseModel): explanation: str output: str class MathResponse(BaseModel): steps: list[Step] final_answer: str client = AzureOpenAI(api_key='[redacted]', api_version='2024-05-01-preview', azure_endpoint='[redacted]') completion = client.beta.chat.completions.parse( model=""gpt-4omini-2024-07-18-name"", messages=[ {""role"": ""system"", ""content"": ""You are a helpful math tutor.""}, {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""}, ], response_format=MathResponse, ) message = completion.choices[0].message if message.parsed: print(message.parsed.steps) print(message.parsed.final_answer) else: print(message.refusal) I get the error: openai.BadRequestError: Error code: 400: { ""error"": { ""message"": ""Invalid parameter: response_format must be one of json_object, text."", ""type"": ""invalid_request_error"", ""param"": ""response_format"", ""code"": ""None"" } } How to fix it? I ran pip install -U openai : I use openai==1.40.1 and Python 3.11. I also tried https://cookbook.openai.com/examples/structured_outputs_intro using using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message: from openai import AzureOpenAI # Replace these variables with your Azure OpenAI endpoint and API key endpoint = ""https://<your-resource-name>.openai.azure.com"" api_key = ""<your-api-key>"" deployment_name = ""<your-deployment-name>"" # Replace with your deployment name MODEL = deployment_name # API endpoint for the completion request api_url = f""{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01"" client = AzureOpenAI(api_key='[redacted]', api_version='2024-07-01-preview', azure_endpoint='https://[redacted].openai.azure.com/') math_tutor_prompt = ''' You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning. ''' def get_math_solution(question): response = client.chat.completions.create( model=MODEL, messages=[ { ""role"": ""system"", ""content"": math_tutor_prompt }, { ""role"": ""user"", ""content"": question } ], response_format={ ""type"": ""json_schema"", ""json_schema"": { ""name"": ""math_reasoning"", ""schema"": { ""type"": ""object"", ""properties"": { ""steps"": { ""type"": ""array"", ""items"": { ""type"": ""object"", ""properties"": { ""explanation"": {""type"": ""string""}, ""output"": {""type"": ""string""} }, ""required"": [""explanation"", ""output""], ""additionalProperties"": False } }, ""final_answer"": {""type"": ""string""} }, ""required"": [""steps"", ""final_answer""], ""additionalProperties"": False }, ""strict"": True } } ) return response.choices[0].message # Testing with an example question question = ""how can I solve 8x + 7 = -23"" result = get_math_solution(question) print(result.content)","['python', 'nlp', 'azure-openai', 'gpt-4']",2,"Using gpt-4o-2024-08-06 , which finally got deployed today (2024-09-03) on Azure, made it work. Code example from learn.microsoft.com : from pydantic import BaseModel from openai import AzureOpenAI endpoint = ""https://your-azure-openai-endpoint.com"" api_key = ""your-azure-openai-key"" deployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name client = AzureOpenAI(api_key=api_key, api_version='2024-08-01-preview', azure_endpoint=endpoint) class CalendarEvent(BaseModel): name: str date: str participants: list[str] completion = client.beta.chat.completions.parse( model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment messages=[ {""role"": ""system"", ""content"": ""Extract the event information.""}, {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""}, ], response_format=CalendarEvent, ) event = completion.choices[0].message.parsed print(event) print(completion.model_dump_json(indent=2)) output: name='Science Fair' date='Friday' participants=['Alice', 'Bob'] { ""id"": ""chatcmpl-A3XDRVolXpjeAAQIGddswI990weid"", ""choices"": [ { ""finish_reason"": ""stop"", ""index"": 0, ""logprobs"": null, ""message"": { ""content"": ""{\""name\"":\""Science Fair\"",\""date\"":\""Friday\"",\""participants\"":[\""Alice\"",\""Bob\""]}"", ""refusal"": null, ""role"": ""assistant"", ""function_call"": null, ""tool_calls"": [], ""parsed"": { ""name"": ""Science Fair"", ""date"": ""Friday"", ""participants"": [ ""Alice"", ""Bob"" ] } }, ""content_filter_results"": { ""hate"": { ""filtered"": false, ""severity"": ""safe"" }, ""self_harm"": { ""filtered"": false, ""severity"": ""safe"" }, ""sexual"": { ""filtered"": false, ""severity"": ""safe"" }, ""violence"": { ""filtered"": false, ""severity"": ""safe"" } } } ], ""created"": 1725406029, ""model"": ""gpt-4o-2024-08-06"", ""object"": ""chat.completion"", ""service_tier"": null, ""system_fingerprint"": ""fp_b2ffeb31ff"", ""usage"": { ""completion_tokens"": 17, ""prompt_tokens"": 32, ""total_tokens"": 49 }, ""prompt_filter_results"": [ { ""prompt_index"": 0, ""content_filter_results"": { ""hate"": { ""filtered"": false, ""severity"": ""safe"" }, ""self_harm"": { ""filtered"": false, ""severity"": ""safe"" }, ""sexual"": { ""filtered"": false, ""severity"": ""safe"" }, ""violence"": { ""filtered"": false, ""severity"": ""safe"" } } } ] } Tested with Python 3.11.7 and openai==1.43.0.",2024-08-07 23:14:19,2024-09-03 23:28:44,1226,https://stackoverflow.com/questions/78846004/how-can-i-use-structured-output-with-azure-openai-with-the-openai-python-library,"How can I use structured_output with Azure OpenAI with the openai Python library? I want to use structured output with Azure OpenAI. I tried the following code, based on the code given in https://openai.com/index/introducing-structured-outputs-in-the-api/ : from pydantic import BaseModel from openai import AzureOpenAI class Step(BaseModel): explanation: str output: str class MathResponse(BaseModel): steps: list[Step] final_answer: str client = AzureOpenAI(api_key='[redacted]', api_version='2024-05-01-preview', azure_endpoint='[redacted]') completion = client.beta.chat.completions.parse( model=""gpt-4omini-2024-07-18-name"", messages=[ {""role"": ""system"", ""content"": ""You are a helpful math tutor.""}, {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""}, ], response_format=MathResponse, ) message = completion.choices[0].message if message.parsed: print(message.parsed.steps) print(message.parsed.final_answer) else: print(message.refusal) I get the error: openai.BadRequestError: Error code: 400: { ""error"": { ""message"": ""Invalid parameter: response_format must be one of json_object, text."", ""type"": ""invalid_request_error"", ""param"": ""response_format"", ""code"": ""None"" } } How to fix it? I ran pip install -U openai : I use openai==1.40.1 and Python 3.11. I also tried https://cookbook.openai.com/examples/structured_outputs_intro using using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message: from openai import AzureOpenAI # Replace these variables with your Azure OpenAI endpoint and API key endpoint = ""https://<your-resource-name>.openai.azure.com"" api_key = ""<your-api-key>"" deployment_name = ""<your-deployment-name>"" # Replace with your deployment name MODEL = deployment_name # API endpoint for the completion request api_url = f""{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01"" client = AzureOpenAI(api_key='[redacted]', api_version='2024-07-01-preview', azure_endpoint='https://[redacted].openai.azure.com/') math_tutor_prompt = ''' You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning. ''' def get_math_solution(question): response = client.chat.completions.create( model=MODEL, messages=[ { ""role"": ""system"", ""content"": math_tutor_prompt }, { ""role"": ""user"", ""content"": question } ], response_format={ ""type"": ""json_schema"", ""json_schema"": { ""name"": ""math_reasoning"", ""schema"": { ""type"": ""object"", ""properties"": { ""steps"": { ""type"": ""array"", ""items"": { ""type"": ""object"", ""properties"": { ""explanation"": {""type"": ""string""}, ""output"": {""type"": ""string""} }, ""required"": [""explanation"", ""output""], ""additionalProperties"": False } }, ""final_answer"": {""type"": ""string""} }, ""required"": [""steps"", ""final_answer""], ""additionalProperties"": False }, ""strict"": True } } ) return response.choices[0].message # Testing with an example question question = ""how can I solve 8x + 7 = -23"" result = get_math_solution(question) print(result.content)",use structuredoutput azure openai openai python library  want use structured output azure openai  try follow code  base code give   openaicom  index  introducing  structure  outputs  in  the  api  pydantic import basemodel openai import azureopenai class step  basemodel   explanation  str output  str class mathresponse  basemodel   step  list  step  finalanswer  str client  azureopenai  apikey   redact    apiversion2024  05  01  preview   azureendpoint   redact    completion  clientbetachatcompletionsparse  model  gpt4omini2024  07  18  name   messages     role     system     content     helpful math tutor         role     user     content     solve 8x  31  2      responseformat  mathresponse   message  completionchoice  0  message messageparse  print  messageparsedstep  print  messageparsedfinalanswer  else  print  messagerefusal  get error  openai  badrequesterror  error code  400     error      message     invalid parameter  responseformat must one jsonobject  text       type     invalidrequesterror     param     responseformat     code     none    fix  run pip install u openai  use openai1401 python 311  also try   cookbookopenaicom  example  structuredoutputsintro use use azure gpt4o mini  2024  07  18   not work either  error message  openai import azureopenai  replace variable azure openai endpoint api key endpoint        your  resource  name  openaiazurecom  apikey     your  api  key   deploymentname     your  deployment  name    replace deployment name model  deploymentname  api endpoint completion request apiurl  f   endpoint  openai  deployments  deploymentname  chat  completion  api  version2024  06  01  client  azureopenai  apikey   redact    apiversion2024  07  01  preview   azureendpoint    redact  openaiazurecom   mathtutorprompt     helpful math tutor  provide math problem  goal output step step solution  along final answer  step  provide output equation use explanation field detail reasoning     def getmathsolution  question   response  clientchatcompletionscreate  model  model  messages     role     system     content   mathtutorprompt      role     user     content   question    responseformat    type     jsonschema     jsonschema      name     mathreasoning     schema      type     object     property      step      type     array     item      type     object     property      explanation      type     string      output      type     string       require      explanation     output      additionalproperties   false      finalanswer      type     string       require      step     finalanswer      additionalproperties   false     strict   true    return responsechoice  0  message  testing example question question    solve 8x  7  23  result  getmathsolution  question  print  resultcontent ,use gpt4o2024  08  06  finally get deploy today  2024  09  03  azure  make work  code example learnmicrosoftcom  pydantic import basemodel openai import azureopenai endpoint      your  azure  openai  endpointcom  apikey    your  azure  openai  key  deploymentname   deployment name   replace gpt4o 2024  08  06 deployment name client  azureopenai  apikey  apikey  apiversion2024  08  01  preview   azureendpoint  endpoint  class calendarevent  basemodel   name  str date  str participant  list  str  completion  clientbetachatcompletionsparse  model  deploymentname   replace model deployment name gpt4o 2024  08  06 deployment messages     role     system     content     extract event information         role     user     content     alice bob go science fair friday        responseformat  calendarevent   event  completionchoice  0  messageparse print  event  print  completionmodeldumpjson  indent2   output  namescience fair  datefriday  participants   alice    bob      i d     chatcmpl  a3xdrvolxpjeaaqigddswi990weid     choice       finishreason     stop     index   0    logprob   null    message      content        name     science fair     date     friday     participants      alice     bob        refusal   null    role     assistant     functioncall   null    toolcalls        parse      name     science fair     date     friday     participant      alice     bob        contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe         create   1725406029    model     gpt4o2024  08  06     object     chatcompletion     servicetier   null    systemfingerprint     fpb2ffeb31ff     usage      completiontokens   17    prompttoken   32    totaltoken   49     promptfilterresult       promptindex   0    contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe       test python 3117 openai1430 ,use structuredoutput azure openai openai python library  want use structured output azure openai  try follow code  base code give   openaicom  index  introducing  structure  outputs  in  the  api  pydantic import basemodel openai import azureopenai class step  basemodel   explanation  str output  str class mathresponse  basemodel   step  list  step  finalanswer  str client  azureopenai  apikey   redact    apiversion2024  05  01  preview   azureendpoint   redact    completion  clientbetachatcompletionsparse  model  gpt4omini2024  07  18  name   messages     role     system     content     helpful math tutor         role     user     content     solve 8x  31  2      responseformat  mathresponse   message  completionchoice  0  message messageparse  print  messageparsedstep  print  messageparsedfinalanswer  else  print  messagerefusal  get error  openai  badrequesterror  error code  400     error      message     invalid parameter  responseformat must one jsonobject  text       type     invalidrequesterror     param     responseformat     code     none    fix  run pip install u openai  use openai1401 python 311  also try   cookbookopenaicom  example  structuredoutputsintro use use azure gpt4o mini  2024  07  18   not work either  error message  openai import azureopenai  replace variable azure openai endpoint api key endpoint        your  resource  name  openaiazurecom  apikey     your  api  key   deploymentname     your  deployment  name    replace deployment name model  deploymentname  api endpoint completion request apiurl  f   endpoint  openai  deployments  deploymentname  chat  completion  api  version2024  06  01  client  azureopenai  apikey   redact    apiversion2024  07  01  preview   azureendpoint    redact  openaiazurecom   mathtutorprompt     helpful math tutor  provide math problem  goal output step step solution  along final answer  step  provide output equation use explanation field detail reasoning     def getmathsolution  question   response  clientchatcompletionscreate  model  model  messages     role     system     content   mathtutorprompt      role     user     content   question    responseformat    type     jsonschema     jsonschema      name     mathreasoning     schema      type     object     property      step      type     array     item      type     object     property      explanation      type     string      output      type     string       require      explanation     output      additionalproperties   false      finalanswer      type     string       require      step     finalanswer      additionalproperties   false     strict   true    return responsechoice  0  message  testing example question question    solve 8x  7  23  result  getmathsolution  question  print  resultcontent  use gpt4o2024  08  06  finally get deploy today  2024  09  03  azure  make work  code example learnmicrosoftcom  pydantic import basemodel openai import azureopenai endpoint      your  azure  openai  endpointcom  apikey    your  azure  openai  key  deploymentname   deployment name   replace gpt4o 2024  08  06 deployment name client  azureopenai  apikey  apikey  apiversion2024  08  01  preview   azureendpoint  endpoint  class calendarevent  basemodel   name  str date  str participant  list  str  completion  clientbetachatcompletionsparse  model  deploymentname   replace model deployment name gpt4o 2024  08  06 deployment messages     role     system     content     extract event information         role     user     content     alice bob go science fair friday        responseformat  calendarevent   event  completionchoice  0  messageparse print  event  print  completionmodeldumpjson  indent2   output  namescience fair  datefriday  participants   alice    bob      i d     chatcmpl  a3xdrvolxpjeaaqigddswi990weid     choice       finishreason     stop     index   0    logprob   null    message      content        name     science fair     date     friday     participants      alice     bob        refusal   null    role     assistant     functioncall   null    toolcalls        parse      name     science fair     date     friday     participant      alice     bob        contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe         create   1725406029    model     gpt4o2024  08  06     object     chatcompletion     servicetier   null    systemfingerprint     fpb2ffeb31ff     usage      completiontokens   17    prompttoken   32    totaltoken   49     promptfilterresult       promptindex   0    contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe       test python 3117 openai1430 ,Library/Tool-Based Queries
Removing bi-grams after tokenization for TfidfVectorizer,"I'm attempting to remove bi-grams that are created by TfidfVectorizer . I'm using text.TfidfVectorizer so that I can use my own preprocessor function. Test strings and preprocessor function: doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 'another that has aa aa and start date and hurricane hitting south carolina'] def remove_bigrams(doc): gram_2 = ['past performance', 'start date', 'aa aa'] res = [] for record in doc: the_string = record for phrase in gram_2: the_string = the_string.replace(phrase, """") res.append(the_string) return res remove_bigrams(doc2) My TfidfVectorizer instantiation and fit_transform : from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import text custom_stop_words = [i for i in stop_words] vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range=(2, 2), preprocessor=remove_bigrams, ) features = vec.fit_transform(doc2) Here is my error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Input In [49], in <cell line: 5>() 3 #t3_cv = CountVectorizer(t2, stop_words = stop_words) 4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams) ----> 5 features = vec.fit_transform(doc2) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y) 2072 self._check_params() 2073 self._tfidf = TfidfTransformer( 2074 norm=self.norm, 2075 use_idf=self.use_idf, 2076 smooth_idf=self.smooth_idf, 2077 sublinear_tf=self.sublinear_tf, 2078 ) -> 2079 X = super().fit_transform(raw_documents) 2080 self._tfidf.fit(X) 2081 # X is already a transformed view of raw_documents so 2082 # we set copy to False File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y) 1330 warnings.warn( 1331 ""Upper case characters found in"" 1332 "" vocabulary while 'lowercase'"" 1333 "" is True. These entries will not"" 1334 "" be matched with any documents"" 1335 ) 1336 break -> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_) 1340 if self.binary: 1341 X.data.fill(1) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab) 1207 for doc in raw_documents: 1208 feature_counter = {} -> 1209 for feature in analyze(doc): 1210 try: 1211 feature_idx = vocabulary[feature] File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words) 111 doc = preprocessor(doc) 112 if tokenizer is not None: --> 113 doc = tokenizer(doc) 114 if ngrams is not None: 115 if stop_words is not None: TypeError: expected string or bytes-like object How to resolve it?","['python', 'scikit-learn', 'nlp', 'preprocessor', 'tfidfvectorizer']",1,"The preprocessor should handle documents, not the whole corpus. (The clues are the ""expected string"" in the error, and the fact that the TfidfVectorizer docs refer to ""the preprocessing (string transformation) stage"". The docs could definitely be clearer.) This should fix it: def remove_bigrams(doc: str) -> str: """"""Remove certain bi-grams from a document."""""" gram_2 = ['past performance', 'start date', 'aa aa'] for phrase in gram_2: doc = doc.replace(phrase, """") return doc",2024-08-05 19:46:40,2024-08-06 06:57:17,41,https://stackoverflow.com/questions/78836208/removing-bi-grams-after-tokenization-for-tfidfvectorizer,"Removing bi-grams after tokenization for TfidfVectorizer I'm attempting to remove bi-grams that are created by TfidfVectorizer . I'm using text.TfidfVectorizer so that I can use my own preprocessor function. Test strings and preprocessor function: doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 'another that has aa aa and start date and hurricane hitting south carolina'] def remove_bigrams(doc): gram_2 = ['past performance', 'start date', 'aa aa'] res = [] for record in doc: the_string = record for phrase in gram_2: the_string = the_string.replace(phrase, """") res.append(the_string) return res remove_bigrams(doc2) My TfidfVectorizer instantiation and fit_transform : from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import text custom_stop_words = [i for i in stop_words] vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range=(2, 2), preprocessor=remove_bigrams, ) features = vec.fit_transform(doc2) Here is my error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Input In [49], in <cell line: 5>() 3 #t3_cv = CountVectorizer(t2, stop_words = stop_words) 4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams) ----> 5 features = vec.fit_transform(doc2) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y) 2072 self._check_params() 2073 self._tfidf = TfidfTransformer( 2074 norm=self.norm, 2075 use_idf=self.use_idf, 2076 smooth_idf=self.smooth_idf, 2077 sublinear_tf=self.sublinear_tf, 2078 ) -> 2079 X = super().fit_transform(raw_documents) 2080 self._tfidf.fit(X) 2081 # X is already a transformed view of raw_documents so 2082 # we set copy to False File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y) 1330 warnings.warn( 1331 ""Upper case characters found in"" 1332 "" vocabulary while 'lowercase'"" 1333 "" is True. These entries will not"" 1334 "" be matched with any documents"" 1335 ) 1336 break -> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_) 1340 if self.binary: 1341 X.data.fill(1) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab) 1207 for doc in raw_documents: 1208 feature_counter = {} -> 1209 for feature in analyze(doc): 1210 try: 1211 feature_idx = vocabulary[feature] File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words) 111 doc = preprocessor(doc) 112 if tokenizer is not None: --> 113 doc = tokenizer(doc) 114 if ngrams is not None: 115 if stop_words is not None: TypeError: expected string or bytes-like object How to resolve it?",remove bi  grams tokenization tfidfvectorizer  m attempt remove bi  gram create tfidfvectorizer   m use text  tfidfvectorizer use preprocessor function  test string preprocessor function  doc2    this test past performance another aa aa add buile cat dog horse hurricane    another aa aa start date hurricane hit south carolina   def removebigram  doc   gram2    past performance    start date    aa aa   re    record doc  thestring  record phrase gram2  thestre  thestringreplace  phrase      resappend  thestring  return re removebigrams  doc2  tfidfvectorizer instantiation fittransform  sklearnfeatureextractiontext import englishstopwords stopwords sklearnfeatureextractiontext import tfidfvectorizer sklearnfeatureextraction import text customstopwords   stopwords  vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange  2  2   preprocessor  removebigram   feature  vecfittransform  doc2  error                                        typeerror traceback  recent call last  input  49    cell line  5    3  t3cv  countvectorizer  t2  stopword  stopwords  4 vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange   22   preprocessor  removebigrams     5 feature  vecfittransform  doc2  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy2079  tfidfvectorizerfittransform  self  rawdocument   2072 selfcheckparams   2073 selftfidf  tfidftransformer  2074 norm  selfnorm  2075 useidf  selfuseidf  2076 smoothidf  selfsmoothidf  2077 sublineartf  selfsublineartf  2078    2079 x  super   fittransform  rawdocument  2080 selftfidffit  x  2081  x already transform view rawdocument 2082  set copy false file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1338  countvectorizerfittransform  self  rawdocument   1330 warningswarn  1331   upper case character find  1332   vocabulary  lowercase   1333   true  entry  1334   match document  1335  1336 break   1338 vocabulary  x  selfcountvocab  rawdocument  selffixedvocabulary   1340 selfbinary  1341 xdatafill  1  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1209  countvectorizercountvocab  self  rawdocument  fixedvocab  1207 doc rawdocument  1208 featurecounter      1209 feature analyze  doc   1210 try  1211 featureidx  vocabulary  feature  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy113   analyze  doc  analyzer  tokenizer  ngram  preprocessor  decoder  stopwords  111 doc  preprocessor  doc  112 tokenizer none    113 doc  tokenizer  doc  114 ngram none  115 stopwords none  typeerror  expect string byte  like object resolve ,preprocessor handle document  whole corpus   clue   expect string  error  fact tfidfvectorizer docs refer   preprocesse  string transformation  stage   doc could definitely clear   fix  def removebigram  doc  str    str      remove certain bi  gram document     gram2    past performance    start date    aa aa   phrase gram2  doc  docreplace  phrase      return doc,remove bi  grams tokenization tfidfvectorizer  m attempt remove bi  gram create tfidfvectorizer   m use text  tfidfvectorizer use preprocessor function  test string preprocessor function  doc2    this test past performance another aa aa add buile cat dog horse hurricane    another aa aa start date hurricane hit south carolina   def removebigram  doc   gram2    past performance    start date    aa aa   re    record doc  thestring  record phrase gram2  thestre  thestringreplace  phrase      resappend  thestring  return re removebigrams  doc2  tfidfvectorizer instantiation fittransform  sklearnfeatureextractiontext import englishstopwords stopwords sklearnfeatureextractiontext import tfidfvectorizer sklearnfeatureextraction import text customstopwords   stopwords  vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange  2  2   preprocessor  removebigram   feature  vecfittransform  doc2  error                                        typeerror traceback  recent call last  input  49    cell line  5    3  t3cv  countvectorizer  t2  stopword  stopwords  4 vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange   22   preprocessor  removebigrams     5 feature  vecfittransform  doc2  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy2079  tfidfvectorizerfittransform  self  rawdocument   2072 selfcheckparams   2073 selftfidf  tfidftransformer  2074 norm  selfnorm  2075 useidf  selfuseidf  2076 smoothidf  selfsmoothidf  2077 sublineartf  selfsublineartf  2078    2079 x  super   fittransform  rawdocument  2080 selftfidffit  x  2081  x already transform view rawdocument 2082  set copy false file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1338  countvectorizerfittransform  self  rawdocument   1330 warningswarn  1331   upper case character find  1332   vocabulary  lowercase   1333   true  entry  1334   match document  1335  1336 break   1338 vocabulary  x  selfcountvocab  rawdocument  selffixedvocabulary   1340 selfbinary  1341 xdatafill  1  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1209  countvectorizercountvocab  self  rawdocument  fixedvocab  1207 doc rawdocument  1208 featurecounter      1209 feature analyze  doc   1210 try  1211 featureidx  vocabulary  feature  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy113   analyze  doc  analyzer  tokenizer  ngram  preprocessor  decoder  stopwords  111 doc  preprocessor  doc  112 tokenizer none    113 doc  tokenizer  doc  114 ngram none  115 stopwords none  typeerror  expect string byte  like object resolve  preprocessor handle document  whole corpus   clue   expect string  error  fact tfidfvectorizer docs refer   preprocesse  string transformation  stage   doc could definitely clear   fix  def removebigram  doc  str    str      remove certain bi  gram document     gram2    past performance    start date    aa aa   phrase gram2  doc  docreplace  phrase      return doc,Basic Understanding
HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long,"Context : I tried to create an evaluation pipeline for a text summary task using HuggingFace evaluate packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this. Here is what I did: My code : from transformers import pipeline from evaluate import evaluator from datasets import load_dataset # Load data: booksum = load_dataset(""kmfoda/booksum"", split=""validation[:1000]"") # Load pipeline pipe = pipeline( task=""summarization"", model=""pszemraj/led-base-book-summary"", device=""mps"" ) # Setup Evaluate task using Rouge task_evaluator = evaluator(""summarization"") # The code that yield issue: eval_results = task_evaluator.compute( model_or_pipeline=pipe, data=booksum, metric=""rouge"", input_column=""chapter"", label_column=""summary_text"" ) This gives me the value error below: Short Error message: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Full Error message: --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 eval_results = task_evaluator.compute( 2 model_or_pipeline=pipe, 3 data=booksum, 4 metric=""rouge"", 6 input_column=""chapter"", 7 label_column=""summary_text"" 8 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 166 @add_start_docstrings( 167 EVALUTOR_COMPUTE_START_DOCSTRING, 168 TASK_DOCUMENTATION_KWARGS, (...) 189 generation_kwargs: dict = None, 190 ) -> Tuple[Dict[str, float], Any]: --> 191 result = super().compute( 192 model_or_pipeline=model_or_pipeline, 193 data=data, 194 subset=subset, 195 split=split, 196 metric=metric, 197 tokenizer=tokenizer, 198 strategy=strategy, 199 confidence_level=confidence_level, 200 n_resamples=n_resamples, 201 device=device, 202 random_state=random_state, 203 input_column=input_column, 204 label_column=label_column, 205 generation_kwargs=generation_kwargs, 206 ) 208 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 130 if generation_kwargs is not None: 131 self.PIPELINE_KWARGS.update(generation_kwargs) --> 133 result = super().compute( 134 model_or_pipeline=model_or_pipeline, 135 data=data, 136 subset=subset, 137 split=split, 138 metric=metric, 139 tokenizer=tokenizer, 140 strategy=strategy, 141 confidence_level=confidence_level, 142 n_resamples=n_resamples, 143 device=device, 144 random_state=random_state, 145 input_column=input_column, 146 label_column=label_column, 147 ) 149 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping) 252 metric = self.prepare_metric(metric) 254 # Compute predictions --> 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs) 256 predictions = self.predictions_processor(predictions, label_mapping) 258 metric_inputs.update(predictions) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs) 511 def call_pipeline(self, pipe, *args, **kwargs): 512 start_time = perf_counter() --> 513 pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS) 514 end_time = perf_counter() 515 return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output)) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs) 245 def __call__(self, *args, **kwargs): 246 r"""""" 247 Summarize the text(s) given as inputs. 248 (...) 267 ids of the summary. 268 """""" --> 269 return super().__call__(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs) 138 def __call__(self, *args, **kwargs): 139 r"""""" 140 Generate the output text(s) using text(s) given as inputs. 141 (...) 164 ids of the generated text. 165 """""" --> 167 result = super().__call__(*args, **kwargs) 168 if ( 169 isinstance(args[0], list) 170 and all(isinstance(el, str) for el in args[0]) 171 and all(len(res) == 1 for res in result) 172 ): 173 return [res[0] for res in result] File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs) 1231 if can_use_iterator: 1232 final_iterator = self.get_iterator( 1233 inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params 1234 ) -> 1235 outputs = list(final_iterator) 1236 return outputs 1237 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self) 121 return self.loader_batch_item() 123 # We're out of items within a batch --> 124 item = next(self.iterator) 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self) 123 # We're out of items within a batch 124 item = next(self.iterator) --> 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". 127 if self.loader_batch_size is not None: 128 # Try to infer the size of the batch File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params) 1159 with inference_context(): 1160 model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device) -> 1161 model_outputs = self._forward(model_inputs, **forward_params) 1162 model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu"")) 1163 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs) 184 in_b, input_length = tf.shape(model_inputs[""input_ids""]).numpy() 186 self.check_inputs( 187 input_length, 188 generate_kwargs.get(""min_length"", self.model.config.min_length), 189 generate_kwargs.get(""max_length"", self.model.config.max_length), 190 ) --> 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs) 192 out_b = output_ids.shape[0] 193 if self.framework == ""pt"": File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs) 113 @functools.wraps(func) 114 def decorate_context(*args, **kwargs): 115 with ctx_factory(): --> 116 return func(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs) 2020 input_ids, model_kwargs = self._expand_inputs_for_generation( 2021 input_ids=input_ids, 2022 expand_size=generation_config.num_beams, 2023 is_encoder_decoder=self.config.is_encoder_decoder, 2024 **model_kwargs, 2025 ) 2027 # 14. run beam sample -> 2028 result = self._beam_search( 2029 input_ids, 2030 beam_scorer, 2031 logits_processor=prepared_logits_processor, 2032 logits_warper=prepared_logits_warper, 2033 stopping_criteria=prepared_stopping_criteria, 2034 generation_config=generation_config, 2035 synced_gpus=synced_gpus, 2036 **model_kwargs, 2037 ) 2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH: 2040 # 11. prepare beam search scorer 2041 beam_scorer = BeamSearchScorer( 2042 batch_size=batch_size, 2043 num_beams=generation_config.num_beams, (...) 2049 max_length=generation_config.max_length, 2050 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs) 3195 next_token_logits = outputs.logits[:, -1, :].clone() 3196 next_token_scores = nn.functional.log_softmax( 3197 next_token_logits, dim=-1 3198 ) # (batch_size * num_beams, vocab_size) -> 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores) 3201 if do_sample: 3202 next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs) 96 scores = processor(input_ids, scores, **kwargs) 97 else: ---> 98 scores = processor(input_ids, scores) 100 return scores File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Notes : I did try to add device=""mps"" to the task_evaluator.compute but it gave me another error of ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.","['pytorch', 'nlp', 'apple-m1', 'huggingface', 'huggingface-evaluate']",1,I ran into a similar issue trying to run Facebook's nougat OCR tool. The error message mentions macOS 14 (Sonoma) and I was on macOS 13 (Ventura). Upgrading to macOS 14 fixed the issue for me.,2024-08-01 21:19:16,2024-08-15 13:01:25,859,https://stackoverflow.com/questions/78823069/huggingface-llm-evaluate-runtimeerror-isin-tensor-tensor-out-only-works-on-flo,"HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Context : I tried to create an evaluation pipeline for a text summary task using HuggingFace evaluate packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this. Here is what I did: My code : from transformers import pipeline from evaluate import evaluator from datasets import load_dataset # Load data: booksum = load_dataset(""kmfoda/booksum"", split=""validation[:1000]"") # Load pipeline pipe = pipeline( task=""summarization"", model=""pszemraj/led-base-book-summary"", device=""mps"" ) # Setup Evaluate task using Rouge task_evaluator = evaluator(""summarization"") # The code that yield issue: eval_results = task_evaluator.compute( model_or_pipeline=pipe, data=booksum, metric=""rouge"", input_column=""chapter"", label_column=""summary_text"" ) This gives me the value error below: Short Error message: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Full Error message: --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 eval_results = task_evaluator.compute( 2 model_or_pipeline=pipe, 3 data=booksum, 4 metric=""rouge"", 6 input_column=""chapter"", 7 label_column=""summary_text"" 8 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 166 @add_start_docstrings( 167 EVALUTOR_COMPUTE_START_DOCSTRING, 168 TASK_DOCUMENTATION_KWARGS, (...) 189 generation_kwargs: dict = None, 190 ) -> Tuple[Dict[str, float], Any]: --> 191 result = super().compute( 192 model_or_pipeline=model_or_pipeline, 193 data=data, 194 subset=subset, 195 split=split, 196 metric=metric, 197 tokenizer=tokenizer, 198 strategy=strategy, 199 confidence_level=confidence_level, 200 n_resamples=n_resamples, 201 device=device, 202 random_state=random_state, 203 input_column=input_column, 204 label_column=label_column, 205 generation_kwargs=generation_kwargs, 206 ) 208 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 130 if generation_kwargs is not None: 131 self.PIPELINE_KWARGS.update(generation_kwargs) --> 133 result = super().compute( 134 model_or_pipeline=model_or_pipeline, 135 data=data, 136 subset=subset, 137 split=split, 138 metric=metric, 139 tokenizer=tokenizer, 140 strategy=strategy, 141 confidence_level=confidence_level, 142 n_resamples=n_resamples, 143 device=device, 144 random_state=random_state, 145 input_column=input_column, 146 label_column=label_column, 147 ) 149 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping) 252 metric = self.prepare_metric(metric) 254 # Compute predictions --> 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs) 256 predictions = self.predictions_processor(predictions, label_mapping) 258 metric_inputs.update(predictions) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs) 511 def call_pipeline(self, pipe, *args, **kwargs): 512 start_time = perf_counter() --> 513 pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS) 514 end_time = perf_counter() 515 return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output)) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs) 245 def __call__(self, *args, **kwargs): 246 r"""""" 247 Summarize the text(s) given as inputs. 248 (...) 267 ids of the summary. 268 """""" --> 269 return super().__call__(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs) 138 def __call__(self, *args, **kwargs): 139 r"""""" 140 Generate the output text(s) using text(s) given as inputs. 141 (...) 164 ids of the generated text. 165 """""" --> 167 result = super().__call__(*args, **kwargs) 168 if ( 169 isinstance(args[0], list) 170 and all(isinstance(el, str) for el in args[0]) 171 and all(len(res) == 1 for res in result) 172 ): 173 return [res[0] for res in result] File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs) 1231 if can_use_iterator: 1232 final_iterator = self.get_iterator( 1233 inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params 1234 ) -> 1235 outputs = list(final_iterator) 1236 return outputs 1237 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self) 121 return self.loader_batch_item() 123 # We're out of items within a batch --> 124 item = next(self.iterator) 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self) 123 # We're out of items within a batch 124 item = next(self.iterator) --> 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". 127 if self.loader_batch_size is not None: 128 # Try to infer the size of the batch File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params) 1159 with inference_context(): 1160 model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device) -> 1161 model_outputs = self._forward(model_inputs, **forward_params) 1162 model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu"")) 1163 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs) 184 in_b, input_length = tf.shape(model_inputs[""input_ids""]).numpy() 186 self.check_inputs( 187 input_length, 188 generate_kwargs.get(""min_length"", self.model.config.min_length), 189 generate_kwargs.get(""max_length"", self.model.config.max_length), 190 ) --> 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs) 192 out_b = output_ids.shape[0] 193 if self.framework == ""pt"": File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs) 113 @functools.wraps(func) 114 def decorate_context(*args, **kwargs): 115 with ctx_factory(): --> 116 return func(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs) 2020 input_ids, model_kwargs = self._expand_inputs_for_generation( 2021 input_ids=input_ids, 2022 expand_size=generation_config.num_beams, 2023 is_encoder_decoder=self.config.is_encoder_decoder, 2024 **model_kwargs, 2025 ) 2027 # 14. run beam sample -> 2028 result = self._beam_search( 2029 input_ids, 2030 beam_scorer, 2031 logits_processor=prepared_logits_processor, 2032 logits_warper=prepared_logits_warper, 2033 stopping_criteria=prepared_stopping_criteria, 2034 generation_config=generation_config, 2035 synced_gpus=synced_gpus, 2036 **model_kwargs, 2037 ) 2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH: 2040 # 11. prepare beam search scorer 2041 beam_scorer = BeamSearchScorer( 2042 batch_size=batch_size, 2043 num_beams=generation_config.num_beams, (...) 2049 max_length=generation_config.max_length, 2050 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs) 3195 next_token_logits = outputs.logits[:, -1, :].clone() 3196 next_token_scores = nn.functional.log_softmax( 3197 next_token_logits, dim=-1 3198 ) # (batch_size * num_beams, vocab_size) -> 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores) 3201 if do_sample: 3202 next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs) 96 scores = processor(input_ids, scores, **kwargs) 97 else: ---> 98 scores = processor(input_ids, scores) 100 return scores File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Notes : I did try to add device=""mps"" to the task_evaluator.compute but it gave me another error of ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.",huggingface llm evaluate  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long context  try create evaluation pipeline text summary task use huggingface evaluate package  get issue receive dtype long tensor  feed long type two column specify evaluate pipeline text  investigation look like issue root torch version mac  m1    m sure proceed   code  transformer import pipeline evaluate import evaluator dataset import loaddataset  load datum  booksum  loaddataset    kmfoda  booksum   split  validation   1000     load pipeline pipe  pipeline  task  summarization   model  pszemraj  lead  base  book  summary   device  mp    setup evaluate task use rouge taskevaluator  evaluator    summarization    code yield issue  evalresult  taskevaluatorcompute  modelorpipeline  pipe  datum  booksum  metric  rouge   inputcolumn  chapter   labelcolumn  summarytext   give value error  short error message  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long full error message                                        runtimeerror traceback  recent call last  cell  10   line 1    1 evalresult  taskevaluatorcompute  2 modelorpipeline  pipe  3 datum  booksum  4 metric  rouge   6 inputcolumn  chapter   7 labelcolumn  summarytext  8  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy191  summarizationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  166  addstartdocstring  167 evalutorcomputestartdocstring  168 taskdocumentationkwargs     189 generationkwargs  dict  none  190    tuple  dict  str  float       191 result  super   compute  192 modelorpipeline  modelorpipeline  193 datum  datum  194 subset  subset  195 split  split  196 metric  metric  197 tokenizer  tokenizer  198 strategy  strategy  199 confidencelevel  confidencelevel  200 nresample  nresample  201 device  device  202 randomstate  randomstate  203 inputcolumn  inputcolumn  204 labelcolumn  labelcolumn  205 generationkwarg  generationkwargs  206  208 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy133  text2textgenerationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  130 generationkwargs none  131 selfpipelinekwargsupdate  generationkwargs    133 result  super   compute  134 modelorpipeline  modelorpipeline  135 datum  datum  136 subset  subset  137 split  split  138 metric  metric  139 tokenizer  tokenizer  140 strategy  strategy  141 confidencelevel  confidencelevel  142 nresample  nresample  143 device  device  144 randomstate  randomstate  145 inputcolumn  inputcolumn  146 labelcolumn  labelcolumn  147  149 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy255  evaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  featureextractor  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  labelmapping  252 metric  selfpreparemetric  metric  254  compute prediction   255 prediction  perfresult  selfcallpipeline  pipe  pipeinput  256 prediction  selfpredictionsprocessor  prediction  labelmapping  258 metricinputsupdate  prediction  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy513  evaluatorcallpipeline  self  pipe   args    kwargs  511 def callpipeline  self  pipe   args    kwargs   512 starttime  perfcounter     513 pipeoutput  pipe   args    kwarg    self  pipelinekwargs  514 endtime  perfcounter   515 return pipeoutput  selfcomputetimeperf  starttime  endtime  len  pipeoutput   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy269  summarizationpipelinecall    self   args    kwargs  245 def   call    self   args    kwargs   246 r    247 summarize text   give input  248    267 id summary  268       269 return super   call     args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy167  text2textgenerationpipelinecall    self   args    kwargs  138 def   call    self   args    kwargs   139 r    140 generate output text   use text   give input  141    164 id generate text  165       167 result  super   call     args    kwargs  168  169 isinstance  args  0   list  170  isinstance  el  str  el args  0   171  len  re    1 re result  172   173 return  re  0  re result  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1235  pipelinecall    self  input  numworker  batchsize   args    kwargs  1231 canuseiterator  1232 finaliterator  selfgetiterator  1233 input  numworker  batchsize  preprocessparam  forwardparams  postprocessparams 1234    1235 output  list  finaliterator  1236 return output 1237 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy124  pipelineiteratornext    self  121 return selfloaderbatchitem   123  be item within batch   124 item  next  selfiterator  125 process  selfinf  item    selfparam  126  batch   infer thing   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy125  pipelineiteratornext    self  123  be item within batch 124 item  next  selfiterator    125 process  selfinf  item    selfparam  126  batch   infer thing   127 selfloaderbatchsize none  128  try infer size batch file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1161  pipelineforward  self  modelinput    forwardparams  1159 inferencecontext    1160 modelinput  selfensuretensorondevice  modelinput  device  selfdevice    1161 modeloutputs  selfforward  modelinput    forwardparams  1162 modeloutputs  selfensuretensorondevice  modeloutput  device  torchdevice    cpu    1163 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy191  text2textgenerationpipelineforward  self  modelinput    generatekwargs  184 inb  inputlength  tfshape  modelinput    inputids    numpy   186 selfcheckinput  187 inputlength  188 generatekwargsget    minlength   selfmodelconfigminlength   189 generatekwargsget    maxlength   selfmodelconfigmaxlength   190    191 outputid  selfmodelgenerate    modelinput    generatekwargs  192 outb  outputidsshape  0  193 selfframework     pt   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  torch  utilscontextlibpy116  contextdecorator   local  decoratecontext   args    kwargs  113  functoolswraps  func  114 def decoratecontext   args    kwargs   115 ctxfactory      116 return func   args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy2028  generationmixingenerate  self  input  generationconfig  logitsprocessor  stoppingcriteria  prefixallowedtokensfn  syncedgpus  assistantmodel  streamer  negativepromptid  negativepromptattentionmask    kwargs  2020 inputids  modelkwarg  selfexpandinputsforgeneration  2021 inputid  inputids  2022 expandsize  generationconfignumbeams  2023 isencoderdecoder  selfconfigisencoderdecoder  2024   modelkwarg  2025  2027  14  run beam sample   2028 result  selfbeamsearch  2029 inputids  2030 beamscorer  2031 logitsprocessor  preparedlogitsprocessor  2032 logitswarper  preparedlogitswarper  2033 stoppingcriteria  preparedstoppingcriteria  2034 generationconfig  generationconfig  2035 syncedgpus  syncedgpus  2036   modelkwarg  2037  2039 elif generationmode   generationmode  groupbeamsearch  2040  11  prepare beam search scorer 2041 beamscorer  beamsearchscorer  2042 batchsize  batchsize  2043 numbeam  generationconfignumbeams     2049 maxlength  generationconfigmaxlength  2050  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy3200  generationmixinbeamsearch  self  inputid  beamscorer  logitsprocessor  stoppingcriteria  generationconfig  syncedgpus  logitswarper    modelkwarg  3195 nexttokenlogits  outputslogits    1    clone   3196 nexttokenscores  nnfunctionallogsoftmax  3197 nexttokenlogits  dim1 3198    batchsize  numbeams  vocabsize    3200 nexttokenscoresprocesse  logitsprocessor  inputids  nexttokenscores  3201 dosample  3202 nexttokenscoresprocesse  logitswarper  inputids  nexttokenscoresprocesse  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy98  logitsprocessorlistcall    self  inputid  score    kwargs  96 score  processor  inputids  score    kwargs  97 else     98 score  processor  inputids  score  100 return score file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long note  try add device  mp  taskevaluatorcompute give another error valueerror  pipeline instantiate device none device  mp pass  compute  ,run similar issue try run facebook s nougat ocr tool  error message mention macos 14  sonoma  macos 13  ventura   upgrade macos 14 fix issue ,huggingface llm evaluate  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long context  try create evaluation pipeline text summary task use huggingface evaluate package  get issue receive dtype long tensor  feed long type two column specify evaluate pipeline text  investigation look like issue root torch version mac  m1    m sure proceed   code  transformer import pipeline evaluate import evaluator dataset import loaddataset  load datum  booksum  loaddataset    kmfoda  booksum   split  validation   1000     load pipeline pipe  pipeline  task  summarization   model  pszemraj  lead  base  book  summary   device  mp    setup evaluate task use rouge taskevaluator  evaluator    summarization    code yield issue  evalresult  taskevaluatorcompute  modelorpipeline  pipe  datum  booksum  metric  rouge   inputcolumn  chapter   labelcolumn  summarytext   give value error  short error message  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long full error message                                        runtimeerror traceback  recent call last  cell  10   line 1    1 evalresult  taskevaluatorcompute  2 modelorpipeline  pipe  3 datum  booksum  4 metric  rouge   6 inputcolumn  chapter   7 labelcolumn  summarytext  8  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy191  summarizationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  166  addstartdocstring  167 evalutorcomputestartdocstring  168 taskdocumentationkwargs     189 generationkwargs  dict  none  190    tuple  dict  str  float       191 result  super   compute  192 modelorpipeline  modelorpipeline  193 datum  datum  194 subset  subset  195 split  split  196 metric  metric  197 tokenizer  tokenizer  198 strategy  strategy  199 confidencelevel  confidencelevel  200 nresample  nresample  201 device  device  202 randomstate  randomstate  203 inputcolumn  inputcolumn  204 labelcolumn  labelcolumn  205 generationkwarg  generationkwargs  206  208 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy133  text2textgenerationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  130 generationkwargs none  131 selfpipelinekwargsupdate  generationkwargs    133 result  super   compute  134 modelorpipeline  modelorpipeline  135 datum  datum  136 subset  subset  137 split  split  138 metric  metric  139 tokenizer  tokenizer  140 strategy  strategy  141 confidencelevel  confidencelevel  142 nresample  nresample  143 device  device  144 randomstate  randomstate  145 inputcolumn  inputcolumn  146 labelcolumn  labelcolumn  147  149 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy255  evaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  featureextractor  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  labelmapping  252 metric  selfpreparemetric  metric  254  compute prediction   255 prediction  perfresult  selfcallpipeline  pipe  pipeinput  256 prediction  selfpredictionsprocessor  prediction  labelmapping  258 metricinputsupdate  prediction  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy513  evaluatorcallpipeline  self  pipe   args    kwargs  511 def callpipeline  self  pipe   args    kwargs   512 starttime  perfcounter     513 pipeoutput  pipe   args    kwarg    self  pipelinekwargs  514 endtime  perfcounter   515 return pipeoutput  selfcomputetimeperf  starttime  endtime  len  pipeoutput   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy269  summarizationpipelinecall    self   args    kwargs  245 def   call    self   args    kwargs   246 r    247 summarize text   give input  248    267 id summary  268       269 return super   call     args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy167  text2textgenerationpipelinecall    self   args    kwargs  138 def   call    self   args    kwargs   139 r    140 generate output text   use text   give input  141    164 id generate text  165       167 result  super   call     args    kwargs  168  169 isinstance  args  0   list  170  isinstance  el  str  el args  0   171  len  re    1 re result  172   173 return  re  0  re result  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1235  pipelinecall    self  input  numworker  batchsize   args    kwargs  1231 canuseiterator  1232 finaliterator  selfgetiterator  1233 input  numworker  batchsize  preprocessparam  forwardparams  postprocessparams 1234    1235 output  list  finaliterator  1236 return output 1237 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy124  pipelineiteratornext    self  121 return selfloaderbatchitem   123  be item within batch   124 item  next  selfiterator  125 process  selfinf  item    selfparam  126  batch   infer thing   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy125  pipelineiteratornext    self  123  be item within batch 124 item  next  selfiterator    125 process  selfinf  item    selfparam  126  batch   infer thing   127 selfloaderbatchsize none  128  try infer size batch file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1161  pipelineforward  self  modelinput    forwardparams  1159 inferencecontext    1160 modelinput  selfensuretensorondevice  modelinput  device  selfdevice    1161 modeloutputs  selfforward  modelinput    forwardparams  1162 modeloutputs  selfensuretensorondevice  modeloutput  device  torchdevice    cpu    1163 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy191  text2textgenerationpipelineforward  self  modelinput    generatekwargs  184 inb  inputlength  tfshape  modelinput    inputids    numpy   186 selfcheckinput  187 inputlength  188 generatekwargsget    minlength   selfmodelconfigminlength   189 generatekwargsget    maxlength   selfmodelconfigmaxlength   190    191 outputid  selfmodelgenerate    modelinput    generatekwargs  192 outb  outputidsshape  0  193 selfframework     pt   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  torch  utilscontextlibpy116  contextdecorator   local  decoratecontext   args    kwargs  113  functoolswraps  func  114 def decoratecontext   args    kwargs   115 ctxfactory      116 return func   args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy2028  generationmixingenerate  self  input  generationconfig  logitsprocessor  stoppingcriteria  prefixallowedtokensfn  syncedgpus  assistantmodel  streamer  negativepromptid  negativepromptattentionmask    kwargs  2020 inputids  modelkwarg  selfexpandinputsforgeneration  2021 inputid  inputids  2022 expandsize  generationconfignumbeams  2023 isencoderdecoder  selfconfigisencoderdecoder  2024   modelkwarg  2025  2027  14  run beam sample   2028 result  selfbeamsearch  2029 inputids  2030 beamscorer  2031 logitsprocessor  preparedlogitsprocessor  2032 logitswarper  preparedlogitswarper  2033 stoppingcriteria  preparedstoppingcriteria  2034 generationconfig  generationconfig  2035 syncedgpus  syncedgpus  2036   modelkwarg  2037  2039 elif generationmode   generationmode  groupbeamsearch  2040  11  prepare beam search scorer 2041 beamscorer  beamsearchscorer  2042 batchsize  batchsize  2043 numbeam  generationconfignumbeams     2049 maxlength  generationconfigmaxlength  2050  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy3200  generationmixinbeamsearch  self  inputid  beamscorer  logitsprocessor  stoppingcriteria  generationconfig  syncedgpus  logitswarper    modelkwarg  3195 nexttokenlogits  outputslogits    1    clone   3196 nexttokenscores  nnfunctionallogsoftmax  3197 nexttokenlogits  dim1 3198    batchsize  numbeams  vocabsize    3200 nexttokenscoresprocesse  logitsprocessor  inputids  nexttokenscores  3201 dosample  3202 nexttokenscoresprocesse  logitswarper  inputids  nexttokenscoresprocesse  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy98  logitsprocessorlistcall    self  inputid  score    kwargs  96 score  processor  inputids  score    kwargs  97 else     98 score  processor  inputids  score  100 return score file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long note  try add device  mp  taskevaluatorcompute give another error valueerror  pipeline instantiate device none device  mp pass  compute   run similar issue try run facebook s nougat ocr tool  error message mention macos 14  sonoma  macos 13  ventura   upgrade macos 14 fix issue ,Task-Specific Queries
ImportError: cannot import name &#39;pre_init&#39; from &#39;langchain_core.utils&#39;,"When I want to install langchain libraries from requirements.txt I'm getting ImportError: cannot import name 'pre_init' from 'langchain_core.utils' I've tried to install libraries from terminal using these commands : pip install gigachain pip install gigachat pip install -U langchain-community and it's working, so I used pip freeze And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help","['python', 'pip', 'nlp', 'langchain', 'large-language-model']",1,"Your import error suggests that the langchain-core module has not been installed. You can confirm whether this is the case by checking the output from the following command: pip show langchain-core If it hasn't been installed, ensure it is installed via the following command: pip install langchain-core As for using a requirements.txt file, simply creating it doesn't automatically install the packages listed within. You still have to run the following command in your terminal to install the listed packages: python -m pip install -r /path/to/requirements.txt Finally, you don't have to manually create the requirements.txt file by coping and pasting the output from pip freeze . You can simply run: python -m pip freeze > /path/to/requirements.txt You can read more about requirement files under the requirements files section of the pip user guide.",2024-07-29 22:59:13,2024-07-29 23:00:42,1812,https://stackoverflow.com/questions/78809281/importerror-cannot-import-name-pre-init-from-langchain-core-utils,"ImportError: cannot import name &#39;pre_init&#39; from &#39;langchain_core.utils&#39; When I want to install langchain libraries from requirements.txt I'm getting ImportError: cannot import name 'pre_init' from 'langchain_core.utils' I've tried to install libraries from terminal using these commands : pip install gigachain pip install gigachat pip install -U langchain-community and it's working, so I used pip freeze And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help",importerror  import name   39  preinit   39    39  langchaincoreutils   39  want install langchain library requirementstxt  m get importerror  import name  preinit   langchaincoreutils   ve try install library terminal use command  pip install gigachain pip install gigachat pip install u langchain  community s working  use pip freeze pasted library terminal requirementstxt  not work  would nice someone could help,import error suggest langchain  core module instal  confirm whether case check output follow command  pip show langchain  core not instal  ensure instal via follow command  pip install langchain  core use requirementstxt file  simply creating not automatically install package list within  still run follow command terminal install list package  python m pip install r path  to  requirementstxt finally  not manually create requirementstxt file cope paste output pip freeze  simply run  python m pip freeze  path  to  requirementstxt read requirement file requirement file section pip user guide ,importerror  import name   39  preinit   39    39  langchaincoreutils   39  want install langchain library requirementstxt  m get importerror  import name  preinit   langchaincoreutils   ve try install library terminal use command  pip install gigachain pip install gigachat pip install u langchain  community s working  use pip freeze pasted library terminal requirementstxt  not work  would nice someone could help import error suggest langchain  core module instal  confirm whether case check output follow command  pip show langchain  core not instal  ensure instal via follow command  pip install langchain  core use requirementstxt file  simply creating not automatically install package list within  still run follow command terminal install list package  python m pip install r path  to  requirementstxt finally  not manually create requirementstxt file cope paste output pip freeze  simply run  python m pip freeze  path  to  requirementstxt read requirement file requirement file section pip user guide ,Library/Tool-Based Queries
Do I need to use Named Entity Recognition (NER) in tokenization?,"I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the documentation , I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching. The thing I am trying to understand is how to embody it ( if I should ) in my tokenization process. I am giving an example. text = ""Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it. A significant portion of Apple's user base wasn't able to use it even if they wanted to. As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased."" sentence = sp(text) # sp = spacy.load('en_core_web_sm') for word in sentence: print(word.text) # Let # 's # not # forget # that # Apple # Pay # in # etc... for word in sentence.ents: print(word.text + "" _ "" + word.label_ + "" _ "" + str(spacy.explain(word.label_))) # Apple Pay _ ORG _ Companies, agencies, institutions, etc. # 2014 _ DATE _ Absolute or relative dates or periods # iPhone _ ORG _ Companies, agencies, institutions, etc. # Apple _ ORG _ Companies, agencies, institutions, etc. # iPhones _ ORG _ Companies, agencies, institutions, etc. The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) ""type"" of tokenization? My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').","['python', 'python-3.x', 'nlp', 'spacy', 'named-entity-recognition']",1,"Tokenization typically is the splitting of a sentence into words or even subwords. I am not sure what you later plan to do with the data, but it is a convention in NLP to stick to either the document level, sentence level or word/token level. Having some mix of token and n-gram level (like [""Apple Pay"", ""required"", ""an"", ""iPhone"", ""to"", ""use"", ""it"", "".""] in my opinion will not help you in most later use cases. If you later train a classifier (assuming you're talking about fine-tuning a transformer based language model on a token classification task) would then use something like the IOB format to handle n-grams, e.g. like so: Token Label Apple B Pay I required O an O iPhone B to O use O it O . O Of course this depends on your application and directly merging to n-grams might work well for you. If you have some application where you are searching for frequent n-grams, you could use collocation metrics to extract those n-grams, e.g. using NLTK's CollocationFinder . Or as you mentioned use SpaCy either for noun chunk extraction or named entity recognition . For the latter one, you could access the token level ent_type_ and ent_iob_ attributes to iterate over the tokens in the processed docs once and then merge these n-grams together based on their IOB-tags.",2024-07-22 13:28:14,2024-07-22 21:24:34,384,https://stackoverflow.com/questions/78778988/do-i-need-to-use-named-entity-recognition-ner-in-tokenization,"Do I need to use Named Entity Recognition (NER) in tokenization? I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the documentation , I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching. The thing I am trying to understand is how to embody it ( if I should ) in my tokenization process. I am giving an example. text = ""Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it. A significant portion of Apple's user base wasn't able to use it even if they wanted to. As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased."" sentence = sp(text) # sp = spacy.load('en_core_web_sm') for word in sentence: print(word.text) # Let # 's # not # forget # that # Apple # Pay # in # etc... for word in sentence.ents: print(word.text + "" _ "" + word.label_ + "" _ "" + str(spacy.explain(word.label_))) # Apple Pay _ ORG _ Companies, agencies, institutions, etc. # 2014 _ DATE _ Absolute or relative dates or periods # iPhone _ ORG _ Companies, agencies, institutions, etc. # Apple _ ORG _ Companies, agencies, institutions, etc. # iPhones _ ORG _ Companies, agencies, institutions, etc. The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) ""type"" of tokenization? My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').",need use name entity recognition  ner  tokenization  work nlp project sentiment analysis  use spacy tokenize sentence  read documentation  learn ner   ve read use extract entity text aid user s search  thing try understand embody   tokenization process  give example  text    let us forget apple pay 2014 require brand new iphone order use  significant portion apple s user base not able use even want  successive iphone incorporate technology old iphone replace number people could use technology increase   sentence  sp  text   sp  spacyload   encorewebsm   word sentence  print  wordtext   let  s   forget   apple  pay   etc  word sentenceent  print  wordtext        wordlabel         str  spacyexplain  wordlabel      apple pay  org  company  agency  institution  etc   2014  date  absolute relative date period  iphone  org  company  agency  institution  etc   apple  org  company  agency  institution  etc   iphones  org  company  agency  institution  etc  first loops show  apple   pay  different token  printing discover entity second loop  understand  apply pay  org  yes  could achieve  let us say    type  tokenization  think  not  apple   pay  tokenized single word together  create classifier recognize entity recognize fruit   apple   verb   pay   ,tokenization typically split sentence word even subword  sure later plan datum  convention nlp stick either document level  sentence level word  token level  mix token n  gram level  like    apple pay     require         iphone         use            opinion help later use case  later train classifier  assume be talk fine  tuning transformer base language model token classification task  would use something like iob format handle n  gram  eg  like  token label apple b pay require iphone b use  course depend application directly merge n  gram might work well  application search frequent n  gram  could use collocation metric extract n  gram  eg  use nltk s collocationfinder  mention use spacy either noun chunk extraction name entity recognition  latter one  could access token level enttype  entiob  attribute iterate token process docs merge n  gram together base iob  tag ,need use name entity recognition  ner  tokenization  work nlp project sentiment analysis  use spacy tokenize sentence  read documentation  learn ner   ve read use extract entity text aid user s search  thing try understand embody   tokenization process  give example  text    let us forget apple pay 2014 require brand new iphone order use  significant portion apple s user base not able use even want  successive iphone incorporate technology old iphone replace number people could use technology increase   sentence  sp  text   sp  spacyload   encorewebsm   word sentence  print  wordtext   let  s   forget   apple  pay   etc  word sentenceent  print  wordtext        wordlabel         str  spacyexplain  wordlabel      apple pay  org  company  agency  institution  etc   2014  date  absolute relative date period  iphone  org  company  agency  institution  etc   apple  org  company  agency  institution  etc   iphones  org  company  agency  institution  etc  first loops show  apple   pay  different token  printing discover entity second loop  understand  apply pay  org  yes  could achieve  let us say    type  tokenization  think  not  apple   pay  tokenized single word together  create classifier recognize entity recognize fruit   apple   verb   pay    tokenization typically split sentence word even subword  sure later plan datum  convention nlp stick either document level  sentence level word  token level  mix token n  gram level  like    apple pay     require         iphone         use            opinion help later use case  later train classifier  assume be talk fine  tuning transformer base language model token classification task  would use something like iob format handle n  gram  eg  like  token label apple b pay require iphone b use  course depend application directly merge n  gram might work well  application search frequent n  gram  could use collocation metric extract n  gram  eg  use nltk s collocationfinder  mention use spacy either noun chunk extraction name entity recognition  latter one  could access token level enttype  entiob  attribute iterate token process docs merge n  gram together base iob  tag ,Task-Specific Queries
"IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface","i am trying to learn on how to fine tune a pretrained model and use it. this is my code from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) now, i am trying to make a prediction on the fine tuned model, like this inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"") predictions = trainer.predict(test_dataset=inputs) i am getting this error when i am trying to make a prediction, IndexError Traceback (most recent call last) Cell In[8], line 7 3 inputs=tokenizer(dataset[0][‘text’], padding=“max_length”, truncation=True,return_tensors=“pt”) 6 # Make predictions ----> 7 predictions = trainer.predict(test_dataset=inputs) File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix) 3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305 output = eval_loop( 3306 test_dataloader, description=“Prediction”, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 ) 3308 total_batch_size = self.args.eval_batch_size * self.args.world_size 3309 if f""{metric_key_prefix}_jit_compilation_time"" in output.metrics: File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix) 3406 observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step, inputs in enumerate(dataloader): 3409 # Update the observed num examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if observed_batch_size is not None: File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check when we are at the end 453 try: → 454 current_batch = next(dataloader_iter) 455 except StopIteration: 456 yield File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in _BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] → 631 data = self._next_data() 632 self._num_yielded += 1 633 if self._dataset_kind == _DatasetKind.Iterable and 634 self._IterableDataset_len_called is not None and 635 self._num_yielded self._IterableDataset_len_called: File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in _SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 676 if self._pin_memory: 677 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device) File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in (.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255, in BatchEncoding.getitem(self, item) 253 return self.data[item] 254 elif self._encodings is not None: → 255 return self._encodings[item] 256 elif isinstance(item, slice): 257 return {key: self.data[key][item] for key in self.data.keys()} IndexError: list index out of range","['nlp', 'huggingface-transformers', 'huggingface', 'fine-tuning']",1,"The error you are encountering is because the trainer.predict method expects a dataset as input, but you are passing a single example that has been tokenized into tensors. To perform predictions on a single input, you need to prepare it similarly to how the dataset was prepared before training, and then use the model directly for prediction. Here's how you can modify your code to make predictions on a single input: Prepare the input correctly Use the model directly for prediction Here's the revised code: from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) # Make a prediction on a single input inputs = tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True, return_tensors=""pt"") model.eval() # Set the model to evaluation mode with torch.no_grad(): # Disable gradient calculation outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=-1) print(f""Predicted label: {predictions.item()}"")",2024-07-20 19:44:43,2024-07-21 11:40:54,115,https://stackoverflow.com/questions/78773758/indexerror-list-index-out-of-range-when-trying-to-predict-from-the-fine-tuned,"IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface i am trying to learn on how to fine tune a pretrained model and use it. this is my code from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) now, i am trying to make a prediction on the fine tuned model, like this inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"") predictions = trainer.predict(test_dataset=inputs) i am getting this error when i am trying to make a prediction, IndexError Traceback (most recent call last) Cell In[8], line 7 3 inputs=tokenizer(dataset[0][‘text’], padding=“max_length”, truncation=True,return_tensors=“pt”) 6 # Make predictions ----> 7 predictions = trainer.predict(test_dataset=inputs) File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix) 3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305 output = eval_loop( 3306 test_dataloader, description=“Prediction”, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 ) 3308 total_batch_size = self.args.eval_batch_size * self.args.world_size 3309 if f""{metric_key_prefix}_jit_compilation_time"" in output.metrics: File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix) 3406 observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step, inputs in enumerate(dataloader): 3409 # Update the observed num examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if observed_batch_size is not None: File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check when we are at the end 453 try: → 454 current_batch = next(dataloader_iter) 455 except StopIteration: 456 yield File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in _BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] → 631 data = self._next_data() 632 self._num_yielded += 1 633 if self._dataset_kind == _DatasetKind.Iterable and 634 self._IterableDataset_len_called is not None and 635 self._num_yielded self._IterableDataset_len_called: File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in _SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 676 if self._pin_memory: 677 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device) File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in (.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255, in BatchEncoding.getitem(self, item) 253 return self.data[item] 254 elif self._encodings is not None: → 255 return self._encodings[item] 256 elif isinstance(item, slice): 257 return {key: self.data[key][item] for key in self.data.keys()} IndexError: list index out of range",indexerror  list index range  try predict fine tuned model use hugginface try learn fine tune pretraine model use  code transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   try make prediction fine tune model  like input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   prediction  trainerpredict  testdataset  input  get error try make prediction  indexerror traceback  recent call last  cell  8   line 7 3 input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   6  make prediction    7 prediction  trainerpredict  testdataset  input  file c  python311libsite  packagestransformerstrainerpy3305  trainerpredict  self  testdataset  ignorekeys  metrickeyprefix  3302 starttime  timetime   3304 evalloop  selfpredictionloop selfargsuselegacypredictionloop else selfevaluationloop  3305 output  evalloop  3306 testdataloader  description  prediction   ignorekey  ignorekey  metrickeyprefix  metrickeyprefix 3307  3308 totalbatchsize  selfargsevalbatchsize  selfargsworldsize 3309 f   metrickeyprefix   jitcompilationtime  outputmetric  file c  python311libsite  packagestransformerstrainerpy3408  trainerevaluationloop  self  dataloader  description  predictionlossonly  ignorekey  metrickeyprefix  3406 observednumexample  0 3407  main evaluation loop  3408 step  input enumerate  dataloader   3409  update observe num example 3410 observedbatchsize  findbatchsize  input  3411 observedbatchsize none  file c  python311libsite  packagesacceleratedataloaderpy454  dataloadersharditer  self  452  iterate one batch ahead check end 453 try   454 currentbatch  next  dataloaderiter  455 except stopiteration  456 yield file c  python311libsite  packagestorchutilsdatadataloaderpy631   basedataloaderiternext  self  628 selfsamplerit none  629  todo  bug dataloader iterator find mypy  issue  76750  pytorch  pytorch  github  630 selfreset    type  ignore  call  arg   631 datum  selfnextdata   632 selfnumyielde   1 633 selfdatasetkind    datasetkind  iterable 634 selfiterabledatasetlencalle none 635 selfnumyielded selfiterabledatasetlencalle  file c  python311libsite  packagestorchutilsdatadataloaderpy675   singleprocessdataloaderiternextdata  self  673 def  nextdata  self   674 index  selfnextindex    may raise stopiteration  675 datum  selfdatasetfetcherfetch  index   may raise stopiteration 676 selfpinmemory  677 datum   utilspinmemorypinmemory  datum  selfpinmemorydevice  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   mapdatasetfetcherfetch  self  possiblybatchedindex  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   0  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestransformerstokenizationutilsbasepy255  batchencodinggetitem  self  item  253 return selfdata  item  254 elif selfencoding none   255 return selfencoding  item  256 elif isinstance  item  slice   257 return  key  selfdata  key   item  key selfdatakey    indexerror  list index range,error encounter trainerpredict method expect dataset input  pass single example tokenized tensor  perform prediction single input  need prepare similarly dataset prepared training  use model directly prediction  s modify code make prediction single input  prepare input correctly use model directly prediction s revise code  transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   make prediction single input input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   modeleval    set model evaluation mode torchnograd     disable gradient calculation output  model    input  prediction  torchargmax  outputslogit  dim1  print  f  predict label   predictionsitem     ,indexerror  list index range  try predict fine tuned model use hugginface try learn fine tune pretraine model use  code transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   try make prediction fine tune model  like input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   prediction  trainerpredict  testdataset  input  get error try make prediction  indexerror traceback  recent call last  cell  8   line 7 3 input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   6  make prediction    7 prediction  trainerpredict  testdataset  input  file c  python311libsite  packagestransformerstrainerpy3305  trainerpredict  self  testdataset  ignorekeys  metrickeyprefix  3302 starttime  timetime   3304 evalloop  selfpredictionloop selfargsuselegacypredictionloop else selfevaluationloop  3305 output  evalloop  3306 testdataloader  description  prediction   ignorekey  ignorekey  metrickeyprefix  metrickeyprefix 3307  3308 totalbatchsize  selfargsevalbatchsize  selfargsworldsize 3309 f   metrickeyprefix   jitcompilationtime  outputmetric  file c  python311libsite  packagestransformerstrainerpy3408  trainerevaluationloop  self  dataloader  description  predictionlossonly  ignorekey  metrickeyprefix  3406 observednumexample  0 3407  main evaluation loop  3408 step  input enumerate  dataloader   3409  update observe num example 3410 observedbatchsize  findbatchsize  input  3411 observedbatchsize none  file c  python311libsite  packagesacceleratedataloaderpy454  dataloadersharditer  self  452  iterate one batch ahead check end 453 try   454 currentbatch  next  dataloaderiter  455 except stopiteration  456 yield file c  python311libsite  packagestorchutilsdatadataloaderpy631   basedataloaderiternext  self  628 selfsamplerit none  629  todo  bug dataloader iterator find mypy  issue  76750  pytorch  pytorch  github  630 selfreset    type  ignore  call  arg   631 datum  selfnextdata   632 selfnumyielde   1 633 selfdatasetkind    datasetkind  iterable 634 selfiterabledatasetlencalle none 635 selfnumyielded selfiterabledatasetlencalle  file c  python311libsite  packagestorchutilsdatadataloaderpy675   singleprocessdataloaderiternextdata  self  673 def  nextdata  self   674 index  selfnextindex    may raise stopiteration  675 datum  selfdatasetfetcherfetch  index   may raise stopiteration 676 selfpinmemory  677 datum   utilspinmemorypinmemory  datum  selfpinmemorydevice  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   mapdatasetfetcherfetch  self  possiblybatchedindex  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   0  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestransformerstokenizationutilsbasepy255  batchencodinggetitem  self  item  253 return selfdata  item  254 elif selfencoding none   255 return selfencoding  item  256 elif isinstance  item  slice   257 return  key  selfdata  key   item  key selfdatakey    indexerror  list index range error encounter trainerpredict method expect dataset input  pass single example tokenized tensor  perform prediction single input  need prepare similarly dataset prepared training  use model directly prediction  s modify code make prediction single input  prepare input correctly use model directly prediction s revise code  transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   make prediction single input input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   modeleval    set model evaluation mode torchnograd     disable gradient calculation output  model    input  prediction  torchargmax  outputslogit  dim1  print  f  predict label   predictionsitem     ,Implementation Issues
not able END function using &quot;add_conditional_edges&quot; in lang graph,"this is my code: import os from dotenv import load_dotenv load_ dotenv() from langchain_openai import ChatOpenAI from langgraph.graph import StateGraph, END from langgraph.graph import Graph, MessagesState from typing import Annotated, Any, Dict, Optional, List,Sequence, TypedDict from langgraph.graph.message import add_messages class AgentState(TypedDict): # The `add_messages` function within the annotation defines # *how* updates should be merged into the state. messages: Annotated[list, add_messages] def function1(state): return {""messages"": ""Hi""} def function2(state): return {""messages"": ""Hello""} def my_condition(state): return ""end"" workflow=StateGraph(AgentState) workflow.add_node(""agent"", function1) workflow.add_node(""tool"", function2) workflow.add_edge('agent','tool') workflow.set_entry_point(""agent"") workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) app=workflow.compile() print(app.invoke({""messages"": ""tell me about you""})) In above code I want to END function at ""function1"" and get this result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463')]} But I am getting this below result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463'), HumanMessage(content='Hello', id='7ea9ab2a-635f-46eb-8f17-d9a6af79688e')]}","['nlp', 'langchain', 'langgraph']",1,"Edges tell lang graph where to look after a node. If it's ""conditional edges,"" it will go to different next nodes in different conditions. If it's just a normal edge, it is a direct line from that node to the next node. In your code, you are adding both conditional edges and a normal edge from the same ""agent"" node. workflow.add_edge('agent','tool') ... workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) You need to decide, after the ""agent"" function is called, which thing do you want to happen next? If you want to reach END, you could put the conditional edges from ""tool"" instead of ""agent,"" which already has an edge, and all of the functions will be called.",2024-07-05 07:29:18,2024-07-06 02:56:30,1990,https://stackoverflow.com/questions/78710185/not-able-end-function-using-add-conditional-edges-in-lang-graph,"not able END function using &quot;add_conditional_edges&quot; in lang graph this is my code: import os from dotenv import load_dotenv load_ dotenv() from langchain_openai import ChatOpenAI from langgraph.graph import StateGraph, END from langgraph.graph import Graph, MessagesState from typing import Annotated, Any, Dict, Optional, List,Sequence, TypedDict from langgraph.graph.message import add_messages class AgentState(TypedDict): # The `add_messages` function within the annotation defines # *how* updates should be merged into the state. messages: Annotated[list, add_messages] def function1(state): return {""messages"": ""Hi""} def function2(state): return {""messages"": ""Hello""} def my_condition(state): return ""end"" workflow=StateGraph(AgentState) workflow.add_node(""agent"", function1) workflow.add_node(""tool"", function2) workflow.add_edge('agent','tool') workflow.set_entry_point(""agent"") workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) app=workflow.compile() print(app.invoke({""messages"": ""tell me about you""})) In above code I want to END function at ""function1"" and get this result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463')]} But I am getting this below result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463'), HumanMessage(content='Hello', id='7ea9ab2a-635f-46eb-8f17-d9a6af79688e')]}",able end function use  quot  addconditionaledge  quot  lang graph code  import os dotenv import loaddotenv load  dotenv   langchainopenai import chatopenai langgraphgraph import stategraph  end langgraphgraph import graph  messagesstate type import annotated   dict  optional  list  sequence  typeddict langgraphgraphmessage import addmessage class agentstate  typeddict     addmessage  function within annotation define    update merge state  message  annotated  list  addmessage  def function1  state   return    message     hi   def function2  state   return    message     hello   def mycondition  state   return   end  workflow  stategraph  agentstate  workflowaddnode    agent   function1  workflowaddnode    tool   function2  workflowaddedge   agent    tool   workflowsetentrypoint    agent   workflowaddconditionaledge    agent   mycondition     end   end   app  workflowcompile   print  appinvoke     message     tell     code want end function   function1  get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463     get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463    humanmessage  contenthello   id7ea9ab2a635f46eb8f17  d9a6af79688e    ,edge tell lang graph look node  s   conditional edge   go different next node different condition  s normal edge  direct line node next node  code  add conditional edge normal edge   agent  node  workflowaddedge   agent    tool    workflowaddconditionaledge    agent   mycondition     end   end   need decide    agent  function call  thing want happen next  want reach end  could put conditional edge   tool  instead   agent   already edge  function call ,able end function use  quot  addconditionaledge  quot  lang graph code  import os dotenv import loaddotenv load  dotenv   langchainopenai import chatopenai langgraphgraph import stategraph  end langgraphgraph import graph  messagesstate type import annotated   dict  optional  list  sequence  typeddict langgraphgraphmessage import addmessage class agentstate  typeddict     addmessage  function within annotation define    update merge state  message  annotated  list  addmessage  def function1  state   return    message     hi   def function2  state   return    message     hello   def mycondition  state   return   end  workflow  stategraph  agentstate  workflowaddnode    agent   function1  workflowaddnode    tool   function2  workflowaddedge   agent    tool   workflowsetentrypoint    agent   workflowaddconditionaledge    agent   mycondition     end   end   app  workflowcompile   print  appinvoke     message     tell     code want end function   function1  get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463     get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463    humanmessage  contenthello   id7ea9ab2a635f46eb8f17  d9a6af79688e     edge tell lang graph look node  s   conditional edge   go different next node different condition  s normal edge  direct line node next node  code  add conditional edge normal edge   agent  node  workflowaddedge   agent    tool    workflowaddconditionaledge    agent   mycondition     end   end   need decide    agent  function call  thing want happen next  want reach end  could put conditional edge   tool  instead   agent   already edge  function call ,Implementation Issues
How to make dynamic API calls based on user input in a Gemini application python nlp?,"I'm working on a Gemini application where I need to make dynamic API calls based on user input. Specifically, I want to perform different API requests depending on the user's query. For example, if the user asks for the latest news, the application should make an API call to a news service. Similarly, if the user wants to know the current weather, the application should fetch data from a weather API. Here's a basic outline of what I'm trying to achieve: Capture the user's input. Determine the type of request based on the input (e.g., news or weather). Make the appropriate API call and return the data to the user. I want to avoid using third-party libraries like RAG for this purpose. How can I implement this functionality in a clean and efficient way within my Gemini application? And i dont want to use a approach like this ' def handle_user_input(user_input): if ""news"" in user_input: # Call news API pass elif ""weather"" in user_input: # Call weather API pass else: return ""I can't handle that request.""","['python', 'nlp', 'request', 'google-cloud-vertex-ai', 'google-gemini']",1,"I suggest using a dictionary, basically what you u are looking to do is something called a ""strategy pattern"". You want to choose a different ""strategy"" in your case api based on different input. so the way it will look is you have a dictionary {""news"": ""news-url"", ""weather"": ""weather-url""} and then your code will be very simple user_input = """" url = api_dict[user_input] # make api call and at the start of your program you need to initialize your dict. having it in this structure makes it so that you could even write a json file, parse it and have it used as your dictionary meaning you dont even need to edit code to update your available API's At the end of the day when making a dynamic program, the options will always need to be inputted somehow, worst case is within the code like the if that you successfully understood is bad, as it makes the code long, unreadable and hard to modify. An improvement like I suggested is creating that dynamic options map as a file or a python dictionary on the start of the program and using that. the most dynamic option would be if the code is able to generate different responses to different input purely by the input. such an example would be gemini and chat-gpt where you can ask the some api route both ""what is the weather in france"" and ""how much is 4 + 4"" and it will provide a fitting answer (although not always correct) EDIT (based on last comment) Now I understand your question better. I cant say I understand your reasoning but the implementation is very simple, here is a chat I had with: chatGPT here are a few api routes http://test/weather http://test/stocks http://test/facts in the next message I will put user input and based on the input you need to output the appropriate url ChatGPT Got it! Please provide the user input in your next message, and I'll give you the appropriate URL. what should I wear tomorrow ChatGPT For the query ""what should I wear tomorrow,"" the appropriate URL is: http://test/weather you will need chatGPT to alaways know the context for your URLs, meaning always provide or from a longer session have it available. then you query with the user input and get your URL as a response key issues: chat gpt makes stuff up, it can easily make stuff up, best case it will provide no url result for such a case, but im sure some edge cases will make it generate a url that doesnt exist in your context expansive, using chat gpt to process user input simply to make a http request is expansive, it means a lot of requests, a lot of queries, a big context window (tokens) depending on your url count and input length. whatever you are building will be quite expansive to run",2024-07-04 15:11:18,2024-07-04 16:47:22,458,https://stackoverflow.com/questions/78707861/how-to-make-dynamic-api-calls-based-on-user-input-in-a-gemini-application-python,"How to make dynamic API calls based on user input in a Gemini application python nlp? I'm working on a Gemini application where I need to make dynamic API calls based on user input. Specifically, I want to perform different API requests depending on the user's query. For example, if the user asks for the latest news, the application should make an API call to a news service. Similarly, if the user wants to know the current weather, the application should fetch data from a weather API. Here's a basic outline of what I'm trying to achieve: Capture the user's input. Determine the type of request based on the input (e.g., news or weather). Make the appropriate API call and return the data to the user. I want to avoid using third-party libraries like RAG for this purpose. How can I implement this functionality in a clean and efficient way within my Gemini application? And i dont want to use a approach like this ' def handle_user_input(user_input): if ""news"" in user_input: # Call news API pass elif ""weather"" in user_input: # Call weather API pass else: return ""I can't handle that request.""",make dynamic api call base user input gemini application python nlp   m work gemini application need make dynamic api call base user input  specifically  want perform different api request depend user s query  example  user ask late news  application make api call news service  similarly  user want know current weather  application fetch datum weather api  s basic outline  m try achieve  capture user s input  determine type request base input  eg  news weather   make appropriate api call return datum user  want avoid use third  party library like rag purpose  implement functionality clean efficient way within gemini application  do not want use approach like  def handleuserinput  userinput     news  userinput   call news api pass elif   weather  userinput   call weather api pass else  return   can not handle request  ,suggest use dictionary  basically u look something call   strategy pattern   want choose different   strategy  case api base different input  way look dictionary    news     news  url     weather     weather  url   code simple userinput     url  apidict  userinput   make api call start program need initialize dict  structure make could even write json file  parse use dictionary meaning do not even need edit code update available api s end day make dynamic program  option always need inputte somehow  bad case within code like successfully understand bad  make code long  unreadable hard modify  improvement like suggest create dynamic option map file python dictionary start program use  dynamic option would code able generate different response different input purely input  example would gemini chat  gpt ask api route   weather france    much 4  4  provide fitting answer  although always correct  edit  base last comment  understand question well  can not say understand reasoning implementation simple  chat  chatgpt api route http  test  weather http  test  stock http  test  fact next message put user input base input need output appropriate url chatgpt got  please provide user input next message  will give appropriate url  wear tomorrow chatgpt query   wear tomorrow   appropriate url  http  test  weather need chatgpt alaway know context url  meaning always provide long session available  query user input get url response key issue  chat gpt make stuff  easily make stuff  good case provide url result case  i m sure edge case make generate url do not exist context expansive  use chat gpt process user input simply make http request expansive  mean lot request  lot query  big context window  tokens  depend url count input length  whatever build quite expansive run,make dynamic api call base user input gemini application python nlp   m work gemini application need make dynamic api call base user input  specifically  want perform different api request depend user s query  example  user ask late news  application make api call news service  similarly  user want know current weather  application fetch datum weather api  s basic outline  m try achieve  capture user s input  determine type request base input  eg  news weather   make appropriate api call return datum user  want avoid use third  party library like rag purpose  implement functionality clean efficient way within gemini application  do not want use approach like  def handleuserinput  userinput     news  userinput   call news api pass elif   weather  userinput   call weather api pass else  return   can not handle request   suggest use dictionary  basically u look something call   strategy pattern   want choose different   strategy  case api base different input  way look dictionary    news     news  url     weather     weather  url   code simple userinput     url  apidict  userinput   make api call start program need initialize dict  structure make could even write json file  parse use dictionary meaning do not even need edit code update available api s end day make dynamic program  option always need inputte somehow  bad case within code like successfully understand bad  make code long  unreadable hard modify  improvement like suggest create dynamic option map file python dictionary start program use  dynamic option would code able generate different response different input purely input  example would gemini chat  gpt ask api route   weather france    much 4  4  provide fitting answer  although always correct  edit  base last comment  understand question well  can not say understand reasoning implementation simple  chat  chatgpt api route http  test  weather http  test  stock http  test  fact next message put user input base input need output appropriate url chatgpt got  please provide user input next message  will give appropriate url  wear tomorrow chatgpt query   wear tomorrow   appropriate url  http  test  weather need chatgpt alaway know context url  meaning always provide long session available  query user input get url response key issue  chat gpt make stuff  easily make stuff  good case provide url result case  i m sure edge case make generate url do not exist context expansive  use chat gpt process user input simply make http request expansive  mean lot request  lot query  big context window  tokens  depend url count input length  whatever build quite expansive run,Implementation Issues
Alternative to Receptive field in Transformers and what factors impact it,"I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting? In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images. Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?","['nlp', 'huggingface-transformers', 'receptive-field']",1,"How do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties? For modern Transformer-based Language Models (LMs), there are some empirical ""scaling laws,"" such as the Chinchilla scaling laws ( Wikipedia ), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. So far, most LMs seem to roughly follow Chinchilla scaling. There is another kind of scaling, which is closer to a ""receptive field"", that I talk about below. Do you guys have something similar in NLP? Kind of. Transformer-based LMs can be thought to have a ""receptive field"" similar to CNN layers, as the attention mechanism in the Transformer operates on a pre-defined ""context window"" or ""context length"", which is the maximum number of tokens the layer can look at (""attend to"") at any given time, similar to a CNN kernel. However, with the introduction of new positional encoding (PE) approaches, such as Rotary Positional Encoding (RoPE) , and modified attention architectures, like Sliding Window Attention (SWA) , this is not strictly accurate. Scaling in terms of ""context length"" is of much interest, but usually, it is very difficult to scale Transformers this way, because of attention being a ($\mathcal{O}(N^2)$) (O(N^2)) operation. So, usually, researchers go towards deeper architectures with more parameters (""over-parameterization"") that can allow the model to ""memorize"" as much of the large training corpus as it can (""overfitting""), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).",2024-06-29 04:58:30,2024-06-29 20:38:23,147,https://stackoverflow.com/questions/78685093/alternative-to-receptive-field-in-transformers-and-what-factors-impact-it,"Alternative to Receptive field in Transformers and what factors impact it I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting? In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images. Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?",alternative receptive field transformers factor impact two transformer network  one 3 head per attention 15 layer total second one 5 head per layer 30 layer total  give arbitrary set document  2048 token per   find  network go well use less prone overfitting  computer vision concept call    receptive field   allow we understand big small network need use  instance  cnn 120 layer cnn 70 layer  calculate receptive field understand one go perform well particular dataset image  guy something similar nlp  understand whether one architecture optimal use versus anotherhave set text document unique property ,understand whether one architecture optimal use versus another  set text document unique property  modern transformer  base language models  lms   empirical   scale law   chinchilla scale law  wikipedia   essentially say large  deep  model layer  ie  parameter tend perform well  far  lms seem roughly follow chinchilla scaling  another kind scaling  close   receptive field   talk  guy something similar nlp  kind  transformer  base lm think   receptive field  similar cnn layer  attention mechanism transformer operate pre  define   context window    context length   maximum number token layer look    attend   give time  similar cnn kernel  however  introduction new positional encoding  pe  approach  rotary positional encoding  rope   modify attention architecture  like sliding window attention  swa   strictly accurate  scale term   context length  much interest  usually  difficult scale transformers way  attention   mathcal    n2      n2   operation   usually  researcher go towards deep architecture parameter    over  parameterization   allow model   memorize  much large training corpus    overfitte    perform reasonably well  fine  tune down  stream task  least representative example training corpus  ,alternative receptive field transformers factor impact two transformer network  one 3 head per attention 15 layer total second one 5 head per layer 30 layer total  give arbitrary set document  2048 token per   find  network go well use less prone overfitting  computer vision concept call    receptive field   allow we understand big small network need use  instance  cnn 120 layer cnn 70 layer  calculate receptive field understand one go perform well particular dataset image  guy something similar nlp  understand whether one architecture optimal use versus anotherhave set text document unique property  understand whether one architecture optimal use versus another  set text document unique property  modern transformer  base language models  lms   empirical   scale law   chinchilla scale law  wikipedia   essentially say large  deep  model layer  ie  parameter tend perform well  far  lms seem roughly follow chinchilla scaling  another kind scaling  close   receptive field   talk  guy something similar nlp  kind  transformer  base lm think   receptive field  similar cnn layer  attention mechanism transformer operate pre  define   context window    context length   maximum number token layer look    attend   give time  similar cnn kernel  however  introduction new positional encoding  pe  approach  rotary positional encoding  rope   modify attention architecture  like sliding window attention  swa   strictly accurate  scale term   context length  much interest  usually  difficult scale transformers way  attention   mathcal    n2      n2   operation   usually  researcher go towards deep architecture parameter    over  parameterization   allow model   memorize  much large training corpus    overfitte    perform reasonably well  fine  tune down  stream task  least representative example training corpus  ,Basic Understanding
implement a search engine chain using tavily in langchain,"I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts: Tavily search platform Parser with the aim of converting search output data into standard LangChai documents. I wrote this code but I don't know how to change tavily output format into standard form of document: from langchain_core.documents.base import Document from langchain_community.tools.tavily_search import TavilySearchResults search = TavilySearchResults(max_results=5) class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") search_parser = PydanticOutputParser(pydantic_object=ParsedDocument) search_engine_chain = search | search_parser I would be grateful if you could help me how to change this code.","['python', 'nlp', 'search-engine', 'langchain', 'chain']",1,"I finally found the answer: class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") # Define a custom parser def custom_parser(search_results): parsed_documents = [] for result in search_results: # Adjust this line based on the actual structure of search_results parsed_document = ParsedDocument(content=result['content'], url=result['url']) document = Document(page_content=parsed_document.content, metadata={'url': parsed_document.url}) parsed_documents.append(document) return parsed_documents search_engine_chain = search | custom_parser",2024-06-24 10:01:06,2024-08-25 06:24:37,517,https://stackoverflow.com/questions/78661728/implement-a-search-engine-chain-using-tavily-in-langchain,"implement a search engine chain using tavily in langchain I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts: Tavily search platform Parser with the aim of converting search output data into standard LangChai documents. I wrote this code but I don't know how to change tavily output format into standard form of document: from langchain_core.documents.base import Document from langchain_community.tools.tavily_search import TavilySearchResults search = TavilySearchResults(max_results=5) class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") search_parser = PydanticOutputParser(pydantic_object=ParsedDocument) search_engine_chain = search | search_parser I would be grateful if you could help me how to change this code.",implement search engine chain use tavily langchain want implement search engine chain use tavily langchain  chain give user s query input return 5 relate document  retrieve document must content document pagecontent url correspond site metadata definition langchain documents  must use langchaincoredocumentsbase  document class define document  chain two main part  tavily search platform parser aim convert search output datum standard langchai document  write code not know change tavily output format standard form document  langchaincoredocumentsbase import document langchaincommunitytoolstavilysearch import tavilysearchresult search  tavilysearchresult  maxresults5  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search    searchparser  pydanticoutputparser  pydanticobject  parseddocument  searchenginechain  search  searchparser would grateful could help change code ,finally find answer  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search     define custom parser def custompars  searchresults   parseddocument    result searchresult   adjust line base actual structure searchresult parseddocument  parseddocument  content  result   content    url  result   url    document  document  pagecontent  parseddocumentcontent  metadata   url   parseddocumenturl   parseddocumentsappend  document  return parseddocument searchenginechain  search  customparser,implement search engine chain use tavily langchain want implement search engine chain use tavily langchain  chain give user s query input return 5 relate document  retrieve document must content document pagecontent url correspond site metadata definition langchain documents  must use langchaincoredocumentsbase  document class define document  chain two main part  tavily search platform parser aim convert search output datum standard langchai document  write code not know change tavily output format standard form document  langchaincoredocumentsbase import document langchaincommunitytoolstavilysearch import tavilysearchresult search  tavilysearchresult  maxresults5  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search    searchparser  pydanticoutputparser  pydanticobject  parseddocument  searchenginechain  search  searchparser would grateful could help change code  finally find answer  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search     define custom parser def custompars  searchresults   parseddocument    result searchresult   adjust line base actual structure searchresult parseddocument  parseddocument  content  result   content    url  result   url    document  document  pagecontent  parseddocumentcontent  metadata   url   parseddocumenturl   parseddocumentsappend  document  return parseddocument searchenginechain  search  customparser,Implementation Issues
Understanding the results of Transformers Learn In Context with Gradient Descent,"I'm trying to implement this paper: https://arxiv.org/pdf/2212.07677 (Here's their code): https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually. As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?","['machine-learning', 'nlp', 'large-language-model', 'transformer-model', 'meta-learning']",1,"What data are you using in your replication? As far as I can tell this paper does not mention explicitly the parameters of the data used for the particular result you are trying to replicate. Indeed, it tests a variety of alpha values for the distributions used in figure 6. It is feasible for the loss to be low even after one step of GD if the alpha value is low. If you find the same trends in relative behavior of GD and transformer layers, I don't think it's important to match the exact loss values.",2024-06-18 20:43:45,2024-06-20 01:45:25,136,https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent,"Understanding the results of Transformers Learn In Context with Gradient Descent I'm trying to implement this paper: https://arxiv.org/pdf/2212.07677 (Here's their code): https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually. As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?",understand result transformer learn context gradient descent  m try implement paper    arxivorg  pdf221207677  s code     githubcom  google  research  self  organise  system  tree  master  transformerslearniclbygd  m struggle match experimental result  specifically  simple gd model  single layer single head softmax   obtain constant low loss roughly 020 test datum  not quite understand case  conceptually  understand  model single iteration gradient descent datum  would reach low loss  would loss constant  near constant training step  not train learning rate gd model ,datum use replication  far tell paper mention explicitly parameter datum use particular result try replicate  indeed  test variety alpha value distribution use figure 6  feasible loss low even one step gd alpha value low  find trend relative behavior gd transformer layer  not think s important match exact loss value ,understand result transformer learn context gradient descent  m try implement paper    arxivorg  pdf221207677  s code     githubcom  google  research  self  organise  system  tree  master  transformerslearniclbygd  m struggle match experimental result  specifically  simple gd model  single layer single head softmax   obtain constant low loss roughly 020 test datum  not quite understand case  conceptually  understand  model single iteration gradient descent datum  would reach low loss  would loss constant  near constant training step  not train learning rate gd model  datum use replication  far tell paper mention explicitly parameter datum use particular result try replicate  indeed  test variety alpha value distribution use figure 6  feasible loss low even one step gd alpha value low  find trend relative behavior gd transformer layer  not think s important match exact loss value ,Implementation Issues
Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline,"I'm currently working on a project using spaCy with the German trained pipeline de_dep_news_trf . Unfortunately, I'm having issues with named entity recognition (NER). When I run a simple sentence like ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" , no entities are detected. I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project: python.exe -m pip install --upgrade pip pip install -U pip setuptools wheel pip install -U spacy python -m spacy download de_dep_news_trf --timeout 600 pip install spacy[transformers] Here is a snippet of my code: import spacy def process_text_with_spacy(text_to_process): doc = nlp(text_to_process) data = { ""text"": text_to_process, ""sentences"": [] } for sent in doc.sents: process_sentence_data = { ""sentence"": sent.text, ""entities"": [] } for ent in sent.ents: process_sentence_data[""entities""].append({ ""text"": ent.text, ""start"": ent.start_char, ""end"": ent.end_char, ""label"": ent.label_ }) data[""sentences""].append(process_sentence_data) return data nlp = spacy.load('de_dep_news_trf') sample_text = ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" processed_data = process_text_with_spacy(sample_text) print(""Text:"", sample_text) for sentence_data in processed_data[""sentences""]: print(""Sentence:"", sentence_data[""sentence""]) print(""Entities:"", sentence_data[""entities""]) Output: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [] When using de_core_news_lg , the output for each sentence is: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}] However, when I use de_dep_news_trf , the results are empty. Model de_dep_news_trf is selected based on ""accuracy"" from the SpaCy website. Could someone explain why de_dep_news_trf does not return the same result? Is there a specific reason or setting that could cause this difference? Thank you for your help!","['python', 'nlp', 'spacy']",1,"Problem is because this model doesn't have function to recognize entities. See documentation for de_dep_news_trf - it has components transformer, tagger, morphologizer, parser, lemmatizer, attribute_ruler but no ner for EntityRecognizer So it may need to use one of other models : de_core_news_sm de_core_news_md de_core_news_lg",2024-06-17 09:14:29,2024-06-17 11:34:37,201,https://stackoverflow.com/questions/78631769/problems-with-named-entity-recognition-in-spacy-using-german-de-dep-news-trf-pip,"Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline I'm currently working on a project using spaCy with the German trained pipeline de_dep_news_trf . Unfortunately, I'm having issues with named entity recognition (NER). When I run a simple sentence like ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" , no entities are detected. I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project: python.exe -m pip install --upgrade pip pip install -U pip setuptools wheel pip install -U spacy python -m spacy download de_dep_news_trf --timeout 600 pip install spacy[transformers] Here is a snippet of my code: import spacy def process_text_with_spacy(text_to_process): doc = nlp(text_to_process) data = { ""text"": text_to_process, ""sentences"": [] } for sent in doc.sents: process_sentence_data = { ""sentence"": sent.text, ""entities"": [] } for ent in sent.ents: process_sentence_data[""entities""].append({ ""text"": ent.text, ""start"": ent.start_char, ""end"": ent.end_char, ""label"": ent.label_ }) data[""sentences""].append(process_sentence_data) return data nlp = spacy.load('de_dep_news_trf') sample_text = ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" processed_data = process_text_with_spacy(sample_text) print(""Text:"", sample_text) for sentence_data in processed_data[""sentences""]: print(""Sentence:"", sentence_data[""sentence""]) print(""Entities:"", sentence_data[""entities""]) Output: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [] When using de_core_news_lg , the output for each sentence is: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}] However, when I use de_dep_news_trf , the results are empty. Model de_dep_news_trf is selected based on ""accuracy"" from the SpaCy website. Could someone explain why de_dep_news_trf does not return the same result? Is there a specific reason or setting that could cause this difference? Thank you for your help!",problem name entity recognition spacy use german dedepnewstrf pipeline  m currently work project use spacy german train pipeline dedepnewstrf  unfortunately   m issue name entity recognition  ner   run simple sentence like   berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin    entity detect   ve follow step set python environment  312   windows  pycharm community project  pythonexe m pip install  upgrade pip pip install u pip setuptools wheel pip install u spacy python m spacy download dedepnewstrf  timeout 600 pip install spacy  transformer  snippet code  import spacy def processtextwithspacy  texttoprocess   doc  nlp  texttoprocess  datum     text   texttoprocess    sentence      send docsent  processsentencedata     sentence   senttext    entity      ent sentent  processsentencedata    entity   append     text   enttext    start   entstartchar    end   entendchar    label   entlabel    datum    sentence   append  processsentencedata  return datum nlp  spacyload   dedepnewstrf   sampletext    berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin   processeddata  processtextwithspacy  sampletext  print    text    sampletext  sentencedata processeddata    sentence    print    sentence    sentencedata    sentence    print    entities    sentencedata    entity    output  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity    sentence  angela merkel war die bundeskanzlerin  entity    use decorenewslg  output sentence  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity     text    berlin    start   0   end   6   label    loc      text    deutschland    start   30   end   41   label    loc    sentence  angela merkel war die bundeskanzlerin  entity     text    angela merkel    start   43   end   56   label    per    however  use dedepnewstrf  result empty  model dedepnewstrf select base   accuracy  spacy website  could someone explain dedepnewstrf return result  specific reason setting could cause difference  thank help ,problem model not function recognize entity  see documentation dedepnewstrf  component transformer  tagger  morphologizer  parser  lemmatizer  attributeruler ner entityrecognizer may need use one model  decorenewssm decorenewsmd decorenewslg,problem name entity recognition spacy use german dedepnewstrf pipeline  m currently work project use spacy german train pipeline dedepnewstrf  unfortunately   m issue name entity recognition  ner   run simple sentence like   berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin    entity detect   ve follow step set python environment  312   windows  pycharm community project  pythonexe m pip install  upgrade pip pip install u pip setuptools wheel pip install u spacy python m spacy download dedepnewstrf  timeout 600 pip install spacy  transformer  snippet code  import spacy def processtextwithspacy  texttoprocess   doc  nlp  texttoprocess  datum     text   texttoprocess    sentence      send docsent  processsentencedata     sentence   senttext    entity      ent sentent  processsentencedata    entity   append     text   enttext    start   entstartchar    end   entendchar    label   entlabel    datum    sentence   append  processsentencedata  return datum nlp  spacyload   dedepnewstrf   sampletext    berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin   processeddata  processtextwithspacy  sampletext  print    text    sampletext  sentencedata processeddata    sentence    print    sentence    sentencedata    sentence    print    entities    sentencedata    entity    output  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity    sentence  angela merkel war die bundeskanzlerin  entity    use decorenewslg  output sentence  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity     text    berlin    start   0   end   6   label    loc      text    deutschland    start   30   end   41   label    loc    sentence  angela merkel war die bundeskanzlerin  entity     text    angela merkel    start   43   end   56   label    per    however  use dedepnewstrf  result empty  model dedepnewstrf select base   accuracy  spacy website  could someone explain dedepnewstrf return result  specific reason setting could cause difference  thank help  problem model not function recognize entity  see documentation dedepnewstrf  component transformer  tagger  morphologizer  parser  lemmatizer  attributeruler ner entityrecognizer may need use one model  decorenewssm decorenewsmd decorenewslg,Implementation Issues
How do we add/modify the normalizer in a pretrained Huggingface tokenizer?,"Given a Huggingface tokenizer that already have a normalizer, e.g. ""mistralai/Mistral-7B-v0.1"" , we can do this to modify the normalizer import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.1"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) assert old_tok.backend_tokenizer.normalizer != None new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) [out]: >>> print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) <s> I foo you < br > hello world >>> print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) <s> I bar you hello world But when this hot-plug normalizer modification don't always work, if we change it to ""mistralai/Mistral-7B-v0.3"" , it fails to work: import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.3"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) [out]: <s> I foo you < br > hello world <s> I foo you < br > hello world How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Can any normalizer from a pretrained tokenizer be modified or just specific ones? If the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?","['python', 'nlp', 'large-language-model', 'huggingface-tokenizers']",1,"This looks like a bug. The v0.1 tokenizer has a normalizer by default, which can be seen by looking at the mistral-78-v0.1/tokenizer.json file: { ... ""normalizer"": { ""type"": ""Sequence"", ""normalizers"": [ { ""type"": ""Prepend"", ""prepend"": ""▁"" }, { ""type"": ""Replace"", ""pattern"": { ""String"": "" "" }, ""content"": ""▁"" } ] }, ... } After modifying the .backend_tokenizer.normalizer object, the modification are saved to the tokenizer.json file. In the v0.3 version, the mistral-78-v0.1/tokenizer.json file has no value for the normalizer: { ... ""normalizer"": null, ... } Modifying the normalizer and saving the model does write the changes to the JSON file, but it is not getting picked up on reload using AutoTokenizer.from_pretrained . I am not sure why, but it is entirely possible the tokenizer.model file indicates no normalizer is the default and it simply does not load it. However, you can get the tokenizer to load correctly - with the custom normalizer - by instantiating the matched tokenizer class explicitly and passing in the tokenizer.model and tokenizer.json paths along with the values from the tokenizer_config.json file. In this case it is the LlamaTokenizerFast class. from transformers import AutoTokenizer, LlamaTokenizerFast, AddedToken from tokenizers.normalizers import Sequence, Replace, Prepend ### load, modify, and save tok = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.3"") tok.backend_tokenizer.normalizer = Sequence([ Prepend('_'), Replace('_', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n') ]) tok.save_pretrained(""mistral-7B-v0.3-custom/"") ### read in config, construct the AddedToken objects with open('mistral-7B-v0.3-custom/tokenizer_config.json') as fp: config = json.load(fp) config['added_tokens_decoder'] = { int(k): AddedToken(**v) for k, v in config.pop('added_tokens_decoder').items() } ### load from saved files tok_custom = LlamaTokenizerFast( 'mistral-7B-v0.3-custom/tokenizer.model', 'mistral-7B-v0.3-custom/tokenizer.json', **config, ) test_str = ""I foo you<br>hello world"" print(' '.join(tok_custom.batch_decode(tok_custom(test_str)['input_ids']))) # prints: #<s> I bar you # hello world If you don't want to specify the tokenizer class explicitly, you can load the model the the AutoTokenizer , and then load it again using from the resulting class. It is a hacky work-around. tok_path = ""path/to/mistral-7B-v0.3-custom/"" with open(f'{tok_path}/tokenizer_config.json') as fp: config = json.load(fp) config['added_tokens_decoder'] = { int(k): AddedToken(**v) for k, v in config.pop('added_tokens_decoder').items() } tok = AutoTokenizer.from_pretrained(tok_path).__class__( f'{tok_path}/tokenizer.model', f'{tok_path}/tokenizer.json', **config, )",2024-06-12 11:03:59,2024-06-14 17:18:38,311,https://stackoverflow.com/questions/78612251/how-do-we-add-modify-the-normalizer-in-a-pretrained-huggingface-tokenizer,"How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Given a Huggingface tokenizer that already have a normalizer, e.g. ""mistralai/Mistral-7B-v0.1"" , we can do this to modify the normalizer import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.1"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) assert old_tok.backend_tokenizer.normalizer != None new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) [out]: >>> print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) <s> I foo you < br > hello world >>> print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) <s> I bar you hello world But when this hot-plug normalizer modification don't always work, if we change it to ""mistralai/Mistral-7B-v0.3"" , it fails to work: import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.3"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) [out]: <s> I foo you < br > hello world <s> I foo you < br > hello world How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Can any normalizer from a pretrained tokenizer be modified or just specific ones? If the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?",add  modify normalizer pretraine huggingface tokenizer  give huggingface tokenizer already normalizer  eg    mistralai  mistral7b  v01   modify normalizer import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v01  oldtok  autotokenizerfrompretraine  tokenizername  assert oldtokbackendtokenizernormalizer   none newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname        print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids        foo  br  hello world    print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids        bar hello world hot  plug normalizer modification not always work  change   mistralai  mistral7b  v03   fail work  import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v03  oldtok  autotokenizerfrompretraine  tokenizername  newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname  print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids      print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids           foo  br  hello world   foo  br  hello world add  modify normalizer pretraine huggingface tokenizer  normalizer pretraine tokenizer modify specific one  latter  know pretraine tokenizer s normalizer extend modify ,look like bug  v01 tokenizer normalizer default  see look mistral78  v01  tokenizerjson file      normalizer      type     sequence     normalizer       type     prepend     prepend            type     replace     pattern      string           content             modify backendtokenizernormalizer object  modification save tokenizerjson file  v03 version  mistral78  v01  tokenizerjson file value normalizer      normalizer   null    modify normalizer saving model write change json file  getting pick reload use autotokenizerfrompretrained  sure  entirely possible tokenizermodel file indicate normalizer default simply load  however  get tokenizer load correctly  custom normalizer  instantiate match tokenizer class explicitly pass tokenizermodel tokenizerjson path along value tokenizerconfigjson file  case llamatokenizerfast class  transformer import autotokenizer  llamatokenizerfast  addedtoken tokenizersnormalizer import sequence  replace  prepend    load  modify  save tok  autotokenizerfrompretraine    mistralai  mistral7b  v03   tokbackendtokenizernormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     toksavepretraine    mistral7b  v03  custom      read config  construct addedtoken object open   mistral7b  v03  custom  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items       load save file tokcustom  llamatokenizerfast   mistral7b  v03  custom  tokenizermodel    mistral7b  v03  custom  tokenizerjson     config   teststr    foo  br  hello world  print    join  tokcustombatchdecode  tokcustom  teststr    inputids       print     bar  hello world not want specify tokenizer class explicitly  load model autotokenizer  load use result class  hacky work  around  tokpath    path  to  mistral7b  v03  custom  open  f   tokpath  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items    tok  autotokenizerfrompretraine  tokpath  class    f   tokpath  tokenizermodel   f   tokpath  tokenizerjson     config  ,add  modify normalizer pretraine huggingface tokenizer  give huggingface tokenizer already normalizer  eg    mistralai  mistral7b  v01   modify normalizer import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v01  oldtok  autotokenizerfrompretraine  tokenizername  assert oldtokbackendtokenizernormalizer   none newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname        print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids        foo  br  hello world    print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids        bar hello world hot  plug normalizer modification not always work  change   mistralai  mistral7b  v03   fail work  import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v03  oldtok  autotokenizerfrompretraine  tokenizername  newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname  print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids      print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids           foo  br  hello world   foo  br  hello world add  modify normalizer pretraine huggingface tokenizer  normalizer pretraine tokenizer modify specific one  latter  know pretraine tokenizer s normalizer extend modify  look like bug  v01 tokenizer normalizer default  see look mistral78  v01  tokenizerjson file      normalizer      type     sequence     normalizer       type     prepend     prepend            type     replace     pattern      string           content             modify backendtokenizernormalizer object  modification save tokenizerjson file  v03 version  mistral78  v01  tokenizerjson file value normalizer      normalizer   null    modify normalizer saving model write change json file  getting pick reload use autotokenizerfrompretrained  sure  entirely possible tokenizermodel file indicate normalizer default simply load  however  get tokenizer load correctly  custom normalizer  instantiate match tokenizer class explicitly pass tokenizermodel tokenizerjson path along value tokenizerconfigjson file  case llamatokenizerfast class  transformer import autotokenizer  llamatokenizerfast  addedtoken tokenizersnormalizer import sequence  replace  prepend    load  modify  save tok  autotokenizerfrompretraine    mistralai  mistral7b  v03   tokbackendtokenizernormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     toksavepretraine    mistral7b  v03  custom      read config  construct addedtoken object open   mistral7b  v03  custom  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items       load save file tokcustom  llamatokenizerfast   mistral7b  v03  custom  tokenizermodel    mistral7b  v03  custom  tokenizerjson     config   teststr    foo  br  hello world  print    join  tokcustombatchdecode  tokcustom  teststr    inputids       print     bar  hello world not want specify tokenizer class explicitly  load model autotokenizer  load use result class  hacky work  around  tokpath    path  to  mistral7b  v03  custom  open  f   tokpath  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items    tok  autotokenizerfrompretraine  tokpath  class    f   tokpath  tokenizermodel   f   tokpath  tokenizerjson     config  ,Basic Understanding
HuggingFace pipeline - Debug prompt,"I've defined a pipeline using Huggingface transformer library. pipe = pipeline( ""text-generation"", model=myllm, tokenizer=tokenizer, max_new_tokens=512, ) I'd like to test it: result = pipe(""Some input prompt for the LLM"") How can I debug the prompt actually sent to the LLM? I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?","['python', 'nlp', 'huggingface-transformers']",1,"you may use preprocess method and check generated token_ids. Generally would suggest to more closely look on the code of the method, it will explain what is happening with the prompt before model forward pass. params = pipe._preprocess_params pipe.preprocess(""I can't believe you did such a "", **params) # Returns: # {'input_ids': tensor([[ 40, 460, 470, 1975, 345, 750, 884, 257, 220]]), # 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), # 'prompt_text': ""I can't believe you did such a ""} Internally preprocess is calling either for chats tokenizer.chat_template for simple text prompts tokenizer(prompt_text) . For example for ""gpt-2"" model default tokenizer outputs token_ids and masks : pipe.tokenizer(""I can't believe you did such a "") # Returns: # {'input_ids': [40, 460, 470, 1975, 345, 750, 884, 257, 220], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} Another thing to consider during prompts debug is to look what will happen if you'll invert token ids: pipe = pipeline( ""text-generation"", model=""openai-community/gpt2"" ) inputs = pipe.tokenizer(""I can't believe you did such a "") pipe.tokenizer.convert_ids_to_tokens(inputs['input_ids']) # ['I', 'Ġcan', ""'t"", 'Ġbelieve', 'Ġyou', 'Ġdid', 'Ġsuch', 'Ġa', 'Ġ']",2024-06-06 12:36:47,2024-06-06 14:21:59,384,https://stackoverflow.com/questions/78586621/huggingface-pipeline-debug-prompt,"HuggingFace pipeline - Debug prompt I've defined a pipeline using Huggingface transformer library. pipe = pipeline( ""text-generation"", model=myllm, tokenizer=tokenizer, max_new_tokens=512, ) I'd like to test it: result = pipe(""Some input prompt for the LLM"") How can I debug the prompt actually sent to the LLM? I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?",huggingface pipeline  debug prompt  ve define pipeline use huggingface transformer library  pipe  pipeline    text  generation   model  myllm  tokenizer  tokenizer  maxnewtokens512   would like test  result  pipe    input prompt llm   debug prompt actually send llm  expect pipeline apply prompt template  tokenizerdefaultchattemplate  verify prompt template apply ,may use preprocess method check generate tokenids  generally would suggest closely look code method  explain happen prompt model forward pass  param  pipepreprocessparams pipepreprocess    can not believe      param   return     inputids   tensor    40  460  470  1975  345  750  884  257  220       attentionmask   tensor    1  1  1  1  1  1  1  1  1       prompttext     can not believe    internally preprocess call either chat tokenizerchattemplate simple text prompt tokenizer  prompttext   example   gpt2  model default tokenizer output tokenids mask  pipetokenizer    can not believe     return     inputids    40  460  470  1975  345  750  884  257  220     attentionmask    1  1  1  1  1  1  1  1  1   another thing consider prompt debug look happen will invert token id  pipe  pipeline    text  generation   model  openai  community  gpt2   input  pipetokenizer    can not believe    pipetokenizerconvertidstotoken  input   inputids          can      t    believe    you    did    such    a      ,huggingface pipeline  debug prompt  ve define pipeline use huggingface transformer library  pipe  pipeline    text  generation   model  myllm  tokenizer  tokenizer  maxnewtokens512   would like test  result  pipe    input prompt llm   debug prompt actually send llm  expect pipeline apply prompt template  tokenizerdefaultchattemplate  verify prompt template apply  may use preprocess method check generate tokenids  generally would suggest closely look code method  explain happen prompt model forward pass  param  pipepreprocessparams pipepreprocess    can not believe      param   return     inputids   tensor    40  460  470  1975  345  750  884  257  220       attentionmask   tensor    1  1  1  1  1  1  1  1  1       prompttext     can not believe    internally preprocess call either chat tokenizerchattemplate simple text prompt tokenizer  prompttext   example   gpt2  model default tokenizer output tokenids mask  pipetokenizer    can not believe     return     inputids    40  460  470  1975  345  750  884  257  220     attentionmask    1  1  1  1  1  1  1  1  1   another thing consider prompt debug look happen will invert token id  pipe  pipeline    text  generation   model  openai  community  gpt2   input  pipetokenizer    can not believe    pipetokenizerconvertidstotoken  input   inputids          can      t    believe    you    did    such    a      ,Library/Tool-Based Queries
AttributeError: &#39;TrainingArguments&#39; object has no attribute &#39;model_init_kwargs&#39;,"While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Code: Loading the libraries from enum import Enum from functools import partial import pandas as pd import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig from datasets import load_dataset from trl import SFTTrainer from peft import get_peft_model, LoraConfig, TaskType seed = 42 set_seed(seed) Loading the dataset and preprocess it. model_name = ""gg-hf/gemma-2b-it"" dataset_name = ""FinGPT/fingpt-fiqa_qa"" tokenizer = AutoTokenizer.from_pretrained(model_name) template = """"""{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"""""" tokenizer.chat_template = template def preprocess(samples): batch = [] for system_prompt, input, output in zip(samples[""instruction""], samples[""input""], samples[""output""]): conversation = [{""content"": system_prompt, ""role"": ""system""}, {""content"": input, ""role"": ""user""}, {""content"": output, ""role"": ""assistant""}] batch.append(tokenizer.apply_chat_template(conversation, tokenize=False)) return {""content"": batch} dataset = load_dataset(dataset_name) dataset = dataset.map( preprocess, batched=True, remove_columns=dataset[""train""].column_names ) dataset = dataset[""train""].train_test_split(0.1) print(dataset) print(dataset[""train""][0]) Create PEFT configurations peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""gate_proj"",""q_proj"",""lm_head"",""o_proj"",""k_proj"",""embed_tokens"",""down_proj"",""up_proj"",""v_proj""], task_type=TaskType.CAUSAL_LM) Create Quantization configurations bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, ) Load the model and tokenizer class ChatmlSpecialTokens(str, Enum): user = ""<|im_start|>user"" assistant = ""<|im_start|>assistant"" system = ""<|im_start|>system"" eos_token = ""<|im_end|>"" bos_token = ""<s>"" pad_token = ""<pad>"" @classmethod def list(cls): return [c.value for c in cls] tokenizer = AutoTokenizer.from_pretrained( model_name, pad_token=ChatmlSpecialTokens.pad_token.value, bos_token=ChatmlSpecialTokens.bos_token.value, eos_token=ChatmlSpecialTokens.eos_token.value, additional_special_tokens=ChatmlSpecialTokens.list(), trust_remote_code=True ) tokenizer.chat_template = template model = AutoModelForCausalLM.from_pretrained(model_name) model.resize_token_embeddings(len(tokenizer)) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # cast non-trainable params in fp16 for p in model.parameters(): if not p.requires_grad: p.data = p.to(torch.float16) Training Configurations output_dir = ""Gemma2B_finetune_QLoRA"" per_device_train_batch_size = 1 per_device_eval_batch_size = 1 gradient_accumulation_steps = 8 logging_steps = 5 learning_rate = 5e-4 max_grad_norm = 1.0 max_steps = 250 num_train_epochs=10 warmup_ratio = 0.1 lr_scheduler_type = ""cosine"" max_seq_length = 2048 training_arguments = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=per_device_train_batch_size, per_device_eval_batch_size=per_device_eval_batch_size, gradient_accumulation_steps=gradient_accumulation_steps, save_strategy=""no"", evaluation_strategy=""epoch"", logging_steps=logging_steps, learning_rate=learning_rate, max_grad_norm=max_grad_norm, weight_decay=0.1, warmup_ratio=warmup_ratio, lr_scheduler_type=lr_scheduler_type, fp16=True, report_to=[""tensorboard"", ""wandb""], hub_private_repo=True, push_to_hub=True, num_train_epochs=num_train_epochs, gradient_checkpointing=True, gradient_checkpointing_kwargs={""use_reentrant"": False} ) Create trainer trainer = SFTTrainer( model=model, args=training_arguments, train_dataset=dataset[""train""], eval_dataset=dataset[""test""], tokenizer=tokenizer, packing=True, dataset_text_field=""content"", max_seq_length=max_seq_length, peft_config=peft_config, dataset_kwargs={ ""append_concat_token"": False, ""add_special_tokens"": False, }, ) The error I'm getting is like :- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 trainer = SFTTrainer( 2 model=model, 3 args=training_arguments, 4 train_dataset=dataset[""train""], 5 eval_dataset=dataset[""test""], 6 tokenizer=tokenizer, 7 packing=True, 8 dataset_text_field=""content"", 9 max_seq_length=max_seq_length, 10 peft_config=peft_config, 11 dataset_kwargs={ 12 ""append_concat_token"": False, 13 ""add_special_tokens"": False, 14 }, 15 ) File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f(*args, **kwargs) 99 message += ""\n\n"" + custom_message 100 warnings.warn(message, FutureWarning) --> 101 return f(*args, **kwargs) File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing) 150 warnings.warn( 151 ""You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`."" 152 ) 153 args.model_init_kwargs = model_init_kwargs --> 154 if args.model_init_kwargs is None: 155 model_init_kwargs = {} 156 elif not isinstance(model, str): AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Do let me know if there's any solution for this? Thanks.","['python', 'nlp', 'huggingface-transformers', 'large-language-model', 'peft']",1,"Just replace your TrainingArguments constructor with SFTConfig constructor, and pass this to SFTTrainer. from trl import SFTConfig training_arguments = SFTConfig(your training args ...) trainer = SFTTrainer(args=training_arguments, rest of the args...)",2024-06-04 12:06:34,2024-06-05 05:02:36,4587,https://stackoverflow.com/questions/78575305/attributeerror-trainingarguments-object-has-no-attribute-model-init-kwargs,"AttributeError: &#39;TrainingArguments&#39; object has no attribute &#39;model_init_kwargs&#39; While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Code: Loading the libraries from enum import Enum from functools import partial import pandas as pd import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig from datasets import load_dataset from trl import SFTTrainer from peft import get_peft_model, LoraConfig, TaskType seed = 42 set_seed(seed) Loading the dataset and preprocess it. model_name = ""gg-hf/gemma-2b-it"" dataset_name = ""FinGPT/fingpt-fiqa_qa"" tokenizer = AutoTokenizer.from_pretrained(model_name) template = """"""{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"""""" tokenizer.chat_template = template def preprocess(samples): batch = [] for system_prompt, input, output in zip(samples[""instruction""], samples[""input""], samples[""output""]): conversation = [{""content"": system_prompt, ""role"": ""system""}, {""content"": input, ""role"": ""user""}, {""content"": output, ""role"": ""assistant""}] batch.append(tokenizer.apply_chat_template(conversation, tokenize=False)) return {""content"": batch} dataset = load_dataset(dataset_name) dataset = dataset.map( preprocess, batched=True, remove_columns=dataset[""train""].column_names ) dataset = dataset[""train""].train_test_split(0.1) print(dataset) print(dataset[""train""][0]) Create PEFT configurations peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""gate_proj"",""q_proj"",""lm_head"",""o_proj"",""k_proj"",""embed_tokens"",""down_proj"",""up_proj"",""v_proj""], task_type=TaskType.CAUSAL_LM) Create Quantization configurations bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, ) Load the model and tokenizer class ChatmlSpecialTokens(str, Enum): user = ""<|im_start|>user"" assistant = ""<|im_start|>assistant"" system = ""<|im_start|>system"" eos_token = ""<|im_end|>"" bos_token = ""<s>"" pad_token = ""<pad>"" @classmethod def list(cls): return [c.value for c in cls] tokenizer = AutoTokenizer.from_pretrained( model_name, pad_token=ChatmlSpecialTokens.pad_token.value, bos_token=ChatmlSpecialTokens.bos_token.value, eos_token=ChatmlSpecialTokens.eos_token.value, additional_special_tokens=ChatmlSpecialTokens.list(), trust_remote_code=True ) tokenizer.chat_template = template model = AutoModelForCausalLM.from_pretrained(model_name) model.resize_token_embeddings(len(tokenizer)) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # cast non-trainable params in fp16 for p in model.parameters(): if not p.requires_grad: p.data = p.to(torch.float16) Training Configurations output_dir = ""Gemma2B_finetune_QLoRA"" per_device_train_batch_size = 1 per_device_eval_batch_size = 1 gradient_accumulation_steps = 8 logging_steps = 5 learning_rate = 5e-4 max_grad_norm = 1.0 max_steps = 250 num_train_epochs=10 warmup_ratio = 0.1 lr_scheduler_type = ""cosine"" max_seq_length = 2048 training_arguments = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=per_device_train_batch_size, per_device_eval_batch_size=per_device_eval_batch_size, gradient_accumulation_steps=gradient_accumulation_steps, save_strategy=""no"", evaluation_strategy=""epoch"", logging_steps=logging_steps, learning_rate=learning_rate, max_grad_norm=max_grad_norm, weight_decay=0.1, warmup_ratio=warmup_ratio, lr_scheduler_type=lr_scheduler_type, fp16=True, report_to=[""tensorboard"", ""wandb""], hub_private_repo=True, push_to_hub=True, num_train_epochs=num_train_epochs, gradient_checkpointing=True, gradient_checkpointing_kwargs={""use_reentrant"": False} ) Create trainer trainer = SFTTrainer( model=model, args=training_arguments, train_dataset=dataset[""train""], eval_dataset=dataset[""test""], tokenizer=tokenizer, packing=True, dataset_text_field=""content"", max_seq_length=max_seq_length, peft_config=peft_config, dataset_kwargs={ ""append_concat_token"": False, ""add_special_tokens"": False, }, ) The error I'm getting is like :- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 trainer = SFTTrainer( 2 model=model, 3 args=training_arguments, 4 train_dataset=dataset[""train""], 5 eval_dataset=dataset[""test""], 6 tokenizer=tokenizer, 7 packing=True, 8 dataset_text_field=""content"", 9 max_seq_length=max_seq_length, 10 peft_config=peft_config, 11 dataset_kwargs={ 12 ""append_concat_token"": False, 13 ""add_special_tokens"": False, 14 }, 15 ) File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f(*args, **kwargs) 99 message += ""\n\n"" + custom_message 100 warnings.warn(message, FutureWarning) --> 101 return f(*args, **kwargs) File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing) 150 warnings.warn( 151 ""You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`."" 152 ) 153 args.model_init_kwargs = model_init_kwargs --> 154 if args.model_init_kwargs is None: 155 model_init_kwargs = {} 156 elif not isinstance(model, str): AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Do let me know if there's any solution for this? Thanks.",attributeerror    39  trainingarguments   39  object attribute   39  modelinitkwargs   39  finetune gemma2b model use qlora  m get error attributeerror   trainingarguments  object attribute  modelinitkwargs  code  loading library enum import enum functool import partial import panda pd import torch transformer import autotokenizer  automodelforcausallm  trainingarguments  setseed  bitsandbytesconfig dataset import loaddataset trl import sfttrainer peft import getpeftmodel  loraconfig  tasktype seed  42 setseed  seed  loading dataset preprocess  modelname    gg  hf  gemma2b  it  datasetname    fingpt  fingpt  fiqaqa  tokenizer  autotokenizerfrompretraine  modelname  template        message message   n     imstart    message   role     n   message   content      imend     n      looplast addgenerationprompt       imstart  assistantn      endif     endfor      tokenizerchattemplate  template def preprocess  sample   batch    systemprompt  input  output zip  sample    instruction    sample    input    sample    output     conversation      content   systemprompt    role     system       content   input    role     user       content   output    role     assistant    batchappend  tokenizerapplychattemplate  conversation  tokenize  false   return    content   batch  dataset  loaddataset  datasetname  dataset  datasetmap  preprocess  batch  true  removecolumn  dataset    train   columnnames  dataset  dataset    train   traintestsplit  01  print  dataset  print  dataset    train    0   create peft configuration peftconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    gateproj    qproj    lmhead    oproj    kproj    embedtoken    downproj    upproj    vproj    tasktype  tasktype  causallm  create quantization configuration bnbconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchbfloat16  bnb4bitusedoublequant  true   load model tokenizer class chatmlspecialtokens  str  enum   user     imstart  user  assistant     imstart  assistant  system     imstart  system  eostoken     imend   bostoken       padtoken     pad    classmethod def list  cls   return  cvalue c cls  tokenizer  autotokenizerfrompretraine  modelname  padtoken  chatmlspecialtokenspadtokenvalue  bostoken  chatmlspecialtokensbostokenvalue  eostoken  chatmlspecialtokenseostokenvalue  additionalspecialtokens  chatmlspecialtokenslist    trustremotecode  true  tokenizerchattemplate  template model  automodelforcausallmfrompretraine  modelname  modelresizetokenembeddings  len  tokenizer   model  getpeftmodel  model  peftconfig  modelprinttrainableparameters    cast non  trainable param fp16 p modelparameter    prequiresgrad  pdata  pto  torchfloat16  training configurations outputdir    gemma2bfinetuneqlora  perdevicetrainbatchsize  1 perdeviceevalbatchsize  1 gradientaccumulationsteps  8 loggingstep  5 learningrate  5e4 maxgradnorm  10 maxstep  250 numtrainepochs10 warmupratio  01 lrschedulertype    cosine  maxseqlength  2048 trainingargument  trainingarguments  outputdir  outputdir  perdevicetrainbatchsize  perdevicetrainbatchsize  perdeviceevalbatchsize  perdeviceevalbatchsize  gradientaccumulationsteps  gradientaccumulationsteps  savestrategy    evaluationstrategy  epoch   loggingstep  loggingsteps  learningrate  learningrate  maxgradnorm  maxgradnorm  weightdecay01  warmupratio  warmupratio  lrschedulertype  lrschedulertype  fp16  true  reportto    tensorboard     wandb    hubprivaterepo  true  pushtohub  true  numtrainepochs  numtrainepochs  gradientcheckpointing  true  gradientcheckpointingkwargs    usereentrant   false   create trainer trainer  sfttrainer  model  model  arg  trainingarguments  traindataset  dataset    train    evaldataset  dataset    test    tokenizer  tokenizer  pack  true  datasettextfield  content   maxseqlength  maxseqlength  peftconfig  peftconfig  datasetkwargs    appendconcattoken   false    addspecialtoken   false     error  m get like                                         attributeerror traceback  recent call last  cell  10   line 1    1 trainer  sfttrainer  2 model  model  3 arg  trainingarguments  4 traindataset  dataset    train    5 evaldataset  dataset    test    6 tokenizer  tokenizer  7 pack  true  8 datasettextfield  content   9 maxseqlength  maxseqlength  10 peftconfig  peftconfig  11 datasetkwargs  12   appendconcattoken   false  13   addspecialtoken   false  14   15  file usr  local  lib  python310  dist  package  huggingfacehub  utilsdeprecationpy101   deprecatearguments   local  innerdeprecatepositionalargs   local  innerf   args    kwargs  99 message     nn   custommessage 100 warningswarn  message  futurewarning    101 return f   args    kwargs  file usr  local  lib  python310  dist  package  trl  trainer  sfttrainerpy154  sfttrainerinit    self  model  args  datacollator  traindataset  evaldataset  tokenizer  modelinit  computemetric  callback  optimizer  preprocesslogitsformetric  peftconfig  datasettextfield  packing  formattingfunc  maxseqlength  infinite  numofsequence  charspertoken  datasetnumproc  datasetbatchsize  neftunenoisealpha  modelinitkwargs  datasetkwargs  evalpacke  150 warningswarn  151   pass  modelinitkwargs  sfttrainer  value pass override one  sftconfig    152  153 argsmodelinitkwargs  modelinitkwargs   154 argsmodelinitkwargs none  155 modelinitkwargs    156 elif isinstance  model  str   attributeerror   trainingarguments  object attribute  modelinitkwargs  let know s solution  thank ,replace trainingarguments constructor sftconfig constructor  pass sfttrainer  trl import sftconfig trainingargument  sftconfig  training args   trainer  sfttrainer  arg  trainingargument  rest args  ,attributeerror    39  trainingarguments   39  object attribute   39  modelinitkwargs   39  finetune gemma2b model use qlora  m get error attributeerror   trainingarguments  object attribute  modelinitkwargs  code  loading library enum import enum functool import partial import panda pd import torch transformer import autotokenizer  automodelforcausallm  trainingarguments  setseed  bitsandbytesconfig dataset import loaddataset trl import sfttrainer peft import getpeftmodel  loraconfig  tasktype seed  42 setseed  seed  loading dataset preprocess  modelname    gg  hf  gemma2b  it  datasetname    fingpt  fingpt  fiqaqa  tokenizer  autotokenizerfrompretraine  modelname  template        message message   n     imstart    message   role     n   message   content      imend     n      looplast addgenerationprompt       imstart  assistantn      endif     endfor      tokenizerchattemplate  template def preprocess  sample   batch    systemprompt  input  output zip  sample    instruction    sample    input    sample    output     conversation      content   systemprompt    role     system       content   input    role     user       content   output    role     assistant    batchappend  tokenizerapplychattemplate  conversation  tokenize  false   return    content   batch  dataset  loaddataset  datasetname  dataset  datasetmap  preprocess  batch  true  removecolumn  dataset    train   columnnames  dataset  dataset    train   traintestsplit  01  print  dataset  print  dataset    train    0   create peft configuration peftconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    gateproj    qproj    lmhead    oproj    kproj    embedtoken    downproj    upproj    vproj    tasktype  tasktype  causallm  create quantization configuration bnbconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchbfloat16  bnb4bitusedoublequant  true   load model tokenizer class chatmlspecialtokens  str  enum   user     imstart  user  assistant     imstart  assistant  system     imstart  system  eostoken     imend   bostoken       padtoken     pad    classmethod def list  cls   return  cvalue c cls  tokenizer  autotokenizerfrompretraine  modelname  padtoken  chatmlspecialtokenspadtokenvalue  bostoken  chatmlspecialtokensbostokenvalue  eostoken  chatmlspecialtokenseostokenvalue  additionalspecialtokens  chatmlspecialtokenslist    trustremotecode  true  tokenizerchattemplate  template model  automodelforcausallmfrompretraine  modelname  modelresizetokenembeddings  len  tokenizer   model  getpeftmodel  model  peftconfig  modelprinttrainableparameters    cast non  trainable param fp16 p modelparameter    prequiresgrad  pdata  pto  torchfloat16  training configurations outputdir    gemma2bfinetuneqlora  perdevicetrainbatchsize  1 perdeviceevalbatchsize  1 gradientaccumulationsteps  8 loggingstep  5 learningrate  5e4 maxgradnorm  10 maxstep  250 numtrainepochs10 warmupratio  01 lrschedulertype    cosine  maxseqlength  2048 trainingargument  trainingarguments  outputdir  outputdir  perdevicetrainbatchsize  perdevicetrainbatchsize  perdeviceevalbatchsize  perdeviceevalbatchsize  gradientaccumulationsteps  gradientaccumulationsteps  savestrategy    evaluationstrategy  epoch   loggingstep  loggingsteps  learningrate  learningrate  maxgradnorm  maxgradnorm  weightdecay01  warmupratio  warmupratio  lrschedulertype  lrschedulertype  fp16  true  reportto    tensorboard     wandb    hubprivaterepo  true  pushtohub  true  numtrainepochs  numtrainepochs  gradientcheckpointing  true  gradientcheckpointingkwargs    usereentrant   false   create trainer trainer  sfttrainer  model  model  arg  trainingarguments  traindataset  dataset    train    evaldataset  dataset    test    tokenizer  tokenizer  pack  true  datasettextfield  content   maxseqlength  maxseqlength  peftconfig  peftconfig  datasetkwargs    appendconcattoken   false    addspecialtoken   false     error  m get like                                         attributeerror traceback  recent call last  cell  10   line 1    1 trainer  sfttrainer  2 model  model  3 arg  trainingarguments  4 traindataset  dataset    train    5 evaldataset  dataset    test    6 tokenizer  tokenizer  7 pack  true  8 datasettextfield  content   9 maxseqlength  maxseqlength  10 peftconfig  peftconfig  11 datasetkwargs  12   appendconcattoken   false  13   addspecialtoken   false  14   15  file usr  local  lib  python310  dist  package  huggingfacehub  utilsdeprecationpy101   deprecatearguments   local  innerdeprecatepositionalargs   local  innerf   args    kwargs  99 message     nn   custommessage 100 warningswarn  message  futurewarning    101 return f   args    kwargs  file usr  local  lib  python310  dist  package  trl  trainer  sfttrainerpy154  sfttrainerinit    self  model  args  datacollator  traindataset  evaldataset  tokenizer  modelinit  computemetric  callback  optimizer  preprocesslogitsformetric  peftconfig  datasettextfield  packing  formattingfunc  maxseqlength  infinite  numofsequence  charspertoken  datasetnumproc  datasetbatchsize  neftunenoisealpha  modelinitkwargs  datasetkwargs  evalpacke  150 warningswarn  151   pass  modelinitkwargs  sfttrainer  value pass override one  sftconfig    152  153 argsmodelinitkwargs  modelinitkwargs   154 argsmodelinitkwargs none  155 modelinitkwargs    156 elif isinstance  model  str   attributeerror   trainingarguments  object attribute  modelinitkwargs  let know s solution  thank  replace trainingarguments constructor sftconfig constructor  pass sfttrainer  trl import sftconfig trainingargument  sftconfig  training args   trainer  sfttrainer  arg  trainingargument  rest args  ,Basic Understanding
Unable to fit new documents without running out of memory in STM topic modeling,"I'm trying to label new texts based on a previous topic model using the fitNewDocuments() function form the stm package in R. I've tried fitting 10 new documents based on topic models trained on 20000, 10000 and 3000 documents, and the function always ends up using way too much memory (from 20gb to even 50gb), crashing the R session. I'm not finding anything online about using the fitNewDocuments() properly. I'm following the documentation to the letter, but the process just never finishes. I've only noticed that the documentation says the origData argument should be out$meta , but it returns an error if I supply that, and I have to supply just out instead. That being said, I'm able to reproduce the example in the documentation using Gadarian data. But it fails with my own data. I could share code, but it would be useless without access to the data, which sadly I can't provide.","['r', 'nlp', 'lda', 'stm']",1,"After trying a million things, somehow I was able to fix this by removing the prevalencePrior = ""Covariate"" argument from fitNewDocuments() , and the new documents were properly fit based on the models.",2024-06-04 03:41:55,2024-06-04 03:58:53,25,https://stackoverflow.com/questions/78573280/unable-to-fit-new-documents-without-running-out-of-memory-in-stm-topic-modeling,"Unable to fit new documents without running out of memory in STM topic modeling I'm trying to label new texts based on a previous topic model using the fitNewDocuments() function form the stm package in R. I've tried fitting 10 new documents based on topic models trained on 20000, 10000 and 3000 documents, and the function always ends up using way too much memory (from 20gb to even 50gb), crashing the R session. I'm not finding anything online about using the fitNewDocuments() properly. I'm following the documentation to the letter, but the process just never finishes. I've only noticed that the documentation says the origData argument should be out$meta , but it returns an error if I supply that, and I have to supply just out instead. That being said, I'm able to reproduce the example in the documentation using Gadarian data. But it fails with my own data. I could share code, but it would be useless without access to the data, which sadly I can't provide.",unable fit new document without run memory stm topic modeling  m try label new text base previous topic model use fitnewdocument   function form stm package r  ve try fit 10 new document base topic model train 20000  10000 3000 document  function always end use way much memory  20 gb even 50 gb   crash r session   m find anything online use fitnewdocument   properly   m follow documentation letter  process never finish   ve notice documentation say origdata argument  meta  return error supply  supply instead  say   m able reproduce example documentation use gadarian datum  fail datum  could share code  would useless without access datum  sadly can not provide ,try million thing  somehow able fix remove prevalenceprior    covariate  argument fitnewdocument    new document properly fit base model ,unable fit new document without run memory stm topic modeling  m try label new text base previous topic model use fitnewdocument   function form stm package r  ve try fit 10 new document base topic model train 20000  10000 3000 document  function always end use way much memory  20 gb even 50 gb   crash r session   m find anything online use fitnewdocument   properly   m follow documentation letter  process never finish   ve notice documentation say origdata argument  meta  return error supply  supply instead  say   m able reproduce example documentation use gadarian datum  fail datum  could share code  would useless without access datum  sadly can not provide  try million thing  somehow able fix remove prevalenceprior    covariate  argument fitnewdocument    new document properly fit base model ,Implementation Issues
How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model?,"Preface I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace. The directories are filled with these files: - .gitattributes - adapter_config.json - adapter_model.safetensors - special_tokens_map.json - tokenizer.json - tokenizer_config.json - training_args.bin Implementation I am trying to load this model through this: model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, ) But it shows the error OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files. So then I am trying to load the config.json from the original model which is meta-llama/Meta-Llama-3-8B : original_model = ""meta-llama/Meta-Llama-3-8B"" model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) original_config = AutoConfig.from_pretrained(original_model) original_config.save_pretrained(model_id_1) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, config = original_config ) But still, it shows another error OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3. Questions How to load the fine-tuned model properly?","['pytorch', 'nlp', 'huggingface-transformers', 'large-language-model', 'peft']",1,"Your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. I assume you have used the save_pretrained method from peft. This method only saves the adapter weights and config (I use a smaller model for my answer and a different task type!): from peft import LoraConfig, TaskType, get_peft_model, PeftModel from transformers import AutoModelForTokenClassification from pathlib import Path # ferguso/llama-8b-pcl-v3 in your case adapter_path = 'bla' # meta-llama/Meta-Llama-3-8B in your case base_model_id = ""distilbert/distilbert-base-uncased"" peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, target_modules=""all-linear"") # AutoModelForCausalLM in your case model = AutoModelForTokenClassification.from_pretrained(base_model_id) model = get_peft_model(model, peft_config) model.save_pretrained(adapter_path) print(*list(Path(adapter_path).iterdir()), sep='\n') Output: bla/adapter_config.json bla/README.md bla/adapter_model.safetensors To load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter: model = AutoModelForTokenClassification.from_pretrained(base_model_id) model = PeftModel.from_pretrained(model, adapter_path) You can also merge the adapter weights back with merge_and_unload and save it: model.merge_and_unload().save_pretrained('bla2') print(*list(Path('bla2').iterdir()), sep='\n') Output: bla2/config.json bla2/model.safetensors This way you will be able to load the model without peft and only transformers as you tried in the example code of your question.",2024-05-30 02:36:56,2024-06-01 15:31:59,3057,https://stackoverflow.com/questions/78552651/how-to-fix-error-oserror-model-does-not-appear-to-have-a-file-named-config-j,"How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model? Preface I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace. The directories are filled with these files: - .gitattributes - adapter_config.json - adapter_model.safetensors - special_tokens_map.json - tokenizer.json - tokenizer_config.json - training_args.bin Implementation I am trying to load this model through this: model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, ) But it shows the error OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files. So then I am trying to load the config.json from the original model which is meta-llama/Meta-Llama-3-8B : original_model = ""meta-llama/Meta-Llama-3-8B"" model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) original_config = AutoConfig.from_pretrained(original_model) original_config.save_pretrained(model_id_1) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, config = original_config ) But still, it shows another error OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3. Questions How to load the fine-tuned model properly?",fix error  oserror   lt  model  gt  appear file name configjson   load custom fine  tune model  preface new implement nlp model  successfully fine  tune llama 3  8b variant qlora upload huggingface  directory fill file   gitattributes  adapterconfigjson  adaptermodelsafetensors  specialtokensmapjson  tokenizerjson  tokenizerconfigjson  trainingargsbin implementation try load model  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig   show error oserror  ferguso  llama8b  pcl  v3 appear file name configjson  checkout    huggingfaceco  ferguso  llama8b  pcl  v3  tree  main  available file  try load configjson original model meta  llama  meta  llama3  8b  originalmodel    meta  llama  meta  llama3  8b  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   originalconfig  autoconfigfrompretrained  originalmodel  originalconfigsavepretraine  modelid1  model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig  config  originalconfig  still  show another error oserror  error file name pytorchmodelbin  modelsafetensor  tfmodelh5  modelckptindex flaxmodelmsgpack find directory ferguso  llama8b  pcl  v3  question load fine  tuned model properly ,directory contain file peft  adapter file require load tokenizer  base model weight missing  assume use savepretrained method peft  method save adapter weight config  use small model answer different task type    peft import loraconfig  tasktype  getpeftmodel  peftmodel transformer import automodelfortokenclassification pathlib import path  ferguso  llama8b  pcl  v3 case adapterpath   bla   meta  llama  meta  llama3  8b case basemodelid    distilbert  distilbert  base  uncased  peftconfig  loraconfig  tasktype  tasktype  tokencls  targetmodules  all  linear    automodelforcausallm case model  automodelfortokenclassificationfrompretraine  basemodelid  model  getpeftmodel  model  peftconfig  modelsavepretraine  adapterpath  print   list  path  adapterpath  iterdir     sepn   output  bla  adapterconfigjson bla  readmemd bla  adaptermodelsafetensors load pretraine model successfully  need load basemodel weight well use peft model class load adapter  model  automodelfortokenclassificationfrompretraine  basemodelid  model  peftmodelfrompretraine  model  adapterpath  also merge adapter weight back mergeandunload save  modelmergeandunload   savepretrained   bla2   print   list  path   bla2   iterdir     sepn   output  bla2  configjson bla2  modelsafetensor way able load model without peft transformer try example code question ,fix error  oserror   lt  model  gt  appear file name configjson   load custom fine  tune model  preface new implement nlp model  successfully fine  tune llama 3  8b variant qlora upload huggingface  directory fill file   gitattributes  adapterconfigjson  adaptermodelsafetensors  specialtokensmapjson  tokenizerjson  tokenizerconfigjson  trainingargsbin implementation try load model  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig   show error oserror  ferguso  llama8b  pcl  v3 appear file name configjson  checkout    huggingfaceco  ferguso  llama8b  pcl  v3  tree  main  available file  try load configjson original model meta  llama  meta  llama3  8b  originalmodel    meta  llama  meta  llama3  8b  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   originalconfig  autoconfigfrompretrained  originalmodel  originalconfigsavepretraine  modelid1  model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig  config  originalconfig  still  show another error oserror  error file name pytorchmodelbin  modelsafetensor  tfmodelh5  modelckptindex flaxmodelmsgpack find directory ferguso  llama8b  pcl  v3  question load fine  tuned model properly  directory contain file peft  adapter file require load tokenizer  base model weight missing  assume use savepretrained method peft  method save adapter weight config  use small model answer different task type    peft import loraconfig  tasktype  getpeftmodel  peftmodel transformer import automodelfortokenclassification pathlib import path  ferguso  llama8b  pcl  v3 case adapterpath   bla   meta  llama  meta  llama3  8b case basemodelid    distilbert  distilbert  base  uncased  peftconfig  loraconfig  tasktype  tasktype  tokencls  targetmodules  all  linear    automodelforcausallm case model  automodelfortokenclassificationfrompretraine  basemodelid  model  getpeftmodel  model  peftconfig  modelsavepretraine  adapterpath  print   list  path  adapterpath  iterdir     sepn   output  bla  adapterconfigjson bla  readmemd bla  adaptermodelsafetensors load pretraine model successfully  need load basemodel weight well use peft model class load adapter  model  automodelfortokenclassificationfrompretraine  basemodelid  model  peftmodelfrompretraine  model  adapterpath  also merge adapter weight back mergeandunload save  modelmergeandunload   savepretrained   bla2   print   list  path   bla2   iterdir     sepn   output  bla2  configjson bla2  modelsafetensor way able load model without peft transformer try example code question ,Task-Specific Queries
Not able to install spacy==2.3.5 version,"I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error. Using Python 3.12.3 Also it gives a E053 config file error when running the program regarding pyresparser: ""OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg"" `(.venv) PS C:\Smart_Resume_Analyser_App-master> pip install spacy==2.3.5 Getting requirements to build wheel did not run successfully. exit code: 1 [267 lines of output] Error compiling Cython file: ------------------------------------------------------------ ... len_t* widths int i int nr_layer int batch_size __init__(len_t* widths, int nr_layer, int batch_size) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this._nr_feat = <len_t*>calloc(batch_size, sizeof(len_t)) this._is_valid = <int*>calloc(batch_size * widths[nr_layer-1], sizeof(int)) this._costs = <weight_t*>calloc(batch_size * widths[nr_layer-1], sizeof(weight_t)) this.signatures = <uint64_t*>calloc(batch_size, sizeof(uint64_t)) __dealloc__() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... free(this._nr_feat) free(this._is_valid) free(this._costs) free(this.signatures) void reset() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.i): free(this._feats[i]) this._feats[i] = NULL this.i = 0 int nr_in() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this.i = 0 int nr_in() nogil: return this.widths[0] int nr_out() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.widths[0] int nr_out() nogil: return this.widths[this.nr_layer - 1] int push_back(const FeatureC* feats, int nr_feat, ^ ------------------------------------------------------------ thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.nr_out()): this.is_valid(this.i)[i] = 1 this.i += 1 return this.i >= this.batch_size FeatureC* features(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.i >= this.batch_size FeatureC* features(int i) nogil: return this._feats[i] int nr_feat(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._feats[i] int nr_feat(int i) nogil: return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out()) int best(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline' warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 Compiling thinc/linalg.pyx because it changed. Compiling thinc/structs.pyx because it changed. Compiling thinc/typedefs.pyx because it changed. Compiling thinc/linear/avgtron.pyx because it changed. Compiling thinc/linear/features.pyx because it changed. Compiling thinc/linear/serialize.pyx because it changed. Compiling thinc/linear/sparse.pyx because it changed. Compiling thinc/linear/linear.pyx because it changed. Compiling thinc/neural/optimizers.pyx because it changed. Compiling thinc/neural/ops.pyx because it changed. Compiling thinc/neural/_aligned_alloc.pyx because it changed. Compiling thinc/extra/eg.pyx because it changed. Compiling thinc/extra/mb.pyx because it changed. Compiling thinc/extra/search.pyx because it changed. Compiling thinc/extra/cache.pyx because it changed. [ 1/15] Cythonizing thinc/extra/cache.pyx [ 2/15] Cythonizing thinc/extra/eg.pyx Traceback (most recent call last): File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module> main() File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 325, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 295, in _get_build_requires self.run_setup() File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 311, in run_setup exec(code, locals()) File ""<string>"", line 258, in <module> File ""<string>"", line 195, in setup_package File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1154, in cythonize cythonize_one(*args) File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1321, in cythonize_one raise CompileError(None, pyx_file) Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error Getting requirements to build wheel did not run successfully. exit code: 1 See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. `","['python', 'parsing', 'pip', 'nlp', 'spacy']",1,try: pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz or pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz,2024-05-27 11:20:09,2024-05-28 00:47:10,213,https://stackoverflow.com/questions/78538749/not-able-to-install-spacy-2-3-5-version,"Not able to install spacy==2.3.5 version I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error. Using Python 3.12.3 Also it gives a E053 config file error when running the program regarding pyresparser: ""OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg"" `(.venv) PS C:\Smart_Resume_Analyser_App-master> pip install spacy==2.3.5 Getting requirements to build wheel did not run successfully. exit code: 1 [267 lines of output] Error compiling Cython file: ------------------------------------------------------------ ... len_t* widths int i int nr_layer int batch_size __init__(len_t* widths, int nr_layer, int batch_size) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this._nr_feat = <len_t*>calloc(batch_size, sizeof(len_t)) this._is_valid = <int*>calloc(batch_size * widths[nr_layer-1], sizeof(int)) this._costs = <weight_t*>calloc(batch_size * widths[nr_layer-1], sizeof(weight_t)) this.signatures = <uint64_t*>calloc(batch_size, sizeof(uint64_t)) __dealloc__() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... free(this._nr_feat) free(this._is_valid) free(this._costs) free(this.signatures) void reset() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.i): free(this._feats[i]) this._feats[i] = NULL this.i = 0 int nr_in() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this.i = 0 int nr_in() nogil: return this.widths[0] int nr_out() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.widths[0] int nr_out() nogil: return this.widths[this.nr_layer - 1] int push_back(const FeatureC* feats, int nr_feat, ^ ------------------------------------------------------------ thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.nr_out()): this.is_valid(this.i)[i] = 1 this.i += 1 return this.i >= this.batch_size FeatureC* features(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.i >= this.batch_size FeatureC* features(int i) nogil: return this._feats[i] int nr_feat(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._feats[i] int nr_feat(int i) nogil: return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out()) int best(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline' warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 Compiling thinc/linalg.pyx because it changed. Compiling thinc/structs.pyx because it changed. Compiling thinc/typedefs.pyx because it changed. Compiling thinc/linear/avgtron.pyx because it changed. Compiling thinc/linear/features.pyx because it changed. Compiling thinc/linear/serialize.pyx because it changed. Compiling thinc/linear/sparse.pyx because it changed. Compiling thinc/linear/linear.pyx because it changed. Compiling thinc/neural/optimizers.pyx because it changed. Compiling thinc/neural/ops.pyx because it changed. Compiling thinc/neural/_aligned_alloc.pyx because it changed. Compiling thinc/extra/eg.pyx because it changed. Compiling thinc/extra/mb.pyx because it changed. Compiling thinc/extra/search.pyx because it changed. Compiling thinc/extra/cache.pyx because it changed. [ 1/15] Cythonizing thinc/extra/cache.pyx [ 2/15] Cythonizing thinc/extra/eg.pyx Traceback (most recent call last): File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module> main() File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 325, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 295, in _get_build_requires self.run_setup() File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 311, in run_setup exec(code, locals()) File ""<string>"", line 258, in <module> File ""<string>"", line 195, in setup_package File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1154, in cythonize cythonize_one(*args) File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1321, in cythonize_one raise CompileError(None, pyx_file) Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error Getting requirements to build wheel did not run successfully. exit code: 1 See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. `",able install spacy235 version try install spacy235 resume analyser program  encounter pip subprocess install build dependency run successfully error  use python 3123 also give e053 config file error run program regard pyresparser    oserror   e053  could read config file c  smartresumeanalyserapp  mastervenvlibsite  packagespyresparserconfigcfg    venv  ps c  smartresumeanalyserapp  master  pip install spacy235 get requirement build wheel run successfully  exit code  1  267 line output  error compile cython file                                 lent  width int int nrlayer int batchsize   init    lent  width  int nrlayer  int batchsize  nogil                                 thincstructspxd1404  function definition pxd file must declare  cdef inline  error compile cython file                                 thisnrfeat   lent   calloc  batchsize  sizeof  lent   thisisvalid   int   calloc  batchsize  width  nrlayer1   sizeof  int   thiscost   weightt   calloc  batchsize  width  nrlayer1   sizeof  weightt   thissignature   uint64t   calloc  batchsize  sizeof  uint64t     dealloc     nogil                                 thincstructspxd1574  function definition pxd file must declare  cdef inline  error compile cython file                                 free  thisnrfeat  free  thisisvalid  free  thiscost  free  thissignature  void reset   nogil                                 thincstructspxd1724  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisi   free  thisfeat    thisfeat    null thisi  0 int nrin   nogil                                 thincstructspxd1894  function definition pxd file must declare  cdef inline  error compile cython file                                 thisi  0 int nrin   nogil  return thiswidth  0  int nrout   nogil                                 thincstructspxd1924  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiswidth  0  int nrout   nogil  return thiswidth  thisnrlayer  1  int pushback  const featurec  feat  int nrfeat                                 thincstructspxd1954  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisnrout     thisisvalid  thisi     1 thisi   1 return thisi   thisbatchsize featurec  feature  int  nogil                                 thincstructspxd2264  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisi   thisbatchsize featurec  feature  int  nogil  return thisfeat   int nrfeat  int  nogil                                 thincstructspxd2294  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfeat   int nrfeat  int  nogil  return thisnrfeat   weightt  fwd  int  int j  nogil                                 thincstructspxd2324  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisnrfeat   weightt  fwd  int  int j  nogil  return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil                                 thincstructspxd2354  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil  return thisbwd     j  thiswidth    weightt  score  int  nogil                                 thincstructspxd2384  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd     j  thiswidth    weightt  score  int  nogil  return thisfwd  thisnrlayer1   weightt  loss  int  nogil                                 thincstructspxd2414  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd  thisnrlayer1   weightt  loss  int  nogil  return thisbwd  thisnrlayer1   weightt  cost  int  nogil                                 thincstructspxd2444  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd  thisnrlayer1   weightt  cost  int  nogil  return thiscost    thisnrout    int  isvalid  int  nogil                                 thincstructspxd2474  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiscost    thisnrout    int  isvalid  int  nogil  return thisisvalid    thisnrout    int guess  int  nogil                                 thincstructspxd2504  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisisvalid    thisnrout    int guess  int  nogil  return vecvecargmaxiftrue  thisscore    thisisvalid    thisnrout    int good  int  nogil                                 thincstructspxd2534  function definition pxd file must declare  cdef inline  warning  thinclinalgpxd140   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd908   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd1748   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 compiling thinc  linalgpyx change  compile thinc  structspyx change  compile thinc  typedefspyx change  compile thinc  linear  avgtronpyx change  compile thinc  linear  featurespyx change  compile thinc  linear  serializepyx change  compile thinc  linear  sparsepyx change  compile thinc  linear  linearpyx change  compile thinc  neural  optimizerspyx change  compile thinc  neural  opspyx change  compile thinc  neuralalignedallocpyx change  compile thinc  extra  egpyx change  compile thinc  extra  mbpyx change  compile thinc  extra  searchpyx change  compile thinc  extra  cachepyx change   115  cythonize thinc  extra  cachepyx  215  cythonize thinc  extra  egpyx traceback  recent call last   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 353   module  main   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 335  main jsonout   returnval    hook    hookinput   kwarg     file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 118  getrequiresforbuildwheel return hook  configsetting   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 325  getrequiresforbuildwheel return selfgetbuildrequire  configsetting  requirements   wheel     file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 295   getbuildrequire selfrunsetup   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 311  runsetup exec  code  local    file    string    line 258   module  file    string    line 195  setuppackage file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1154  cythonize cythonizeone   args  file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1321  cythonizeone raise compileerror  none  pyxfile  cython  compiler  error  compileerror  thinc  extra  egpyx  end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error getting requirement build wheel run successfully  exit code  1 see output  note  error originate subprocess  likely problem pip   end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error  pip subprocess install build dependency run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip  ,try  pip install   githubcom  explosion  spacy  model  release  download  encorewebsm231  encorewebsm231targz pip install   githubcom  explosion  spacy  model  release  download  encorewebsm230  encorewebsm230targz,able install spacy235 version try install spacy235 resume analyser program  encounter pip subprocess install build dependency run successfully error  use python 3123 also give e053 config file error run program regard pyresparser    oserror   e053  could read config file c  smartresumeanalyserapp  mastervenvlibsite  packagespyresparserconfigcfg    venv  ps c  smartresumeanalyserapp  master  pip install spacy235 get requirement build wheel run successfully  exit code  1  267 line output  error compile cython file                                 lent  width int int nrlayer int batchsize   init    lent  width  int nrlayer  int batchsize  nogil                                 thincstructspxd1404  function definition pxd file must declare  cdef inline  error compile cython file                                 thisnrfeat   lent   calloc  batchsize  sizeof  lent   thisisvalid   int   calloc  batchsize  width  nrlayer1   sizeof  int   thiscost   weightt   calloc  batchsize  width  nrlayer1   sizeof  weightt   thissignature   uint64t   calloc  batchsize  sizeof  uint64t     dealloc     nogil                                 thincstructspxd1574  function definition pxd file must declare  cdef inline  error compile cython file                                 free  thisnrfeat  free  thisisvalid  free  thiscost  free  thissignature  void reset   nogil                                 thincstructspxd1724  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisi   free  thisfeat    thisfeat    null thisi  0 int nrin   nogil                                 thincstructspxd1894  function definition pxd file must declare  cdef inline  error compile cython file                                 thisi  0 int nrin   nogil  return thiswidth  0  int nrout   nogil                                 thincstructspxd1924  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiswidth  0  int nrout   nogil  return thiswidth  thisnrlayer  1  int pushback  const featurec  feat  int nrfeat                                 thincstructspxd1954  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisnrout     thisisvalid  thisi     1 thisi   1 return thisi   thisbatchsize featurec  feature  int  nogil                                 thincstructspxd2264  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisi   thisbatchsize featurec  feature  int  nogil  return thisfeat   int nrfeat  int  nogil                                 thincstructspxd2294  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfeat   int nrfeat  int  nogil  return thisnrfeat   weightt  fwd  int  int j  nogil                                 thincstructspxd2324  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisnrfeat   weightt  fwd  int  int j  nogil  return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil                                 thincstructspxd2354  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil  return thisbwd     j  thiswidth    weightt  score  int  nogil                                 thincstructspxd2384  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd     j  thiswidth    weightt  score  int  nogil  return thisfwd  thisnrlayer1   weightt  loss  int  nogil                                 thincstructspxd2414  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd  thisnrlayer1   weightt  loss  int  nogil  return thisbwd  thisnrlayer1   weightt  cost  int  nogil                                 thincstructspxd2444  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd  thisnrlayer1   weightt  cost  int  nogil  return thiscost    thisnrout    int  isvalid  int  nogil                                 thincstructspxd2474  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiscost    thisnrout    int  isvalid  int  nogil  return thisisvalid    thisnrout    int guess  int  nogil                                 thincstructspxd2504  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisisvalid    thisnrout    int guess  int  nogil  return vecvecargmaxiftrue  thisscore    thisisvalid    thisnrout    int good  int  nogil                                 thincstructspxd2534  function definition pxd file must declare  cdef inline  warning  thinclinalgpxd140   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd908   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd1748   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 compiling thinc  linalgpyx change  compile thinc  structspyx change  compile thinc  typedefspyx change  compile thinc  linear  avgtronpyx change  compile thinc  linear  featurespyx change  compile thinc  linear  serializepyx change  compile thinc  linear  sparsepyx change  compile thinc  linear  linearpyx change  compile thinc  neural  optimizerspyx change  compile thinc  neural  opspyx change  compile thinc  neuralalignedallocpyx change  compile thinc  extra  egpyx change  compile thinc  extra  mbpyx change  compile thinc  extra  searchpyx change  compile thinc  extra  cachepyx change   115  cythonize thinc  extra  cachepyx  215  cythonize thinc  extra  egpyx traceback  recent call last   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 353   module  main   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 335  main jsonout   returnval    hook    hookinput   kwarg     file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 118  getrequiresforbuildwheel return hook  configsetting   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 325  getrequiresforbuildwheel return selfgetbuildrequire  configsetting  requirements   wheel     file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 295   getbuildrequire selfrunsetup   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 311  runsetup exec  code  local    file    string    line 258   module  file    string    line 195  setuppackage file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1154  cythonize cythonizeone   args  file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1321  cythonizeone raise compileerror  none  pyxfile  cython  compiler  error  compileerror  thinc  extra  egpyx  end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error getting requirement build wheel run successfully  exit code  1 see output  note  error originate subprocess  likely problem pip   end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error  pip subprocess install build dependency run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip   try  pip install   githubcom  explosion  spacy  model  release  download  encorewebsm231  encorewebsm231targz pip install   githubcom  explosion  spacy  model  release  download  encorewebsm230  encorewebsm230targz,Implementation Issues
Filtering stop words out of a multiple text files (using a list of stop words),"I have a folder named cleaned_texts . The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format: ['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar'] . Example: a.txt contains ['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India'] and b.txt contains ['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada'] . I also have another folder named StopWords which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc). Example: currency.txt contains names of currencies (Eg: BAHT | Thailand, PESO | Mexico, YEN | Japan etc) . geographic.txt contains names of countries (Eg: Canada, China, India, Germany etc) . I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder. I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it. Here is my script: import glob import codecs import os #Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') clean_text_data = [] for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: clean_text_data.append(info.read()) #Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) for file in stopwords_files: with open(file, 'r') as w: stop_words = w.read() map_dict = {'|': ''} res = ''.join( idx if idx not in map_dict else map_dict[idx] for idx in stop_words) new_list = res.split() #new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada'] #Trying to save the filtered texts folder_name = ""new_texts"" Path(folder).mkdir(parents=True, exist_ok=True) filtered_sentence = [] for index, word in enumerate(clean_text_data): if word not in new_list: #print(filtered_sentence.append(word)) file_path = Path(folder_name, f""{index}.txt"") with pathlib.Path.open(file_path, ""w"", encoding=""utf-8"") as f: f.write(f""{filtered_sentence }"") Actual/Resulting Output: ""None"" is printing in all the text files. a.txt = None b.txt = None c.txt = None Expected Output: a.txt = ['Rise', 'of', 'e-health', 'and', 'its'] b.txt = ['Man', 'development', 'never','year', 'date']","['python', 'nlp']",1,"You are not correctly combining stopwords from different files. Also, you never assigned any value to filtered_sentence so you end up writing an empty list to your files, resulting in the unexpected output. Try the following instead: import glob import codecs import os from pathlib import Path # Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') # Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) # Combine all stop words into a single list stop_words = [] for file in stopwords_files: with open(file, 'r', encoding='utf-8') as w: stop_words_in_file = [word.strip() for word in w.read().split('|')] stop_words.extend(stop_words_in_file) # Remove duplicates and convert to set for faster lookup stop_words = set(stop_words) # Loop through cleaned texts and filter out stop words folder_name = ""new_texts"" Path(folder_name).mkdir(parents=True, exist_ok=True) for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: content = eval(info.read()) # Convert string to list filtered_content = [word for word in content if word not in stop_words] file_path = os.path.join(folder_name, os.path.basename(path)) with open(file_path, ""w"", encoding=""utf-8"") as f: f.write(str(filtered_content))",2024-05-23 22:15:12,2024-05-24 09:46:37,44,https://stackoverflow.com/questions/78525770/filtering-stop-words-out-of-a-multiple-text-files-using-a-list-of-stop-words,"Filtering stop words out of a multiple text files (using a list of stop words) I have a folder named cleaned_texts . The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format: ['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar'] . Example: a.txt contains ['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India'] and b.txt contains ['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada'] . I also have another folder named StopWords which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc). Example: currency.txt contains names of currencies (Eg: BAHT | Thailand, PESO | Mexico, YEN | Japan etc) . geographic.txt contains names of countries (Eg: Canada, China, India, Germany etc) . I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder. I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it. Here is my script: import glob import codecs import os #Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') clean_text_data = [] for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: clean_text_data.append(info.read()) #Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) for file in stopwords_files: with open(file, 'r') as w: stop_words = w.read() map_dict = {'|': ''} res = ''.join( idx if idx not in map_dict else map_dict[idx] for idx in stop_words) new_list = res.split() #new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada'] #Trying to save the filtered texts folder_name = ""new_texts"" Path(folder).mkdir(parents=True, exist_ok=True) filtered_sentence = [] for index, word in enumerate(clean_text_data): if word not in new_list: #print(filtered_sentence.append(word)) file_path = Path(folder_name, f""{index}.txt"") with pathlib.Path.open(file_path, ""w"", encoding=""utf-8"") as f: f.write(f""{filtered_sentence }"") Actual/Resulting Output: ""None"" is printing in all the text files. a.txt = None b.txt = None c.txt = None Expected Output: a.txt = ['Rise', 'of', 'e-health', 'and', 'its'] b.txt = ['Man', 'development', 'never','year', 'date']",filtering stop word multiple text file  use list stop word  folder name cleanedtexts  folder contain text file  atxt  btxt  ctxt etc  text file contain tokenized word format    rise    of    e  health    and    its    germany    dollar    example  atxt contain   rise    of    e  health    and    its    thailand    yen    india   btxt contain   peso    man    development    never    japan    year    date    canada    also another folder name stopwords also contain text file text file contain stop word  text file name format  currencytxt  namestxt  geographictxt etc   example  currencytxt contain name currency  eg  baht  thailand  peso  mexico  yen  japan etc   geographictxt contain name country  eg  canada  china  india  germany etc   want filter stop word contain text file inside stopwords folder  text file cleanedtexts folder  loop stop word folder  combine stop word convert list  challenge filter stop word cleanedtexts file  day could not figure  script  import glob import codec import os  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts   cleantextdata    root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  cleantextdataappend  inforead     stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt    file stopwordsfile  open  file   r   w  stopwords  wread   mapdict          re    join  idx idx mapdict else mapdict  idx  idx stopwords  newlist  ressplit    newlist output   smith    surname    from    1990    thailand    yen    india    peso    japan    canada    try save filter text foldername    newtexts  path  folder  mkdir  parent  true  existok  true  filteredsentence    index  word enumerate  cleantextdata   word newlist   print  filteredsentenceappend  word   filepath  path  foldername  f   index  txt   pathlibpathopen  filepath    w   encoding  utf8   f  fwrite  f   filteredsentence    actual  result output    none  print text file  atxt  none btxt  none ctxt  none expect output  atxt    rise    of    e  health    and    its   btxt    man    development    never    year    date  ,correctly combine stopword different file  also  never assign value filteredsentence end write empty list file  result unexpected output  try follow instead  import glob import codec import os pathlib import path  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts    stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt     combine stop word single list stopwords    file stopwordsfile  open  file   r   encodingutf8   w  stopwordsinfile   wordstrip   word wread   split       stopwordsextend  stopwordsinfile   remove duplicate convert set fast lookup stopwords  set  stopwords   loop clean text filter stop word foldername    newtexts  path  foldername  mkdir  parent  true  existok  true  root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  content  eval  inforead     convert string list filteredcontent   word word content word stopwords  filepath  ospathjoin  foldername  ospathbasename  path   open  filepath    w   encoding  utf8   f  fwrite  str  filteredcontent  ,filtering stop word multiple text file  use list stop word  folder name cleanedtexts  folder contain text file  atxt  btxt  ctxt etc  text file contain tokenized word format    rise    of    e  health    and    its    germany    dollar    example  atxt contain   rise    of    e  health    and    its    thailand    yen    india   btxt contain   peso    man    development    never    japan    year    date    canada    also another folder name stopwords also contain text file text file contain stop word  text file name format  currencytxt  namestxt  geographictxt etc   example  currencytxt contain name currency  eg  baht  thailand  peso  mexico  yen  japan etc   geographictxt contain name country  eg  canada  china  india  germany etc   want filter stop word contain text file inside stopwords folder  text file cleanedtexts folder  loop stop word folder  combine stop word convert list  challenge filter stop word cleanedtexts file  day could not figure  script  import glob import codec import os  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts   cleantextdata    root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  cleantextdataappend  inforead     stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt    file stopwordsfile  open  file   r   w  stopwords  wread   mapdict          re    join  idx idx mapdict else mapdict  idx  idx stopwords  newlist  ressplit    newlist output   smith    surname    from    1990    thailand    yen    india    peso    japan    canada    try save filter text foldername    newtexts  path  folder  mkdir  parent  true  existok  true  filteredsentence    index  word enumerate  cleantextdata   word newlist   print  filteredsentenceappend  word   filepath  path  foldername  f   index  txt   pathlibpathopen  filepath    w   encoding  utf8   f  fwrite  f   filteredsentence    actual  result output    none  print text file  atxt  none btxt  none ctxt  none expect output  atxt    rise    of    e  health    and    its   btxt    man    development    never    year    date   correctly combine stopword different file  also  never assign value filteredsentence end write empty list file  result unexpected output  try follow instead  import glob import codec import os pathlib import path  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts    stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt     combine stop word single list stopwords    file stopwordsfile  open  file   r   encodingutf8   w  stopwordsinfile   wordstrip   word wread   split       stopwordsextend  stopwordsinfile   remove duplicate convert set fast lookup stopwords  set  stopwords   loop clean text filter stop word foldername    newtexts  path  foldername  mkdir  parent  true  existok  true  root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  content  eval  inforead     convert string list filteredcontent   word word content word stopwords  filepath  ospathjoin  foldername  ospathbasename  path   open  filepath    w   encoding  utf8   f  fwrite  str  filteredcontent  ,Implementation Issues
How to lemmatize text column in pandas dataframes using stanza?,I read csv file into pandas dataframe. my text column is df['story']. how do I lemmatize this colummn ? should I tokenize before?,"['pandas', 'nlp', 'tokenize', 'lemmatization', 'stanza']",1,"No, you don't necessarily have to tokenize before lemmatizing. You can try the following code: import stanza import pandas as pd nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma') def lemmatize_text(text): doc = nlp(text) lemmas = [word.lemma for sent in doc.sentences for word in sent.words] return ' '.join(lemmas) df['lemmatized_story'] = df['story'].apply(lemmatize_text)",2024-05-16 12:34:50,2024-05-16 17:14:59,72,https://stackoverflow.com/questions/78489915/how-to-lemmatize-text-column-in-pandas-dataframes-using-stanza,How to lemmatize text column in pandas dataframes using stanza? I read csv file into pandas dataframe. my text column is df['story']. how do I lemmatize this colummn ? should I tokenize before?,lemmatize text column panda dataframe use stanza  read csv file panda dataframe  text column df   story    lemmatize colummn  tokenize , not necessarily tokenize lemmatizing  try follow code  import stanza import panda pd nlp  stanza  pipeline  langen   processorstokenize  mwt  pos  lemma   def lemmatizetext  text   doc  nlp  text  lemma   wordlemma send docsentence word sentword  return   join  lemmas  df   lemmatizedstory    df   story   apply  lemmatizetext ,lemmatize text column panda dataframe use stanza  read csv file panda dataframe  text column df   story    lemmatize colummn  tokenize   not necessarily tokenize lemmatizing  try follow code  import stanza import panda pd nlp  stanza  pipeline  langen   processorstokenize  mwt  pos  lemma   def lemmatizetext  text   doc  nlp  text  lemma   wordlemma send docsentence word sentword  return   join  lemmas  df   lemmatizedstory    df   story   apply  lemmatizetext ,Library/Tool-Based Queries
Encode a list of sentences into embeddings using a HuggingFace model not in its hub,"I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's M2M100 model, I do not get the expected results. When using a model within SentenceTransformer() , my results look like this: from sentence_transformers import SentenceTransformer dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_1 = SentenceTransformer('all-distilroberta-v1') embeddings_1 = model_1.encode(dat) embeddings_1.shape > (2, 768) However, when I use the M2M100 model, my results do not look right at all, specifically I would expect 2 rows of results: from transformers import M2M100Tokenizer model_m2m = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"") model_m2m.src_lang = ""en"" embeddings_m2m = model_m2m.encode(dat, return_tensors=""pt"") embeddings_m2m.shape > torch.Size([1, 4]) How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding? (As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)","['nlp', 'huggingface-transformers', 'encode', 'embedding', 'huggingface']",1,"The code you provided only uses the tokenizer of the model, which maps the text to integer ids that don't represent any kind of (semantical) meaning. To retrieve sentence embeddings (i.e. a vector that represents the text) from facebook/m2m100_418M , which is an encoder-decoder model, you need to perform some kind of pooling over the last-hidden-state of the encoder. Common approaches, which are cls and mean pooling are shown in the example below: import torch from transformers import M2M100Tokenizer, M2M100Model def mean_pooling(last_hidden_state, attention_mask): non_pad_tokens = attention_mask.sum(1) sum_embeddings = torch.sum(attention_mask.unsqueeze(-1) * last_hidden_state, 1) return sum_embeddings/non_pad_tokens.unsqueeze(-1) def cls_pooling(last_hidden_state): return last_hidden_state[:,0] dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_id = ""facebook/m2m100_418M"" t_m2m = M2M100Tokenizer.from_pretrained(model_id) t_m2m.src_lang = ""en"" m_m2m = M2M100Model.from_pretrained(model_id) tokenized = t_m2m(dat, padding=True, return_tensors='pt') with torch.inference_mode(): encoder_o = m_m2m.encoder(**tokenized) encoder_last_hidden_state = encoder_o.last_hidden_state print(encoder_last_hidden_state.shape) mean_pooling_embeddings = mean_pooling(encoder_last_hidden_state, tokenized.attention_mask) print(mean_pooling_embeddings.shape) cls_pooling_embeddings = cls_pooling(encoder_last_hidden_state) print(cls_pooling_embeddings.shape) Output: torch.Size([2, 9, 1024]) torch.Size([2, 1024]) torch.Size([2, 1024]) Which of the two approaches works better for your downstream task, must be tested with your data. Please also note that even when you have the sentence embeddings now, it doesn't mean they are semantically meaningful (i.e. the embeddings are useless for your downstream task). Refer to this StackOverflow answer for further explanation.",2024-05-15 16:39:39,2024-05-19 10:57:24,771,https://stackoverflow.com/questions/78485347/encode-a-list-of-sentences-into-embeddings-using-a-huggingface-model-not-in-its,"Encode a list of sentences into embeddings using a HuggingFace model not in its hub I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's M2M100 model, I do not get the expected results. When using a model within SentenceTransformer() , my results look like this: from sentence_transformers import SentenceTransformer dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_1 = SentenceTransformer('all-distilroberta-v1') embeddings_1 = model_1.encode(dat) embeddings_1.shape > (2, 768) However, when I use the M2M100 model, my results do not look right at all, specifically I would expect 2 rows of results: from transformers import M2M100Tokenizer model_m2m = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"") model_m2m.src_lang = ""en"" embeddings_m2m = model_m2m.encode(dat, return_tensors=""pt"") embeddings_m2m.shape > torch.Size([1, 4]) How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding? (As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)",encode list sentence embedding use huggingface model hub try encode list sentence list embedding  use model huggingface hub  work expect  use model hub  case facebook s m2m100 model  get expect result  use model within sentencetransformer    result look like  sentencetransformer import sentencetransformer dat    meteorite fall road    go wrong direction   model1  sentencetransformer   all  distilroberta  v1   embeddings1  model1encode  dat  embeddings1shape   2  768  however  use m2m100 model  result look right  specifically would expect 2 row result  transformer import m2m100tokenizer modelm2 m  m2m100tokenizerfrompretrained    facebook  m2m100418 m   modelm2msrclang    en  embeddingsm2 m  modelm2mencode  dat  returntensors  pt   embeddingsm2mshape  torch  size   1  4   format return n  dimensional list embedding  row correspond sentence number column equal dimensionality embed   note  eventually sentence language   m use multi  lingual model  ,code provide use tokenizer model  map text integer id not represent kind  semantical  meaning  retrieve sentence embedding  ie  vector represent text  facebook  m2m100418 m  encoder  decoder model  need perform kind pool last  hide  state encoder  common approach  cls mean pooling show example  import torch transformer import m2m100tokenizer  m2m100model def meanpooling  lasthiddenstate  attentionmask   nonpadtoken  attentionmasksum  1  sumembedding  torchsum  attentionmaskunsqueeze  1   lasthiddenstate  1  return sumembedding  nonpadtokensunsqueeze  1  def clspooling  lasthiddenstate   return lasthiddenstate    0  dat    meteorite fall road    go wrong direction   modelid    facebook  m2m100418 m  tm2 m  m2m100tokenizerfrompretrained  modelid  tm2msrclang    en  mm2 m  m2m100modelfrompretraine  modelid  tokenized  tm2 m  dat  pad  true  returntensorspt   torchinferencemode    encodero  mm2mencoder    tokenized  encoderlasthiddenstate  encoderolasthiddenstate print  encoderlasthiddenstateshape  meanpoolingembeddings  meanpooling  encoderlasthiddenstate  tokenizedattentionmask  print  meanpoolingembeddingsshape  clspoolingembedding  clspoole  encoderlasthiddenstate  print  clspoolingembeddingsshape  output  torch  size   2  9  1024   torch  size   2  1024   torch  size   2  1024   two approach work well downstream task  must test datum  please also note even sentence embedding  not mean semantically meaningful  ie  embedding useless downstream task   refer stackoverflow answer explanation ,encode list sentence embedding use huggingface model hub try encode list sentence list embedding  use model huggingface hub  work expect  use model hub  case facebook s m2m100 model  get expect result  use model within sentencetransformer    result look like  sentencetransformer import sentencetransformer dat    meteorite fall road    go wrong direction   model1  sentencetransformer   all  distilroberta  v1   embeddings1  model1encode  dat  embeddings1shape   2  768  however  use m2m100 model  result look right  specifically would expect 2 row result  transformer import m2m100tokenizer modelm2 m  m2m100tokenizerfrompretrained    facebook  m2m100418 m   modelm2msrclang    en  embeddingsm2 m  modelm2mencode  dat  returntensors  pt   embeddingsm2mshape  torch  size   1  4   format return n  dimensional list embedding  row correspond sentence number column equal dimensionality embed   note  eventually sentence language   m use multi  lingual model   code provide use tokenizer model  map text integer id not represent kind  semantical  meaning  retrieve sentence embedding  ie  vector represent text  facebook  m2m100418 m  encoder  decoder model  need perform kind pool last  hide  state encoder  common approach  cls mean pooling show example  import torch transformer import m2m100tokenizer  m2m100model def meanpooling  lasthiddenstate  attentionmask   nonpadtoken  attentionmasksum  1  sumembedding  torchsum  attentionmaskunsqueeze  1   lasthiddenstate  1  return sumembedding  nonpadtokensunsqueeze  1  def clspooling  lasthiddenstate   return lasthiddenstate    0  dat    meteorite fall road    go wrong direction   modelid    facebook  m2m100418 m  tm2 m  m2m100tokenizerfrompretrained  modelid  tm2msrclang    en  mm2 m  m2m100modelfrompretraine  modelid  tokenized  tm2 m  dat  pad  true  returntensorspt   torchinferencemode    encodero  mm2mencoder    tokenized  encoderlasthiddenstate  encoderolasthiddenstate print  encoderlasthiddenstateshape  meanpoolingembeddings  meanpooling  encoderlasthiddenstate  tokenizedattentionmask  print  meanpoolingembeddingsshape  clspoolingembedding  clspoole  encoderlasthiddenstate  print  clspoolingembeddingsshape  output  torch  size   2  9  1024   torch  size   2  1024   torch  size   2  1024   two approach work well downstream task  must test datum  please also note even sentence embedding  not mean semantically meaningful  ie  embedding useless downstream task   refer stackoverflow answer explanation ,Implementation Issues
How to optimize this function and improve running time?,"I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time? Here's my code: def pmi_count_phrase_create(pmi_tups,freq_list): import pandas as pd """"""pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)] freq_list is a result of running freq_list= finder.ngram_fd.items() -> df made up of columns for pmi list, count list, phrase list"""""" pmi3_list =[] count3_list =[] phrase3_list =[] for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..] for item in freq_list: quadgram,count = item if quadgram == phrase: pmi3_list.append(pmi) count3_list.append(count) phrase3_list.append(phrase) # create dataframe df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list}) return df Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.","['python', 'performance', 'optimization', 'nlp', 'nltk']",1,"Ended up changing my function to convert freq_list to a dictionary and list comprehensions instead of for loops and this code instantly returned a data-frame: def quicker_func(pmi_tups, freq_list): import pandas as pd freq_dict = dict(freq_list) # Create a dictionary for faster lookups pmi_list = [pmi for phrase, pmi in pmi_tups if phrase in freq_dict] count_list = [freq_dict[phrase] for phrase, pmi in pmi_tups if phrase in freq_dict] phrase_list = [phrase for phrase, pmi in pmi_tups if phrase in freq_dict] df = pd.DataFrame({'Phrase': phrase_list, 'PMI': pmi_list, 'Count': count_list}) return df",2024-05-10 15:18:00,2024-05-10 17:42:18,47,https://stackoverflow.com/questions/78461078/how-to-optimize-this-function-and-improve-running-time,"How to optimize this function and improve running time? I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time? Here's my code: def pmi_count_phrase_create(pmi_tups,freq_list): import pandas as pd """"""pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)] freq_list is a result of running freq_list= finder.ngram_fd.items() -> df made up of columns for pmi list, count list, phrase list"""""" pmi3_list =[] count3_list =[] phrase3_list =[] for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..] for item in freq_list: quadgram,count = item if quadgram == phrase: pmi3_list.append(pmi) count3_list.append(count) phrase3_list.append(phrase) # create dataframe df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list}) return df Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.",optimize function improve run time  function aim create data  frame three column  bigram  phrase  count  bigram  phrase   pmi score  bigram  phrase   since want run large dataset million phrase  compute time incredibly long  recognize nest for  loop matching condition contribute computation difficulty  alternative way thing cut run  time  s code  def pmicountphrasecreate  pmitup  freqlist   import panda pd     pmitup result run pmitup   finderscorengrams  bigrammeasurespmi   freqlist result run freqlist finderngramfditems     df make column pmi list  count list  phrase list    pmi3list    count3list    phrase3list    phrase  pmi pmitup   pmitup list tuple form     phrase   pmi     item freqlist  quadgram  count  item quadgram   phrase  pmi3listappend  pmi  count3listappend  count  phrase3listappend  phrase   create dataframe df  pd  dataframe    phrase   phrase3list   pmi   pmi3list   count   count3list   return df running code pmitup freqlist  still run s 1000 minute   m open also use different library evaluate bi  gram phrase  pmi s frequency ,end change function convert freqlist dictionary list comprehension instead loop code instantly return data  frame  def quickerfunc  pmitup  freqlist   import panda pd freqdict  dict  freqlist   create dictionary fast lookup pmilist   pmi phrase  pmi pmitup phrase freqdict  countlist   freqdict  phrase  phrase  pmi pmitup phrase freqdict  phraselist   phrase phrase  pmi pmitup phrase freqdict  df  pd  dataframe    phrase   phraselist   pmi   pmilist   count   countlist   return df,optimize function improve run time  function aim create data  frame three column  bigram  phrase  count  bigram  phrase   pmi score  bigram  phrase   since want run large dataset million phrase  compute time incredibly long  recognize nest for  loop matching condition contribute computation difficulty  alternative way thing cut run  time  s code  def pmicountphrasecreate  pmitup  freqlist   import panda pd     pmitup result run pmitup   finderscorengrams  bigrammeasurespmi   freqlist result run freqlist finderngramfditems     df make column pmi list  count list  phrase list    pmi3list    count3list    phrase3list    phrase  pmi pmitup   pmitup list tuple form     phrase   pmi     item freqlist  quadgram  count  item quadgram   phrase  pmi3listappend  pmi  count3listappend  count  phrase3listappend  phrase   create dataframe df  pd  dataframe    phrase   phrase3list   pmi   pmi3list   count   count3list   return df running code pmitup freqlist  still run s 1000 minute   m open also use different library evaluate bi  gram phrase  pmi s frequency  end change function convert freqlist dictionary list comprehension instead loop code instantly return data  frame  def quickerfunc  pmitup  freqlist   import panda pd freqdict  dict  freqlist   create dictionary fast lookup pmilist   pmi phrase  pmi pmitup phrase freqdict  countlist   freqdict  phrase  phrase  pmi pmitup phrase freqdict  phraselist   phrase phrase  pmi pmitup phrase freqdict  df  pd  dataframe    phrase   phraselist   pmi   pmilist   count   countlist   return df,Library/Tool-Based Queries
Why doesn&#39;t fuzzywuzzy&#39;s process.extractBests give a 100% score when the tested string 100% contains the query string?,"I'm testing fuzzywuzzy 's process.extractBests() as follows: from fuzzywuzzy import process # Define the query string query = ""Apple"" # Define the list of choices choices = [""Apple"", ""Apple Inc."", ""Apple Computer"", ""Apple Records"", ""Apple TV""] # Call the process.extractBests function results = process.extractBests(query, choices) # Print the results for result in results: print(result) It outputs: ('Apple', 100) ('Apple Inc.', 90) ('Apple Computer', 90) ('Apple Records', 90) ('Apple TV', 90) Why didn't the scorer give 100 to all strings since they all 100% contain the query string (""Apple"")? I use fuzzywuzzy==0.18.0 with Python 3.11.7.","['python', 'nlp', 'string-matching', 'fuzzywuzzy']",1,"The fuzzywuzzy 's extractBests() function does not give 100% because it does not check for a match, it checks for similarity, such as length of string, contents of string compared to the query, positions of the query string, and a few other factors. In your case, it does not output 100% because ""Apple Inc."" is not an exact match of your query, ""Apple"". This is why only the ""Apple"" choice outputs 100%, because it 100% matches with the query, ""Apple"". I hoped this helped!",2024-05-09 14:24:36,2024-05-09 14:36:54,76,https://stackoverflow.com/questions/78455102/why-doesnt-fuzzywuzzys-process-extractbests-give-a-100-score-when-the-tested,"Why doesn&#39;t fuzzywuzzy&#39;s process.extractBests give a 100% score when the tested string 100% contains the query string? I'm testing fuzzywuzzy 's process.extractBests() as follows: from fuzzywuzzy import process # Define the query string query = ""Apple"" # Define the list of choices choices = [""Apple"", ""Apple Inc."", ""Apple Computer"", ""Apple Records"", ""Apple TV""] # Call the process.extractBests function results = process.extractBests(query, choices) # Print the results for result in results: print(result) It outputs: ('Apple', 100) ('Apple Inc.', 90) ('Apple Computer', 90) ('Apple Records', 90) ('Apple TV', 90) Why didn't the scorer give 100 to all strings since they all 100% contain the query string (""Apple"")? I use fuzzywuzzy==0.18.0 with Python 3.11.7.",  39  fuzzywuzzy   39  processextractbest give 100  score test string 100  contain query string   m test fuzzywuzzy s processextractbest   follow  fuzzywuzzy import process  define query string query    apple   define list choice choice     apple     apple inc     apple computer     apple records     apple tv    call processextractbest function result  processextractbest  query  choice   print result result result  print  result  output    apple   100    apple inc   90    apple computer   90    apple records   90    apple tv   90  not scorer give 100 string since 100  contain query string    apple    use fuzzywuzzy0180 python 3117 ,fuzzywuzzy s extractbest   function give 100  check match  check similarity  length string  content string compare query  position query string  factor  case  output 100    apple inc  exact match query    apple     apple  choice output 100   100  match query    apple   hoped help ,  39  fuzzywuzzy   39  processextractbest give 100  score test string 100  contain query string   m test fuzzywuzzy s processextractbest   follow  fuzzywuzzy import process  define query string query    apple   define list choice choice     apple     apple inc     apple computer     apple records     apple tv    call processextractbest function result  processextractbest  query  choice   print result result result  print  result  output    apple   100    apple inc   90    apple computer   90    apple records   90    apple tv   90  not scorer give 100 string since 100  contain query string    apple    use fuzzywuzzy0180 python 3117  fuzzywuzzy s extractbest   function give 100  check match  check similarity  length string  content string compare query  position query string  factor  case  output 100    apple inc  exact match query    apple     apple  choice output 100   100  match query    apple   hoped help ,Basic Understanding
FastText language_identification in R returns too many arguments - how to match to texts?,"FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document. There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts? Code: DF = data.frame(doc_id = seq(1, 5), speechtext = c(""Hello. Fake text entry 1."", ""Fake text entry 2"", ""more text"", ""Text in a different language"", ""Hola"")) library(fastText) # download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html file_ftz = system.file(""language_identification/lid.176.ftz"", package = ""fastText"") lang1 = language_identification(DF$speechtext, pre_trained_language_model_path = file_ftz, verbose = T) I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with. Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug. (I tried adding intern = T as an argument per R - fasttext how to load output into a dataframe from command line -- this is not recognized as an argument).","['r', 'nlp', 'fasttext', 'language-detection']",1,"The first argument to fastText::language_identification() is defined as: either a valid character string to a valid path where each line represents a different text extract or a vector of text extracts (emphasis mine) You have line breaks in your input data: DF$speechtext[4] [1] ""Text in a\ndifferent language"" As one prediction is generated per line, you'll get two predictions from this element. You have two options: Remove new lines in your input data. This makes sense in this case. Keep new lines and map document IDs to each line. This makes sense if new lines might actually be in different languages. Remove new lines If you replace new lines with spaces you will get the same number of predictions returned as input rows. In the regex below, I have used the PCRE \v which matches newlines and any character considered vertical whitespace. This now produces five rows, one relating to each input row. language_identification(gsub(""\\v"", "" "", DF$speechtext, perl = TRUE), file_ftz) # iso_lang_1 prob_1 # <char> <num> # 1: en 0.220767 # 2: en 0.388695 # 3: en 0.613707 # 4: en 0.757671 # 5: es 0.721487 \v includes several vertical space characters (such as form feed and line separator), so should cover all possible types of new line. For full details see the table here . Keep new lines and map document ID to each line Alternatively, if different lines of each input document might be in different languages, you may not want to remove new lines. In this case, you can predict each line separately and then map the document IDs to each line: # As before lang1 <- language_identification(DF$speechtext, file_ftz) # Add document IDs lang1$doc_id <- rep( DF$doc_id, lengths(strsplit(DF$speechtext, ""\\v"", perl = TRUE)) ) lang1 # iso_lang_1 prob_1 doc_id # <char> <num> <int> # 1: en 0.220767 1 # 2: en 0.388695 2 # 3: en 0.613707 3 # 4: en 0.932691 4 # 5: en 0.571937 4 # 6: es 0.721487 5",2024-05-07 16:46:21,2024-05-07 16:56:33,132,https://stackoverflow.com/questions/78443980/fasttext-language-identification-in-r-returns-too-many-arguments-how-to-match,"FastText language_identification in R returns too many arguments - how to match to texts? FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document. There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts? Code: DF = data.frame(doc_id = seq(1, 5), speechtext = c(""Hello. Fake text entry 1."", ""Fake text entry 2"", ""more text"", ""Text in a different language"", ""Hola"")) library(fastText) # download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html file_ftz = system.file(""language_identification/lid.176.ftz"", package = ""fastText"") lang1 = language_identification(DF$speechtext, pre_trained_language_model_path = file_ftz, verbose = T) I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with. Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug. (I tried adding intern = T as an argument per R - fasttext how to load output into a dataframe from command line -- this is not recognized as an argument).",fasttext languageidentification r return many argument  match text  fasttext languageidentification return multiple prediction per original text  also fails indicate belong original document  differ number prediction per original document  github forum close  anyone know match output original text  code  df  dataframe  docid  seq  1  5   speechtext  c    hello  fake text entry 1       fake text entry 2     text     text different language     hola    library  fasttext   download ftz pretraine model   fasttextcc  docs  en  language  identificationhtml fileftz  systemfile    languageidentification  lid176ftz   package    fasttext   lang1  languageidentification  df  speechtext  pretrainedlanguagemodelpath  fileftz  verbose   expect one prediction per original text  least consistent number  way mark document prediction align  really could guess base large number per series element output  not seem optimal  seem like bug   try add intern  argument per r  fasttext load output dataframe command line  recognize argument  ,first argument fasttext   languageidentification   define  either valid character string valid path line represent different text extract vector text extract  emphasis mine  line break input datum  df  speechtext  4   1    text andifferent language  one prediction generate per line  will get two prediction element  two option  remove new line input datum  make sense case  keep new line map document id line  make sense new line might actually different language  remove new line replace new line space get number prediction return input row  regex  use pcre v match newline character consider vertical whitespace  produce five row  one relate input row  languageidentification  gsub    v        df  speechtext  perl  true   fileftz   isolang1 prob1   char   num   1  en 0220767  2  en 0388695  3  en 0613707  4  en 0757671  5  es 0721487 v include several vertical space character  form feed line separator   cover possible type new line  full detail see table  keep new line map document id line alternatively  different line input document might different language  may want remove new line  case  predict line separately map document id line   lang1   languageidentification  df  speechtext  fileftz   add document id lang1  docid   rep  df  docid  length  strsplit  df  speechtext    v   perl  true    lang1  isolang1 prob1 docid   char   num   int   1  en 0220767 1  2  en 0388695 2  3  en 0613707 3  4  en 0932691 4  5  en 0571937 4  6  es 0721487 5,fasttext languageidentification r return many argument  match text  fasttext languageidentification return multiple prediction per original text  also fails indicate belong original document  differ number prediction per original document  github forum close  anyone know match output original text  code  df  dataframe  docid  seq  1  5   speechtext  c    hello  fake text entry 1       fake text entry 2     text     text different language     hola    library  fasttext   download ftz pretraine model   fasttextcc  docs  en  language  identificationhtml fileftz  systemfile    languageidentification  lid176ftz   package    fasttext   lang1  languageidentification  df  speechtext  pretrainedlanguagemodelpath  fileftz  verbose   expect one prediction per original text  least consistent number  way mark document prediction align  really could guess base large number per series element output  not seem optimal  seem like bug   try add intern  argument per r  fasttext load output dataframe command line  recognize argument   first argument fasttext   languageidentification   define  either valid character string valid path line represent different text extract vector text extract  emphasis mine  line break input datum  df  speechtext  4   1    text andifferent language  one prediction generate per line  will get two prediction element  two option  remove new line input datum  make sense case  keep new line map document id line  make sense new line might actually different language  remove new line replace new line space get number prediction return input row  regex  use pcre v match newline character consider vertical whitespace  produce five row  one relate input row  languageidentification  gsub    v        df  speechtext  perl  true   fileftz   isolang1 prob1   char   num   1  en 0220767  2  en 0388695  3  en 0613707  4  en 0757671  5  es 0721487 v include several vertical space character  form feed line separator   cover possible type new line  full detail see table  keep new line map document id line alternatively  different line input document might different language  may want remove new line  case  predict line separately map document id line   lang1   languageidentification  df  speechtext  fileftz   add document id lang1  docid   rep  df  docid  length  strsplit  df  speechtext    v   perl  true    lang1  isolang1 prob1 docid   char   num   int   1  en 0220767 1  2  en 0388695 2  3  en 0613707 3  4  en 0932691 4  5  en 0571937 4  6  es 0721487 5,Library/Tool-Based Queries
Determining contents of decoder_hidden_states from T5ForConditionalGeneration,"I'm using the Huggingface T5ForConditionalGeneration model without modification. I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the decoder_hidden_states contains what I'm looking for. I want to do something like this: # Prepare batch data sources = batch_df['Source'].tolist() tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda') input_ids = tokenized_input['input_ids'].to('cuda') attention_mask = tokenized_input['attention_mask'].to('cuda') input_batch = { 'input_ids': input_ids, 'attention_mask': attention_mask, 'do_sample': False, 'num_beams': 1, 'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id, 'max_length': self.max_output_length, 'output_scores': True, 'return_dict_in_generate': True, 'output_hidden_states': True, } outputs = self.model.generate(**input_batch) # Retrieve the decoder hidden states decoder_last_hidden_state = outputs.decoder_hidden_states[-1] # Last layer's hidden states # Compute the mean of the hidden states across the sequence length dimension mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False) This approach works for the encoder, but for the decoder, decoder_hidden_states[-1] is a tuple of tensors, not a tensor. When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors. When I inspected the dimensions, like this: for tuple_number in range(n): # Checking the tuples print(f""Tuple {layer_number}:"") for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]): print(f"" Tuple {i} in Layer {layer_number}: shape {tensor.shape}"") the outputs were all like this: Tuple 0: Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512]) Tuple 1: Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512]) . . . 512 is the max_length of my tokenizer, and 2 is my batch size. (I verified that 2 is the batch size because that number changed when I modified my batch size.) Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39. Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits. So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple. However, I don't understand exactly how the token length corresponds to the number of tuples. How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.","['pytorch', 'nlp', 'huggingface-transformers']",1,"I think whats happening is that T5 returns the hidden state per step of decoding. Therefore, the number of tuples should correspond to the longest generated sequence. You are most likely interested in the last decoding step and could take the last tuple. In that tuple you have a tuple of size num_layers + 1 (+1 for the final LayerNorm). The output of the last layer should be the last tuple entry.",2024-05-04 22:13:37,2024-05-08 10:14:16,121,https://stackoverflow.com/questions/78430524/determining-contents-of-decoder-hidden-states-from-t5forconditionalgeneration,"Determining contents of decoder_hidden_states from T5ForConditionalGeneration I'm using the Huggingface T5ForConditionalGeneration model without modification. I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the decoder_hidden_states contains what I'm looking for. I want to do something like this: # Prepare batch data sources = batch_df['Source'].tolist() tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda') input_ids = tokenized_input['input_ids'].to('cuda') attention_mask = tokenized_input['attention_mask'].to('cuda') input_batch = { 'input_ids': input_ids, 'attention_mask': attention_mask, 'do_sample': False, 'num_beams': 1, 'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id, 'max_length': self.max_output_length, 'output_scores': True, 'return_dict_in_generate': True, 'output_hidden_states': True, } outputs = self.model.generate(**input_batch) # Retrieve the decoder hidden states decoder_last_hidden_state = outputs.decoder_hidden_states[-1] # Last layer's hidden states # Compute the mean of the hidden states across the sequence length dimension mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False) This approach works for the encoder, but for the decoder, decoder_hidden_states[-1] is a tuple of tensors, not a tensor. When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors. When I inspected the dimensions, like this: for tuple_number in range(n): # Checking the tuples print(f""Tuple {layer_number}:"") for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]): print(f"" Tuple {i} in Layer {layer_number}: shape {tensor.shape}"") the outputs were all like this: Tuple 0: Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512]) Tuple 1: Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512]) . . . 512 is the max_length of my tokenizer, and 2 is my batch size. (I verified that 2 is the batch size because that number changed when I modified my batch size.) Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39. Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits. So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple. However, I don't understand exactly how the token length corresponds to the number of tuples. How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.",determine content decoderhiddenstates t5forconditionalgeneration  m use huggingface t5forconditionalgeneration model without modification  want compute mean pool last hide state t5 decoder  can not determine part decoderhiddenstates contain  m look  want something like   prepare batch data source  batchdf   source   tolist   tokenizedinput  selftokenizer  source  returntensorspt   pad  true  truncation  true  maxlength  selfmaxlength  to   cuda   inputids  tokenizedinput   inputids   to   cuda   attentionmask  tokenizedinput   attentionmask   to   cuda   inputbatch    inputids   inputids   attentionmask   attentionmask   dosample   false   numbeams   1   eostokenid   selftokenizereostokenid   padtokenid   selftokenizerpadtokenid   maxlength   selfmaxoutputlength   outputscore   true   returndictingenerate   true   outputhiddenstate   true   output  selfmodelgenerate    inputbatch   retrieve decoder hide state decoderlasthiddenstate  outputsdecoderhiddenstate  1   last layer s hide state  compute mean hide state across sequence length dimension meanpooledoutput  torchmean  decoderlasthiddenstate  dim1  keepdim  false  approach work encoder  decoder  decoderhiddenstates  1  tuple tensor  tensor  first inspect tuple  10 tuple  tuple contain 7 tensor  inspect dimension  like  tuplenumber range  n    check tuple print  f  tuple  layernumber      tensor enumerate  outputsdecoderhiddenstate  layernumber    print  f  tuple   layer  layernumber   shape  tensorshape    output like  tuple 0  tensor 0 tuple 0  shape torch  size   2  1  512   tensor 1 tuple 0  shape torch  size   2  1  512   tensor 2 tuple 0  shape torch  size   2  1  512   tensor 3 tuple 0  shape torch  size   2  1  512   tensor 4 tuple 0  shape torch  size   2  1  512   tensor 5 tuple 0  shape torch  size   2  1  512   tensor 6 tuple 0  shape torch  size   2  1  512   tuple 1  tensor 0 tuple 1  shape torch  size   2  1  512   tensor 1 tuple 1  shape torch  size   2  1  512   tensor 2 tuple 1  shape torch  size   2  1  512   tensor 3 tuple 1  shape torch  size   2  1  512   tensor 4 tuple 1  shape torch  size   2  1  512   tensor 5 tuple 1  shape torch  size   2  1  512   tensor 6 tuple 1  shape torch  size   2  1  512      512 maxlength tokenizer  2 batch size   verify 2 batch size number change modify batch size    trim length input string 10 character  surprise  number tuple go 10 39  trim string 2 char per string  number tuple not increase beyond 39   double input string length instead  number tuple go 7   appear like number tuple correspond iteration decoder chunk size limit   wanted compute mean pool first token  seem like would compute mean last tensor first tuple  however  not understand exactly token length correspond number tuple  determine exactly represent tuple tensor  successful finding information go t5 source code ,think what s happen t5 return hide state per step decode  therefore  number tuple correspond long generate sequence  likely interested last decode step could take last tuple  tuple tuple size numlayer  1  1 final layernorm   output last layer last tuple entry ,determine content decoderhiddenstates t5forconditionalgeneration  m use huggingface t5forconditionalgeneration model without modification  want compute mean pool last hide state t5 decoder  can not determine part decoderhiddenstates contain  m look  want something like   prepare batch data source  batchdf   source   tolist   tokenizedinput  selftokenizer  source  returntensorspt   pad  true  truncation  true  maxlength  selfmaxlength  to   cuda   inputids  tokenizedinput   inputids   to   cuda   attentionmask  tokenizedinput   attentionmask   to   cuda   inputbatch    inputids   inputids   attentionmask   attentionmask   dosample   false   numbeams   1   eostokenid   selftokenizereostokenid   padtokenid   selftokenizerpadtokenid   maxlength   selfmaxoutputlength   outputscore   true   returndictingenerate   true   outputhiddenstate   true   output  selfmodelgenerate    inputbatch   retrieve decoder hide state decoderlasthiddenstate  outputsdecoderhiddenstate  1   last layer s hide state  compute mean hide state across sequence length dimension meanpooledoutput  torchmean  decoderlasthiddenstate  dim1  keepdim  false  approach work encoder  decoder  decoderhiddenstates  1  tuple tensor  tensor  first inspect tuple  10 tuple  tuple contain 7 tensor  inspect dimension  like  tuplenumber range  n    check tuple print  f  tuple  layernumber      tensor enumerate  outputsdecoderhiddenstate  layernumber    print  f  tuple   layer  layernumber   shape  tensorshape    output like  tuple 0  tensor 0 tuple 0  shape torch  size   2  1  512   tensor 1 tuple 0  shape torch  size   2  1  512   tensor 2 tuple 0  shape torch  size   2  1  512   tensor 3 tuple 0  shape torch  size   2  1  512   tensor 4 tuple 0  shape torch  size   2  1  512   tensor 5 tuple 0  shape torch  size   2  1  512   tensor 6 tuple 0  shape torch  size   2  1  512   tuple 1  tensor 0 tuple 1  shape torch  size   2  1  512   tensor 1 tuple 1  shape torch  size   2  1  512   tensor 2 tuple 1  shape torch  size   2  1  512   tensor 3 tuple 1  shape torch  size   2  1  512   tensor 4 tuple 1  shape torch  size   2  1  512   tensor 5 tuple 1  shape torch  size   2  1  512   tensor 6 tuple 1  shape torch  size   2  1  512      512 maxlength tokenizer  2 batch size   verify 2 batch size number change modify batch size    trim length input string 10 character  surprise  number tuple go 10 39  trim string 2 char per string  number tuple not increase beyond 39   double input string length instead  number tuple go 7   appear like number tuple correspond iteration decoder chunk size limit   wanted compute mean pool first token  seem like would compute mean last tensor first tuple  however  not understand exactly token length correspond number tuple  determine exactly represent tuple tensor  successful finding information go t5 source code  think what s happen t5 return hide state per step decode  therefore  number tuple correspond long generate sequence  likely interested last decode step could take last tuple  tuple tuple size numlayer  1  1 final layernorm   output last layer last tuple entry ,Task-Specific Queries
Extracting only technical keywords from a text using RAKE library in Python,"I want to use rake to extract technical keywords from a job description that I've found on Linkedin, which looks like this: input = ""In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch.Experience with front-end development using HTML, CSS, and JavaScript. Familiarity with database technologies such as SQL and NoSQL.Excellent problem-solving ability with solid communication and collaboration skills. Preferred Skills And QualificationsExperience with popular Python frameworks such as Django, Flask or Pyramid."" I run this code, as it's supposed to return the keywords. from rake_nltk import Rake r = Rake() r.extract_keywords_from_text(input) keywords = r.get_ranked_phrases_with_scores() for score, keyword in keywords: if len(keyword.split()) == 1: # Check if the keyword is one word print(f""{keyword}: {score}"") But the output is this: frameworks: 2.0 tools: 1.0 sql: 1.0 spacy: 1.0 scipy: 1.0 sci: 1.0 qualificationsexperience: 1.0 pytorch: 1.0 pyramid: 1.0 pandas: 1.0 numpy: 1.0 nosql: 1.0 nltk: 1.0 learn: 1.0 kit: 1.0 javascript: 1.0 front: 1.0 flask: 1.0 familiarity: 1.0 experience: 1.0 ecosystems: 1.0 django: 1.0 dask: 1.0 css: 1.0 Simply I just want the explicit name of tools, skills and frameworks. Such as ""Numpy"", ""Scipy"", ""HTML"", etc That are used in the text and NOT every single word that's found in it (such as ""experience"" or ""tools""). Is there any way to do so? Or should I just provide a list of all possible python frameworks and related skill and then filter the output of rake? If the latter one is the solution, How can I find/make a thorough list? Any help is appreciated.","['python', 'python-3.x', 'nlp', 'nltk', 'rake']",2,"You can utilize skill and knowledge token classification from Hugging Face's library from transformers import pipeline token_skill_classifier = pipeline(model=""jjzha/jobbert_skill_extraction"", aggregation_strategy=""first"") token_knowledge_classifier = pipeline(model=""jjzha/jobbert_knowledge_extraction"", aggregation_strategy=""first"") def aggregate_span(results): new_results = [] current_result = results[0] for result in results[1:]: if result[""start""] == current_result[""end""] + 1: current_result[""word""] += "" "" + result[""word""] current_result[""end""] = result[""end""] else: new_results.append(current_result) current_result = result new_results.append(current_result) return new_results def ner(text): output_skills = token_skill_classifier(text) for result in output_skills: if result.get(""entity_group""): result[""entity""] = ""Skill"" del result[""entity_group""] output_knowledge = token_knowledge_classifier(text) for result in output_knowledge: if result.get(""entity_group""): result[""entity""] = ""Knowledge"" del result[""entity_group""] if len(output_skills) > 0: output_skills = aggregate_span(output_skills) if len(output_knowledge) > 0: output_knowledge = aggregate_span(output_knowledge) return {""text"": text, ""entities"": output_skills}, {""text"": text, ""entities"": output_knowledge}",2024-04-28 05:52:10,2024-04-30 04:55:18,328,https://stackoverflow.com/questions/78397201/extracting-only-technical-keywords-from-a-text-using-rake-library-in-python,"Extracting only technical keywords from a text using RAKE library in Python I want to use rake to extract technical keywords from a job description that I've found on Linkedin, which looks like this: input = ""In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch.Experience with front-end development using HTML, CSS, and JavaScript. Familiarity with database technologies such as SQL and NoSQL.Excellent problem-solving ability with solid communication and collaboration skills. Preferred Skills And QualificationsExperience with popular Python frameworks such as Django, Flask or Pyramid."" I run this code, as it's supposed to return the keywords. from rake_nltk import Rake r = Rake() r.extract_keywords_from_text(input) keywords = r.get_ranked_phrases_with_scores() for score, keyword in keywords: if len(keyword.split()) == 1: # Check if the keyword is one word print(f""{keyword}: {score}"") But the output is this: frameworks: 2.0 tools: 1.0 sql: 1.0 spacy: 1.0 scipy: 1.0 sci: 1.0 qualificationsexperience: 1.0 pytorch: 1.0 pyramid: 1.0 pandas: 1.0 numpy: 1.0 nosql: 1.0 nltk: 1.0 learn: 1.0 kit: 1.0 javascript: 1.0 front: 1.0 flask: 1.0 familiarity: 1.0 experience: 1.0 ecosystems: 1.0 django: 1.0 dask: 1.0 css: 1.0 Simply I just want the explicit name of tools, skills and frameworks. Such as ""Numpy"", ""Scipy"", ""HTML"", etc That are used in the text and NOT every single word that's found in it (such as ""experience"" or ""tools""). Is there any way to do so? Or should I just provide a list of all possible python frameworks and related skill and then filter the output of rake? If the latter one is the solution, How can I find/make a thorough list? Any help is appreciated.",extract technical keyword text use rake library python want use rake extract technical keyword job description  ve find linkedin  look like  input    in  depth understand python software development stack  ecosystem  framework tool numpy  scipy  pandas  dask  spacy  nltk  sci  kit  learn pytorch  experience front  end development use html  css  javascript  familiarity database technologie sql nosqlexcellent problem  solve ability solid communication collaboration skill  preferred skills qualificationsexperience popular python framework django  flask pyramid   run code  s suppose return keyword  rakenltk import rake r  rake   rextractkeywordsfromtext  input  keyword  rgetrankedphraseswithscores   score  keyword keyword  len  keywordsplit      1   check keyword one word print  f   keyword    score    output  framework  20 tool  10 sql  10 spacy  10 scipy  10 sci  10 qualificationsexperience  10 pytorch  10 pyramid  10 panda  10 numpy  10 nosql  10 nltk  10 learn  10 kit  10 javascript  10 front  10 flask  10 familiarity  10 experience  10 ecosystem  10 django  10 dask  10 css  10 simply want explicit name tool  skill framework    numpy     scipy     html   etc use text every single word be find    experience    tool    way  provide list possible python framework relate skill filter output rake  latter one solution  find  make thorough list  help appreciate ,utilize skill knowledge token classification hugging face s library transformer import pipeline tokenskillclassifier  pipeline  model  jjzha  jobbertskillextraction   aggregationstrategy  first   tokenknowledgeclassifier  pipeline  model  jjzha  jobbertknowledgeextraction   aggregationstrategy  first   def aggregatespan  result   newresult    currentresult  result  0  result result  1    result    start     currentresult    end    1  currentresult    word          result    word   currentresult    end    result    end   else  newresultsappend  currentresult  currentresult  result newresultsappend  currentresult  return newresults def ner  text   outputskills  tokenskillclassifier  text  result outputskill  resultget    entitygroup    result    entity      skill  del result    entitygroup   outputknowledge  tokenknowledgeclassifier  text  result outputknowledge  resultget    entitygroup    result    entity      knowledge  del result    entitygroup   len  outputskills   0  outputskills  aggregatespan  outputskills  len  outputknowledge   0  outputknowledge  aggregatespan  outputknowledge  return    text   text    entity   outputskills      text   text    entity   outputknowledge ,extract technical keyword text use rake library python want use rake extract technical keyword job description  ve find linkedin  look like  input    in  depth understand python software development stack  ecosystem  framework tool numpy  scipy  pandas  dask  spacy  nltk  sci  kit  learn pytorch  experience front  end development use html  css  javascript  familiarity database technologie sql nosqlexcellent problem  solve ability solid communication collaboration skill  preferred skills qualificationsexperience popular python framework django  flask pyramid   run code  s suppose return keyword  rakenltk import rake r  rake   rextractkeywordsfromtext  input  keyword  rgetrankedphraseswithscores   score  keyword keyword  len  keywordsplit      1   check keyword one word print  f   keyword    score    output  framework  20 tool  10 sql  10 spacy  10 scipy  10 sci  10 qualificationsexperience  10 pytorch  10 pyramid  10 panda  10 numpy  10 nosql  10 nltk  10 learn  10 kit  10 javascript  10 front  10 flask  10 familiarity  10 experience  10 ecosystem  10 django  10 dask  10 css  10 simply want explicit name tool  skill framework    numpy     scipy     html   etc use text every single word be find    experience    tool    way  provide list possible python framework relate skill filter output rake  latter one solution  find  make thorough list  help appreciate  utilize skill knowledge token classification hugging face s library transformer import pipeline tokenskillclassifier  pipeline  model  jjzha  jobbertskillextraction   aggregationstrategy  first   tokenknowledgeclassifier  pipeline  model  jjzha  jobbertknowledgeextraction   aggregationstrategy  first   def aggregatespan  result   newresult    currentresult  result  0  result result  1    result    start     currentresult    end    1  currentresult    word          result    word   currentresult    end    result    end   else  newresultsappend  currentresult  currentresult  result newresultsappend  currentresult  return newresults def ner  text   outputskills  tokenskillclassifier  text  result outputskill  resultget    entitygroup    result    entity      skill  del result    entitygroup   outputknowledge  tokenknowledgeclassifier  text  result outputknowledge  resultget    entitygroup    result    entity      knowledge  del result    entitygroup   len  outputskills   0  outputskills  aggregatespan  outputskills  len  outputknowledge   0  outputknowledge  aggregatespan  outputknowledge  return    text   text    entity   outputskills      text   text    entity   outputknowledge ,Library/Tool-Based Queries
Langchain sql agent with context,"I am working on a langchain based SQL chat application and wanted my agent to understand context w.r.t the user session. For e.g. User - What is highest order placed in last placed? Bot - Order id : XYZ User - When was this placed? Here, bot should be able to deduce that 'this' refers to 'order id XYZ' from previous question. How can I incorporate this in my code? I am tried using ChatHistory but getting context from session history is where I am stuck.","['text', 'nlp', 'langchain']",1,"I had a similar issue and I solved by “Contextualizing” the questions. Here is an example (not for SQL) : https://python.langchain.com/docs/use_cases/question_answering/chat_history/ Read the part on Contextualizing the question to reformulate your question based on the history. In the context of a RAG system that uses a SQL agent, contextualizing questions involves modifying or framing these questions based on the specific data structure and contents of the SQL database. Here, the questions are crafted to match the structure of the database to efficiently retrieve relevant information. This means incorporating the correct table names, field names, and specific terms used within the database into the query. By doing so, the RAG system leverages the SQL agent to execute precise database queries that fetch data relevant to the ongoing conversation or task, thus enhancing the response accuracy and relevance. Hope this helps!",2024-04-27 07:49:59,2024-04-29 11:25:07,1083,https://stackoverflow.com/questions/78394225/langchain-sql-agent-with-context,"Langchain sql agent with context I am working on a langchain based SQL chat application and wanted my agent to understand context w.r.t the user session. For e.g. User - What is highest order placed in last placed? Bot - Order id : XYZ User - When was this placed? Here, bot should be able to deduce that 'this' refers to 'order id XYZ' from previous question. How can I incorporate this in my code? I am tried using ChatHistory but getting context from session history is where I am stuck.",langchain sql agent context work langchain base sql chat application want agent understand context wrt user session  eg  user  high order place last place  bot  order i d  xyz user  place   bot able deduce  this  refer  order i d xyz  previous question  incorporate code  try use chathistory get context session history stick ,similar issue solve  contextualizing  question  example  sql     pythonlangchaincom  docs  usecases  questionanswering  chathistory read part contextualizing question reformulate question base history  context rag system use sql agent  contextualizing question involve modify framing question base specific datum structure content sql database   question craft match structure database efficiently retrieve relevant information  mean incorporate correct table name  field name  specific term use within database query   rag system leverage sql agent execute precise database query fetch datum relevant ongoing conversation task  thus enhance response accuracy relevance  hope help ,langchain sql agent context work langchain base sql chat application want agent understand context wrt user session  eg  user  high order place last place  bot  order i d  xyz user  place   bot able deduce  this  refer  order i d xyz  previous question  incorporate code  try use chathistory get context session history stick  similar issue solve  contextualizing  question  example  sql     pythonlangchaincom  docs  usecases  questionanswering  chathistory read part contextualizing question reformulate question base history  context rag system use sql agent  contextualizing question involve modify framing question base specific datum structure content sql database   question craft match structure database efficiently retrieve relevant information  mean incorporate correct table name  field name  specific term use within database query   rag system leverage sql agent execute precise database query fetch datum relevant ongoing conversation task  thus enhance response accuracy relevance  hope help ,Implementation Issues
Performance of textSimilarity() from R&#39;s text library,"I have a large data.frame with about 4 million rows and 2 columns. The two columns contain long character strings, texts representing recipes. For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the text library in R. I like the textSimilarity() function, because it uses text embeddings that ""comprises values that represent the latent meaning of a word"". Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU? Example data - with way shorter texts: df <- data.frame( columnA= c(""tomato sauce is very tasty to use"", ""without garlic, this dish is not chinese"", ""British food is as tasteless as it can get""), columnB= c(""pizza is the source of life"", ""a nice xiaolongbao is steamed until it is soft"", ""braised pork can be very healthy if prepared well"") ) > df columnA columnB 1 tomato sauce is very tasty to use pizza is the source of life 2 without garlic, this dish is not chinese a nice xiaolongbao is steamed until it is soft 3 British food is as tasteless as it can get braised pork can be very healthy if prepared will To get the similarity, I use: df$sim <- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts) In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?","['r', 'performance', 'nlp', 'huggingface-transformers']",2,"Contextual embedding models are computationally expensive The default model used by the R text package is bert-base-uncased , which has 110m parameters, including a 12 layer feed-forward neural network. To compare similarity of sentences, each sentence is split into tokens (which are words or parts of words) and each token is represented as a 768-dimensional vector. Then the token vectors from each sentence are aggregated to create a single, 768-dimensional vector to represent that sentence in vector space. The idea is that with decent token (or word) embeddings and a sensible way of aggregating them this representation captures meaning, so similar sentences appear closer together in vector space. You can then calculate the distance between sentence vectors as a measure of their semantic similarity. The expensive step is creating the token vectors. Unlike older models such as Word2Vec, with BERT the vector representation of each token depends on the context. A canonical example of this is that the representation of the first word in Apple Inc. was founded by Steve Jobs will not be the same as the first word in Apple is my favourite fruit . This is a strength of this family of models. These sentences should not appear close together in vector space. But calculating the representation of each token based on the others around it, and working out which tokens affect the meaning of other tokens, is computationally expensive. Use a more lightweight context-dependent model A good way to speed up things a lot without sacrificing too much accuracy is by using a more lightweight model, e.g. distilbert which has 66m parameters and 6 rather than 12 feed-forward neural network layers. This is still a transformer model. You still get context-specific embeddings with an attention mechanism to appropriately weight each token based on the surrounding ones. With distilled models trained on larger models like BERT, the results tend to be fairly close to the model from which they're derived, and they're much faster: DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark. Benchmarking the default model We need a little more text to measure the performance. I'll use the Biomedical Semantic Similarity Estimation System (BIOSSES) dataset of 100 sentence pairs. # Read in the data biosses <- jsonlite::read_json(""https://datasets-server.huggingface.co/rows?dataset=biosses&config=default&split=train&offset=0&length=100"") df2 <- data.frame( sentence1 = sapply(biosses$rows, \(row) row$row$sentence1), sentence2 = sapply(biosses$rows, \(row) row$row$sentence2) ) # Define function to calculate similarity get_similarity <- function(model, dat = df2, col1 = ""sentence1"", col2 = ""sentence2"") { embeds_a <- text::textEmbed(dat[[col1]], model = model) embeds_b <- text::textEmbed(dat[[col2]], model = model) text::textSimilarity( embeds_a$texts$texts, embeds_b$texts$texts ) } No need to microbenchmark here as it takes so long (about 16 minutes): system.time( base_result <- get_similarity(model = ""bert-base-uncased"") ) # 956.136 seconds Speed of smaller BERT models Let's try three other BERT derivatives: DistilBERT . Bert tiny intel CPU optimized . DistilBERT distilled . models <- c( ""distilbert"" = ""distilbert-base-uncased"", ""tiny_distilbert"" = ""muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"", ""distilled_distilbert"" = ""distilbert/distilbert-base-uncased-distilled-squad"" ) l <- lapply(models, \(model) list( time = system.time(x <- get_similarity(model))[""elapsed""], result = x, diff_from_base = abs(base_result - x) ) ) To compare the results: sapply(l, \(x) x$time) # distilbert.elapsed tiny_distilbert.elapsed distilled_distilbert.elapsed # 622.071 61.956 569.000 So the tiny model is by far the fastest at just over a minute to run, with the other two much slower at around 9-10 mins. All are substantially faster than the base model. Accuracy of smaller BERT models Of course, speed is not everything. Let's compare the distribution of similarity scores to the bert-base-uncased scores: res <- cbind(sentence = seq(nrow(df2)), data.frame(l)) |> tidyr::pivot_longer( cols = !sentence, names_pattern = ""(.+)\\.(.+)"", names_to = c(""model"", ""var"") ) |> tidyr::pivot_wider( names_from = var ) library(ggplot2) ggplot(res) + geom_density(aes(x = diff_from_base, color = model, fill = model), alpha = 0.2) + theme(legend.position = ""bottom"") + labs(x = ""Absolute difference"", title = ""Distribution of differences from bert-base-uncased similarity score"") Interestingly, the fastest model also appears to have the closest results to the base model, at least on this task. You may want to do more robust checks with a subset of your actual data to find the optimal trade-off between speed and similarity to BERT (or whatever your gold standard of similarity is). Other approaches to speeding up this task Parallel processing You suggested using parallel processing. This should not speed things up, as under the hood, text::textEmbed() wraps the Python Hugging Face transformers library, which in turn calls the Python torch library, which already multi-threaded by default. When I run this code, CPU utilisation spikes for all cores. You might want to check the same happens on your machine but I suspect it will. GPU You suggested using a GPU. If you have one you should use it. This may speed things up significantly, though how much depends on the model. It looks like the way to do this is to call text::textEmbed(..., device = ""gpu"") . I would not feel completely confident this will work out of the box though. Getting torch to recognise a GPU generally requires specifying the Cuda version during installation. This cannot be done for you from R when you run text::textrpp_initialize() , as it will not know which hardware or drivers you have installed. You might want to enter the Python conda virtual environment created by the R text package and check whether torch.cuda.is_available() == True . If not, see this question for how to find out whether your GPU is compatible with Cuda and if so how to find the compatible torch version. A note on using R There is a cost to using R here. I replicated these results in Python and it was about twice as fast again. Each sentence is represented as a 768 dimensional vector. There's some work going on to take the Python list of Torch.tensor s representing each sentence, coerce to a numpy array, then pass this to R where ultimately it's stored as a row of a data frame with 768 columns. I suspect that as the size of the data increases, the time spent on translating the vectors from Python to R reduces as a proportion of the total time, but it would certainly be somewhat faster still to do all this in Python. Nevertheless, if muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu will do the job and there are benefits to keeping everything in R, hopefully a sixteen-fold speed up is enough to be able to do this.",2024-04-27 03:15:58,2024-04-30 08:30:55,246,https://stackoverflow.com/questions/78393709/performance-of-textsimilarity-from-rs-text-library,"Performance of textSimilarity() from R&#39;s text library I have a large data.frame with about 4 million rows and 2 columns. The two columns contain long character strings, texts representing recipes. For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the text library in R. I like the textSimilarity() function, because it uses text embeddings that ""comprises values that represent the latent meaning of a word"". Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU? Example data - with way shorter texts: df <- data.frame( columnA= c(""tomato sauce is very tasty to use"", ""without garlic, this dish is not chinese"", ""British food is as tasteless as it can get""), columnB= c(""pizza is the source of life"", ""a nice xiaolongbao is steamed until it is soft"", ""braised pork can be very healthy if prepared well"") ) > df columnA columnB 1 tomato sauce is very tasty to use pizza is the source of life 2 without garlic, this dish is not chinese a nice xiaolongbao is steamed until it is soft 3 British food is as tasteless as it can get braised pork can be very healthy if prepared will To get the similarity, I use: df$sim <- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts) In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?",performance textsimilarity   r   39  text library large dataframe 4 million row 2 column  two column contain long character string  text represent recipe  row  compare similarity recipe column column b  use textsimilarity   text library r like textsimilarity   function  use text embedding   comprise value represent latent meaning word   yet  performance slow  way speeding  code wrong  can  should set parallel  use gpu  example data  way short text  df   dataframe  columna c    tomato sauce tasty use     without garlic  dish chinese     british food tasteless get    columnb c    pizza source life     nice xiaolongbao steamed soft     braise pork healthy prepare well     df columna columnb 1 tomato sauce tasty use pizza source life 2 without garlic  dish chinese nice xiaolongbao steam soft 3 british food tasteless get braise pork healthy prepared get similarity  use  df  sim   textsimilarity  textembed  df  columna   text  text  textembed  df  columnb   text  text  current set  up  process take day rather hour  speed  parallelization  gpu  alternative ,contextual embed model computationally expensive default model use r text package bert  base  uncased  110 m parameter  include 12 layer feed  forward neural network  compare similarity sentence  sentence split token  word part word  token represent 768  dimensional vector  token vectors sentence aggregate create single  768  dimensional vector represent sentence vector space  idea decent token  word  embedding sensible way aggregate representation capture mean  similar sentence appear close together vector space  calculate distance sentence vector measure semantic similarity  expensive step create token vector  unlike old model word2vec  bert vector representation token depend context  canonical example representation first word apple inc found steve jobs first word apple favourite fruit  strength family model  sentence appear close together vector space  calculate representation token base other around  work token affect mean token  computationally expensive  use lightweight context  dependent model good way speed thing lot without sacrifice much accuracy use lightweight model  eg  distilbert 66 m parameter 6 rather 12 feed  forward neural network layer  still transformer model  still get context  specific embedding attention mechanism appropriately weight token base surround one  distilled model train large model like bert  result tend fairly close model be derive  be much fast  distilbert small  fast  cheap light transformer model train distilling bert base  40  less parameter google  bert  bert  base  uncased  run 60  fast preserve 95  bert  performance measure glue language understand benchmark  benchmarke default model need little text measure performance  will use biomedical semantic similarity estimation system  biosse  dataset 100 sentence pair   read data biosse   jsonlite   readjson      datasets  serverhuggingfaceco  row  dataset  biosse  config  default  split  train  offset0  length100   df2   dataframe  sentence1  sapply  biosse  row    row  row  row  sentence1   sentence2  sapply  biosse  row    row  row  row  sentence2    define function calculate similarity getsimilarity   function  model  dat  df2  col1    sentence1   col2    sentence2    embedsa   text   textembed  dat   col1    model  model  embedsb   text   textembed  dat   col2    model  model  text   textsimilarity  embedsa  text  text  embedsb  text  text   need microbenchmark take long  16 minute   systemtime  baseresult   getsimilarity  model    bert  base  uncased     956136 second speed small bert model let us try three bert derivative  distilbert  bert tiny intel cpu optimize  distilbert distil  model   c    distilbert     distilbert  base  uncased     tinydistilbert     muhtasham  bert  tiny  finetune  finer139  full  intel  cpu     distilleddistilbert     distilbert  distilbert  base  uncase  distil  squad   l   lapply  model    model  list  time  systemtime  x   getsimilarity  model      elapse    result  x  difffrombase  ab  baseresult  x    compare result  sapply  l    x  x  time   distilbertelapse tinydistilbertelapsed distilleddistilbertelapsed  622071 61956 569000 tiny model far fast minute run  two much slow around 9  10 min  substantially fast base model  accuracy small bert model course  speed everything  let us compare distribution similarity score bert  base  uncased score  re   cbind  sentence  seq  nrow  df2    dataframe  l     tidyr   pivotlonger  col   sentence  namespattern              namesto  c    model     var      tidyr   pivotwider  namesfrom  var  library  ggplot2  ggplot  re   geomdensity  aes  x  difffrombase  color  model  fill  model   alpha  02   theme  legendposition    bottom    lab  x    absolute difference   title    distribution difference bert  base  uncased similarity score   interestingly  fast model also appear close result base model  least task  may want robust check subset actual datum find optimal trade  off speed similarity bert  whatever gold standard similarity   approach speed task parallel processing suggest use parallel processing  speed thing  hood  text   textembed   wrap python hug face transformer library  turn call python torch library  already multi  thread default  run code  cpu utilisation spike core  might want check happen machine suspect  gpu suggest use gpu  one use  may speed thing significantly  though much depend model  look like way call text   textembed    device    gpu    would feel completely confident work box though  get torch recognise gpu generally require specify cuda version installation  do r run text   textrppinitialize    know hardware driver instal  might want enter python conda virtual environment create r text package check whether torchcudaisavailable     true   see question find whether gpu compatible cuda find compatible torch version  note use r cost use r  replicate result python twice fast  sentence represent 768 dimensional vector  s work go take python list torchtensor represent sentence  coerce numpy array  pass r ultimately be store row datum frame 768 column  suspect size data increase  time spend translate vector python r reduce proportion total time  would certainly somewhat fast still python  nevertheless  muhtasham  bert  tiny  finetune  finer139  full  intel  cpu job benefit keep everything r  hopefully sixteen  fold speed enough able ,performance textsimilarity   r   39  text library large dataframe 4 million row 2 column  two column contain long character string  text represent recipe  row  compare similarity recipe column column b  use textsimilarity   text library r like textsimilarity   function  use text embedding   comprise value represent latent meaning word   yet  performance slow  way speeding  code wrong  can  should set parallel  use gpu  example data  way short text  df   dataframe  columna c    tomato sauce tasty use     without garlic  dish chinese     british food tasteless get    columnb c    pizza source life     nice xiaolongbao steamed soft     braise pork healthy prepare well     df columna columnb 1 tomato sauce tasty use pizza source life 2 without garlic  dish chinese nice xiaolongbao steam soft 3 british food tasteless get braise pork healthy prepared get similarity  use  df  sim   textsimilarity  textembed  df  columna   text  text  textembed  df  columnb   text  text  current set  up  process take day rather hour  speed  parallelization  gpu  alternative  contextual embed model computationally expensive default model use r text package bert  base  uncased  110 m parameter  include 12 layer feed  forward neural network  compare similarity sentence  sentence split token  word part word  token represent 768  dimensional vector  token vectors sentence aggregate create single  768  dimensional vector represent sentence vector space  idea decent token  word  embedding sensible way aggregate representation capture mean  similar sentence appear close together vector space  calculate distance sentence vector measure semantic similarity  expensive step create token vector  unlike old model word2vec  bert vector representation token depend context  canonical example representation first word apple inc found steve jobs first word apple favourite fruit  strength family model  sentence appear close together vector space  calculate representation token base other around  work token affect mean token  computationally expensive  use lightweight context  dependent model good way speed thing lot without sacrifice much accuracy use lightweight model  eg  distilbert 66 m parameter 6 rather 12 feed  forward neural network layer  still transformer model  still get context  specific embedding attention mechanism appropriately weight token base surround one  distilled model train large model like bert  result tend fairly close model be derive  be much fast  distilbert small  fast  cheap light transformer model train distilling bert base  40  less parameter google  bert  bert  base  uncased  run 60  fast preserve 95  bert  performance measure glue language understand benchmark  benchmarke default model need little text measure performance  will use biomedical semantic similarity estimation system  biosse  dataset 100 sentence pair   read data biosse   jsonlite   readjson      datasets  serverhuggingfaceco  row  dataset  biosse  config  default  split  train  offset0  length100   df2   dataframe  sentence1  sapply  biosse  row    row  row  row  sentence1   sentence2  sapply  biosse  row    row  row  row  sentence2    define function calculate similarity getsimilarity   function  model  dat  df2  col1    sentence1   col2    sentence2    embedsa   text   textembed  dat   col1    model  model  embedsb   text   textembed  dat   col2    model  model  text   textsimilarity  embedsa  text  text  embedsb  text  text   need microbenchmark take long  16 minute   systemtime  baseresult   getsimilarity  model    bert  base  uncased     956136 second speed small bert model let us try three bert derivative  distilbert  bert tiny intel cpu optimize  distilbert distil  model   c    distilbert     distilbert  base  uncased     tinydistilbert     muhtasham  bert  tiny  finetune  finer139  full  intel  cpu     distilleddistilbert     distilbert  distilbert  base  uncase  distil  squad   l   lapply  model    model  list  time  systemtime  x   getsimilarity  model      elapse    result  x  difffrombase  ab  baseresult  x    compare result  sapply  l    x  x  time   distilbertelapse tinydistilbertelapsed distilleddistilbertelapsed  622071 61956 569000 tiny model far fast minute run  two much slow around 9  10 min  substantially fast base model  accuracy small bert model course  speed everything  let us compare distribution similarity score bert  base  uncased score  re   cbind  sentence  seq  nrow  df2    dataframe  l     tidyr   pivotlonger  col   sentence  namespattern              namesto  c    model     var      tidyr   pivotwider  namesfrom  var  library  ggplot2  ggplot  re   geomdensity  aes  x  difffrombase  color  model  fill  model   alpha  02   theme  legendposition    bottom    lab  x    absolute difference   title    distribution difference bert  base  uncased similarity score   interestingly  fast model also appear close result base model  least task  may want robust check subset actual datum find optimal trade  off speed similarity bert  whatever gold standard similarity   approach speed task parallel processing suggest use parallel processing  speed thing  hood  text   textembed   wrap python hug face transformer library  turn call python torch library  already multi  thread default  run code  cpu utilisation spike core  might want check happen machine suspect  gpu suggest use gpu  one use  may speed thing significantly  though much depend model  look like way call text   textembed    device    gpu    would feel completely confident work box though  get torch recognise gpu generally require specify cuda version installation  do r run text   textrppinitialize    know hardware driver instal  might want enter python conda virtual environment create r text package check whether torchcudaisavailable     true   see question find whether gpu compatible cuda find compatible torch version  note use r cost use r  replicate result python twice fast  sentence represent 768 dimensional vector  s work go take python list torchtensor represent sentence  coerce numpy array  pass r ultimately be store row datum frame 768 column  suspect size data increase  time spend translate vector python r reduce proportion total time  would certainly somewhat fast still python  nevertheless  muhtasham  bert  tiny  finetune  finer139  full  intel  cpu job benefit keep everything r  hopefully sixteen  fold speed enough able ,Library/Tool-Based Queries
How to Generate Text with Specific Length Using AI API,"I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far: Setting Max Tokens : I've used the max_tokens parameter hoping to limit the output length, but then text is truncated. Explicit Prompt Requests : I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800). I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length? I need this because I inject the text in slides. Any alternative solution?","['python', 'nlp', 'openai-api', 'text-generation']",1,This won't work. LLMs are bad at counting. There are a lot of humorous examples over internet.,2024-04-25 09:30:56,2024-04-25 09:32:51,236,https://stackoverflow.com/questions/78383619/how-to-generate-text-with-specific-length-using-ai-api,"How to Generate Text with Specific Length Using AI API I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far: Setting Max Tokens : I've used the max_tokens parameter hoping to limit the output length, but then text is truncated. Explicit Prompt Requests : I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800). I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length? I need this because I inject the text in slides. Any alternative solution?",generate text specific length use ai api attempt generate text output exactly certain number character word long use ai api  openai gpt  claude  gemini     m face difficulty  s  ve try far  setting max tokens   ve use maxtokens parameter hope limit output length  text truncate  explicit prompt request   ve try include explicit instruction prompt desire length  however  approach produce precise output length need  ex  give 600 word instead 800   look suggestion configure api call promtp achieve exact output length  way well utilize openai  claude  parameter  method post  process text fit require length  need inject text slide  alternative solution ,will not work  llms bad counting  lot humorous example internet ,generate text specific length use ai api attempt generate text output exactly certain number character word long use ai api  openai gpt  claude  gemini     m face difficulty  s  ve try far  setting max tokens   ve use maxtokens parameter hope limit output length  text truncate  explicit prompt request   ve try include explicit instruction prompt desire length  however  approach produce precise output length need  ex  give 600 word instead 800   look suggestion configure api call promtp achieve exact output length  way well utilize openai  claude  parameter  method post  process text fit require length  need inject text slide  alternative solution  will not work  llms bad counting  lot humorous example internet ,Implementation Issues
ValueError: Cannot use a compiled regex as replacement pattern with regex=False,"I'm doing a project, on Google Colab, where I use the following version: !pip install ""gensim==4.2.0"" !pip install ""texthero==1.0.5"" Until recently, I received the following warning: FutureWarning: The default value of regex will change from True to False in a future version. return input.str.replace(r""^\d+\s|\s\d+\s|\s\d+$"", "" "") But the execution worked normally. Now, I'm getting the following error: How should I proceed? I tried different versions, but the problem persists.","['python', 'text', 'nlp']",1,"This is a texthero bug triggering a pandas error. Pandas str.replace now uses regex=False by default: Texthero's replace_digits function hasn't been updated in two years and doesn't explicitly pass regex=True : if only_blocks: pattern = r""\b\d+\b"" return s.str.replace(pattern, symbols) else: return s.str.replace(r""\d+"", symbols) You should fill a bug report to texthero, there are probably several other occurrences of str.replace to fix. In there meantime you can patch the library by changing the code to: if only_blocks: pattern = r""\b\d+\b"" return s.str.replace(pattern, symbols, regex=True) else: return s.str.replace(r""\d+"", symbols, regex=True) Or use a pandas version prior to 2 (e.g. 1.5.2 )",2024-04-24 01:52:07,2024-04-24 02:41:14,333,https://stackoverflow.com/questions/78375631/valueerror-cannot-use-a-compiled-regex-as-replacement-pattern-with-regex-false,"ValueError: Cannot use a compiled regex as replacement pattern with regex=False I'm doing a project, on Google Colab, where I use the following version: !pip install ""gensim==4.2.0"" !pip install ""texthero==1.0.5"" Until recently, I received the following warning: FutureWarning: The default value of regex will change from True to False in a future version. return input.str.replace(r""^\d+\s|\s\d+\s|\s\d+$"", "" "") But the execution worked normally. Now, I'm getting the following error: How should I proceed? I tried different versions, but the problem persists.",valueerror  use compile regex replacement pattern regex  false  m project  google colab  use follow version   pip install   gensim420   pip install   texthero105  recently  receive follow warning  futurewarning  default value regex change true false future version  return inputstrreplace  r  dssdssd         execution work normally    m get follow error  proceed  try different version  problem persist ,texthero bug trigger panda error  panda strreplace use regex  false default  texthero s replacedigits function not update two year not explicitly pass regex  true  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  else  return sstrreplace  r  d   symbol  fill bug report texthero  probably several occurrence strreplace fix  meantime patch library change code  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  regex  true  else  return sstrreplace  r  d   symbol  regex  true  use panda version prior 2  eg  152 ,valueerror  use compile regex replacement pattern regex  false  m project  google colab  use follow version   pip install   gensim420   pip install   texthero105  recently  receive follow warning  futurewarning  default value regex change true false future version  return inputstrreplace  r  dssdssd         execution work normally    m get follow error  proceed  try different version  problem persist  texthero bug trigger panda error  panda strreplace use regex  false default  texthero s replacedigits function not update two year not explicitly pass regex  true  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  else  return sstrreplace  r  d   symbol  fill bug report texthero  probably several occurrence strreplace fix  meantime patch library change code  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  regex  true  else  return sstrreplace  r  d   symbol  regex  true  use panda version prior 2  eg  152 ,Library/Tool-Based Queries
No Attention returned even when output_attentions= True,"I'm using a pretrained model based BERT (github link: DNABERT-2 ) It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base. I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem. output is of length 2 and each element is of shape: torch.Size([1, 7, 768]) and torch.Size([1, 768]) . When trying to get output.attentions I get None . I'm not sure where to search and what a solution would be. I'm providing my whole code: Defining model, trainer, data, tokenizer: from copy import deepcopy from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback # END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset @dataclass class ModelArguments: model_name_or_path: Optional[str] = field(default=""facebook/opt-125m"") use_lora: bool = field(default=False, metadata={""help"": ""whether to use LoRA""}) lora_r: int = field(default=8, metadata={""help"": ""hidden dimension for LoRA""}) lora_alpha: int = field(default=32, metadata={""help"": ""alpha for LoRA""}) lora_dropout: float = field(default=0.05, metadata={""help"": ""dropout rate for LoRA""}) lora_target_modules: str = field(default=""query,value"", metadata={""help"": ""where to perform LoRA""}) @dataclass class DataArguments: data_path: str = field(default=None, metadata={""help"": ""Path to the training data.""}) kmer: int = field(default=-1, metadata={""help"": ""k-mer for input sequence. -1 means not using k-mer.""}) @dataclass class TrainingArguments(transformers.TrainingArguments): cache_dir: Optional[str] = field(default=None) run_name: str = field(default=""run"") optim: str = field(default=""adamw_torch"") model_max_length: int = field(default=512, metadata={""help"": ""Maximum sequence length.""}) gradient_accumulation_steps: int = field(default=1) per_device_train_batch_size: int = field(default=1) per_device_eval_batch_size: int = field(default=1) num_train_epochs: int = field(default=1) logging_steps: int = field(default=100) save_steps: int = field(default=100) fp16: bool = field(default=False) # START NEW # eval_steps: int = field(default=100) eval_steps: int = field(default=0.1) # END NEW evaluation_strategy: str = field(default=""steps"") warmup_steps: int = field(default=50) weight_decay: float = field(default=0.01) learning_rate: float = field(default=1e-4) save_total_limit: int = field(default=3) load_best_model_at_end: bool = field(default=True) output_dir: str = field(default=""output"") find_unused_parameters: bool = field(default=False) checkpointing: bool = field(default=False) dataloader_pin_memory: bool = field(default=False) eval_and_save_results: bool = field(default=True) save_model: bool = field(default=False) seed: int = field(default=42) def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str): """"""Collects the state dict and dump to disk."""""" state_dict = trainer.model.state_dict() if trainer.args.should_save: cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()} del state_dict trainer._save(output_dir, state_dict=cpu_state_dict) # noqa """""" Get the reversed complement of the original DNA sequence. """""" def get_alter_of_dna_sequence(sequence: str): MAP = {""A"": ""T"", ""T"": ""A"", ""C"": ""G"", ""G"": ""C""} # return """".join([MAP[c] for c in reversed(sequence)]) return """".join([MAP[c] for c in sequence]) """""" Transform a dna sequence to k-mer string """""" def generate_kmer_str(sequence: str, k: int) -> str: """"""Generate k-mer string from DNA sequence."""""" return "" "".join([sequence[i:i + k] for i in range(len(sequence) - k + 1)]) """""" Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of ""_{k}mer"". """""" def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]: """"""Load or generate k-mer string for each DNA sequence."""""" kmer_path = data_path.tokenizerreplace("".csv"", f""_{k}mer.json"") if os.path.exists(kmer_path): logging.warning(f""Loading k-mer from {kmer_path}..."") with open(kmer_path, ""r"") as f: kmer = json.load(f) else: logging.warning(f""Generating k-mer..."") kmer = [generate_kmer_str(text, k) for text in texts] with open(kmer_path, ""w"") as f: logging.warning(f""Saving k-mer to {kmer_path}..."") json.dump(kmer, f) return kmer class SupervisedDataset(Dataset): """"""Dataset for supervised fine-tuning."""""" def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, kmer: int = -1): super(SupervisedDataset, self).__init__() # load data from the disk with open(data_path, ""r"") as f: data = list(csv.reader(f))[1:] if len(data[0]) == 2: # data is in the format of [text, label] logging.warning(""Perform single sequence classification..."") texts = [d[0] for d in data] labels = [int(d[1]) for d in data] # All genes sequences are concat: we don't work with the sequence-pair, # But we are tricking the model to think it is single sequence. elif len(data[0]) == 3: # data is in the format of [text1, text2, label] logging.warning(""Perform sequence-pair classification..."") texts = [[d[0], d[1]] for d in data] labels = [int(d[2]) for d in data] else: raise ValueError(""Data format not supported."") if kmer != -1: # only write file on the first process if torch.distributed.get_rank() not in [0, -1]: torch.distributed.barrier() logging.warning(f""Using {kmer}-mer as input..."") texts = load_or_generate_kmer(data_path, texts, kmer) if torch.distributed.get_rank() == 0: torch.distributed.barrier() output = tokenizer( texts, return_tensors=""pt"", padding=""longest"", max_length=tokenizer.model_max_length, truncation=True, ) self.input_ids = output[""input_ids""] # CHANGE self.input_ids[0][self.input_ids[0] == 0] = 2 # Change to which tokens we want to attend and to which we don't self.attention_mask = output[""attention_mask""] self.labels = labels self.num_labels = len(set(labels)) def __len__(self): return len(self.input_ids) def __getitem__(self, i) -> Dict[str, torch.Tensor]: return dict(input_ids=self.input_ids[i], labels=self.labels[i]) @dataclass class DataCollatorForSupervisedDataset(object): """"""Collate examples for supervised fine-tuning."""""" tokenizer: transformers.PreTrainedTokenizer def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]: input_ids, labels = tuple([instance[key] for instance in instances] for key in (""input_ids"", ""labels"")) input_ids = torch.nn.utils.rnn.pad_sequence( input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id ) labels = torch.Tensor(labels).long() return dict( input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ) """""" Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. """""" def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray): if logits.ndim == 3: # Reshape logits to 2D if needed logits = logits.reshape(-1, logits.shape[-1]) predictions = np.argmax(logits, axis=-1) valid_mask = labels != -100 # Exclude padding tokens (assuming -100 is the padding token ID) valid_predictions = predictions[valid_mask] valid_labels = labels[valid_mask] return { # START NEW ""sum prediction"": f'{sum(valid_predictions)}/{len(valid_predictions)}', # END NEW ""accuracy"": sklearn.metrics.accuracy_score(valid_labels, valid_predictions), ""f1"": sklearn.metrics.f1_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""matthews_correlation"": sklearn.metrics.matthews_corrcoef( valid_labels, valid_predictions ), ""precision"": sklearn.metrics.precision_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""recall"": sklearn.metrics.recall_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), } """""" Compute metrics used for huggingface trainer. """""" def compute_metrics(eval_pred): logits, labels = eval_pred if isinstance(logits, tuple): # Unpack logits if it's a tuple logits = logits[0] return calculate_metric_with_sklearn(logits, labels) class CustomTrainer(transformers.Trainer): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.epoch_predictions = [] self.epoch_labels = [] self.epoch_loss = [] def compute_loss(self, model, inputs, return_outputs=False): """""" MAX: Subclassed to compute training accuracy. How the loss is computed by Trainer. By default, all models return the loss in the first element. Subclass and override for custom behavior. """""" if self.label_smoother is not None and ""labels"" in inputs: labels = inputs.pop(""labels"") else: labels = None outputs = model(**inputs, output_attentions=True) # TEST try: print(f""Attention: {outputs.attentions}"") except Exception: print(""No Attention returned"") if ""labels"" in inputs: preds = outputs.logits.detach() # Log accuracy acc = ( (preds.argmax(axis=1) == inputs[""labels""]) .type(torch.float) .mean() .item() ) # Uncomment it if you want to plot the batch accuracy # wandb.log({""batch_accuracy"": acc}) # Log accuracy # Store predictions and labels for epoch-level metrics self.epoch_predictions.append(preds.cpu().numpy()) self.epoch_labels.append(inputs[""labels""].cpu().numpy()) # Save past state if it exists if self.args.past_index >= 0: self._past = outputs[self.args.past_index] if labels is not None: loss = self.label_smoother(outputs, labels) else: loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0] # Uncomment it if you want to plot the batch loss # wandb.log({""batch_loss"": loss}) self.epoch_loss.append(loss.item()) # Store loss for epoch-level metrics return (loss, outputs) if return_outputs else loss # Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback): def __init__(self, trainer) -> None: super().__init__() self._trainer = trainer def on_epoch_end(self, args, state, control, **kwargs): # Aggregate predictions and labels for the entire epoch epoch_predictions = np.concatenate(self._trainer.epoch_predictions) epoch_labels = np.concatenate(self._trainer.epoch_labels) # Compute accuracy accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels) # Compute mean loss mean_loss = np.mean(self._trainer.epoch_loss) # Compute precision, recall, and F1-score precision, recall, f1, _ = precision_recall_fscore_support( epoch_labels, epoch_predictions.argmax(axis=1), average=""weighted"" ) # Log epoch-level metrics wandb.log({""epoch_accuracy"": accuracy, ""epoch_loss"": mean_loss}) wandb.log({""precision"": precision, ""recall"": recall, ""f1"": f1}) # Clear stored predictions, labels, and loss for the next epoch self._trainer.epoch_predictions = [] self._trainer.epoch_labels = [] self._trainer.epoch_loss = [] return None # TODO: use this function to gather the prediction and labels and get the metrics #%% Instantiating and training: from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \ TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \ compute_metrics from copy import deepcopy from transformers import TrainerCallback # END NEW import os import json import torch import transformers from peft import ( LoraConfig, get_peft_model, get_peft_model_state_dict, ) import wandb run = wandb.init() assert run is wandb.run def train(device): parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments)) model_args, data_args, training_args = parser.parse_args_into_dataclasses() # load tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=""right"", use_fast=True, trust_remote_code=True, ) if ""InstaDeepAI"" in model_args.model_name_or_path: tokenizer.eos_token = tokenizer.pad_token # define datasets and data collator train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""train.csv""), kmer=data_args.kmer) val_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""dev.csv""), kmer=data_args.kmer) test_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""test.csv""), kmer=data_args.kmer) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) # load model model = transformers.AutoModelForSequenceClassification.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, num_labels=train_dataset.num_labels, trust_remote_code=True, output_attentions = True ).to(device) # configure LoRA if model_args.use_lora: lora_config = LoraConfig( r=model_args.lora_r, lora_alpha=model_args.lora_alpha, target_modules=list(model_args.lora_target_modules.split("","")), lora_dropout=model_args.lora_dropout, bias=""none"", task_type=""SEQ_CLS"", inference_mode=False, ) model = get_peft_model(model, lora_config) model.print_trainable_parameters() trainer = CustomTrainer(model=model, tokenizer=tokenizer, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator ) trainer.add_callback(CustomCallback(trainer)) trainer.train() # train_result = trainer.train() # loss = train_result[""loss""] # print(f""loss issss: {loss}"") # print(f""Train reusults: {train_result}"") # NEW: result: only returns metrics at the end of training if training_args.save_model: trainer.save_state() safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir) # get the evaluation results from trainer if training_args.eval_and_save_results: results_path = os.path.join(training_args.output_dir, ""results"", training_args.run_name) results = trainer.evaluate(eval_dataset=test_dataset) os.makedirs(results_path, exist_ok=True) with open(os.path.join(results_path, ""eval_results.json""), ""w"") as f: json.dump(results, f) if __name__ == ""__main__"": # Define device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device) # Call the train function with the device train(device) After training, I try to run it on an example: model_path = './finetune/output/dnabert2' tokenizer = AutoTokenizer.from_pretrained(model_path) # Load the model with output_attention=True model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True) model_input = tokenizer(""ACTGACGGGTAGTGACTG"", return_tensors=""pt"") with torch.inference_mode(): output = model(**model_input, output_attentions=True) My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.","['nlp', 'huggingface-transformers', 'bert-language-model', 'transformer-model', 'attention-model']",2,"The problem was deep in the structure: attention was discarded early in the model, I had therefore to go through the code to understand what is happening, and change it. https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted I had to extract attention_probs. Here are the changes I have done.",2024-04-23 10:48:50,2024-05-16 20:48:46,422,https://stackoverflow.com/questions/78371741/no-attention-returned-even-when-output-attentions-true,"No Attention returned even when output_attentions= True I'm using a pretrained model based BERT (github link: DNABERT-2 ) It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base. I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem. output is of length 2 and each element is of shape: torch.Size([1, 7, 768]) and torch.Size([1, 768]) . When trying to get output.attentions I get None . I'm not sure where to search and what a solution would be. I'm providing my whole code: Defining model, trainer, data, tokenizer: from copy import deepcopy from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback # END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset @dataclass class ModelArguments: model_name_or_path: Optional[str] = field(default=""facebook/opt-125m"") use_lora: bool = field(default=False, metadata={""help"": ""whether to use LoRA""}) lora_r: int = field(default=8, metadata={""help"": ""hidden dimension for LoRA""}) lora_alpha: int = field(default=32, metadata={""help"": ""alpha for LoRA""}) lora_dropout: float = field(default=0.05, metadata={""help"": ""dropout rate for LoRA""}) lora_target_modules: str = field(default=""query,value"", metadata={""help"": ""where to perform LoRA""}) @dataclass class DataArguments: data_path: str = field(default=None, metadata={""help"": ""Path to the training data.""}) kmer: int = field(default=-1, metadata={""help"": ""k-mer for input sequence. -1 means not using k-mer.""}) @dataclass class TrainingArguments(transformers.TrainingArguments): cache_dir: Optional[str] = field(default=None) run_name: str = field(default=""run"") optim: str = field(default=""adamw_torch"") model_max_length: int = field(default=512, metadata={""help"": ""Maximum sequence length.""}) gradient_accumulation_steps: int = field(default=1) per_device_train_batch_size: int = field(default=1) per_device_eval_batch_size: int = field(default=1) num_train_epochs: int = field(default=1) logging_steps: int = field(default=100) save_steps: int = field(default=100) fp16: bool = field(default=False) # START NEW # eval_steps: int = field(default=100) eval_steps: int = field(default=0.1) # END NEW evaluation_strategy: str = field(default=""steps"") warmup_steps: int = field(default=50) weight_decay: float = field(default=0.01) learning_rate: float = field(default=1e-4) save_total_limit: int = field(default=3) load_best_model_at_end: bool = field(default=True) output_dir: str = field(default=""output"") find_unused_parameters: bool = field(default=False) checkpointing: bool = field(default=False) dataloader_pin_memory: bool = field(default=False) eval_and_save_results: bool = field(default=True) save_model: bool = field(default=False) seed: int = field(default=42) def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str): """"""Collects the state dict and dump to disk."""""" state_dict = trainer.model.state_dict() if trainer.args.should_save: cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()} del state_dict trainer._save(output_dir, state_dict=cpu_state_dict) # noqa """""" Get the reversed complement of the original DNA sequence. """""" def get_alter_of_dna_sequence(sequence: str): MAP = {""A"": ""T"", ""T"": ""A"", ""C"": ""G"", ""G"": ""C""} # return """".join([MAP[c] for c in reversed(sequence)]) return """".join([MAP[c] for c in sequence]) """""" Transform a dna sequence to k-mer string """""" def generate_kmer_str(sequence: str, k: int) -> str: """"""Generate k-mer string from DNA sequence."""""" return "" "".join([sequence[i:i + k] for i in range(len(sequence) - k + 1)]) """""" Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of ""_{k}mer"". """""" def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]: """"""Load or generate k-mer string for each DNA sequence."""""" kmer_path = data_path.tokenizerreplace("".csv"", f""_{k}mer.json"") if os.path.exists(kmer_path): logging.warning(f""Loading k-mer from {kmer_path}..."") with open(kmer_path, ""r"") as f: kmer = json.load(f) else: logging.warning(f""Generating k-mer..."") kmer = [generate_kmer_str(text, k) for text in texts] with open(kmer_path, ""w"") as f: logging.warning(f""Saving k-mer to {kmer_path}..."") json.dump(kmer, f) return kmer class SupervisedDataset(Dataset): """"""Dataset for supervised fine-tuning."""""" def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, kmer: int = -1): super(SupervisedDataset, self).__init__() # load data from the disk with open(data_path, ""r"") as f: data = list(csv.reader(f))[1:] if len(data[0]) == 2: # data is in the format of [text, label] logging.warning(""Perform single sequence classification..."") texts = [d[0] for d in data] labels = [int(d[1]) for d in data] # All genes sequences are concat: we don't work with the sequence-pair, # But we are tricking the model to think it is single sequence. elif len(data[0]) == 3: # data is in the format of [text1, text2, label] logging.warning(""Perform sequence-pair classification..."") texts = [[d[0], d[1]] for d in data] labels = [int(d[2]) for d in data] else: raise ValueError(""Data format not supported."") if kmer != -1: # only write file on the first process if torch.distributed.get_rank() not in [0, -1]: torch.distributed.barrier() logging.warning(f""Using {kmer}-mer as input..."") texts = load_or_generate_kmer(data_path, texts, kmer) if torch.distributed.get_rank() == 0: torch.distributed.barrier() output = tokenizer( texts, return_tensors=""pt"", padding=""longest"", max_length=tokenizer.model_max_length, truncation=True, ) self.input_ids = output[""input_ids""] # CHANGE self.input_ids[0][self.input_ids[0] == 0] = 2 # Change to which tokens we want to attend and to which we don't self.attention_mask = output[""attention_mask""] self.labels = labels self.num_labels = len(set(labels)) def __len__(self): return len(self.input_ids) def __getitem__(self, i) -> Dict[str, torch.Tensor]: return dict(input_ids=self.input_ids[i], labels=self.labels[i]) @dataclass class DataCollatorForSupervisedDataset(object): """"""Collate examples for supervised fine-tuning."""""" tokenizer: transformers.PreTrainedTokenizer def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]: input_ids, labels = tuple([instance[key] for instance in instances] for key in (""input_ids"", ""labels"")) input_ids = torch.nn.utils.rnn.pad_sequence( input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id ) labels = torch.Tensor(labels).long() return dict( input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ) """""" Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. """""" def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray): if logits.ndim == 3: # Reshape logits to 2D if needed logits = logits.reshape(-1, logits.shape[-1]) predictions = np.argmax(logits, axis=-1) valid_mask = labels != -100 # Exclude padding tokens (assuming -100 is the padding token ID) valid_predictions = predictions[valid_mask] valid_labels = labels[valid_mask] return { # START NEW ""sum prediction"": f'{sum(valid_predictions)}/{len(valid_predictions)}', # END NEW ""accuracy"": sklearn.metrics.accuracy_score(valid_labels, valid_predictions), ""f1"": sklearn.metrics.f1_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""matthews_correlation"": sklearn.metrics.matthews_corrcoef( valid_labels, valid_predictions ), ""precision"": sklearn.metrics.precision_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""recall"": sklearn.metrics.recall_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), } """""" Compute metrics used for huggingface trainer. """""" def compute_metrics(eval_pred): logits, labels = eval_pred if isinstance(logits, tuple): # Unpack logits if it's a tuple logits = logits[0] return calculate_metric_with_sklearn(logits, labels) class CustomTrainer(transformers.Trainer): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.epoch_predictions = [] self.epoch_labels = [] self.epoch_loss = [] def compute_loss(self, model, inputs, return_outputs=False): """""" MAX: Subclassed to compute training accuracy. How the loss is computed by Trainer. By default, all models return the loss in the first element. Subclass and override for custom behavior. """""" if self.label_smoother is not None and ""labels"" in inputs: labels = inputs.pop(""labels"") else: labels = None outputs = model(**inputs, output_attentions=True) # TEST try: print(f""Attention: {outputs.attentions}"") except Exception: print(""No Attention returned"") if ""labels"" in inputs: preds = outputs.logits.detach() # Log accuracy acc = ( (preds.argmax(axis=1) == inputs[""labels""]) .type(torch.float) .mean() .item() ) # Uncomment it if you want to plot the batch accuracy # wandb.log({""batch_accuracy"": acc}) # Log accuracy # Store predictions and labels for epoch-level metrics self.epoch_predictions.append(preds.cpu().numpy()) self.epoch_labels.append(inputs[""labels""].cpu().numpy()) # Save past state if it exists if self.args.past_index >= 0: self._past = outputs[self.args.past_index] if labels is not None: loss = self.label_smoother(outputs, labels) else: loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0] # Uncomment it if you want to plot the batch loss # wandb.log({""batch_loss"": loss}) self.epoch_loss.append(loss.item()) # Store loss for epoch-level metrics return (loss, outputs) if return_outputs else loss # Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback): def __init__(self, trainer) -> None: super().__init__() self._trainer = trainer def on_epoch_end(self, args, state, control, **kwargs): # Aggregate predictions and labels for the entire epoch epoch_predictions = np.concatenate(self._trainer.epoch_predictions) epoch_labels = np.concatenate(self._trainer.epoch_labels) # Compute accuracy accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels) # Compute mean loss mean_loss = np.mean(self._trainer.epoch_loss) # Compute precision, recall, and F1-score precision, recall, f1, _ = precision_recall_fscore_support( epoch_labels, epoch_predictions.argmax(axis=1), average=""weighted"" ) # Log epoch-level metrics wandb.log({""epoch_accuracy"": accuracy, ""epoch_loss"": mean_loss}) wandb.log({""precision"": precision, ""recall"": recall, ""f1"": f1}) # Clear stored predictions, labels, and loss for the next epoch self._trainer.epoch_predictions = [] self._trainer.epoch_labels = [] self._trainer.epoch_loss = [] return None # TODO: use this function to gather the prediction and labels and get the metrics #%% Instantiating and training: from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \ TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \ compute_metrics from copy import deepcopy from transformers import TrainerCallback # END NEW import os import json import torch import transformers from peft import ( LoraConfig, get_peft_model, get_peft_model_state_dict, ) import wandb run = wandb.init() assert run is wandb.run def train(device): parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments)) model_args, data_args, training_args = parser.parse_args_into_dataclasses() # load tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=""right"", use_fast=True, trust_remote_code=True, ) if ""InstaDeepAI"" in model_args.model_name_or_path: tokenizer.eos_token = tokenizer.pad_token # define datasets and data collator train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""train.csv""), kmer=data_args.kmer) val_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""dev.csv""), kmer=data_args.kmer) test_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""test.csv""), kmer=data_args.kmer) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) # load model model = transformers.AutoModelForSequenceClassification.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, num_labels=train_dataset.num_labels, trust_remote_code=True, output_attentions = True ).to(device) # configure LoRA if model_args.use_lora: lora_config = LoraConfig( r=model_args.lora_r, lora_alpha=model_args.lora_alpha, target_modules=list(model_args.lora_target_modules.split("","")), lora_dropout=model_args.lora_dropout, bias=""none"", task_type=""SEQ_CLS"", inference_mode=False, ) model = get_peft_model(model, lora_config) model.print_trainable_parameters() trainer = CustomTrainer(model=model, tokenizer=tokenizer, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator ) trainer.add_callback(CustomCallback(trainer)) trainer.train() # train_result = trainer.train() # loss = train_result[""loss""] # print(f""loss issss: {loss}"") # print(f""Train reusults: {train_result}"") # NEW: result: only returns metrics at the end of training if training_args.save_model: trainer.save_state() safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir) # get the evaluation results from trainer if training_args.eval_and_save_results: results_path = os.path.join(training_args.output_dir, ""results"", training_args.run_name) results = trainer.evaluate(eval_dataset=test_dataset) os.makedirs(results_path, exist_ok=True) with open(os.path.join(results_path, ""eval_results.json""), ""w"") as f: json.dump(results, f) if __name__ == ""__main__"": # Define device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device) # Call the train function with the device train(device) After training, I try to run it on an example: model_path = './finetune/output/dnabert2' tokenizer = AutoTokenizer.from_pretrained(model_path) # Load the model with output_attention=True model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True) model_input = tokenizer(""ACTGACGGGTAGTGACTG"", return_tensors=""pt"") with torch.inference_mode(): output = model(**model_input, output_attentions=True) My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.",attention return even outputattentions true  m use pretraine model base bert  github link  dnabert2  use automodelforsequenceclassification mosaicml  mosaic  bert  base   m problem extract attention  read many post show way deal activate outputattention  true model  none post solve problem  output length 2 element shape  torch  size   1  7  768   torch  size   1  768    try get outputattention get none   m sure search solution would   m provide whole code  defining model  trainer  datum  tokenizer  copy import deepcopy sklearnmetrics import precisionrecallfscoresupport import wandb transformer import trainercallback  end new import os import csv import json import logging dataclasse import dataclass  field type import optional  dict  sequence  tuple  list import torch import transformer import sklearn import numpy np torchutilsdata import dataset  dataclass class modelarguments  modelnameorpath  optional  str   field  default  facebook  opt125 m   uselora  bool  field  default  false  metadata    help     whether use lora    lorar  int  field  default8  metadata    help     hidden dimension lora    loraalpha  int  field  default32  metadata    help     alpha lora    loradropout  float  field  default005  metadata    help     dropout rate lora    loratargetmodule  str  field  default  query  value   metadata    help     perform lora     dataclass class dataarguments  datapath  str  field  default  none  metadata    help     path training datum     kmer  int  field  default1  metadata    help     k  mer input sequence  1 mean use k  mer      dataclass class trainingarguments  transformer  trainingarguments   cachedir  optional  str   field  default  none  runname  str  field  default  run   optim  str  field  default  adamwtorch   modelmaxlength  int  field  default512  metadata    help     maximum sequence length     gradientaccumulationsteps  int  field  default1  perdevicetrainbatchsize  int  field  default1  perdeviceevalbatchsize  int  field  default1  numtrainepochs  int  field  default1  loggingstep  int  field  default100  savesteps  int  field  default100  fp16  bool  field  default  false   start new  evalstep  int  field  default100  evalstep  int  field  default01   end new evaluationstrategy  str  field  default  step   warmupsteps  int  field  default50  weightdecay  float  field  default001  learningrate  float  field  default1e4  savetotallimit  int  field  default3  loadbestmodelatend  bool  field  default  true  outputdir  str  field  default  output   findunusedparameter  bool  field  default  false  checkpointing  bool  field  default  false  dataloaderpinmemory  bool  field  default  false  evalandsaveresult  bool  field  default  true  savemodel  bool  field  default  false  seed  int  field  default42  def safesavemodelforhftrainer  trainer  transformer  trainer  outputdir  str       collect state dict dump disk     statedict  trainermodelstatedict   trainerargsshouldsave  cpustatedict   key  valuecpu   key  value statedictitems    del statedict trainersave  outputdir  statedict  cpustatedict   noqa     get reverse complement original dna sequence      def getalterofdnasequence  sequence  str   map                     c     g     g     c    return    join   map  c  c reverse  sequence    return    join   map  c  c sequence       transform dna sequence k  mer string     def generatekmerstr  sequence  str  k  int    str      generate k  mer string dna sequence     return     join   sequence    k  range  len  sequence   k  1        load generate k  mer string dna sequence  generate k  mer string save directory original datum name suffix     k  mer       def loadorgeneratekmer  datapath  str  text  list  str   k  int    list  str       load generate k  mer string dna sequence     kmerpath  datapathtokenizerreplace    csv   f    k  merjson   ospathexist  kmerpath   loggingwarne  f  loading k  mer  kmerpath     open  kmerpath    r   f  kmer  jsonload  f  else  loggingwarning  f  generate k  mer    kmer   generatekmerstr  text  k  text text  open  kmerpath    w   f  loggingwarning  f  saving k  mer  kmerpath     jsondump  kmer  f  return kmer class superviseddataset  dataset       dataset supervise fine  tuning     def   init    self  datapath  str  tokenizer  transformer  pretrainedtokenizer  kmer  int  1   super  superviseddataset  self  init      load data disk open  datapath    r   f  datum  list  csvreader  f    1   len  datum  0     2   datum format  text  label  loggingwarning    perform single sequence classification    text    0  datum  label   int   1   datum   gene sequence concat  not work sequence  pair   tricking model think single sequence  elif len  datum  0     3   datum format  text1  text2  label  loggingwarning    perform sequence  pair classification    text     0    1   datum  label   int   2   datum  else  raise valueerror    data format support    kmer   1   write file first process torchdistributedgetrank    0  1   torchdistributedbarrier   loggingwarning  f  use  kmer  mer input    text  loadorgeneratekmer  datapath  text  kmer  torchdistributedgetrank     0  torchdistributedbarri   output  tokenizer  text  returntensors  pt   padding  long   maxlength  tokenizermodelmaxlength  truncation  true   selfinputid  output    inputids    change selfinputid  0   selfinputid  0    0   2  change token want attend not selfattentionmask  output    attentionmask   selflabel  label selfnumlabel  len  set  label   def   len    self   return len  selfinputid  def   getitem    self     dict  str  torch  tensor   return dict  inputid  selfinputid    label  selflabel     dataclass class datacollatorforsuperviseddataset  object       collate example supervise fine  tuning     tokenizer  transformer  pretrainedtokenizer def   call    self  instance  sequence  dict     dict  str  torch  tensor   inputid  label  tuple   instance  key  instance instance  key    inputids     label    inputids  torchnnutilsrnnpadsequence  inputid  batchfirst  true  paddingvalue  selftokenizerpadtokenid  label  torch  tensor  label  long   return dict  inputid  inputids  label  label  attentionmask  inputidsne  selftokenizerpadtokenid        manually calculate accuracy  f1  matthewscorrelation  precision  recall sklearn      def calculatemetricwithsklearn  logit  npndarray  label  npndarray   logitsndim   3   reshape logit 2d need logit  logitsreshape  1  logitsshape  1   prediction  npargmax  logit  axis1  validmask  label   100  exclude padding token  assume 100 pad token id  validprediction  prediction  validmask  validlabel  label  validmask  return   start new   sum prediction   f   sum  validprediction     len  validprediction      end new   accuracy   sklearnmetricsaccuracyscore  validlabel  validprediction     f1   sklearnmetricsf1score  validlabel  validprediction  average  macro   zerodivision0     matthewscorrelation   sklearnmetricsmatthewscorrcoef  validlabel  validprediction     precision   sklearnmetricsprecisionscore  validlabel  validprediction  average  macro   zerodivision0     recall   sklearnmetricsrecallscore  validlabel  validprediction  average  macro   zerodivision0        compute metric use huggingface trainer      def computemetric  evalpred   logit  label  evalpred isinstance  logit  tuple    unpack logit s tuple logit  logit  0  return calculatemetricwithsklearn  logit  label  class customtrainer  transformer  trainer   def   init    self   args    kwargs   super   init     args    kwargs  selfepochprediction    selfepochlabel    selfepochloss    def computelos  self  model  input  returnoutput  false       max  subclassed compute training accuracy  loss compute trainer  default  model return loss first element  subclass override custom behavior      selflabelsmoother none   label  input  label  inputspop    label   else  label  none output  model    input  outputattention  true   test try  print  f  attention   outputsattention    except exception  print    attention return     label  input  pred  outputslogitsdetach    log accuracy acc    predsargmax  axis1    input    label    type  torchfloat  mean   item     uncomment want plot batch accuracy  wandblog     batchaccuracy   acc    log accuracy  store prediction label epoch  level metric selfepochpredictionsappend  predscpu   numpy    selfepochlabelsappend  input    label   cpu   numpy     save past state exist selfargspastindex   0  selfpast  output  selfargspastindex  label none  loss  selflabelsmoother  output  label  else  loss  output    loss   isinstance  output  dict  else output  0   uncomment want plot batch loss  wandblog     batchloss   loss   selfepochlossappend  lossitem     store loss epoch  level metric return  loss  output  returnoutput else loss  define custom callback calculate metric end epoch class customcallback  trainercallback   def   init    self  trainer    none  super   init     selftrainer  trainer def onepochend  self  args  state  control    kwarg    aggregate prediction label entire epoch epochprediction  npconcatenate  selftrainerepochprediction  epochlabels  npconcatenate  selftrainerepochlabel   compute accuracy accuracy  npmean  epochpredictionsargmax  axis1    epochlabel   compute mean loss meanloss  npmean  selftrainerepochloss   compute precision  recall  f1  score precision  recall  f1    precisionrecallfscoresupport  epochlabels  epochpredictionsargmax  axis1   average  weight    log epoch  level metric wandblog     epochaccuracy   accuracy    epochloss   meanloss   wandblog     precision   precision    recall   recall    f1   f1    clear store prediction  label  loss next epoch selftrainerepochprediction    selftrainerepochlabel    selftrainerepochloss    return none  todo  use function gather prediction label get metric    instantiating training  transformermodel import superviseddataset  datacollatorforsuperviseddataset  modelarguments   trainingarguments  dataarguments  safesavemodelforhftrainer  customtrainer  customcallback   computemetrics copy import deepcopy transformer import trainercallback  end new import os import json import torch import transformer peft import  loraconfig  getpeftmodel  getpeftmodelstatedict   import wandb run  wandbinit   assert run wandbrun def train  device   parser  transformer  hfargumentparser   modelarguments  dataarguments  trainingarguments   modelargs  dataargs  trainingargs  parserparseargsintodataclasse    load tokenizer tokenizer  transformer  autotokenizerfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  modelmaxlength  trainingargsmodelmaxlength  paddingside  right   usefast  true  trustremotecode  true     instadeepai  modelargsmodelnameorpath  tokenizereostoken  tokenizerpadtoken  define dataset datum collator traindataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    traincsv    kmer  dataargskmer  valdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    devcsv    kmer  dataargskmer  testdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    testcsv    kmer  dataargskmer  datacollator  datacollatorforsuperviseddataset  tokenizer  tokenizer   load model model  transformer  automodelforsequenceclassificationfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  numlabel  traindatasetnumlabel  trustremotecode  true  outputattention  true  to  device   configure lora modelargsuselora  loraconfig  loraconfig  r  modelargslorar  loraalpha  modelargsloraalpha  targetmodule  list  modelargsloratargetmodulessplit         loradropout  modelargsloradropout  bias  none   tasktype  seqcls   inferencemode  false   model  getpeftmodel  model  loraconfig  modelprinttrainableparameters   trainer  customtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  computemetric  computemetric  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  traineraddcallback  customcallback  trainer   trainertrain    trainresult  trainertrain    loss  trainresult    loss    print  f  loss issss   loss     print  f  train reusult   trainresult     new  result  return metric end training trainingargssavemodel  trainersavestate   safesavemodelforhftrainer  trainer  trainer  outputdir  trainingargsoutputdir   get evaluation result trainer trainingargsevalandsaveresults  resultspath  ospathjoin  trainingargsoutputdir    result   trainingargsrunname  result  trainerevaluate  evaldataset  testdataset  osmakedirs  resultspath  existok  true  open  ospathjoin  resultspath    evalresultsjson      w   f  jsondump  result  f    name         main      define device device  torchdevice   cuda  torchcudaisavailable   else  cpu   print   use device    device   call train function device train  device  training  try run example  modelpath   finetune  output  dnabert2  tokenizer  autotokenizerfrompretraine  modelpath   load model outputattention  true model  automodelfrompretrained  modelpath  trustremotecode  true  outputattention  true  modelinput  tokenizer    actgacgggtagtgactg   returntensors  pt   torchinferencemode    output  model    modelinput  outputattention  true  code might test print  let know anything missing  thank much help ,problem deep structure  attention discard early model  therefore go code understand happen  change    huggingfaceco  jaandoui  dnabert2  attentionextracted extract attentionprob  change do ,attention return even outputattentions true  m use pretraine model base bert  github link  dnabert2  use automodelforsequenceclassification mosaicml  mosaic  bert  base   m problem extract attention  read many post show way deal activate outputattention  true model  none post solve problem  output length 2 element shape  torch  size   1  7  768   torch  size   1  768    try get outputattention get none   m sure search solution would   m provide whole code  defining model  trainer  datum  tokenizer  copy import deepcopy sklearnmetrics import precisionrecallfscoresupport import wandb transformer import trainercallback  end new import os import csv import json import logging dataclasse import dataclass  field type import optional  dict  sequence  tuple  list import torch import transformer import sklearn import numpy np torchutilsdata import dataset  dataclass class modelarguments  modelnameorpath  optional  str   field  default  facebook  opt125 m   uselora  bool  field  default  false  metadata    help     whether use lora    lorar  int  field  default8  metadata    help     hidden dimension lora    loraalpha  int  field  default32  metadata    help     alpha lora    loradropout  float  field  default005  metadata    help     dropout rate lora    loratargetmodule  str  field  default  query  value   metadata    help     perform lora     dataclass class dataarguments  datapath  str  field  default  none  metadata    help     path training datum     kmer  int  field  default1  metadata    help     k  mer input sequence  1 mean use k  mer      dataclass class trainingarguments  transformer  trainingarguments   cachedir  optional  str   field  default  none  runname  str  field  default  run   optim  str  field  default  adamwtorch   modelmaxlength  int  field  default512  metadata    help     maximum sequence length     gradientaccumulationsteps  int  field  default1  perdevicetrainbatchsize  int  field  default1  perdeviceevalbatchsize  int  field  default1  numtrainepochs  int  field  default1  loggingstep  int  field  default100  savesteps  int  field  default100  fp16  bool  field  default  false   start new  evalstep  int  field  default100  evalstep  int  field  default01   end new evaluationstrategy  str  field  default  step   warmupsteps  int  field  default50  weightdecay  float  field  default001  learningrate  float  field  default1e4  savetotallimit  int  field  default3  loadbestmodelatend  bool  field  default  true  outputdir  str  field  default  output   findunusedparameter  bool  field  default  false  checkpointing  bool  field  default  false  dataloaderpinmemory  bool  field  default  false  evalandsaveresult  bool  field  default  true  savemodel  bool  field  default  false  seed  int  field  default42  def safesavemodelforhftrainer  trainer  transformer  trainer  outputdir  str       collect state dict dump disk     statedict  trainermodelstatedict   trainerargsshouldsave  cpustatedict   key  valuecpu   key  value statedictitems    del statedict trainersave  outputdir  statedict  cpustatedict   noqa     get reverse complement original dna sequence      def getalterofdnasequence  sequence  str   map                     c     g     g     c    return    join   map  c  c reverse  sequence    return    join   map  c  c sequence       transform dna sequence k  mer string     def generatekmerstr  sequence  str  k  int    str      generate k  mer string dna sequence     return     join   sequence    k  range  len  sequence   k  1        load generate k  mer string dna sequence  generate k  mer string save directory original datum name suffix     k  mer       def loadorgeneratekmer  datapath  str  text  list  str   k  int    list  str       load generate k  mer string dna sequence     kmerpath  datapathtokenizerreplace    csv   f    k  merjson   ospathexist  kmerpath   loggingwarne  f  loading k  mer  kmerpath     open  kmerpath    r   f  kmer  jsonload  f  else  loggingwarning  f  generate k  mer    kmer   generatekmerstr  text  k  text text  open  kmerpath    w   f  loggingwarning  f  saving k  mer  kmerpath     jsondump  kmer  f  return kmer class superviseddataset  dataset       dataset supervise fine  tuning     def   init    self  datapath  str  tokenizer  transformer  pretrainedtokenizer  kmer  int  1   super  superviseddataset  self  init      load data disk open  datapath    r   f  datum  list  csvreader  f    1   len  datum  0     2   datum format  text  label  loggingwarning    perform single sequence classification    text    0  datum  label   int   1   datum   gene sequence concat  not work sequence  pair   tricking model think single sequence  elif len  datum  0     3   datum format  text1  text2  label  loggingwarning    perform sequence  pair classification    text     0    1   datum  label   int   2   datum  else  raise valueerror    data format support    kmer   1   write file first process torchdistributedgetrank    0  1   torchdistributedbarrier   loggingwarning  f  use  kmer  mer input    text  loadorgeneratekmer  datapath  text  kmer  torchdistributedgetrank     0  torchdistributedbarri   output  tokenizer  text  returntensors  pt   padding  long   maxlength  tokenizermodelmaxlength  truncation  true   selfinputid  output    inputids    change selfinputid  0   selfinputid  0    0   2  change token want attend not selfattentionmask  output    attentionmask   selflabel  label selfnumlabel  len  set  label   def   len    self   return len  selfinputid  def   getitem    self     dict  str  torch  tensor   return dict  inputid  selfinputid    label  selflabel     dataclass class datacollatorforsuperviseddataset  object       collate example supervise fine  tuning     tokenizer  transformer  pretrainedtokenizer def   call    self  instance  sequence  dict     dict  str  torch  tensor   inputid  label  tuple   instance  key  instance instance  key    inputids     label    inputids  torchnnutilsrnnpadsequence  inputid  batchfirst  true  paddingvalue  selftokenizerpadtokenid  label  torch  tensor  label  long   return dict  inputid  inputids  label  label  attentionmask  inputidsne  selftokenizerpadtokenid        manually calculate accuracy  f1  matthewscorrelation  precision  recall sklearn      def calculatemetricwithsklearn  logit  npndarray  label  npndarray   logitsndim   3   reshape logit 2d need logit  logitsreshape  1  logitsshape  1   prediction  npargmax  logit  axis1  validmask  label   100  exclude padding token  assume 100 pad token id  validprediction  prediction  validmask  validlabel  label  validmask  return   start new   sum prediction   f   sum  validprediction     len  validprediction      end new   accuracy   sklearnmetricsaccuracyscore  validlabel  validprediction     f1   sklearnmetricsf1score  validlabel  validprediction  average  macro   zerodivision0     matthewscorrelation   sklearnmetricsmatthewscorrcoef  validlabel  validprediction     precision   sklearnmetricsprecisionscore  validlabel  validprediction  average  macro   zerodivision0     recall   sklearnmetricsrecallscore  validlabel  validprediction  average  macro   zerodivision0        compute metric use huggingface trainer      def computemetric  evalpred   logit  label  evalpred isinstance  logit  tuple    unpack logit s tuple logit  logit  0  return calculatemetricwithsklearn  logit  label  class customtrainer  transformer  trainer   def   init    self   args    kwargs   super   init     args    kwargs  selfepochprediction    selfepochlabel    selfepochloss    def computelos  self  model  input  returnoutput  false       max  subclassed compute training accuracy  loss compute trainer  default  model return loss first element  subclass override custom behavior      selflabelsmoother none   label  input  label  inputspop    label   else  label  none output  model    input  outputattention  true   test try  print  f  attention   outputsattention    except exception  print    attention return     label  input  pred  outputslogitsdetach    log accuracy acc    predsargmax  axis1    input    label    type  torchfloat  mean   item     uncomment want plot batch accuracy  wandblog     batchaccuracy   acc    log accuracy  store prediction label epoch  level metric selfepochpredictionsappend  predscpu   numpy    selfepochlabelsappend  input    label   cpu   numpy     save past state exist selfargspastindex   0  selfpast  output  selfargspastindex  label none  loss  selflabelsmoother  output  label  else  loss  output    loss   isinstance  output  dict  else output  0   uncomment want plot batch loss  wandblog     batchloss   loss   selfepochlossappend  lossitem     store loss epoch  level metric return  loss  output  returnoutput else loss  define custom callback calculate metric end epoch class customcallback  trainercallback   def   init    self  trainer    none  super   init     selftrainer  trainer def onepochend  self  args  state  control    kwarg    aggregate prediction label entire epoch epochprediction  npconcatenate  selftrainerepochprediction  epochlabels  npconcatenate  selftrainerepochlabel   compute accuracy accuracy  npmean  epochpredictionsargmax  axis1    epochlabel   compute mean loss meanloss  npmean  selftrainerepochloss   compute precision  recall  f1  score precision  recall  f1    precisionrecallfscoresupport  epochlabels  epochpredictionsargmax  axis1   average  weight    log epoch  level metric wandblog     epochaccuracy   accuracy    epochloss   meanloss   wandblog     precision   precision    recall   recall    f1   f1    clear store prediction  label  loss next epoch selftrainerepochprediction    selftrainerepochlabel    selftrainerepochloss    return none  todo  use function gather prediction label get metric    instantiating training  transformermodel import superviseddataset  datacollatorforsuperviseddataset  modelarguments   trainingarguments  dataarguments  safesavemodelforhftrainer  customtrainer  customcallback   computemetrics copy import deepcopy transformer import trainercallback  end new import os import json import torch import transformer peft import  loraconfig  getpeftmodel  getpeftmodelstatedict   import wandb run  wandbinit   assert run wandbrun def train  device   parser  transformer  hfargumentparser   modelarguments  dataarguments  trainingarguments   modelargs  dataargs  trainingargs  parserparseargsintodataclasse    load tokenizer tokenizer  transformer  autotokenizerfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  modelmaxlength  trainingargsmodelmaxlength  paddingside  right   usefast  true  trustremotecode  true     instadeepai  modelargsmodelnameorpath  tokenizereostoken  tokenizerpadtoken  define dataset datum collator traindataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    traincsv    kmer  dataargskmer  valdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    devcsv    kmer  dataargskmer  testdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    testcsv    kmer  dataargskmer  datacollator  datacollatorforsuperviseddataset  tokenizer  tokenizer   load model model  transformer  automodelforsequenceclassificationfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  numlabel  traindatasetnumlabel  trustremotecode  true  outputattention  true  to  device   configure lora modelargsuselora  loraconfig  loraconfig  r  modelargslorar  loraalpha  modelargsloraalpha  targetmodule  list  modelargsloratargetmodulessplit         loradropout  modelargsloradropout  bias  none   tasktype  seqcls   inferencemode  false   model  getpeftmodel  model  loraconfig  modelprinttrainableparameters   trainer  customtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  computemetric  computemetric  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  traineraddcallback  customcallback  trainer   trainertrain    trainresult  trainertrain    loss  trainresult    loss    print  f  loss issss   loss     print  f  train reusult   trainresult     new  result  return metric end training trainingargssavemodel  trainersavestate   safesavemodelforhftrainer  trainer  trainer  outputdir  trainingargsoutputdir   get evaluation result trainer trainingargsevalandsaveresults  resultspath  ospathjoin  trainingargsoutputdir    result   trainingargsrunname  result  trainerevaluate  evaldataset  testdataset  osmakedirs  resultspath  existok  true  open  ospathjoin  resultspath    evalresultsjson      w   f  jsondump  result  f    name         main      define device device  torchdevice   cuda  torchcudaisavailable   else  cpu   print   use device    device   call train function device train  device  training  try run example  modelpath   finetune  output  dnabert2  tokenizer  autotokenizerfrompretraine  modelpath   load model outputattention  true model  automodelfrompretrained  modelpath  trustremotecode  true  outputattention  true  modelinput  tokenizer    actgacgggtagtgactg   returntensors  pt   torchinferencemode    output  model    modelinput  outputattention  true  code might test print  let know anything missing  thank much help  problem deep structure  attention discard early model  therefore go code understand happen  change    huggingfaceco  jaandoui  dnabert2  attentionextracted extract attentionprob  change do ,Task-Specific Queries
Fine-tuning BERT with deterministic masking instead of random masking,"I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons). To do so, I created a dataset that has two columns: text in which some tokens have been replaced with [MASK] (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and label where I have the whole text. Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's transformers library, but I do not want to use DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2) where the masking is done randomly and I only can control the probability. What can I do?","['nlp', 'huggingface-transformers', 'bert-language-model']",2,"This is what I did to solve my problem. I created a custom class and changed the tokenization in a way that I needed (mask one of the numerical spans in the input). class CustomDataCollator(DataCollatorForLanguageModeling): mlm: bool = True return_tensors: str = ""pt"" def __post_init__(self): if self.mlm and self.tokenizer.mask_token is None: raise ValueError( ""This tokenizer does not have a mask token which is necessary "" ""for masked language modeling. You should pass `mlm=False` to "" ""train on causal language modeling instead."" ) def torch_mask_tokens(self, inputs, special_tokens_mask): """""" Prepare masked tokens inputs/labels for masked language modeling. NOTE: keep `special_tokens_mask` as an argument for avoiding error """""" # labels is batch_size x length of the sequence tensor # with the original token id # the length of the sequence includes the special tokens (2) labels = inputs.clone() batch_size = inputs.size(0) # seq_len = inputs.size(1) # in each seq, find the indices of the tokens that represent digits dig_ids = [1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023] dig_idx = torch.zeros_like(labels) for dig_id in dig_ids: dig_idx += (labels == dig_id) dig_idx = dig_idx.bool() # in each seq, find the spans of Trues using `find_spans` function spans = [] for i in range(batch_size): spans.append(find_spans(dig_idx[i].tolist())) masked_indices = torch.zeros_like(labels) # spans is a list of lists of tuples # in each tuple, the first element is the start index # and the second element is the length # in each child list, choose a random tuple for i in range(batch_size): if len(spans[i]) > 0: idx = torch.randint(0, len(spans[i]), (1,)) start, length = spans[i][idx[0]] masked_indices[i, start:start + length] = 1 else: print(""No digit found in the sequence!"") masked_indices = masked_indices.bool() # We only compute loss on masked tokens labels[~masked_indices] = -100 # change the input's masked_indices to self.tokenizer.mask_token inputs[masked_indices] = self.tokenizer.mask_token_id return inputs, labels def find_spans(lst): spans = [] for k, g in groupby(enumerate(lst), key=itemgetter(1)): if k: glist = list(g) spans.append((glist[0][0], len(glist))) return spans",2024-04-22 10:39:05,2024-05-21 07:37:41,345,https://stackoverflow.com/questions/78365608/fine-tuning-bert-with-deterministic-masking-instead-of-random-masking,"Fine-tuning BERT with deterministic masking instead of random masking I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons). To do so, I created a dataset that has two columns: text in which some tokens have been replaced with [MASK] (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and label where I have the whole text. Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's transformers library, but I do not want to use DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2) where the masking is done randomly and I only can control the probability. What can I do?",fine  tuning bert deterministic masking instead random masking want fine  tune bert specific dataset  problem want mask tokens training dataset randomly  already choose tokens want mask  certain reason    create dataset two column  text token replace  mask   aware fact word could tokenise one token take care  label whole text  want fine  tune bert model  say  bert  base  uncased  use hugging face s transformer library  want use datacollatorforlanguagemodeling  tokenizer  tokenizer  mlmprobability02  mask do randomly control probability  ,solve problem  create custom class change tokenization way need  mask one numerical spans input   class customdatacollator  datacollatorforlanguagemodele   mlm  bool  true returntensor  str    pt  def   postinit    self   selfmlm selftokenizermasktoken none  raise valueerror    tokenizer mask token necessary     mask language modeling  pass  mlm  false      train causal language modeling instead    def torchmasktoken  self  input  specialtokensmask       prepare mask tokens input  label mask language modeling  note  keep  specialtokensmask  argument avoid error      label batchsize x length sequence tensor  original token i d  length sequence include special token  2  label  inputsclone   batchsize  inputssize  0   seqlen  inputssize  1   seq  find index token represent digit digid   1014  1015  1016  1017  1018  1019  1020  1021  1022  1023  digidx  torchzeroslike  label  digid digid  digidx    label   digid  digidx  digidxbool    seq  find span true use  findspans  function span    range  batchsize   spansappend  findspan  digidx   tolist     maskedindice  torchzeroslike  label   span list list tuple  tuple  first element start index  second element length  child list  choose random tuple range  batchsize   len  span     0  idx  torchrandint  0  len  span      1    start  length  span    idx  0   maskedindice   start  start  length   1 else  print    digit find sequence    maskedindice  maskedindicesbool    compute loss mask tokens label  maskedindices   100  change input s maskedindices selftokenizermasktoken input  maskedindice   selftokenizermasktokenid return input  label def findspans  lst   span    k  g groupby  enumerate  lst   key  itemgetter  1    k  glist  list  g  spansappend   glist  0   0   len  glist    return span,fine  tuning bert deterministic masking instead random masking want fine  tune bert specific dataset  problem want mask tokens training dataset randomly  already choose tokens want mask  certain reason    create dataset two column  text token replace  mask   aware fact word could tokenise one token take care  label whole text  want fine  tune bert model  say  bert  base  uncased  use hugging face s transformer library  want use datacollatorforlanguagemodeling  tokenizer  tokenizer  mlmprobability02  mask do randomly control probability   solve problem  create custom class change tokenization way need  mask one numerical spans input   class customdatacollator  datacollatorforlanguagemodele   mlm  bool  true returntensor  str    pt  def   postinit    self   selfmlm selftokenizermasktoken none  raise valueerror    tokenizer mask token necessary     mask language modeling  pass  mlm  false      train causal language modeling instead    def torchmasktoken  self  input  specialtokensmask       prepare mask tokens input  label mask language modeling  note  keep  specialtokensmask  argument avoid error      label batchsize x length sequence tensor  original token i d  length sequence include special token  2  label  inputsclone   batchsize  inputssize  0   seqlen  inputssize  1   seq  find index token represent digit digid   1014  1015  1016  1017  1018  1019  1020  1021  1022  1023  digidx  torchzeroslike  label  digid digid  digidx    label   digid  digidx  digidxbool    seq  find span true use  findspans  function span    range  batchsize   spansappend  findspan  digidx   tolist     maskedindice  torchzeroslike  label   span list list tuple  tuple  first element start index  second element length  child list  choose random tuple range  batchsize   len  span     0  idx  torchrandint  0  len  span      1    start  length  span    idx  0   maskedindice   start  start  length   1 else  print    digit find sequence    maskedindice  maskedindicesbool    compute loss mask tokens label  maskedindices   100  change input s maskedindices selftokenizermasktoken input  maskedindice   selftokenizermasktokenid return input  label def findspans  lst   span    k  g groupby  enumerate  lst   key  itemgetter  1    k  glist  list  g  spansappend   glist  0   0   len  glist    return span,Library/Tool-Based Queries
Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java,"So we are using Stanford NLP to annotate input text, and these input texts are laughably small. Below is one example of the same. ""Can you give me details about Mohammad Siva John with identifiers 6745-3876-1354-8790 and 313-31-333"" Below is the Java code snippet to annotate. final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); Below is the Maven dependency. <dependency> <groupId>edu.stanford.nlp</groupId> <artifactId>stanford-corenlp</artifactId> <version>4.5.4</version> </dependency> This works fine, but after couple of days, the JVM crashes with a coredump. Coredump analysis shows that below line resulted into OutOfMemoryError pipeline.annotate(document); Any thoughts on how to resolve this? There are no field level variables in the class, and all of them are method level, and so should be 'freed' once the execution is done. So, there should be no OutOfMemoryError first of all to begin with. Quite perplexing. Any thoughts?","['java', 'nlp', 'out-of-memory']",1,"So found this in StanfordNLP Javadoc of void edu.stanford.nlp.pipeline.StanfordCoreNLP.clearAnnotatorPool() Call this if you are no longer using StanfordCoreNLP and want torelease the memory associated with the annotators. Calling this method itself almost has no impact, but the real game changer is calling 'System.gc()'. Below is the fix for this, just call clearAnnotatorPool and gc after annotate . final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); // Below two calls would fix memory issue. StanfordCoreNLP.clearAnnotatorPool(); System.gc(); This is how used memory is with only StanfordCoreNLP.clearAnnotatorPool() call. Note that this is without System.gc() call. Notice that for 1000 calls to 'annotate' in a loop the used memory crosses 2500 MB and then falls down below 500 MB. This is the case when we are letting JVM call gc. However, when I call both StanfordCoreNLP.clearAnnotatorPool(); and System.gc(); the results are drastically different. Note that used memory is within the range of 132 to 133 MB irrespective of whether the annotation pool has been cleared. One starts seeing the real of 'StanfordCoreNLP.clearAnnotatorPool()' around after 150 pipeline.annotate hits. Below is memory utilization observed over 1000 hits to pipeline.annotate. Observe that with 'StanfordCoreNLP.clearAnnotatorPool() and System.gc()', the memory utilization hovers just above 40 MB. If you rightly understand, this only signifies that somehow the JVM default gc execution is not releasing as much memory as when an explicit call is being made. I understand there would be timing difference and all, which only means that one needs to further research on the default gc and the JDK on being used (am on JDK 21 with IDE enforcing JDK 17 compliance level. The graphs are all almost the same when run on a JDK 17 directly too) and how this changes with various gc strategies. But then I'm happy with this and conclude! Hope this helps someone.",2024-04-18 05:13:30,2024-04-18 06:06:33,26,https://stackoverflow.com/questions/78344999/stanford-nlp-annotation-pipeline-annotate-resulting-into-outofmemoryerror-in-jav,"Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java So we are using Stanford NLP to annotate input text, and these input texts are laughably small. Below is one example of the same. ""Can you give me details about Mohammad Siva John with identifiers 6745-3876-1354-8790 and 313-31-333"" Below is the Java code snippet to annotate. final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); Below is the Maven dependency. <dependency> <groupId>edu.stanford.nlp</groupId> <artifactId>stanford-corenlp</artifactId> <version>4.5.4</version> </dependency> This works fine, but after couple of days, the JVM crashes with a coredump. Coredump analysis shows that below line resulted into OutOfMemoryError pipeline.annotate(document); Any thoughts on how to resolve this? There are no field level variables in the class, and all of them are method level, and so should be 'freed' once the execution is done. So, there should be no OutOfMemoryError first of all to begin with. Quite perplexing. Any thoughts?",stanford nlp annotation pipelineannotate result outofmemoryerror java use stanford nlp annotate input text  input text laughably small  one example    give detail mohammad siva john identifier 6745  3876  1354  8790 313  31  333  java code snippet annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document   maven dependency   dependency   groupid  edustanfordnlp  groupid   artifactid  stanford  corenlp  artifactid   version  454  version   dependency  work fine  couple day  jvm crash coredump  coredump analysis show line result outofmemoryerror pipelineannotate  document   thought resolve  field level variable class  method level   free  execution do   outofmemoryerror first begin  quite perplexing  thought ,find stanfordnlp javadoc void edustanfordnlppipeline  stanfordcorenlpclearannotatorpool   call long use stanfordcorenlp want torelease memory associate annotator  call method almost impact  real game changer call  systemgc     fix  call clearannotatorpool gc annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document    two call would fix memory issue  stanfordcorenlpclearannotatorpool    systemgc    use memory stanfordcorenlpclearannotatorpool   call  note without systemgc   call  notice 1000 call  annotate  loop use memory crosse 2500 mb fall 500 mb  case let jvm call gc  however  call stanfordcorenlpclearannotatorpool    systemgc    result drastically different  note use memory within range 132 133 mb irrespective whether annotation pool clear  one start see real  stanfordcorenlpclearannotatorpool    around 150 pipelineannotate hit  memory utilization observe 1000 hit pipelineannotate  observe  stanfordcorenlpclearannotatorpool   systemgc     memory utilization hover 40 mb  rightly understand  signify somehow jvm default gc execution release much memory explicit call make  understand would time difference  mean one need research default gc jdk use  jdk 21 ide enforce jdk 17 compliance level  graph almost run jdk 17 directly  change various gc strategy   m happy conclude  hope help someone ,stanford nlp annotation pipelineannotate result outofmemoryerror java use stanford nlp annotate input text  input text laughably small  one example    give detail mohammad siva john identifier 6745  3876  1354  8790 313  31  333  java code snippet annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document   maven dependency   dependency   groupid  edustanfordnlp  groupid   artifactid  stanford  corenlp  artifactid   version  454  version   dependency  work fine  couple day  jvm crash coredump  coredump analysis show line result outofmemoryerror pipelineannotate  document   thought resolve  field level variable class  method level   free  execution do   outofmemoryerror first begin  quite perplexing  thought  find stanfordnlp javadoc void edustanfordnlppipeline  stanfordcorenlpclearannotatorpool   call long use stanfordcorenlp want torelease memory associate annotator  call method almost impact  real game changer call  systemgc     fix  call clearannotatorpool gc annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document    two call would fix memory issue  stanfordcorenlpclearannotatorpool    systemgc    use memory stanfordcorenlpclearannotatorpool   call  note without systemgc   call  notice 1000 call  annotate  loop use memory crosse 2500 mb fall 500 mb  case let jvm call gc  however  call stanfordcorenlpclearannotatorpool    systemgc    result drastically different  note use memory within range 132 133 mb irrespective whether annotation pool clear  one start see real  stanfordcorenlpclearannotatorpool    around 150 pipelineannotate hit  memory utilization observe 1000 hit pipelineannotate  observe  stanfordcorenlpclearannotatorpool   systemgc     memory utilization hover 40 mb  rightly understand  signify somehow jvm default gc execution release much memory explicit call make  understand would time difference  mean one need research default gc jdk use  jdk 21 ide enforce jdk 17 compliance level  graph almost run jdk 17 directly  change various gc strategy   m happy conclude  hope help someone ,Implementation Issues
How to tag words that not include one specific symbol in Spacy?,"I'm trying to tag one word in Spacy using regex, but I want to add one condition: it can't contain symbol '/' in any place inside. My code looks like this: [{'lower': {""regex"": ""^.*(word).*?""}}] I tried using ^ to exclude this but It didn't work. So examples: 'subwordw' tagged: 'subword' 'subword/w' tagged nothing","['python', 'nlp', 'pattern-matching', 'spacy']",1,"try this: {'lower': {'REGEX': ""^([^\/]*word[^\/]*)$""}}",2024-04-18 04:21:10,2024-04-18 16:12:27,31,https://stackoverflow.com/questions/78344850/how-to-tag-words-that-not-include-one-specific-symbol-in-spacy,"How to tag words that not include one specific symbol in Spacy? I'm trying to tag one word in Spacy using regex, but I want to add one condition: it can't contain symbol '/' in any place inside. My code looks like this: [{'lower': {""regex"": ""^.*(word).*?""}}] I tried using ^ to exclude this but It didn't work. So examples: 'subwordw' tagged: 'subword' 'subword/w' tagged nothing",tag word include one specific symbol spacy   m try tag one word spacy use regex  want add one condition  can not contain symbol    place inside  code look like     low      regex        word         try use  exclude not work  example   subwordw  tagged   subword   subword  w  tagged nothing,try    low     regex           word         ,tag word include one specific symbol spacy   m try tag one word spacy use regex  want add one condition  can not contain symbol    place inside  code look like     low      regex        word         try use  exclude not work  example   subwordw  tagged   subword   subword  w  tagged nothing try    low     regex           word         ,Task-Specific Queries
Sklearm FeatureHasher not working on a single column in a dataframe,"I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error: ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings. from sklearn.feature_extraction import FeatureHasher hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_df = pd.DataFrame(fh.transform(X_train[""Item_Identifier""]).toarray(), columns=['H'+str(i) for i in range (hash_vector_size)]) I was expecting a dataframe of 50 columns where the data would have been hashed in","['pandas', 'machine-learning', 'scikit-learn', 'nlp']",1,"You were almost there: from sklearn.feature_extraction import FeatureHasher import pandas as pd data = {""Item_Identifier"": [""ID1"", ""ID2"", ""ID3"", ""ID4"", ""ID5""]} X_train = pd.DataFrame(data) hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_features = fh.transform([[item] for item in X_train[""Item_Identifier""]]) hashed_df = pd.DataFrame(hashed_features.toarray(), columns=['H'+str(i) for i in range(hash_vector_size)]) print(hashed_df) which gives H0 H1 H2 H3 H4 H5 H6 H7 H8 H9 ... H40 H41 H42 H43 \ 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1.0 ... 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 H44 H45 H46 H47 H48 H49 0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 [5 rows x 50 columns]",2024-04-14 17:20:17,2024-04-14 17:48:30,111,https://stackoverflow.com/questions/78324647/sklearm-featurehasher-not-working-on-a-single-column-in-a-dataframe,"Sklearm FeatureHasher not working on a single column in a dataframe I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error: ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings. from sklearn.feature_extraction import FeatureHasher hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_df = pd.DataFrame(fh.transform(X_train[""Item_Identifier""]).toarray(), columns=['H'+str(i) for i in range (hash_vector_size)]) I was expecting a dataframe of 50 columns where the data would have been hashed in",sklearm featurehasher work single column dataframe try perform feature hasher single column dataframe keep give error  valueerror  samples single string  input must iterable iterable string  sklearnfeatureextraction import featurehasher hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hasheddf  pd  dataframe  fhtransform  xtrain    itemidentifier    toarray    columns   hstr   range  hashvectorsize    expect dataframe 50 column datum would hash,almost  sklearnfeatureextraction import featurehasher import panda pd datum     itemidentifier      id1     id2     id3     id4     id5    xtrain  pd  dataframe  datum  hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hashedfeature  fhtransform    item  item xtrain    itemidentifier     hasheddf  pd  dataframe  hashedfeaturestoarray    columns   hstr   range  hashvectorsize    print  hasheddf  give h0 h1 h2 h3 h4 h5 h6 h7 h8 h9  h40 h41 h42 h43  0 00 00 00 00 00 00 00 00 00 10  00 00 00 00 1 00 00 00 00 00 00 00 00 00 00  00 00 00 00 2 00 00 00 00 00 00 00 00 00 00  00 00 00 00 3 00 00 00 00 00 00 00 00 00 00  00 00 00 00 4 00 00 00 00 00 00 00 00 00 00  00 00 00 00 h44 h45 h46 h47 h48 h49 0 00 00 00 00 00 00 1 00 00 00 00 00 00 2 00 00 00 00 00 00 3 00 00 00 00 00 00 4 00 00 00 00 00 00  5 row x 50 column ,sklearm featurehasher work single column dataframe try perform feature hasher single column dataframe keep give error  valueerror  samples single string  input must iterable iterable string  sklearnfeatureextraction import featurehasher hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hasheddf  pd  dataframe  fhtransform  xtrain    itemidentifier    toarray    columns   hstr   range  hashvectorsize    expect dataframe 50 column datum would hash almost  sklearnfeatureextraction import featurehasher import panda pd datum     itemidentifier      id1     id2     id3     id4     id5    xtrain  pd  dataframe  datum  hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hashedfeature  fhtransform    item  item xtrain    itemidentifier     hasheddf  pd  dataframe  hashedfeaturestoarray    columns   hstr   range  hashvectorsize    print  hasheddf  give h0 h1 h2 h3 h4 h5 h6 h7 h8 h9  h40 h41 h42 h43  0 00 00 00 00 00 00 00 00 00 10  00 00 00 00 1 00 00 00 00 00 00 00 00 00 00  00 00 00 00 2 00 00 00 00 00 00 00 00 00 00  00 00 00 00 3 00 00 00 00 00 00 00 00 00 00  00 00 00 00 4 00 00 00 00 00 00 00 00 00 00  00 00 00 00 h44 h45 h46 h47 h48 h49 0 00 00 00 00 00 00 1 00 00 00 00 00 00 2 00 00 00 00 00 00 3 00 00 00 00 00 00 4 00 00 00 00 00 00  5 row x 50 column ,Implementation Issues
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data. However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model. This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?","['python', 'nlp', 'artificial-intelligence', 'word2vec', 'word-embedding']",2,"As has been pointed out before , there is no ""go-to"" way for fine-tuning Word2Vec type models. I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of sentence-transformers or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that. For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be faiss .",2024-04-12 17:57:56,2024-04-14 12:15:53,1349,https://stackoverflow.com/questions/78317989/is-it-possible-to-fine-tune-a-pretrained-word-embedding-model-like-vec2word,"Is it possible to fine-tune a pretrained word embedding model like vec2word? I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data. However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model. This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?",possible fine  tune pretraine word embed model like vec2word   m work semantic matching search engine system  see word embed use task  however  dataset limited small  not think training word embed model word2vec scratch yield good result   decide fine  tune pre  train model datum  however  can not find lot information  article documentation  fine  tuning  people even say s impossible fine  tune word embed model  raise question  fine  tune pre  train word embed model possible anyone try  currently   m stick look information  try train word embed model scratch approach ,point    go  to  way fine  tune word2vec type model  would suggest training model scratch  combine datum available datum similar domain  word2vec model fairly quick train would probably give good result  need static word  level embedding  would recommend consider contextualize embedding  example use sentence  transformer similar framework  wide selection already pre  train model choose  fine  tune type model specific datum rather easily  ton resource online  use case  embed document dense vector representation use abovementioned library  construct searchable index semantic space  order match query  embe query use model retrieve document high approximate inner product  often refer mips search  example library take look would faiss ,possible fine  tune pretraine word embed model like vec2word   m work semantic matching search engine system  see word embed use task  however  dataset limited small  not think training word embed model word2vec scratch yield good result   decide fine  tune pre  train model datum  however  can not find lot information  article documentation  fine  tuning  people even say s impossible fine  tune word embed model  raise question  fine  tune pre  train word embed model possible anyone try  currently   m stick look information  try train word embed model scratch approach  point    go  to  way fine  tune word2vec type model  would suggest training model scratch  combine datum available datum similar domain  word2vec model fairly quick train would probably give good result  need static word  level embedding  would recommend consider contextualize embedding  example use sentence  transformer similar framework  wide selection already pre  train model choose  fine  tune type model specific datum rather easily  ton resource online  use case  embed document dense vector representation use abovementioned library  construct searchable index semantic space  order match query  embe query use model retrieve document high approximate inner product  often refer mips search  example library take look would faiss ,Library/Tool-Based Queries
R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens,"I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens. When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize(). However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option. Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()? A reprex: library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df%>%head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>%textrecipes:::get_tokens() Output: ""it"", ""be"", ""a"", ""day"", "","", ""Tuesday"", ""."", ""it"", ""be"", ""not"", ""Thursday"", ""!"" Desired output (Removal of ""!"", "","" and "".""): ""it"", ""be"", ""a"", ""day"", ""Tuesday"", ""it"", ""be"", ""not"", ""Thursday""","['r', 'nlp', 'spacy', 'tidymodels']",2,"You want to use the step_pos_filter() to filter the output of spacy by POS. It is a little annoying, because you have to specify the types to keep. Full list of tags found here https://github.com/explosion/spaCy/blob/master/spacy/glossary.py library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) pos <- c(""ADJ"", ""ADP"", ""ADV"", ""AUX"", ""CONJ"", ""CCONJ"", ""DET"", ""INTJ"", ""NOUN"", ""NUM"", ""PART"", ""PRON"", ""PROPN"", ""SCONJ"", ""SYM"", ""VERB"", ""X"", ""EOL"", ""SPACE"") lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df %>% head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_pos_filter(text, keep_tags = pos) %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>% textrecipes:::get_tokens() #> [[1]] #> [1] ""it"" ""be"" ""a"" ""day"" ""Tuesday"" ""it"" ""be"" #> [8] ""not"" ""Thursday""",2024-04-12 07:39:54,2024-04-12 15:00:44,90,https://stackoverflow.com/questions/78314842/r-tidymodels-textrecipes-tokenizing-with-spacyr-how-to-remove-punctuations-f,"R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens. When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize(). However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option. Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()? A reprex: library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df%>%head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>%textrecipes:::get_tokens() Output: ""it"", ""be"", ""a"", ""day"", "","", ""Tuesday"", ""."", ""it"", ""be"", ""not"", ""Thursday"", ""!"" Desired output (Removal of ""!"", "","" and "".""): ""it"", ""be"", ""a"", ""day"", ""Tuesday"", ""it"", ""be"", ""not"", ""Thursday""",r tidymodels textrecipe  tokenize spacyr  remove punctuation produce list token would like tokenize text use steptokenize spacyr engine proceeding lemmatisation use steplemma  follow  would like remove example punctuation list token  use default tokenizer   tokenizewords pass option list option steptokenize    however  understand steptokenize use spacyparse backend provide option  way remove eg  punctuations numeric tokens tokens produce lemmatisation use steplemma    reprex  library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  lexiconfeaturestokenizedlemmatised   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens   output                day          tuesday                       thursday       desire output  removal                                day     tuesday                 thursday ,want use stepposfilter   filter output spacy pos  little annoying  specify type keep  full list tag find   githubcom  explosion  spacy  blob  master  spacy  glossarypy library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  pos   c    adj     adp     adv     aux     conj     cconj     det     intj     noun     num     part     pron     propn     sconj     sym     verb     x     eol     space   lexiconfeaturestokenizedlemmatise   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      stepposfilter  text  keeptag  pos     steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens       1      1             day    tuesday           8       thursday ,r tidymodels textrecipe  tokenize spacyr  remove punctuation produce list token would like tokenize text use steptokenize spacyr engine proceeding lemmatisation use steplemma  follow  would like remove example punctuation list token  use default tokenizer   tokenizewords pass option list option steptokenize    however  understand steptokenize use spacyparse backend provide option  way remove eg  punctuations numeric tokens tokens produce lemmatisation use steplemma    reprex  library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  lexiconfeaturestokenizedlemmatised   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens   output                day          tuesday                       thursday       desire output  removal                                day     tuesday                 thursday  want use stepposfilter   filter output spacy pos  little annoying  specify type keep  full list tag find   githubcom  explosion  spacy  blob  master  spacy  glossarypy library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  pos   c    adj     adp     adv     aux     conj     cconj     det     intj     noun     num     part     pron     propn     sconj     sym     verb     x     eol     space   lexiconfeaturestokenizedlemmatise   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      stepposfilter  text  keeptag  pos     steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens       1      1             day    tuesday           8       thursday ,Library/Tool-Based Queries
"LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit","I am trying to ask GPT 4 to use Wikipedia for a prompt, using agents and tools via LangChain. The difficulty I'm running into is the book I've been using, Developing Apps with GPT-4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More , while published in 2023, already has code examples that are deprecated. For example, I am trying to do something similar to the code provided on page 114 of that book: from langchain.chat_models import ChatOpenAI from langchain.agents import load_tools, initialize_agent, AgentType llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) question = """"""What is the square root of the population of the capital of the Country where the Olympic Games were held in 2016?"""""" agent.run(question) I see much of this is deprecated (e.g., initialize_agent ), so I have looked around StackOverflow, GitHub, and the LangChain Python documents to come up with this: from langchain_openai import ChatOpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain.agents import ( load_tools, create_structured_chat_agent, AgentExecutor ) model = ChatOpenAI(model=""gpt-4"", temperature=0) tools = load_tools([""wikipedia""]) prompt = ChatPromptTemplate.from_template( """""" You are a research assistant, and your job is to retrieve information about movies and movie directors. Use the following tool: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question. You only need to give the number, no other information or explanation is necessary. Begin! Question: How many movies did the director of the {year} movie {name} direct before they made {name}? Thought: {agent_scratchpad} """""" ) agent = create_structured_chat_agent(model, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({""year"": ""1991"", ""name"": ""thelma and louise""}) I'm going to be running this through a loop of many movies, so I'd like it to only return one integer (in this case, 6). But it seems like I need to give it that full thought process prompt; I can't get it to run if I don't include {tools} , {tool_names} , and {agent_scratchpad} in the prompt ( per this GitHub post ). The frustrating thing is I eventually do get the correct answer, but note that it is throwing an error: ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: First, I need to find out who directed the movie ""Thelma and Louise"" in 1991. Action: wikipedia Action Input: {'query': 'Thelma and Louise'} Observation: ""Thelma & Louise"" is a 1991 American female buddy road film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip with unforeseen consequences. The film became a critical and commercial success, receiving six Academy Award nominations and winning one for Best Original Screenplay for Khouri. Scott was nominated for Best Director. Thought: Ridley Scott directed the movie ""Thelma and Louise"". Now I need to find out how many movies he directed before this one. Action: wikipedia Action Input: {'query': 'Ridley Scott filmography'} Observation: Ridley Scott is an English filmmaker. Following his commercial breakthrough with the science fiction horror film Alien (1979), his best known works are the neo-noir dystopian science fiction film Blade Runner (1982), historical drama Gladiator (2000), and science fiction film The Martian (2015). Scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. His films are also known for their strong female characters. Here is a list of his films before ""Thelma & Louise"": 1. The Duellists (1977) 2. Alien (1979) 3. Blade Runner (1982) 4. Legend (1985) 5. Someone to Watch Over Me (1987) 6. Black Rain (1989) Thought: Ridley Scott directed six movies before ""Thelma and Louise"". Final Answer: 6 This seems to be very common ( here , and here , and also here , and lastly here ). So, I do what it tells me ( see docs also ) and update my AgentExecutor to: agent_executor = AgentExecutor( agent=agent, tools=tools, handle_parsing_errors=True ) And that returns: {'year': '1991', 'name': 'thelma and louise', 'output': 'Agent stopped due to iteration limit or time limit.'} My question: How can I use LangChain to combine GPT 4 and Wikipedia to get an answer to a query, when all I want back is an integer?","['python', 'nlp', 'openai-api', 'langchain', 'large-language-model']",2,"Author of the book Developing Apps with GPT-4 and ChatGPT here, I already answered by mail, but just in case someone else stumbles upon this question... You can find updated code at https://github.com/malywut/gpt_examples . The updated code looks like this: from langchain_openai import ChatOpenAI from langchain.agents import load_tools, create_react_agent, AgentExecutor from langchain import hub llm = ChatOpenAI(model_name=""gpt-3.5-turbo"") tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = create_react_agent( tools=tools, llm=llm, prompt = hub.pull(""hwchase17/react""), ) question = ""..."" agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({""input"": question}) Hope this helps.",2024-04-10 20:52:34,2024-04-12 12:46:43,2638,https://stackoverflow.com/questions/78307073/langchain-agent-parsing-error-with-structured-chat-agent-and-wikipedia-tool-han,"LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit I am trying to ask GPT 4 to use Wikipedia for a prompt, using agents and tools via LangChain. The difficulty I'm running into is the book I've been using, Developing Apps with GPT-4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More , while published in 2023, already has code examples that are deprecated. For example, I am trying to do something similar to the code provided on page 114 of that book: from langchain.chat_models import ChatOpenAI from langchain.agents import load_tools, initialize_agent, AgentType llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) question = """"""What is the square root of the population of the capital of the Country where the Olympic Games were held in 2016?"""""" agent.run(question) I see much of this is deprecated (e.g., initialize_agent ), so I have looked around StackOverflow, GitHub, and the LangChain Python documents to come up with this: from langchain_openai import ChatOpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain.agents import ( load_tools, create_structured_chat_agent, AgentExecutor ) model = ChatOpenAI(model=""gpt-4"", temperature=0) tools = load_tools([""wikipedia""]) prompt = ChatPromptTemplate.from_template( """""" You are a research assistant, and your job is to retrieve information about movies and movie directors. Use the following tool: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question. You only need to give the number, no other information or explanation is necessary. Begin! Question: How many movies did the director of the {year} movie {name} direct before they made {name}? Thought: {agent_scratchpad} """""" ) agent = create_structured_chat_agent(model, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({""year"": ""1991"", ""name"": ""thelma and louise""}) I'm going to be running this through a loop of many movies, so I'd like it to only return one integer (in this case, 6). But it seems like I need to give it that full thought process prompt; I can't get it to run if I don't include {tools} , {tool_names} , and {agent_scratchpad} in the prompt ( per this GitHub post ). The frustrating thing is I eventually do get the correct answer, but note that it is throwing an error: ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: First, I need to find out who directed the movie ""Thelma and Louise"" in 1991. Action: wikipedia Action Input: {'query': 'Thelma and Louise'} Observation: ""Thelma & Louise"" is a 1991 American female buddy road film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip with unforeseen consequences. The film became a critical and commercial success, receiving six Academy Award nominations and winning one for Best Original Screenplay for Khouri. Scott was nominated for Best Director. Thought: Ridley Scott directed the movie ""Thelma and Louise"". Now I need to find out how many movies he directed before this one. Action: wikipedia Action Input: {'query': 'Ridley Scott filmography'} Observation: Ridley Scott is an English filmmaker. Following his commercial breakthrough with the science fiction horror film Alien (1979), his best known works are the neo-noir dystopian science fiction film Blade Runner (1982), historical drama Gladiator (2000), and science fiction film The Martian (2015). Scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. His films are also known for their strong female characters. Here is a list of his films before ""Thelma & Louise"": 1. The Duellists (1977) 2. Alien (1979) 3. Blade Runner (1982) 4. Legend (1985) 5. Someone to Watch Over Me (1987) 6. Black Rain (1989) Thought: Ridley Scott directed six movies before ""Thelma and Louise"". Final Answer: 6 This seems to be very common ( here , and here , and also here , and lastly here ). So, I do what it tells me ( see docs also ) and update my AgentExecutor to: agent_executor = AgentExecutor( agent=agent, tools=tools, handle_parsing_errors=True ) And that returns: {'year': '1991', 'name': 'thelma and louise', 'output': 'Agent stopped due to iteration limit or time limit.'} My question: How can I use LangChain to combine GPT 4 and Wikipedia to get an answer to a query, when all I want back is an integer?",langchain agent parse error structuredchatagent wikipedia tool  handleparsingerrors hit limit try ask gpt 4 use wikipedia prompt  use agent tool via langchain  difficulty  m run book  ve use  develop apps gpt4 chatgpt  build intelligent chatbots  content generators   publish 2023  already code example deprecate  example  try something similar code provide page 114 book  langchainchatmodel import chatopenai langchainagent import loadtools  initializeagent  agenttype llm  chatopenai  modelname  gpt35  turbo   temperature0  tool  loadtool     wikipedia     llm  math    llm  llm  agent  initializeagent  tool  llm  agent  agenttype  zeroshotreactdescription  verbose  true  question      square root population capital country olympic games hold 2016     agentrun  question  see much deprecate  eg  initializeagent   look around stackoverflow  github  langchain python document come  langchainopenai import chatopenai langchaincoreoutputparser import stroutputparser langchaincoreprompts import chatprompttemplate langchainagent import  loadtools  createstructuredchatagent  agentexecutor  model  chatopenai  model  gpt4   temperature0  tool  loadtool     wikipedia    prompt  chatprompttemplatefromtemplate      research assistant  job retrieve information movie movie director  use follow tool   tool  use follow format  question  input question must answer thought  always think action  action take  one   toolnames   action input  input action observation  result action   thought  action  action input  observation repeat n time  thought  know final answer final answer  final answer original input question  need give number  information explanation necessary  begin  question  many movie director  year  movie  name  direct make  name   thought   agentscratchpad       agent  createstructuredchatagent  model  tool  prompt  agentexecutor  agentexecutor  agent  agent  tool  tool  agentexecutorinvoke     year     1991     name     thelma louise     m go run loop many movie  would like return one integer  case  6   seem like need give full thought process prompt  can not get run not include  tool    toolnames    agentscratchpad  prompt  per github post   frustrating thing eventually get correct answer  note throw error  valueerror  output parsing error occur  order pass error back agent try  pass  handleparsingerror  true  agentexecutor  error  could parse llm output  first  need find direct movie   thelma louise  1991  action  wikipedia action input    query    thelma louise   observation    thelma  louise  1991 american female buddy road film direct ridley scott write callie khouri  star geena davis thelma susan sarandon louise  two friend embark road trip unforeseen consequence  film become critical commercial success  receive six academy award nomination win one best original screenplay khouri  scott nominate best director  thought  ridley scott direct movie   thelma louise   need find many movie direct one  action  wikipedia action input    query    ridley scott filmography   observation  ridley scott english filmmaker  follow commercial breakthrough science fiction horror film alien  1979   well know work neo  noir dystopian science fiction film blade runner  1982   historical drama gladiator  2000   science fiction film martian  2015   scott direct 25 film know atmospheric  highly concentrated visual style  film also know strong female character  list film   thelma  louise   1  duellist  1977  2  alien  1979  3  blade runner  1982  4  legend  1985  5  someone watch  1987  6  black rain  1989  thought  ridley scott direct six movie   thelma louise   final answer  6 seem common    also  lastly    tell  see doc also  update agentexecutor  agentexecutor  agentexecutor  agent  agent  tool  tool  handleparsingerror  true  return    year    1991    name    thelma louise    output    agent stop due iteration limit time limit    question  use langchain combine gpt 4 wikipedia get answer query  want back integer ,author book develop apps gpt4 chatgpt  already answer mail  case someone else stumble upon question  find update code   githubcom  malywut  gptexample  update code look like  langchainopenai import chatopenai langchainagent import loadtools  createreactagent  agentexecutor langchain import hub llm  chatopenai  modelname  gpt35  turbo   tool  loadtool     wikipedia     llm  math    llm  llm  agent  createreactagent  tool  tool  llm  llm  prompt  hubpull    hwchase17  react     question      agentexecutor  agentexecutor  agent  agent  tool  tool  verbose  true  agentexecutorinvoke     input   question   hope help ,langchain agent parse error structuredchatagent wikipedia tool  handleparsingerrors hit limit try ask gpt 4 use wikipedia prompt  use agent tool via langchain  difficulty  m run book  ve use  develop apps gpt4 chatgpt  build intelligent chatbots  content generators   publish 2023  already code example deprecate  example  try something similar code provide page 114 book  langchainchatmodel import chatopenai langchainagent import loadtools  initializeagent  agenttype llm  chatopenai  modelname  gpt35  turbo   temperature0  tool  loadtool     wikipedia     llm  math    llm  llm  agent  initializeagent  tool  llm  agent  agenttype  zeroshotreactdescription  verbose  true  question      square root population capital country olympic games hold 2016     agentrun  question  see much deprecate  eg  initializeagent   look around stackoverflow  github  langchain python document come  langchainopenai import chatopenai langchaincoreoutputparser import stroutputparser langchaincoreprompts import chatprompttemplate langchainagent import  loadtools  createstructuredchatagent  agentexecutor  model  chatopenai  model  gpt4   temperature0  tool  loadtool     wikipedia    prompt  chatprompttemplatefromtemplate      research assistant  job retrieve information movie movie director  use follow tool   tool  use follow format  question  input question must answer thought  always think action  action take  one   toolnames   action input  input action observation  result action   thought  action  action input  observation repeat n time  thought  know final answer final answer  final answer original input question  need give number  information explanation necessary  begin  question  many movie director  year  movie  name  direct make  name   thought   agentscratchpad       agent  createstructuredchatagent  model  tool  prompt  agentexecutor  agentexecutor  agent  agent  tool  tool  agentexecutorinvoke     year     1991     name     thelma louise     m go run loop many movie  would like return one integer  case  6   seem like need give full thought process prompt  can not get run not include  tool    toolnames    agentscratchpad  prompt  per github post   frustrating thing eventually get correct answer  note throw error  valueerror  output parsing error occur  order pass error back agent try  pass  handleparsingerror  true  agentexecutor  error  could parse llm output  first  need find direct movie   thelma louise  1991  action  wikipedia action input    query    thelma louise   observation    thelma  louise  1991 american female buddy road film direct ridley scott write callie khouri  star geena davis thelma susan sarandon louise  two friend embark road trip unforeseen consequence  film become critical commercial success  receive six academy award nomination win one best original screenplay khouri  scott nominate best director  thought  ridley scott direct movie   thelma louise   need find many movie direct one  action  wikipedia action input    query    ridley scott filmography   observation  ridley scott english filmmaker  follow commercial breakthrough science fiction horror film alien  1979   well know work neo  noir dystopian science fiction film blade runner  1982   historical drama gladiator  2000   science fiction film martian  2015   scott direct 25 film know atmospheric  highly concentrated visual style  film also know strong female character  list film   thelma  louise   1  duellist  1977  2  alien  1979  3  blade runner  1982  4  legend  1985  5  someone watch  1987  6  black rain  1989  thought  ridley scott direct six movie   thelma louise   final answer  6 seem common    also  lastly    tell  see doc also  update agentexecutor  agentexecutor  agentexecutor  agent  agent  tool  tool  handleparsingerror  true  return    year    1991    name    thelma louise    output    agent stop due iteration limit time limit    question  use langchain combine gpt 4 wikipedia get answer query  want back integer  author book develop apps gpt4 chatgpt  already answer mail  case someone else stumble upon question  find update code   githubcom  malywut  gptexample  update code look like  langchainopenai import chatopenai langchainagent import loadtools  createreactagent  agentexecutor langchain import hub llm  chatopenai  modelname  gpt35  turbo   tool  loadtool     wikipedia     llm  math    llm  llm  agent  createreactagent  tool  tool  llm  llm  prompt  hubpull    hwchase17  react     question      agentexecutor  agentexecutor  agent  agent  tool  tool  verbose  true  agentexecutorinvoke     input   question   hope help ,Basic Understanding
"Error when calling Hugging Face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)","I'm following the huggingface tutorial here and it's giving me a strange error. When I run the following code: from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding from torch.utils.data import DataLoader raw_datasets = load_dataset(""glue"", ""mrpc"") Here is what I see: Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00<00:00, 3.35MB/s] Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00<00:00, 6.63MB/s] Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.89s/it] Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 127.92it/s] Traceback (most recent call last): File ""/Users/ameenizhac/Downloads/transformers_playground.py"", line 5, in <module> raw_datasets = load_dataset(""glue"", ""mrpc"") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py"", line 1782, in load_dataset builder_instance.download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 872, in download_and_prepare self._download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 967, in _download_and_prepare self._prepare_split(split_generator, **prepare_split_kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 1709, in _prepare_split split_info = self.info.splits[split_generator.name] ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py"", line 530, in __getitem__ instructions = make_file_instructions( ^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 112, in make_file_instructions name2filenames = { ^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 113, in <dictcomp> info.name: filenames_for_dataset_split( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 70, in filenames_for_dataset_split prefix = filename_prefix_for_split(dataset_name, split) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 54, in filename_prefix_for_split if os.path.basename(name) != name: ^^^^^^^^^^^^^^^^^^^^^^ File ""<frozen posixpath>"", line 142, in basename TypeError: expected str, bytes or os.PathLike object, not NoneType I don't know where to start because I don't understand where the error is coming from.","['python', 'nlp', 'huggingface', 'huggingface-datasets']",1,"I tried on my PC and on Google Colab. The strange thing is that on Colab it works, on my PC it does not. Anyway, a possible workaround is the following: raw_datasets = load_dataset(""SetFit/mrpc"") If you print it, you will see that the dataset is the same, it just has a different name: DatasetDict({ train: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 3668 }) test: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 1725 }) validation: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 408 }) })",2024-04-08 19:21:13,2024-04-11 13:25:21,456,https://stackoverflow.com/questions/78294720/error-when-calling-hugging-face-load-datasetglue-mrpc,"Error when calling Hugging Face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;) I'm following the huggingface tutorial here and it's giving me a strange error. When I run the following code: from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding from torch.utils.data import DataLoader raw_datasets = load_dataset(""glue"", ""mrpc"") Here is what I see: Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00<00:00, 3.35MB/s] Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00<00:00, 6.63MB/s] Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.89s/it] Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 127.92it/s] Traceback (most recent call last): File ""/Users/ameenizhac/Downloads/transformers_playground.py"", line 5, in <module> raw_datasets = load_dataset(""glue"", ""mrpc"") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py"", line 1782, in load_dataset builder_instance.download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 872, in download_and_prepare self._download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 967, in _download_and_prepare self._prepare_split(split_generator, **prepare_split_kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 1709, in _prepare_split split_info = self.info.splits[split_generator.name] ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py"", line 530, in __getitem__ instructions = make_file_instructions( ^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 112, in make_file_instructions name2filenames = { ^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 113, in <dictcomp> info.name: filenames_for_dataset_split( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 70, in filenames_for_dataset_split prefix = filename_prefix_for_split(dataset_name, split) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 54, in filename_prefix_for_split if os.path.basename(name) != name: ^^^^^^^^^^^^^^^^^^^^^^ File ""<frozen posixpath>"", line 142, in basename TypeError: expected str, bytes or os.PathLike object, not NoneType I don't know where to start because I don't understand where the error is coming from.",error call hugging face loaddataset   quot  glue  quot    quot  mrpc  quot    m follow huggingface tutorial s give strange error  run follow code  dataset import loaddataset transformer import autotokenizer  datacollatorwithpadde torchutilsdata import dataloader rawdataset  loaddataset    glue     mrpc   see  download datum  100                                                                                                                                           151k151k  0000  0000  335mb  s  download datum  100                                                                                                                                         111k111k  0000  0000  663mb  s  download datum file  100                                                                                                                                          33  0032  0000  1089s  it  extract data file  100                                                                                                                                          33  0000  0000  12792it  s  traceback  recent call last   file   users  ameenizhac  download  transformersplaygroundpy   line 5   module  rawdataset  loaddataset    glue     mrpc    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  loadpy   line 1782  loaddataset builderinstancedownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 872  downloadandprepare selfdownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 967   downloadandprepare selfpreparesplit  splitgenerator    preparesplitkwargs  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 1709   preparesplit splitinfo  selfinfosplit  splitgeneratorname   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  splitspy   line 530    getitem   instruction  makefileinstructions   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 112  makefileinstructions name2filename    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 113   dictcomp  infoname  filenamesfordatasetsplit   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 70  filenamesfordatasetsplit prefix  filenameprefixforsplit  datasetname  split   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 54  filenameprefixforsplit ospathbasename  name    name   file    frozen posixpath    line 142  basename typeerror  expect str  byte os  pathlike object  nonetype not know start not understand error come ,try pc google colab  strange thing colab work  pc  anyway  possible workaround following  rawdataset  loaddataset    setfit  mrpc   print  see dataset  different name  datasetdict   train  dataset   feature    text1    text2    label    idx    labeltext    numrows  3668   test  dataset   feature    text1    text2    label    idx    labeltext    numrows  1725   validation  dataset   feature    text1    text2    label    idx    labeltext    numrows  408    ,error call hugging face loaddataset   quot  glue  quot    quot  mrpc  quot    m follow huggingface tutorial s give strange error  run follow code  dataset import loaddataset transformer import autotokenizer  datacollatorwithpadde torchutilsdata import dataloader rawdataset  loaddataset    glue     mrpc   see  download datum  100                                                                                                                                           151k151k  0000  0000  335mb  s  download datum  100                                                                                                                                         111k111k  0000  0000  663mb  s  download datum file  100                                                                                                                                          33  0032  0000  1089s  it  extract data file  100                                                                                                                                          33  0000  0000  12792it  s  traceback  recent call last   file   users  ameenizhac  download  transformersplaygroundpy   line 5   module  rawdataset  loaddataset    glue     mrpc    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  loadpy   line 1782  loaddataset builderinstancedownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 872  downloadandprepare selfdownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 967   downloadandprepare selfpreparesplit  splitgenerator    preparesplitkwargs  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 1709   preparesplit splitinfo  selfinfosplit  splitgeneratorname   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  splitspy   line 530    getitem   instruction  makefileinstructions   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 112  makefileinstructions name2filename    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 113   dictcomp  infoname  filenamesfordatasetsplit   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 70  filenamesfordatasetsplit prefix  filenameprefixforsplit  datasetname  split   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 54  filenameprefixforsplit ospathbasename  name    name   file    frozen posixpath    line 142  basename typeerror  expect str  byte os  pathlike object  nonetype not know start not understand error come  try pc google colab  strange thing colab work  pc  anyway  possible workaround following  rawdataset  loaddataset    setfit  mrpc   print  see dataset  different name  datasetdict   train  dataset   feature    text1    text2    label    idx    labeltext    numrows  3668   test  dataset   feature    text1    text2    label    idx    labeltext    numrows  1725   validation  dataset   feature    text1    text2    label    idx    labeltext    numrows  408    ,Basic Understanding
What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax?,"I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?","['machine-learning', 'nlp', 'word2vec', 'hierarchical', 'softmax']",1,"While there are many slightly-different ways to think about it, it may help to consider the values in the (trained, frozen) neural network as associated with edges moreso than vertexes (nodes). The network ""projection weights"" leading from an (abstract) one-hot encoding of each known word into the network are essentially the actual per-word word-vectors. Assuming a common case of 300d vectors, the 300 edges from the single-word node to the inner nodes are that words. Then, there's another set of edge-weights from the internal activation to the ""output"" layer, which is offering the network's training goal of in-context word-projection. In the (more common & usual default) negative-sampling approach, each output node corresponds to a single predictable word. That's a very easy output-shape to visualize. And the value of negative-sampling is that you only check the activations of the desired word, and n more randomly chosen negative words, to perform your training updates. That's way less calculation than if you checked the output values at all V (size of vocabulary) output nodes, and still works pretty well, and doesn't get more expensive with larger vocabularies (unless you choose for other reasons to also increase your choice of n negative samples). In hierarchical softmax, the interpretation of the output nodes is more complicated. Rather than the activation at a single node indicating the prediction of a single word, a (varying) set of nodes must have the right on/off activations to communicate the variable-length huffman-code of a word. So in HS, to backprop the network more towards predicting your desired ""positive"" word (from one input context), the algorithm considers just those nodes involved in that words nique coding – a smaller set of nodes for the most-ommon words, but a larger set of nodes for rarer words – and nudges each of them more towards the pattern that predicts the desired word. Again, you get the sparse training efficiency of updating only a tiny subset, far smaller than all V nodes, each training-step. But, the cost will vary based on the target word, and grow with the log of V as vocabulary-size grows. Further, as the original/naive assignment of codes is based strictly on word-frequency, quite-dissimilar words may have very-similar codings, perhaps causing more word-to-word interference. (There were hints in the original word2vec.c release of refining the HS word-codings over time to ensure similar words share similar Huffman codings, but I've seen litle followup on that idea, perhaps because of the dominance of negative-sampling.) So, in an HS network, the weights from the inner-activations, to the output nodes, are tuned to indicate, by Huffman code, which word is the preferred prediction from a context. In the word2vec implementation I'm most familiar with, Python Gensim, these ""hidden to output"" weights are not even randomly initialized at the beginning, instead left as 0.0 – and I think this was directly copied from the initialization of Google's word2vec.c release. But, as soon as training begins, the explict random initialization of those ""input weights"" (initial random input word-vectors) means those weights are immediately perturbed in a way that at 1st is nearly all random but becomes more helpful over the SGD training. So: those inner weights start 0.0 but quickly start reflecting the influence of the (initially-random) word vectors and training examples they're usually not harvested from the network after training, like the final word-vectors are – but would be kept around if you wanted to continue training later, and perhaps provide a running start to future training runs (In the negative-sampling case, some research suggested those hidden-to-output weights could also be interpreted as same embedding_size per-word vectors, with some usefulness: see paper by Mitra et al at Microsoft about ""Dual Word Embeddings"". But given the varying-length codings of output words in the HS case, extracting/interpretating those output-weights would be trickier.) In the implementation I'm most familiar with, the It may help to think instead of the shallow neural network's ""projection layer"" (effectively the word-vectors themselves, as each virtual ""single node"" 1-hot word has its embedding_size out-weights) and ""hidden layer",2024-04-06 18:15:41,2024-04-11 21:30:15,148,https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax,"What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax? I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?",  39  inside inner vertex word2vec hierarchical softmax  question hierarchical softmax  actually  quite understand store inner vertex  leaf vertex   clearly understand main idea algorithm  step calculate dot product input word embed word embed inner vertice  vector inside inner vertex  randomly initialize vector size equal embeddingsize coordinate change due backpropagation step stop ,many slightly  different way think  may help consider value  train  frozen  neural network associate edge moreso vertex  node   network   projection weight  lead  abstract  one  hot encoding know word network essentially actual per  word word  vector  assume common case 300d vector  300 edge single  word node inner node word   be another set edge  weight internal activation   output  layer  offer network s training goal in  context word  projection   common  usual default  negative  sample approach  output node correspond single predictable word  s easy output  shape visualize  value negative  sample check activation desire word  n randomly choose negative word  perform training update  s way less calculation check output value v  size vocabulary  output node  still work pretty well  not get expensive large vocabulary  unless choose reason also increase choice n negative sample   hierarchical softmax  interpretation output node complicated  rather activation single node indicate prediction single word   vary  set node must right on  off activation communicate variable  length huffman  code word  hs  backprop network towards predict desire   positive  word  one input context   algorithm consider node involve word nique coding  small set node most  ommon word  large set node rare word  nudge towards pattern predict desire word   get sparse training efficiency update tiny subset  far small v node  training  step   cost vary base target word  grow log v vocabulary  size grow   original  naive assignment code base strictly word  frequency  quite  dissimilar word may very  similar coding  perhaps cause word  to  word interference   hint original word2vecc release refine hs word  coding time ensure similar word share similar huffman coding   ve see litle followup idea  perhaps dominance negative  sampling    hs network  weight inner  activation  output node  tune indicate  huffman code  word prefer prediction context  word2vec implementation  m familiar  python gensim    hidden output  weight even randomly initialize beginning  instead leave 00  think directly copy initialization google s word2vecc release   soon training begin  explict random initialization   input weight   initial random input word  vector  mean weight immediately perturb way 1st nearly random become helpful sgd training   inner weight start 00 quickly start reflect influence  initially  random  word vector training example be usually harvest network training  like final word  vector  would keep around want continue training later  perhaps provide run start future training run  negative  sampling case  research suggest hide  to  output weight could also interpret embeddingsize per  word vector  usefulness  see paper mitra et al microsoft   dual word embeddings   give vary  length coding output word hs case  extracting  interpretate output  weight would tricky   implementation  m familiar  may help think instead shallow neural network s   projection layer   effectively word  vector  virtual   single node  1  hot word embeddingsize out  weight    hidden layer,  39  inside inner vertex word2vec hierarchical softmax  question hierarchical softmax  actually  quite understand store inner vertex  leaf vertex   clearly understand main idea algorithm  step calculate dot product input word embed word embed inner vertice  vector inside inner vertex  randomly initialize vector size equal embeddingsize coordinate change due backpropagation step stop  many slightly  different way think  may help consider value  train  frozen  neural network associate edge moreso vertex  node   network   projection weight  lead  abstract  one  hot encoding know word network essentially actual per  word word  vector  assume common case 300d vector  300 edge single  word node inner node word   be another set edge  weight internal activation   output  layer  offer network s training goal in  context word  projection   common  usual default  negative  sample approach  output node correspond single predictable word  s easy output  shape visualize  value negative  sample check activation desire word  n randomly choose negative word  perform training update  s way less calculation check output value v  size vocabulary  output node  still work pretty well  not get expensive large vocabulary  unless choose reason also increase choice n negative sample   hierarchical softmax  interpretation output node complicated  rather activation single node indicate prediction single word   vary  set node must right on  off activation communicate variable  length huffman  code word  hs  backprop network towards predict desire   positive  word  one input context   algorithm consider node involve word nique coding  small set node most  ommon word  large set node rare word  nudge towards pattern predict desire word   get sparse training efficiency update tiny subset  far small v node  training  step   cost vary base target word  grow log v vocabulary  size grow   original  naive assignment code base strictly word  frequency  quite  dissimilar word may very  similar coding  perhaps cause word  to  word interference   hint original word2vecc release refine hs word  coding time ensure similar word share similar huffman coding   ve see litle followup idea  perhaps dominance negative  sampling    hs network  weight inner  activation  output node  tune indicate  huffman code  word prefer prediction context  word2vec implementation  m familiar  python gensim    hidden output  weight even randomly initialize beginning  instead leave 00  think directly copy initialization google s word2vecc release   soon training begin  explict random initialization   input weight   initial random input word  vector  mean weight immediately perturb way 1st nearly random become helpful sgd training   inner weight start 00 quickly start reflect influence  initially  random  word vector training example be usually harvest network training  like final word  vector  would keep around want continue training later  perhaps provide run start future training run  negative  sampling case  research suggest hide  to  output weight could also interpret embeddingsize per  word vector  usefulness  see paper mitra et al microsoft   dual word embeddings   give vary  length coding output word hs case  extracting  interpretate output  weight would tricky   implementation  m familiar  may help think instead shallow neural network s   projection layer   effectively word  vector  virtual   single node  1  hot word embeddingsize out  weight    hidden layer,Implementation Issues
Extracting dates from a sentence in spaCy,"I have a string like so: ""The dates are from 30 June 2019 to 1 January 2022 inclusive"" I want to extract the dates from this string using spaCy. Here is my function so far: def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": dates_with_year.append(ent.text) return dates_with_year This returns the following output: ['30 June 2019 to 1 January 2022'] However, I want output like: ['30 June 2019', '1 January 2022']","['python', 'regex', 'nlp', 'spacy', 'named-entity-recognition']",1,"The issue is that ""to"" is considered part of the date. So when you do for ent in doc.ents , your loop only has one iteration, as ""30 June 2019 to 1 January 2022"" is considered one entity. As you don't want this behaviour, you can amend your function to split on ""to"" : def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": for ent_txt in ent.text.split(""to""): dates_with_year.append(ent_txt.strip()) return dates_with_year This will correctly handle dates like these, as well as single dates, and strings with multiple dates: txt = """""" The dates are from 30 June 2019 to 1 January 2022 inclusive. And oddly also 5 January 2024. And exclude 21 July 2019 until 23 July 2019. """""" extract_dates_with_year(txt) # Output: [ '30 June 2019', '1 January 2022', '5 January 2024', '21 July 2019', '23 July 2019' ]",2024-04-06 17:12:39,2024-04-06 19:15:08,78,https://stackoverflow.com/questions/78285241/extracting-dates-from-a-sentence-in-spacy,"Extracting dates from a sentence in spaCy I have a string like so: ""The dates are from 30 June 2019 to 1 January 2022 inclusive"" I want to extract the dates from this string using spaCy. Here is my function so far: def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": dates_with_year.append(ent.text) return dates_with_year This returns the following output: ['30 June 2019 to 1 January 2022'] However, I want output like: ['30 June 2019', '1 January 2022']",extract date sentence spacy string like    date 30 june 2019 1 january 2022 inclusive  want extract date string use spacy  function far  def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   dateswithyearappend  enttext  return dateswithyear return follow output    30 june 2019 1 january 2022   however  want output like    30 june 2019    1 january 2022  ,issue    consider part date  ent docent  loop one iteration    30 june 2019 1 january 2022  consider one entity  not want behaviour  amend function split     def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   enttxt enttextsplit       dateswithyearappend  enttxtstrip    return dateswithyear correctly handle date like  well single date  string multiple date  txt      date 30 june 2019 1 january 2022 inclusive  oddly also 5 january 2024  exclude 21 july 2019 23 july 2019      extractdateswithyear  txt   output    30 june 2019    1 january 2022    5 january 2024    21 july 2019    23 july 2019  ,extract date sentence spacy string like    date 30 june 2019 1 january 2022 inclusive  want extract date string use spacy  function far  def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   dateswithyearappend  enttext  return dateswithyear return follow output    30 june 2019 1 january 2022   however  want output like    30 june 2019    1 january 2022   issue    consider part date  ent docent  loop one iteration    30 june 2019 1 january 2022  consider one entity  not want behaviour  amend function split     def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   enttxt enttextsplit       dateswithyearappend  enttxtstrip    return dateswithyear correctly handle date like  well single date  string multiple date  txt      date 30 june 2019 1 january 2022 inclusive  oddly also 5 january 2024  exclude 21 july 2019 23 july 2019      extractdateswithyear  txt   output    30 june 2019    1 january 2022    5 january 2024    21 july 2019    23 july 2019  ,Implementation Issues
What is the best function/stage to use tokenizer in Pytorch&#39;s data processing?,"I am subclassing torch.utils.data.Dataset and writing a collate function to be passed to Dataloader's dataset and collate_fn argument respectively. Between Dataset's __getitem__ or collate_fn I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?) collate_fn seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.","['pytorch', 'nlp']",2,"It is more efficient applying it on the collate_fn as the tokenizer accepts a list of texts (the batch), specially if the sequences need to be padded at a fixed length, you would be processing batch-wise",2024-04-06 15:14:45,2024-04-14 18:37:04,1650,https://stackoverflow.com/questions/78284866/what-is-the-best-function-stage-to-use-tokenizer-in-pytorchs-data-processing,"What is the best function/stage to use tokenizer in Pytorch&#39;s data processing? I am subclassing torch.utils.data.Dataset and writing a collate function to be passed to Dataloader's dataset and collate_fn argument respectively. Between Dataset's __getitem__ or collate_fn I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?) collate_fn seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.",good function  stage use tokenizer pytorch   39  datum processing  subclasse torchutilsdata  dataset write collate function pass dataloader s dataset collatefn argument respectively  dataset s   getitem   collatefn wondering would well function use tokenizer  huggingface s fasttokenizer   another well option two not know   collatefn seem good option since use tokenizer tokenize add padding full batch together  also help tokenization speed  sure cause problem  future teammate start use  customize code ,efficient apply collatefn tokenizer accept list text  batch   specially sequence need pad fix length  would process batch  wise,good function  stage use tokenizer pytorch   39  datum processing  subclasse torchutilsdata  dataset write collate function pass dataloader s dataset collatefn argument respectively  dataset s   getitem   collatefn wondering would well function use tokenizer  huggingface s fasttokenizer   another well option two not know   collatefn seem good option since use tokenizer tokenize add padding full batch together  also help tokenization speed  sure cause problem  future teammate start use  customize code  efficient apply collatefn tokenizer accept list text  batch   specially sequence need pad fix length  would process batch  wise,Task-Specific Queries
Python Spacy Pattern- How to tag a word based on another word?,"I'm trying to write a pattern that would tag the whole word as unit based on one substring. Here's example: terms = [{'ent': ""UNIT"", 'patterns':[ [{'lemma':'liter'}]]}] text = ""There were 46 kiloliters of juice available"" I wanna tag 'kiloliters' as Unit based on this pattern. I tried using 'lemma"" but it won't work in this case.","['python', 'nlp', 'spacy']",1,"You haven't said which model you're using so I'll use en_web_core_sm : import spacy from spacy.matcher import Matcher nlp = spacy.load(""en_core_web_sm"") matcher = Matcher(nlp.vocab) doc = nlp(""There were 46 kiloliters of juice available"") The first thing is that none of these have an ent_type of UNIT : for tok in doc: print(f""'{tok}': ent_type: '{tok.ent_type_}', lemma: '{tok.lemma_}'"") 'There': ent_type: '', lemma: 'there' 'were': ent_type: '', lemma: 'be' '46': ent_type: 'CARDINAL', lemma: '46' 'kiloliters': ent_type: '', lemma: 'kiloliter' 'of': ent_type: '', lemma: 'of' 'juice': ent_type: '', lemma: 'juice' 'available': ent_type: '', lemma: 'available' Also, as you can see, the lemma of kiloliters is kiloliter . This is a bit annoying as you don't want to have to specify milliliters, liters etc. separately. One alternative is to look for a CARDINAL token (which also includes words e.g. ""two liters"" ) followed a regex: doc = nlp("""""" There were 46 kiloliters of juice available. I could not drink more than two liters a day. I would only give a child 500 milliliters. """""" ) pattern = [{'ENT_TYPE': 'CARDINAL'}, {""TEXT"": {""REGEX"": ""^.*(liter)s?$""}}] matcher.add(""unit"", [pattern]) matches = matcher(doc, as_spans=True) for span in matches: print(span[-1].text) Output: kiloliters liters milliliters",2024-04-03 08:07:09,2024-04-03 08:54:15,47,https://stackoverflow.com/questions/78266192/python-spacy-pattern-how-to-tag-a-word-based-on-another-word,"Python Spacy Pattern- How to tag a word based on another word? I'm trying to write a pattern that would tag the whole word as unit based on one substring. Here's example: terms = [{'ent': ""UNIT"", 'patterns':[ [{'lemma':'liter'}]]}] text = ""There were 46 kiloliters of juice available"" I wanna tag 'kiloliters' as Unit based on this pattern. I tried using 'lemma"" but it won't work in this case.",python spacy pattern tag word base another word   m try write pattern would tag whole word unit base one substring  s example  term     ent     unit    pattern       lemma    liter       text    46 kiloliter juice available  wan na tag  kiloliter  unit base pattern  try use  lemma  will not work case ,not say model be use will use enwebcoresm  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   matcher  matcher  nlpvocab  doc  nlp    46 kiloliter juice available   first thing none enttype unit  tok doc  print  f    tok    enttype    tokenttype     lemma    toklemma       there   enttype     lemma   there   be   enttype     lemma   be   46   enttype   cardinal   lemma   46   kiloliter   enttype     lemma   kiloliter   of   enttype     lemma   of   juice   enttype     lemma   juice   available   enttype     lemma   available  also  see  lemma kiloliter kiloliter  bit annoying not want specify milliliter  liter etc  separately  one alternative look cardinal token  also include word eg    two liter   follow regex  doc  nlp      46 kiloliter juice available  could drink two liter day  would give child 500 milliliter       pattern     enttype    cardinal       text      regex         liter        matcheradd    unit    pattern   match  matcher  doc  asspan  true  span match  print  span  1  text  output  kiloliter liter milliliter,python spacy pattern tag word base another word   m try write pattern would tag whole word unit base one substring  s example  term     ent     unit    pattern       lemma    liter       text    46 kiloliter juice available  wan na tag  kiloliter  unit base pattern  try use  lemma  will not work case  not say model be use will use enwebcoresm  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   matcher  matcher  nlpvocab  doc  nlp    46 kiloliter juice available   first thing none enttype unit  tok doc  print  f    tok    enttype    tokenttype     lemma    toklemma       there   enttype     lemma   there   be   enttype     lemma   be   46   enttype   cardinal   lemma   46   kiloliter   enttype     lemma   kiloliter   of   enttype     lemma   of   juice   enttype     lemma   juice   available   enttype     lemma   available  also  see  lemma kiloliter kiloliter  bit annoying not want specify milliliter  liter etc  separately  one alternative look cardinal token  also include word eg    two liter   follow regex  doc  nlp      46 kiloliter juice available  could drink two liter day  would give child 500 milliliter       pattern     enttype    cardinal       text      regex         liter        matcheradd    unit    pattern   match  matcher  doc  asspan  true  span match  print  span  1  text  output  kiloliter liter milliliter,Library/Tool-Based Queries
Gensim&#39;s Doc2Vec with documents in multiple languages,"I'm building a content based recommender system using similarities on vector representations of documents. My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents. Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9. Why is this happening? import pandas as pd from pathlib import Path from gensim.models.doc2vec import TaggedDocument, Doc2Vec from gensim.utils import simple_preprocess import numpy as np from tqdm import tqdm from sklearn.metrics.pairwise import cosine_similarity data_folder = Path.cwd().parent / 'data' transformed_folder = data_folder / 'transformed' BOOKS_PATH = Path(transformed_folder, ""books_final.csv"") import seaborn as sns book_df = pd.read_csv(BOOKS_PATH) languages=book_df.language.unique() training_corpus = np.empty(len(book_df), dtype=object) for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))): training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)]) model=Doc2Vec(vector_size=50, min_count=2, epochs=40) model.build_vocab(training_corpus) #train the model model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs) #get the keys of the trained model model_keys=list(model.dv.key_to_index.keys()) matrix = model.dv.vectors similarity_matrix = cosine_similarity(matrix) indices_for_language = {} for language, group_df in book_df.groupby('language'): indices_for_language[language] = group_df.index.to_list() sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']] #make a heatmap sns.heatmap(sim_japanese_italian) Heatmap of similarities between Italian and Japanese documents","['python', 'nlp', 'gensim', 'recommendation-engine', 'doc2vec']",1,"You corpus is a bit small compared to published results for the Doc2Vec (""Paragraph Vectors"") algorithm, which is usually demonstrated on set of tens-of-thousands to millions of documents. In particular, if you only have (4742-4655=) 93 non-English description texts, and those are split over 16 non-English languages, it seems unlikely to me that any of the words from those other languages will have enough in-context usage examples to either (1) survive the min_count cutoff (which generally shouldn't be as low as 2!); or (2) obtain meaningful/generalizable implications in the model. That is: all those very-rare words will have fairly idiosyncratic/arbitrary meanings, and description texts filled with all rare/weakly-understood words will tend to have weak, almost random doc-vectors. So, spurious similarities aren't that much of a surprise. If those ""non-English"" description texts do have a few English-looking or omnilingual tokens in them – stray words or false cognates or punctuation or glitches – those might also dominate the description -to- description comparison. You should check, on your suspiciously-too-similar pairs, how many of the words in each document are actually in the trained model – keeping in mind words not in the trained model are essentially ignored in training or post-rtraining inference. (A document of 100 words, in a language where there are only a few other 100-word documents, might in fact be dominated by single-appearance words, that don't even meet the min_count=2 , and so when slimmed to the surviving words be short docs filled with weakly-understood words.) If your similarity scores between English documents are generally sensible, then the model is doing what it can, where it has enough data. You really want this family of algorithms – word2vec, Doc2Vec , FastText, etc – to have many, varied, representative examples of any word's usage in sensible contrasting contexts for it to be able to do any meaningful later modeling of those words (and the documents that contain them). So: if you can't manage to use a min_count of at least the usual default ( 5 ), or ideally even higher, you likely don't have sufficient data for these algorithms to show their strengths. Extending your corpus – even with other texts not of direct interest, but of sufficient variety to better represent the languages used in your texts of interest – might help. (That is: even if your universe-of-recommendations is <5000 descriptions , mixing another 200,000 similar short blurbs from other sources, so that words and langguages of interest have enough coverage, could be worthwhile.)",2024-04-02 15:47:30,2024-04-12 18:55:31,141,https://stackoverflow.com/questions/78262529/gensims-doc2vec-with-documents-in-multiple-languages,"Gensim&#39;s Doc2Vec with documents in multiple languages I'm building a content based recommender system using similarities on vector representations of documents. My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents. Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9. Why is this happening? import pandas as pd from pathlib import Path from gensim.models.doc2vec import TaggedDocument, Doc2Vec from gensim.utils import simple_preprocess import numpy as np from tqdm import tqdm from sklearn.metrics.pairwise import cosine_similarity data_folder = Path.cwd().parent / 'data' transformed_folder = data_folder / 'transformed' BOOKS_PATH = Path(transformed_folder, ""books_final.csv"") import seaborn as sns book_df = pd.read_csv(BOOKS_PATH) languages=book_df.language.unique() training_corpus = np.empty(len(book_df), dtype=object) for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))): training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)]) model=Doc2Vec(vector_size=50, min_count=2, epochs=40) model.build_vocab(training_corpus) #train the model model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs) #get the keys of the trained model model_keys=list(model.dv.key_to_index.keys()) matrix = model.dv.vectors similarity_matrix = cosine_similarity(matrix) indices_for_language = {} for language, group_df in book_df.groupby('language'): indices_for_language[language] = group_df.index.to_list() sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']] #make a heatmap sns.heatmap(sim_japanese_italian) Heatmap of similarities between Italian and Japanese documents",gensim   39  doc2vec document multiple language  m build content base recommender system use similarity vector representation document  document description book  english  language  use gensim s doc2vec building vector representation document  base understanding model  document different language low similarity  word even overlap  s happen  document different language dataset high similarity 09  happen  import panda pd pathlib import path gensimmodelsdoc2vec import taggeddocument  doc2vec gensimutil import simplepreprocess import numpy np tqdm import tqdm sklearnmetricspairwise import cosinesimilarity datafolder  pathcwd   parent   datum  transformedfolder  datafold   transform  bookspath  path  transformedfolder    booksfinalcsv   import seaborn sns bookdf  pdreadcsv  bookspath  language  bookdflanguageunique   trainingcorpus  npempty  len  bookdf   dtype  object    isbn  desc  tqdm  enumerate  zip  bookdf   isbn    bookdf   description       trainingcorpus    taggeddocument  simplepreprocess  desc    str     model  doc2vec  vectorsize50  mincount2  epochs40  modelbuildvocab  trainingcorpus   train model modeltrain  trainingcorpus  totalexample  modelcorpuscount  epoch  modelepoch   get key train model modelkey  list  modeldvkeytoindexkeys    matrix  modeldvvectors similaritymatrix  cosinesimilarity  matrix  indicesforlanguage    language  groupdf bookdfgroupby   language    indicesforlanguage  language   groupdfindextolist   simjapaneseitalian  similaritymatrix  indicesforlanguage   japanese       indicesforlanguage   italian     make heatmap snsheatmap  simjapaneseitalian  heatmap similaritie italian japanese document,corpus bit small compare publish result doc2vec    paragraph vectors   algorithm  usually demonstrate set ten  of  thousand million document  particular   4742  4655  93 non  english description text  split 16 non  english language  seem unlikely word language enough in  context usage example either  1  survive mincount cutoff  generally not low 2     2  obtain meaningful  generalizable implication model   very  rare word fairly idiosyncratic  arbitrary meaning  description text fill rare  weakly  understand word tend weak  almost random doc  vector   spurious similarity not much surprise    non  english  description text english  looking omnilingual token  stray word false cognate punctuation glitch  might also dominate description to description comparison  check  suspiciously  too  similar pair  many word document actually train model  keep mind word train model essentially ignore training post  rtraining inference   document 100 word  language 100  word document  might fact dominate single  appearance word  not even meet mincount2  slimme surviving word short doc fill weakly  understand word   similarity score english document generally sensible  model  enough datum  really want family algorithm  word2vec  doc2vec  fasttext  etc  many  varied  representative example word s usage sensible contrasting context able meaningful later modeling word  document contain    can not manage use mincount least usual default  5   ideally even higher  likely not sufficient data algorithm show strength  extend corpus  even text direct interest  sufficient variety well represent language use text interest  might help    even universe  of  recommendation  5000 description  mix another 200000 similar short blurb source  word langguage interest enough coverage  could worthwhile  ,gensim   39  doc2vec document multiple language  m build content base recommender system use similarity vector representation document  document description book  english  language  use gensim s doc2vec building vector representation document  base understanding model  document different language low similarity  word even overlap  s happen  document different language dataset high similarity 09  happen  import panda pd pathlib import path gensimmodelsdoc2vec import taggeddocument  doc2vec gensimutil import simplepreprocess import numpy np tqdm import tqdm sklearnmetricspairwise import cosinesimilarity datafolder  pathcwd   parent   datum  transformedfolder  datafold   transform  bookspath  path  transformedfolder    booksfinalcsv   import seaborn sns bookdf  pdreadcsv  bookspath  language  bookdflanguageunique   trainingcorpus  npempty  len  bookdf   dtype  object    isbn  desc  tqdm  enumerate  zip  bookdf   isbn    bookdf   description       trainingcorpus    taggeddocument  simplepreprocess  desc    str     model  doc2vec  vectorsize50  mincount2  epochs40  modelbuildvocab  trainingcorpus   train model modeltrain  trainingcorpus  totalexample  modelcorpuscount  epoch  modelepoch   get key train model modelkey  list  modeldvkeytoindexkeys    matrix  modeldvvectors similaritymatrix  cosinesimilarity  matrix  indicesforlanguage    language  groupdf bookdfgroupby   language    indicesforlanguage  language   groupdfindextolist   simjapaneseitalian  similaritymatrix  indicesforlanguage   japanese       indicesforlanguage   italian     make heatmap snsheatmap  simjapaneseitalian  heatmap similaritie italian japanese document corpus bit small compare publish result doc2vec    paragraph vectors   algorithm  usually demonstrate set ten  of  thousand million document  particular   4742  4655  93 non  english description text  split 16 non  english language  seem unlikely word language enough in  context usage example either  1  survive mincount cutoff  generally not low 2     2  obtain meaningful  generalizable implication model   very  rare word fairly idiosyncratic  arbitrary meaning  description text fill rare  weakly  understand word tend weak  almost random doc  vector   spurious similarity not much surprise    non  english  description text english  looking omnilingual token  stray word false cognate punctuation glitch  might also dominate description to description comparison  check  suspiciously  too  similar pair  many word document actually train model  keep mind word train model essentially ignore training post  rtraining inference   document 100 word  language 100  word document  might fact dominate single  appearance word  not even meet mincount2  slimme surviving word short doc fill weakly  understand word   similarity score english document generally sensible  model  enough datum  really want family algorithm  word2vec  doc2vec  fasttext  etc  many  varied  representative example word s usage sensible contrasting context able meaningful later modeling word  document contain    can not manage use mincount least usual default  5   ideally even higher  likely not sufficient data algorithm show strength  extend corpus  even text direct interest  sufficient variety well represent language use text interest  might help    even universe  of  recommendation  5000 description  mix another 200000 similar short blurb source  word langguage interest enough coverage  could worthwhile  ,Implementation Issues
Get previous sentence while using SpaCy matcher,"I am running a SpaCy Matcher line-by-line on a text file. My file has each text entry on a separate line. I am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. I am able to get the first two, but I am having trouble getting the previous sentence , given that there isn't a sentence index (from this post ). Here is my code: with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: string_id = nlp.vocab.strings[match_id] span = doc[start:end] for sent in doc.sents: if matcher(sent): instances.append(pd.Series({""instance"":str(span.text), ""sentence"":str(sent.text), ""previous_sentence"":str(sent[-1].text)})) I understand that the bolded part is giving me the previous token, not sentence (I tried to get around this with the list, but it doesn't work). Any advice for retrieving the previous sentence would be greatly appreciated. Thank you!","['python', 'nlp', 'spacy']",1,"Adjustments: Track Previous Sentence: We now maintain a prev_sent variable that tracks the previous sentence as we iterate through all sentences in a document. Matcher Usage: We only need to create the Matcher instance once, outside the loop through lines in the file, and then apply it to each sentence within the loop. This is more efficient than recreating it for every line. Check for Previous Sentence: We handle cases where there might not be a previous sentence (e.g., the match is found in the first sentence of the document) by checking if prev_sent is None. If it is, we set the ""previous_sentence"" field to ""N/A"" or any placeholder text you find suitable. import spacy from spacy.matcher import Matcher import pandas as pd # Load the SpaCy model nlp = spacy.load(""en_core_web_sm"") # Adjust model as necessary # Define your pattern here pattern = [{""LOWER"": ""example""}] # Example pattern # Initialize matcher with the vocab matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) instances = [] # List to hold match details with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) prev_sent = None # Variable to keep track of the previous sentence for sent in doc.sents: matches = matcher(sent) if matches: for match_id, start, end in matches: instance_text = sent[start:end].text # The matched instance current_sentence = sent.text previous_sentence = prev_sent.text if prev_sent else ""N/A"" # Handle the case where there's no previous sentence # Append the extracted information to your instances list instances.append(pd.Series({""instance"": instance_text, ""sentence"": current_sentence, ""previous_sentence"": previous_sentence})) prev_sent = sent # Update the previous sentence for the next iteration # Convert instances list to DataFrame df_instances = pd.DataFrame(instances) print(df_instances)",2024-04-02 00:14:51,2024-04-02 05:16:42,50,https://stackoverflow.com/questions/78258373/get-previous-sentence-while-using-spacy-matcher,"Get previous sentence while using SpaCy matcher I am running a SpaCy Matcher line-by-line on a text file. My file has each text entry on a separate line. I am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. I am able to get the first two, but I am having trouble getting the previous sentence , given that there isn't a sentence index (from this post ). Here is my code: with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: string_id = nlp.vocab.strings[match_id] span = doc[start:end] for sent in doc.sents: if matcher(sent): instances.append(pd.Series({""instance"":str(span.text), ""sentence"":str(sent.text), ""previous_sentence"":str(sent[-1].text)})) I understand that the bolded part is giving me the previous token, not sentence (I tried to get around this with the list, but it doesn't work). Any advice for retrieving the previous sentence would be greatly appreciated. Thank you!",get previous sentence use spacy matcher run spacy matcher line  by  line text file  file text entry separate line  try extract 1  match instance  2  full sentence  3  previous sentence  able get first two  trouble get previous sentence  give not sentence index  post   code  open   filetxt    r   f  line iter  freadline      doc  nlp  line  matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   match  matcher  doc  matchid  start  end match  stringid  nlpvocabstrings  matchid  span  doc  start  end  send docsent  matcher  send   instancesappend  pd  series     instance   str  spantext     sentence   str  senttext     previoussentence   str  send  1  text     understand bolde part give previous token  sentence  try get around list  not work   advice retrieve previous sentence would greatly appreciate  thank ,adjustment  track previous sentence  maintain prevsent variable track previous sentence iterate sentence document  matcher usage  need create matcher instance  outside loop line file  apply sentence within loop  efficient recreate every line  check previous sentence  handle case might previous sentence  eg  match find first sentence document  check prevsent none   set   previoussentence  field   n  a  placeholder text find suitable  import spacy spacymatcher import matcher import panda pd  load spacy model nlp  spacyload    encorewebsm    adjust model necessary  define pattern pattern      lower     example     example pattern  initialize matcher vocab matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   instance     list hold match detail open   filetxt    r   f  line iter  freadline      doc  nlp  line  prevsent  none  variable keep track previous sentence send docsent  match  matcher  send  match  matchid  start  end match  instancetext  send  start  end  text  match instance currentsentence  senttext previoussentence  prevsenttext prevsent else   n  a   handle case s previous sentence  append extract information instance list instancesappend  pd  series     instance   instancetext    sentence   currentsentence    previoussentence   previoussentence    prevsent  send  update previous sentence next iteration  convert instance list dataframe dfinstance  pd  dataframe  instance  print  dfinstance ,get previous sentence use spacy matcher run spacy matcher line  by  line text file  file text entry separate line  try extract 1  match instance  2  full sentence  3  previous sentence  able get first two  trouble get previous sentence  give not sentence index  post   code  open   filetxt    r   f  line iter  freadline      doc  nlp  line  matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   match  matcher  doc  matchid  start  end match  stringid  nlpvocabstrings  matchid  span  doc  start  end  send docsent  matcher  send   instancesappend  pd  series     instance   str  spantext     sentence   str  senttext     previoussentence   str  send  1  text     understand bolde part give previous token  sentence  try get around list  not work   advice retrieve previous sentence would greatly appreciate  thank  adjustment  track previous sentence  maintain prevsent variable track previous sentence iterate sentence document  matcher usage  need create matcher instance  outside loop line file  apply sentence within loop  efficient recreate every line  check previous sentence  handle case might previous sentence  eg  match find first sentence document  check prevsent none   set   previoussentence  field   n  a  placeholder text find suitable  import spacy spacymatcher import matcher import panda pd  load spacy model nlp  spacyload    encorewebsm    adjust model necessary  define pattern pattern      lower     example     example pattern  initialize matcher vocab matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   instance     list hold match detail open   filetxt    r   f  line iter  freadline      doc  nlp  line  prevsent  none  variable keep track previous sentence send docsent  match  matcher  send  match  matchid  start  end match  instancetext  send  start  end  text  match instance currentsentence  senttext previoussentence  prevsenttext prevsent else   n  a   handle case s previous sentence  append extract information instance list instancesappend  pd  series     instance   instancetext    sentence   currentsentence    previoussentence   previoussentence    prevsent  send  update previous sentence next iteration  convert instance list dataframe dfinstance  pd  dataframe  instance  print  dfinstance ,Library/Tool-Based Queries
"How to detect if two sentences are simmilar, not in meaning, but in syllables/words?","Here are some examples of the types of sentences that need to be considered ""similar"" there was a most extraordinary noise going on shrinking rapidly she soon made out there was a most extraordinary noise going on shrinking rapid that will be a very little alice knew it was just possible it had thou wilt be very little alice i knew it was possible to add however at last it sat down and looked very anxiously into her face and however that lives in sadtown and look very anxiously into him facing it she went in search of her or of anything to say she simply bowed she went in the search of her own or of anything to say and she squeezed herself up on tiptoe and peeped over the wig he did and she squeezed herself up on the tiptoe and peeped over her wig he did she had not noticed before and behind it was very glad to find that she had not noticed before and behind it it was very glad to find that as soon as the soldiers had to fall a long hookah and taking not soon as the soldiers have to fall along huka and taking knots And here are some examples of more difficult edge cases I would be able to like to catch, but are not as necessary so she tucked it under her arm with its head it would not join she tucked it under her arm with its head let me see four times five is twelve and four times five is twelve let me see times is and times is let me see four times seven is oh dear run home this moment and times is o dear run home this moment and in a minute or two she walked sadly down the middle being held up and then well see you sidely down the middle in health often Sentences that are somewhat different and have no such similarities need to be marked as dissimilar. If there is an algorithm that exists that outputs a ""score"" versus just a boolean similar or not, I could determine what threshold would be necessary through my own testing. The top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. If there is some syllabic comparison method that would be much more accurate given that I have the initial source text as well as the audio, I could also employ that instead of this word comparison technique. My current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. If at least 10 words match in either indexing order, I count the sentences as similar. However, all of the presented examples are cases where this strategy does not work.","['search', 'nlp', 'full-text-search', 'similarity', 'sentence-similarity']",2,"This is exactly similar to my answer above, but this is in nodejs. Apart from the language difference, code works exactly the same. First you need to install the natural module using npm. npm install natural const natural = require('natural'); function dotProduct(vector1, vector2) { return vector1.reduce((acc, val, index) => acc + val * vector2[index], 0); } function magnitude(vector) { return Math.sqrt(vector.reduce((acc, val) => acc + val * val, 0)); } function cosineSimilarity(vector1, vector2) { const dotProd = dotProduct(vector1, vector2); const mag1 = magnitude(vector1); const mag2 = magnitude(vector2); if (mag1 === 0 || mag2 === 0) { return 0; // Avoid division by zero } return dotProd / (mag1 * mag2); } function sentenceSimilarity(sentence1, sentence2) { // Tokenizing sentences const tokenizer = new natural.WordTokenizer(); const sentence1Tokens = tokenizer.tokenize(sentence1); const sentence2Tokens = tokenizer.tokenize(sentence2); // Creating a bade of words from tokens const bagOfWords = new Set([...sentence1Tokens, ...sentence2Tokens]); // Convert tokens to vectors const vector1 = Array.from(bagOfWords).map(word => sentence1Tokens.includes(word) ? 1 : 0); const vector2 = Array.from(bagOfWords).map(word => sentence2Tokens.includes(word) ? 1 : 0); // Calculate cosine similarity const similarity = cosineSimilarity(vector1, vector2); return similarity; } // Example usage const sentence1 = ""This is a sentence.""; const sentence2 = ""This is another sentence.""; const similarityScore = sentenceSimilarity(sentence1, sentence2); console.log(""Similarity score:"", similarityScore); Function dotProduct(), magnitude(), and cosineSimilarity() were needed to be defined since I was not able to find a library that provides these in node unlike in python. Apart from that, all the other logic are similar to the python code above.",2024-03-29 02:08:38,2024-04-02 04:54:45,126,https://stackoverflow.com/questions/78241665/how-to-detect-if-two-sentences-are-simmilar-not-in-meaning-but-in-syllables-wo,"How to detect if two sentences are simmilar, not in meaning, but in syllables/words? Here are some examples of the types of sentences that need to be considered ""similar"" there was a most extraordinary noise going on shrinking rapidly she soon made out there was a most extraordinary noise going on shrinking rapid that will be a very little alice knew it was just possible it had thou wilt be very little alice i knew it was possible to add however at last it sat down and looked very anxiously into her face and however that lives in sadtown and look very anxiously into him facing it she went in search of her or of anything to say she simply bowed she went in the search of her own or of anything to say and she squeezed herself up on tiptoe and peeped over the wig he did and she squeezed herself up on the tiptoe and peeped over her wig he did she had not noticed before and behind it was very glad to find that she had not noticed before and behind it it was very glad to find that as soon as the soldiers had to fall a long hookah and taking not soon as the soldiers have to fall along huka and taking knots And here are some examples of more difficult edge cases I would be able to like to catch, but are not as necessary so she tucked it under her arm with its head it would not join she tucked it under her arm with its head let me see four times five is twelve and four times five is twelve let me see times is and times is let me see four times seven is oh dear run home this moment and times is o dear run home this moment and in a minute or two she walked sadly down the middle being held up and then well see you sidely down the middle in health often Sentences that are somewhat different and have no such similarities need to be marked as dissimilar. If there is an algorithm that exists that outputs a ""score"" versus just a boolean similar or not, I could determine what threshold would be necessary through my own testing. The top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. If there is some syllabic comparison method that would be much more accurate given that I have the initial source text as well as the audio, I could also employ that instead of this word comparison technique. My current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. If at least 10 words match in either indexing order, I count the sentences as similar. However, all of the presented examples are cases where this strategy does not work.",detect two sentence simmilar  meaning  syllable  word  example type sentence need consider   similar  extraordinary noise go shrink rapidly soon make extraordinary noise go shrink rapid little alice know possible thou wilt little alice know possible add however last sat look anxiously face however life sadtown look anxiously face go search anything say simply bow go search anything say squeeze tiptoe peep wig squeeze tiptoe peep wig notice behind glad find notice behind glad find soon soldier fall long hookah take soon soldier fall along huka take knot example difficult edge case would able like catch  necessary tuck arm head would join tuck arm head let see four time five twelve four time five twelve let see time time let see four time seven oh dear run home moment time dear run home moment minute two walk sadly middle hold well see sidely middle health often sentence somewhat different similarity need mark dissimilar  algorithm exist output   score  versus boolean similar  could determine threshold would necessary testing  top sentence example randomly generate  bottom sentence output speech  to  text neural network  audio file someone read top line  syllabic comparison method would much accurate give initial source text well audio  could also employ instead word comparison technique  current method involve indexing word  forwards  reverse  check many word line  least 10 word match either indexing order  count sentence similar  however  present example case strategy work ,exactly similar answer  nodejs  apart language difference  code work exactly  first need install natural module use npm  npm install natural const natural  require   natural    function dotproduct  vector1  vector2   return vector1reduce   acc  val  index    acc  val  vector2  index   0    function magnitude  vector   return mathsqrt  vectorreduce   acc  val    acc  val  val  0     function cosinesimilarity  vector1  vector2   const dotprod  dotproduct  vector1  vector2   const mag1  magnitude  vector1   const mag2  magnitude  vector2    mag1    0  mag2    0   return 0   avoid division zero  return dotprod   mag1  mag2    function sentencesimilarity  sentence1  sentence2    tokenize sentence const tokenizer  new natural  wordtokenizer    const sentence1token  tokenizertokenize  sentence1   const sentence2token  tokenizertokenize  sentence2    create bade word token const bagofword  new set    sentence1tokens   sentence2token     convert token vector const vector1  arrayfrom  bagofword  map  word   sentence1tokensincludes  word   1  0   const vector2  arrayfrom  bagofword  map  word   sentence2tokensincludes  word   1  0    calculate cosine similarity const similarity  cosinesimilarity  vector1  vector2   return similarity    example usage const sentence1    sentence     const sentence2    another sentence     const similarityscore  sentencesimilarity  sentence1  sentence2   consolelog    similarity score    similarityscore   function dotproduct    magnitude    cosinesimilarity   needed define since able find library provide node unlike python  apart  logic similar python code ,detect two sentence simmilar  meaning  syllable  word  example type sentence need consider   similar  extraordinary noise go shrink rapidly soon make extraordinary noise go shrink rapid little alice know possible thou wilt little alice know possible add however last sat look anxiously face however life sadtown look anxiously face go search anything say simply bow go search anything say squeeze tiptoe peep wig squeeze tiptoe peep wig notice behind glad find notice behind glad find soon soldier fall long hookah take soon soldier fall along huka take knot example difficult edge case would able like catch  necessary tuck arm head would join tuck arm head let see four time five twelve four time five twelve let see time time let see four time seven oh dear run home moment time dear run home moment minute two walk sadly middle hold well see sidely middle health often sentence somewhat different similarity need mark dissimilar  algorithm exist output   score  versus boolean similar  could determine threshold would necessary testing  top sentence example randomly generate  bottom sentence output speech  to  text neural network  audio file someone read top line  syllabic comparison method would much accurate give initial source text well audio  could also employ instead word comparison technique  current method involve indexing word  forwards  reverse  check many word line  least 10 word match either indexing order  count sentence similar  however  present example case strategy work  exactly similar answer  nodejs  apart language difference  code work exactly  first need install natural module use npm  npm install natural const natural  require   natural    function dotproduct  vector1  vector2   return vector1reduce   acc  val  index    acc  val  vector2  index   0    function magnitude  vector   return mathsqrt  vectorreduce   acc  val    acc  val  val  0     function cosinesimilarity  vector1  vector2   const dotprod  dotproduct  vector1  vector2   const mag1  magnitude  vector1   const mag2  magnitude  vector2    mag1    0  mag2    0   return 0   avoid division zero  return dotprod   mag1  mag2    function sentencesimilarity  sentence1  sentence2    tokenize sentence const tokenizer  new natural  wordtokenizer    const sentence1token  tokenizertokenize  sentence1   const sentence2token  tokenizertokenize  sentence2    create bade word token const bagofword  new set    sentence1tokens   sentence2token     convert token vector const vector1  arrayfrom  bagofword  map  word   sentence1tokensincludes  word   1  0   const vector2  arrayfrom  bagofword  map  word   sentence2tokensincludes  word   1  0    calculate cosine similarity const similarity  cosinesimilarity  vector1  vector2   return similarity    example usage const sentence1    sentence     const sentence2    another sentence     const similarityscore  sentencesimilarity  sentence1  sentence2   consolelog    similarity score    similarityscore   function dotproduct    magnitude    cosinesimilarity   needed define since able find library provide node unlike python  apart  logic similar python code ,Library/Tool-Based Queries
Is BertForSequenceClassification using the CLS vector?,"In the hugging face source code , pooled_output = outputs[1] is used. outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) pooled_output = outputs[1] Shouldn't it be pooled_output = outputs[0] ? (This answer mentioning BertPooler seems to be outdated) Based on this answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that BertForSequenceClassification freezes the Bert model and only trains the classification head, but maybe that's not the case) Would a sentence embedding be equivalent or even better than the [CLS] token embedding?","['nlp', 'huggingface-transformers', 'bert-language-model']",1,"Would a sentence embedding be equivalent or even better than the [CLS] token embedding? A sentence embedding is everything that represents the input sequence as a numerical vector. The question is whether this embedding is semantical meaningful (e.g. can we use it with similarity metrics). This is for example not the case for the pretrained Bert weights released by google (refer to this answer for more information). Is the CLS token a sentence embedding? Yes. Is some kind of pooling a sentence embedding? Yes. Are they semantically meaningful with the Bert weights release by google? No. Shouldn't it be pooled_output = outputs[0]? No, because when you check the code , you will see that the first element of the tuple is the last_hidden_state sequence_output = encoder_outputs[0] pooled_output = self.pooler(sequence_output) if self.pooler is not None else None if not return_dict: return (sequence_output, pooled_output) + encoder_outputs[1:] I am confused as to why/how masked language modeling would lead to the start token learning a sentence level representation. Because it is included in every training sequence and the [CLS] ""absorbs"" the other tokens. You can also see this in the attention mechanism (compare Revealing the Dark Secrets of BERT paper ). As mentioned above, the questions is if they are semantically meaningful without any further finetuning. No (compare this StackOverflow answer ).",2024-03-28 20:54:17,2024-05-11 14:04:25,216,https://stackoverflow.com/questions/78240828/is-bertforsequenceclassification-using-the-cls-vector,"Is BertForSequenceClassification using the CLS vector? In the hugging face source code , pooled_output = outputs[1] is used. outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) pooled_output = outputs[1] Shouldn't it be pooled_output = outputs[0] ? (This answer mentioning BertPooler seems to be outdated) Based on this answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that BertForSequenceClassification freezes the Bert model and only trains the classification head, but maybe that's not the case) Would a sentence embedding be equivalent or even better than the [CLS] token embedding?",bertforsequenceclassification use cls vector  hug face source code  pooledoutput  output  1  use  output  selfbert  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  inputsembed  inputsembeds  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   pooledoutput  output  1  not pooledoutput  output  0    answer mention bertpooler seem outdated  base answer  seem cls token learn sentence level representation  confuse why  how mask language modelling would lead start token learn sentence level representation   think bertforsequenceclassification freeze bert model train classification head  maybe s case  would sentence embed equivalent even well  cls  token embed ,would sentence embed equivalent even well  cls  token embed  sentence embed everything represent input sequence numerical vector  question whether embed semantical meaningful  eg  use similarity metric   example case pretraine bert weight release google  refer answer information   cls token sentence embed  yes  kind pool sentence embed  yes  semantically meaningful bert weight release google   not pooledoutput  output  0    check code  see first element tuple lasthiddenstate sequenceoutput  encoderoutput  0  pooledoutput  selfpooler  sequenceoutput  selfpool none else none returndict  return  sequenceoutput  pooledoutput   encoderoutput  1   confuse why  how mask language modeling would lead start token learn sentence level representation  include every training sequence  cls    absorb  token  also see attention mechanism  compare revealing dark secrets bert paper   mention  question semantically meaningful without finetune   compare stackoverflow answer  ,bertforsequenceclassification use cls vector  hug face source code  pooledoutput  output  1  use  output  selfbert  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  inputsembed  inputsembeds  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   pooledoutput  output  1  not pooledoutput  output  0    answer mention bertpooler seem outdated  base answer  seem cls token learn sentence level representation  confuse why  how mask language modelling would lead start token learn sentence level representation   think bertforsequenceclassification freeze bert model train classification head  maybe s case  would sentence embed equivalent even well  cls  token embed  would sentence embed equivalent even well  cls  token embed  sentence embed everything represent input sequence numerical vector  question whether embed semantical meaningful  eg  use similarity metric   example case pretraine bert weight release google  refer answer information   cls token sentence embed  yes  kind pool sentence embed  yes  semantically meaningful bert weight release google   not pooledoutput  output  0    check code  see first element tuple lasthiddenstate sequenceoutput  encoderoutput  0  pooledoutput  selfpooler  sequenceoutput  selfpool none else none returndict  return  sequenceoutput  pooledoutput   encoderoutput  1   confuse why  how mask language modeling would lead start token learn sentence level representation  include every training sequence  cls    absorb  token  also see attention mechanism  compare revealing dark secrets bert paper   mention  question semantically meaningful without finetune   compare stackoverflow answer  ,Implementation Issues
How to remove layers in Huggingface&#39;s transformers GPT2 pre-trained models?,"My code: from transformers import GPT2Config, GPT2Model from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""openai-community/gpt2"") print(decoder) Here is the output of the console, listing the model architecture: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) I want to remove the first layer: (wte): Embedding(50257, 768) I've tried the following way: def deleteEncodingLayers(model, num_layers_to_keep): # must pass in the full bert model oldModuleList = model.bert.encoder.layer newModuleList = nn.ModuleList() # Now iterate over all layers, only keepign only the relevant layers. for i in range(0, len(num_layers_to_keep)): newModuleList.append(oldModuleList[i]) # create a copy of the model, modify it with the new list, and return copyOfModel = copy.deepcopy(model) copyOfModel.bert.encoder.layer = newModuleList return copyOfModel But it didn't work. Who knows how to fix it?","['python', 'machine-learning', 'deep-learning', 'nlp', 'transformer-model']",1,"Try these parameters to bypass the embedding layer : class GPT2WithoutWTE(GPT2Model): def __init__(self, config): super().__init__(config) # Remove the word token embedding layer del self.wte def forward( self, inputs_embeds, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, ): # here you will bypass the embedding layer and use inputs_embeds directly return super().forward( inputs_embeds=inputs_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) for the input embeddings you can use the following inputs_embeds = torch.rand(1, 10, config.n_embd) Load the config of GPT2, send it to the class and then use the inputs_embeds for the new model.",2024-03-25 12:28:15,2024-03-25 13:22:28,343,https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models,"How to remove layers in Huggingface&#39;s transformers GPT2 pre-trained models? My code: from transformers import GPT2Config, GPT2Model from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""openai-community/gpt2"") print(decoder) Here is the output of the console, listing the model architecture: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) I want to remove the first layer: (wte): Embedding(50257, 768) I've tried the following way: def deleteEncodingLayers(model, num_layers_to_keep): # must pass in the full bert model oldModuleList = model.bert.encoder.layer newModuleList = nn.ModuleList() # Now iterate over all layers, only keepign only the relevant layers. for i in range(0, len(num_layers_to_keep)): newModuleList.append(oldModuleList[i]) # create a copy of the model, modify it with the new list, and return copyOfModel = copy.deepcopy(model) copyOfModel.bert.encoder.layer = newModuleList return copyOfModel But it didn't work. Who knows how to fix it?",remove layer huggingface   39  transformer gpt2 pre  train model  code  transformer import gpt2config  gpt2model transformer import autotokenizer  automodelformaskedlm  automodelforcausallm model  automodelforcausallmfrompretraine    openai  community  gpt2   print  decoder  output console  list model architecture  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   want remove first layer   wte   embed  50257  768   ve try follow way  def deleteencodinglayer  model  numlayerstokeep    must pass full bert model oldmodulelist  modelbertencoderlayer newmodulelist  nn  modulelist    iterate layer  keepign relevant layer  range  0  len  numlayerstokeep    newmodulelistappend  oldmodulelist     create copy model  modify new list  return copyofmodel  copydeepcopy  model  copyofmodelbertencoderlayer  newmodulelist return copyofmodel not work  know fix ,try parameter bypass embed layer  class gpt2withoutwte  gpt2model   def   init    self  config   super   init    config   remove word token embed layer del selfwte def forward  self  inputsembed  attentionmask  none  tokentypeid  none  positionid  none  headmask  none  input  none  encoderhiddenstate  none  encoderattentionmask  none  pastkeyvalue  none  usecache  none  outputattention  none  outputhiddenstate  none  returndict  none     bypass embed layer use inputsembed directly return super   forward  inputsembed  inputsembeds  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  encoderhiddenstate  encoderhiddenstates  encoderattentionmask  encoderattentionmask  pastkeyvalue  pastkeyvalues  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   input embedding use follow inputsembed  torchrand  1  10  confignembd  load config gpt2  send class use inputsembed new model ,remove layer huggingface   39  transformer gpt2 pre  train model  code  transformer import gpt2config  gpt2model transformer import autotokenizer  automodelformaskedlm  automodelforcausallm model  automodelforcausallmfrompretraine    openai  community  gpt2   print  decoder  output console  list model architecture  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   want remove first layer   wte   embed  50257  768   ve try follow way  def deleteencodinglayer  model  numlayerstokeep    must pass full bert model oldmodulelist  modelbertencoderlayer newmodulelist  nn  modulelist    iterate layer  keepign relevant layer  range  0  len  numlayerstokeep    newmodulelistappend  oldmodulelist     create copy model  modify new list  return copyofmodel  copydeepcopy  model  copyofmodelbertencoderlayer  newmodulelist return copyofmodel not work  know fix  try parameter bypass embed layer  class gpt2withoutwte  gpt2model   def   init    self  config   super   init    config   remove word token embed layer del selfwte def forward  self  inputsembed  attentionmask  none  tokentypeid  none  positionid  none  headmask  none  input  none  encoderhiddenstate  none  encoderattentionmask  none  pastkeyvalue  none  usecache  none  outputattention  none  outputhiddenstate  none  returndict  none     bypass embed layer use inputsembed directly return super   forward  inputsembed  inputsembeds  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  encoderhiddenstate  encoderhiddenstates  encoderattentionmask  encoderattentionmask  pastkeyvalue  pastkeyvalues  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   input embedding use follow inputsembed  torchrand  1  10  confignembd  load config gpt2  send class use inputsembed new model ,Library/Tool-Based Queries
How can i get the first content of a python synsets list?,"enter image description here I have a scrapped text stored under the variable ""message"". I have removed the StopWords and stored the result with the variable ""without_stop_words"". I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS. Currently i am trying to get the MEANINGS but I'm getting an error: ""IndexError: list index out of range"" enter image description here for writeup in writeups: message = writeup.text #Stop Words stop_words = set(stopwords.words('english')) #print(stop_words) tokenized_words = word_tokenize(message) #Filtering Stop Words without_stop_words = [] for word in tokenized_words: if word not in stop_words: without_stop_words.append(word) #Word Meanings word_meanings = [] for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) meaning = sync_words[0].definition() print(meaning) I want to get the MEANING of each word in the ""without_stop_words"".","['python', 'nlp', 'nltk', 'sentiment-analysis', 'synset']",1,"The error comes from this line meaning = sync_words[0].definition() And it indicates that sync_words is empty for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) if sync_words: meaning = sync_words[0].definition() word_meanings.append(meaning) else: # Whatever you want to do if it's empty This will stop the error, but you should try to find out why sync_words is empty in the first place.",2024-03-23 14:53:18,2024-03-23 15:00:56,43,https://stackoverflow.com/questions/78211318/how-can-i-get-the-first-content-of-a-python-synsets-list,"How can i get the first content of a python synsets list? enter image description here I have a scrapped text stored under the variable ""message"". I have removed the StopWords and stored the result with the variable ""without_stop_words"". I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS. Currently i am trying to get the MEANINGS but I'm getting an error: ""IndexError: list index out of range"" enter image description here for writeup in writeups: message = writeup.text #Stop Words stop_words = set(stopwords.words('english')) #print(stop_words) tokenized_words = word_tokenize(message) #Filtering Stop Words without_stop_words = [] for word in tokenized_words: if word not in stop_words: without_stop_words.append(word) #Word Meanings word_meanings = [] for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) meaning = sync_words[0].definition() print(meaning) I want to get the MEANING of each word in the ""without_stop_words"".",get first content python synset list  enter image description scrap text store variable   message   remove stopwords store result variable   withoutstopwords   want loop word  withoutstopwords  get meaning pronouns  currently try get meanings  m get error    indexerror  list index range  enter image description writeup writeup  message  writeuptext  stop word stopwords  set  stopwordsword   english     print  stopwords  tokenizedwords  wordtokenize  message   filtering stop word withoutstopwords    word tokenizedwords  word stopwords  withoutstopwordsappend  word   word meanings wordmeaning    eachword withoutstopwords  syncword  wordnetsynset  eachword  mean  syncword  0  definition   print  meaning  want get meaning word   withoutstopwords  ,error come line meaning  syncword  0  definition   indicate syncwords empty eachword withoutstopwords  syncword  wordnetsynset  eachword  syncword  mean  syncword  0  definition   wordmeaningsappend  mean  else   whatever want s empty stop error  try find syncword empty first place ,get first content python synset list  enter image description scrap text store variable   message   remove stopwords store result variable   withoutstopwords   want loop word  withoutstopwords  get meaning pronouns  currently try get meanings  m get error    indexerror  list index range  enter image description writeup writeup  message  writeuptext  stop word stopwords  set  stopwordsword   english     print  stopwords  tokenizedwords  wordtokenize  message   filtering stop word withoutstopwords    word tokenizedwords  word stopwords  withoutstopwordsappend  word   word meanings wordmeaning    eachword withoutstopwords  syncword  wordnetsynset  eachword  mean  syncword  0  definition   print  meaning  want get meaning word   withoutstopwords   error come line meaning  syncword  0  definition   indicate syncwords empty eachword withoutstopwords  syncword  wordnetsynset  eachword  syncword  mean  syncword  0  definition   wordmeaningsappend  mean  else   whatever want s empty stop error  try find syncword empty first place ,Library/Tool-Based Queries
Solution to solve problem different results when run Doc2vec gensim?,"I try to find information about problem that Doc2vec returns different results when it runs. I saw many previous questions about this and I know It happens because vector is randomly initialize. However, I am creating a website which uses this result to display in frontend. The difference in results makes reliability of systems reduce. I know my dataset is really small. But infer_vector() can't return same vectors with same documents and results most_similar() are different in each run. How do I prevent this problem or having alternative way to apply doc2vec model in my application to avoid difference of results? This is some code: model = gensim.models.doc2vec.Doc2Vec(vector_size=50, dm=1, window=5, min_count=2, epochs=100, negative=0, workers=5) But I received warning: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. I try to set negative=-1 but I see explain from gensim : negative must be integer.","['nlp', 'doc2vec']",1,"These are potentially, two different issues. With regard to the warning you're seeing: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. The warning is complete and truthful, it already describes what you're doing wrong and how to solve it. You must set either hs or negative to be positive or else no training will happen in your model . negative=-1 is an illegal setting, and not positive. If you want to use Doc2Vec , you need to either have the negative parameter as a positive integer (as with its default value negative=5 ), or if you want to set negative=0 then you need to enable the alternative ""hierarchical softmax"" mode with hs=1 . The algorithm will do nothing but error or given nonsense untrained results if you give it illegal configurations. As is explained in the Q12 of the Gensim Project FAQ & other StackOverflow answers , the operation of the Doc2Vec algorithm naturally allows for variance in the vectors returned by infer_vector() from run to run. And, if that ""jitter"" between inferences is s making a big difference in results, there are probably other serious problems in your use of Doc2Vec , such as insufficient data or bad parameters, that you should fix, rather than trying to force a false determinism onto your calculations. In particular, if the model whose changing infer_vector() results was ""trained"" – not really – with the shown parameters ( negative=0 without enabled hs ), ignoring the warning that won't work, that is the first big problem to solve. It will make all inferred vetor random and meaninglfess (as opposed to just ""a little noisy""). But, if after fixing the total failure of training you then insistently want to do the incorrect thing, you can force inference determinism as is described in another answer at: removing randomization of vector initialization for doc2vec",2024-03-19 03:20:17,2024-03-19 12:50:10,43,https://stackoverflow.com/questions/78184077/solution-to-solve-problem-different-results-when-run-doc2vec-gensim,"Solution to solve problem different results when run Doc2vec gensim? I try to find information about problem that Doc2vec returns different results when it runs. I saw many previous questions about this and I know It happens because vector is randomly initialize. However, I am creating a website which uses this result to display in frontend. The difference in results makes reliability of systems reduce. I know my dataset is really small. But infer_vector() can't return same vectors with same documents and results most_similar() are different in each run. How do I prevent this problem or having alternative way to apply doc2vec model in my application to avoid difference of results? This is some code: model = gensim.models.doc2vec.Doc2Vec(vector_size=50, dm=1, window=5, min_count=2, epochs=100, negative=0, workers=5) But I received warning: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. I try to set negative=-1 but I see explain from gensim : negative must be integer.",solution solve problem different result run doc2vec gensim  try find information problem doc2vec return different result run  see many previous question know happen vector randomly initialize  however  create website use result display frontend  difference result make reliability system reduce  know dataset really small  infervector   can not return vector document result mostsimilar   different run  prevent problem alternative way apply doc2vec model application avoid difference result  code  model  gensimmodelsdoc2vec  doc2vec  vectorsize50  dm1  window5  mincount2  epochs100  negative0  workers5  receive warning  must set either  hs   negative  positive proper training   hs0   negative0   training  try set negative1 see explain gensim  negative must integer ,potentially  two different issue  regard warning be see  must set either  hs   negative  positive proper training   hs0   negative0   training  warn complete truthful  already describe be wrong solve  must set either hs negative positive else training happen model  negative1 illegal setting  positive  want use doc2vec  need either negative parameter positive integer  default value negative5   want set negative0 need enable alternative   hierarchical softmax  mode hs1  algorithm nothing error give nonsense untraine result give illegal configuration  explain q12 gensim project faq  stackoverflow answer  operation doc2vec algorithm naturally allow variance vector return infervector   run run     jitter  inference make big difference result  probably serious problem use doc2vec  insufficient datum bad parameter  fix  rather try force false determinism onto calculation  particular  model whose change infervector   result   train   really  show parameter  negative0 without enable hs   ignore warning will not work  first big problem solve  make inferred vetor random meaninglfess  oppose   little noisy     fix total failure training insistently want incorrect thing  force inference determinism describe another answer  remove randomization vector initialization doc2vec,solution solve problem different result run doc2vec gensim  try find information problem doc2vec return different result run  see many previous question know happen vector randomly initialize  however  create website use result display frontend  difference result make reliability system reduce  know dataset really small  infervector   can not return vector document result mostsimilar   different run  prevent problem alternative way apply doc2vec model application avoid difference result  code  model  gensimmodelsdoc2vec  doc2vec  vectorsize50  dm1  window5  mincount2  epochs100  negative0  workers5  receive warning  must set either  hs   negative  positive proper training   hs0   negative0   training  try set negative1 see explain gensim  negative must integer  potentially  two different issue  regard warning be see  must set either  hs   negative  positive proper training   hs0   negative0   training  warn complete truthful  already describe be wrong solve  must set either hs negative positive else training happen model  negative1 illegal setting  positive  want use doc2vec  need either negative parameter positive integer  default value negative5   want set negative0 need enable alternative   hierarchical softmax  mode hs1  algorithm nothing error give nonsense untraine result give illegal configuration  explain q12 gensim project faq  stackoverflow answer  operation doc2vec algorithm naturally allow variance vector return infervector   run run     jitter  inference make big difference result  probably serious problem use doc2vec  insufficient datum bad parameter  fix  rather try force false determinism onto calculation  particular  model whose change infervector   result   train   really  show parameter  negative0 without enable hs   ignore warning will not work  first big problem solve  make inferred vetor random meaninglfess  oppose   little noisy     fix total failure training insistently want incorrect thing  force inference determinism describe another answer  remove randomization vector initialization doc2vec,Implementation Issues
How to optimize the function which uses looping on lists on pandas dataframe?,"I am using a function on a pandas dataframe as : import spacy from collections import Counter # Load English language model nlp = spacy.load(""en_core_web_sm"") # Function to filter out only nouns from a list of words def filter_nouns(words): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' filtered_nouns = [] # Preprocess the text by removing symbols and splitting into words words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp("" "".join(words)) if token.pos_ == ""NOUN""] return filtered_nouns # Apply filtering logic to all rows in the 'NOTE' column df['filtered_nouns'] = sf['NOTE'].apply(lambda x: filter_nouns(x)) I have a dataset containing 6400 rows and df['NOTE'] is a very long paragraph converted from the Oracle CLOB datatype. This function is working quickly for 5-10 rows but for 6400 rows, it is taking a very long time. Any ways to optimize this.","['python', 'python-3.x', 'pandas', 'list', 'nlp']",2,"The first thing you should do is remove all the repetition in your function. In this line: words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] You are building the translation dictionary every time you translate a word, and calling translate for each word in the text. It is far more efficient to do each of those once: tr = str.maketrans('', '', SYMBOLS) words = words.strip().translate(tr).split() This makes about a 50x speed-up on a 1000-word string on my computer. In the next line you are then joining all the words for every call to nlp . You should do that once: text = ' '.join(words) filtered_nouns = [token.text for token in nlp(text) if token.pos_ == ""NOUN""] But note that you just split on spaces, so you might as well skip that step completely. In total: def filter_nouns(text): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' tr = str.maketrans('', '', SYMBOLS) # Preprocess the text by removing symbols words = text.strip().translate(tr) # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp(words) if token.pos_ == ""NOUN""] return filtered_nouns Finally, note that .apply(lambda x: filter_nouns(x)) is the same as .apply(filter_nouns) .",2024-03-14 02:56:32,2024-03-14 05:40:23,94,https://stackoverflow.com/questions/78157864/how-to-optimize-the-function-which-uses-looping-on-lists-on-pandas-dataframe,"How to optimize the function which uses looping on lists on pandas dataframe? I am using a function on a pandas dataframe as : import spacy from collections import Counter # Load English language model nlp = spacy.load(""en_core_web_sm"") # Function to filter out only nouns from a list of words def filter_nouns(words): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' filtered_nouns = [] # Preprocess the text by removing symbols and splitting into words words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp("" "".join(words)) if token.pos_ == ""NOUN""] return filtered_nouns # Apply filtering logic to all rows in the 'NOTE' column df['filtered_nouns'] = sf['NOTE'].apply(lambda x: filter_nouns(x)) I have a dataset containing 6400 rows and df['NOTE'] is a very long paragraph converted from the Oracle CLOB datatype. This function is working quickly for 5-10 rows but for 6400 rows, it is taking a very long time. Any ways to optimize this.",optimize function use loop list panda dataframe  use function panda dataframe  import spacy collection import counter  load english language model nlp  spacyload    encorewebsm    function filter noun list word def filternoun  word   symbols                        1234567890     filterednoun     preprocess text remove symbol split word word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit     process word filter noun filterednouns   tokentext token nlp      join  word   tokenpos      noun   return filterednouns  apply filter logic row  note  column df   filterednoun    sf   note   apply  lambda x  filternouns  x   dataset contain 6400 row df   note   long paragraph convert oracle clob datatype  function work quickly 5  10 row 6400 row  take long time  way optimize ,first thing remove repetition function  line  word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit    build translation dictionary every time translate word  call translate word text  far efficient  tr  strmaketrans        symbols  word  wordsstrip   translate  tr  split   make 50x speed  up 1000  word string computer  next line join word every call nlp   text    join  word  filterednoun   tokentext token nlp  text  tokenpos      noun   note split space  might well skip step completely  total  def filternoun  text   symbols                        1234567890     tr  strmaketrans        symbols   preprocess text remove symbol word  textstrip   translate  tr   process word filter noun filterednouns   tokentext token nlp  word  tokenpos      noun   return filterednouns finally  note apply  lambda x  filternouns  x   apply  filternouns  ,optimize function use loop list panda dataframe  use function panda dataframe  import spacy collection import counter  load english language model nlp  spacyload    encorewebsm    function filter noun list word def filternoun  word   symbols                        1234567890     filterednoun     preprocess text remove symbol split word word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit     process word filter noun filterednouns   tokentext token nlp      join  word   tokenpos      noun   return filterednouns  apply filter logic row  note  column df   filterednoun    sf   note   apply  lambda x  filternouns  x   dataset contain 6400 row df   note   long paragraph convert oracle clob datatype  function work quickly 5  10 row 6400 row  take long time  way optimize  first thing remove repetition function  line  word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit    build translation dictionary every time translate word  call translate word text  far efficient  tr  strmaketrans        symbols  word  wordsstrip   translate  tr  split   make 50x speed  up 1000  word string computer  next line join word every call nlp   text    join  word  filterednoun   tokentext token nlp  text  tokenpos      noun   note split space  might well skip step completely  total  def filternoun  text   symbols                        1234567890     tr  strmaketrans        symbols   preprocess text remove symbol word  textstrip   translate  tr   process word filter noun filterednouns   tokentext token nlp  word  tokenpos      noun   return filterednouns finally  note apply  lambda x  filternouns  x   apply  filternouns  ,Library/Tool-Based Queries
SpaCy: Regex pattern does not work in rule-based matcher,"I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model. The aim is to add tokens with ""COMP"" label whenever it finds words structured like this: XXX-Ynnn XXX Ynnn Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination. To do so, I use the following method def add_component_patterns_re(input_references, model_ruler): ruler = model_ruler ref_patterns = [] letters = ['V', 'B', 'F', 'K', 'S'] print(""Adding component patterns"") for ref in input_references.iloc[:, 0]: # print(f""Adding references for system: {ref}"") for letter in letters: pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}' pattern = {""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}} ref_patterns.append({""label"":""COMP"", ""pattern"":pattern}) ruler.add_patterns(ref_patterns) return ref_patterns Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler. For information, i've also tried to change the pattern variable as a list entry, like this: pattern = [{""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}}] But the result is the same, it can't seem to get any match. Does someone have any suggestion? Thanks in advance!","['python', 'nlp', 'spacy', 'named-entity-recognition']",2,"In the end I got print(f""Adding references for system: {ref}"") for letter in letters: for nnn in range(1000): pattern = f""{ref}-{letter}{nnn:03d}"" ref_patterns.append({""label"": ""COMP"", ""pattern"": pattern}) pattern = f""{ref} {letter}{nnn:03d}"" ref_patterns.append({""label"": ""COMP"", ""pattern"": pattern}) For each pattern. The code is lengthier and a tad slower but it does the job just fine!",2024-03-11 13:19:59,2024-03-15 10:02:03,97,https://stackoverflow.com/questions/78140912/spacy-regex-pattern-does-not-work-in-rule-based-matcher,"SpaCy: Regex pattern does not work in rule-based matcher I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model. The aim is to add tokens with ""COMP"" label whenever it finds words structured like this: XXX-Ynnn XXX Ynnn Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination. To do so, I use the following method def add_component_patterns_re(input_references, model_ruler): ruler = model_ruler ref_patterns = [] letters = ['V', 'B', 'F', 'K', 'S'] print(""Adding component patterns"") for ref in input_references.iloc[:, 0]: # print(f""Adding references for system: {ref}"") for letter in letters: pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}' pattern = {""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}} ref_patterns.append({""label"":""COMP"", ""pattern"":pattern}) ruler.add_patterns(ref_patterns) return ref_patterns Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler. For information, i've also tried to change the pattern variable as a list entry, like this: pattern = [{""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}}] But the result is the same, it can't seem to get any match. Does someone have any suggestion? Thanks in advance!",spacy  regex pattern work rule  base matcher try define regular expression use text pattern entity ruler component spacy model  aim add tokens   comp  label whenever find word structure like  xxx  ynnn xxx ynnn  xxx  trigram list    letter  nnn  digit combination   use follow method def addcomponentpatternsre  inputreferences  modelruler   ruler  modelruler refpatterns    letter    v    b    f    k   s   print    add component pattern   ref inputreferencesiloc    0    print  f  add reference system   ref    letter letter  patterntext  fr   ref      letter   0  9    3    pattern     text      regex   fr   ref      letter   0  9    3      refpatternsappend     label    comp     pattern   pattern   ruleraddpattern  refpattern  return refpattern printing add pattern  seem output list correct  guess something wrong defining pattern add ruler  information   ve also try change pattern variable list entry  like  pattern      text      regex   fr   ref      letter   0  9    3       result  can not seem get match  someone suggestion  thank advance ,end get print  f  add reference system   ref    letter letter  nnn range  1000   pattern  f   ref    letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  f   ref   letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  code lengthy tad slow job fine ,spacy  regex pattern work rule  base matcher try define regular expression use text pattern entity ruler component spacy model  aim add tokens   comp  label whenever find word structure like  xxx  ynnn xxx ynnn  xxx  trigram list    letter  nnn  digit combination   use follow method def addcomponentpatternsre  inputreferences  modelruler   ruler  modelruler refpatterns    letter    v    b    f    k   s   print    add component pattern   ref inputreferencesiloc    0    print  f  add reference system   ref    letter letter  patterntext  fr   ref      letter   0  9    3    pattern     text      regex   fr   ref      letter   0  9    3      refpatternsappend     label    comp     pattern   pattern   ruleraddpattern  refpattern  return refpattern printing add pattern  seem output list correct  guess something wrong defining pattern add ruler  information   ve also try change pattern variable list entry  like  pattern      text      regex   fr   ref      letter   0  9    3       result  can not seem get match  someone suggestion  thank advance  end get print  f  add reference system   ref    letter letter  nnn range  1000   pattern  f   ref    letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  f   ref   letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  code lengthy tad slow job fine ,Implementation Issues
Torchtext functions in Newest version analogue,"Good day all, i'm trying to solve task, where it was used previously torchtext.dataset.TranslationDataset , torch.data.Field and torch.data.BucketIterator . But, after updating they were removed and i don't know how it can be used now. Has anybody faced with this problem, how have you solved it? I'd be the greatful, if you share a link with advice if it's possible. I've tried to read docs and find info on the Internet, but unsuccessfully. It always cite torchtext.legacy , but it doesn't have it in up-to-date versions. I understand, that i can download to it's version, but don't want this yet.","['nlp', 'version', 'torchtext']",1,I think these are the imports in the newer version of torch and their latest documentation torchtext.dataset.TranslationDataset -> torchtext.datasets.TranslationDataset torch.data.Field -> torchtext.data.Field torch.data.BucketIterator -> torchtext.data.BucketIterator Let me know if you have trouble importing them,2024-03-10 13:36:58,2024-03-13 03:11:30,157,https://stackoverflow.com/questions/78136056/torchtext-functions-in-newest-version-analogue,"Torchtext functions in Newest version analogue Good day all, i'm trying to solve task, where it was used previously torchtext.dataset.TranslationDataset , torch.data.Field and torch.data.BucketIterator . But, after updating they were removed and i don't know how it can be used now. Has anybody faced with this problem, how have you solved it? I'd be the greatful, if you share a link with advice if it's possible. I've tried to read docs and find info on the Internet, but unsuccessfully. It always cite torchtext.legacy , but it doesn't have it in up-to-date versions. I understand, that i can download to it's version, but don't want this yet.",torchtext function new version analogue good day   m try solve task  use previously torchtextdataset  translationdataset  torchdata  field torchdata  bucketiterator   update removed not know use  anybody face problem  solve  would greatful  share link advice be possible   ve try read doc find info internet  unsuccessfully  always cite torchtextlegacy  not up  to  date version  understand  download s version  not want yet ,think import new version torch late documentation torchtextdataset  translationdataset   torchtextdataset  translationdataset torchdata  field   torchtextdata  field torchdata  bucketiterator   torchtextdata  bucketiterator let know trouble import,torchtext function new version analogue good day   m try solve task  use previously torchtextdataset  translationdataset  torchdata  field torchdata  bucketiterator   update removed not know use  anybody face problem  solve  would greatful  share link advice be possible   ve try read doc find info internet  unsuccessfully  always cite torchtextlegacy  not up  to  date version  understand  download s version  not want yet  think import new version torch late documentation torchtextdataset  translationdataset   torchtextdataset  translationdataset torchdata  field   torchtextdata  field torchdata  bucketiterator   torchtextdata  bucketiterator let know trouble import,Task-Specific Queries
TypeError: Exception encountered when calling layer &#39;embeddings&#39; (type TFBertEmbeddings),"My model was wholly workable two weeks back, but now it's showing the following error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-23-a3e5a45f06c9> in <cell line: 14>() 12 13 # Encode input using BERT model ---> 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) 15 16 # Get pooled output and pass through dropout layer 8 frames /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs) 68 # To get the full stack trace, call: 69 # `tf.debugging.disable_traceback_filtering()` ---> 70 raise e.with_traceback(filtered_tb) from None 71 finally: 72 del filtered_tb TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings). Could not build a TypeSpec for name: ""tf.debugging.assert_less/assert_less/Assert/Assert"" op: ""Assert"" input: ""tf.debugging.assert_less/assert_less/All"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_0"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_1"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_2"" input: ""Placeholder"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_4"" input: ""tf.debugging.assert_less/assert_less/y"" attr { key: ""T"" value { list { type: DT_STRING type: DT_STRING type: DT_STRING type: DT_INT32 type: DT_STRING type: DT_INT32 } } } attr { key: ""summarize"" value { i: 3 } } of unsupported type <class 'tensorflow.python.framework.ops.Operation'>. Call arguments received by layer 'embeddings' (type TFBertEmbeddings): • input_ids=<KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'input_ids')> • position_ids=None • token_type_ids=<KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'token_type_ids')> • inputs_embeds=None • past_key_values_length=0 • training=False I think this error occurs when the input layers are sent to the corresponding BERT layer. If I use old versions of TensorFlow instead of 2.15.0, the error is resolved. However, with those old versions, I did not get GPU and faced a Graph Execution Error. Can anyone help me with this","['tensorflow', 'deep-learning', 'nlp', 'bert-language-model', 'transformer-model']",2,"it's a problem with the transformers library, I had the same problem and solved it using version 4.31.0",2024-03-08 16:48:31,2024-03-10 12:00:31,1386,https://stackoverflow.com/questions/78129126/typeerror-exception-encountered-when-calling-layer-embeddings-type-tfbertemb,"TypeError: Exception encountered when calling layer &#39;embeddings&#39; (type TFBertEmbeddings) My model was wholly workable two weeks back, but now it's showing the following error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-23-a3e5a45f06c9> in <cell line: 14>() 12 13 # Encode input using BERT model ---> 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) 15 16 # Get pooled output and pass through dropout layer 8 frames /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs) 68 # To get the full stack trace, call: 69 # `tf.debugging.disable_traceback_filtering()` ---> 70 raise e.with_traceback(filtered_tb) from None 71 finally: 72 del filtered_tb TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings). Could not build a TypeSpec for name: ""tf.debugging.assert_less/assert_less/Assert/Assert"" op: ""Assert"" input: ""tf.debugging.assert_less/assert_less/All"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_0"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_1"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_2"" input: ""Placeholder"" input: ""tf.debugging.assert_less/assert_less/Assert/Assert/data_4"" input: ""tf.debugging.assert_less/assert_less/y"" attr { key: ""T"" value { list { type: DT_STRING type: DT_STRING type: DT_STRING type: DT_INT32 type: DT_STRING type: DT_INT32 } } } attr { key: ""summarize"" value { i: 3 } } of unsupported type <class 'tensorflow.python.framework.ops.Operation'>. Call arguments received by layer 'embeddings' (type TFBertEmbeddings): • input_ids=<KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'input_ids')> • position_ids=None • token_type_ids=<KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'token_type_ids')> • inputs_embeds=None • past_key_values_length=0 • training=False I think this error occurs when the input layers are sent to the corresponding BERT layer. If I use old versions of TensorFlow instead of 2.15.0, the error is resolved. However, with those old versions, I did not get GPU and faced a Graph Execution Error. Can anyone help me with this",typeerror  exception encounter call layer   39  embedding   39   type tfbertembedding  model wholly workable two week back  be showing follow error                                        typeerror traceback  recent call last   ipython  input23  a3e5a45f06c9   cell line  14    12 13  encode input use bert model    14 bertoutput  bertmodel  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  15 16  get pool output pass dropout layer 8 frame usr  local  lib  python310  dist  package  keras  src  util  tracebackutilspy errorhandler   args    kwargs  68  get full stack trace  call  69   tfdebuggingdisabletracebackfiltere       70 raise ewithtraceback  filteredtb  none 71 finally  72 del filteredtb typeerror  exception encounter call layer  embedding   type tfbertembedding   could build typespec name    tfdebuggingassertless  assertless  assert  assert  op    assert  input    tfdebuggingassertless  assertless  all  input    tfdebuggingassertless  assertless  assert  assert  data0  input    tfdebuggingassertless  assertless  assert  assert  data1  input    tfdebuggingassertless  assertless  assert  assert  data2  input    placeholder  input    tfdebuggingassertless  assertless  assert  assert  data4  input    tfdebuggingassertless  assertless  y  attr  key     value  list  type  dtstring type  dtstring type  dtstring type  dtint32 type  dtstring type  dtint32    attr  key    summarize  value   3   unsupported type  class  tensorflowpythonframeworkops  operation    call argument receive layer  embedding   type tfbertembedding    inputids  kerastensor  shape  none  50  dtype  int32  create layer  inputids     positionid  none  tokentypeids  kerastensor  shape  none  50  dtype  int32  create layer  tokentypeid     inputsembed  none  pastkeyvalueslength0  training  false think error occur input layer send correspond bert layer  use old version tensorflow instead 2150  error resolve  however  old version  get gpu face graph execution error  anyone help,s problem transformer library  problem solve use version 4310,typeerror  exception encounter call layer   39  embedding   39   type tfbertembedding  model wholly workable two week back  be showing follow error                                        typeerror traceback  recent call last   ipython  input23  a3e5a45f06c9   cell line  14    12 13  encode input use bert model    14 bertoutput  bertmodel  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  15 16  get pool output pass dropout layer 8 frame usr  local  lib  python310  dist  package  keras  src  util  tracebackutilspy errorhandler   args    kwargs  68  get full stack trace  call  69   tfdebuggingdisabletracebackfiltere       70 raise ewithtraceback  filteredtb  none 71 finally  72 del filteredtb typeerror  exception encounter call layer  embedding   type tfbertembedding   could build typespec name    tfdebuggingassertless  assertless  assert  assert  op    assert  input    tfdebuggingassertless  assertless  all  input    tfdebuggingassertless  assertless  assert  assert  data0  input    tfdebuggingassertless  assertless  assert  assert  data1  input    tfdebuggingassertless  assertless  assert  assert  data2  input    placeholder  input    tfdebuggingassertless  assertless  assert  assert  data4  input    tfdebuggingassertless  assertless  y  attr  key     value  list  type  dtstring type  dtstring type  dtstring type  dtint32 type  dtstring type  dtint32    attr  key    summarize  value   3   unsupported type  class  tensorflowpythonframeworkops  operation    call argument receive layer  embedding   type tfbertembedding    inputids  kerastensor  shape  none  50  dtype  int32  create layer  inputids     positionid  none  tokentypeids  kerastensor  shape  none  50  dtype  int32  create layer  tokentypeid     inputsembed  none  pastkeyvalueslength0  training  false think error occur input layer send correspond bert layer  use old version tensorflow instead 2150  error resolve  however  old version  get gpu face graph execution error  anyone help s problem transformer library  problem solve use version 4310,Implementation Issues
How to apply a linear layer atop a sentence transformer,"Hey there I am trying to create a basic Sentence Transformer model for few shot learning, however while fitting I observed that the changes made to the model are miniscule because the model has been trained on 1B+ pairs whereas I train it on around 40 pairs per epochs, to deal with this problem I decided to apply a linear layer on top of the sentence transformer in order to learn the embeddings corresponding to a specific data set. However there seems to be no forward function for the sentence transformers. Their is an alternative with the model.encode() method but it does not change the model parameters. So summarizing I want to create a network that does a forward pass on the sentence transformer, then on the linear layer and then finally get a loss which can be used across the model. Any help would be useful. Thank you.","['python', 'pytorch', 'nlp', 'bert-language-model']",1,"Here is a simple code snippet that adds one simple linear layer on top of a sentence transformer: import torch from sentence_transformers import SentenceTransformer class SentenceTransformerWithLinearLayer(torch.nn.Module): def __init__(self, transformer_model_name): super(SentenceTransformerWithLinearLayer, self).__init__() # Load the sentence transformer model self.sentence_transformer = SentenceTransformer(transformer_model_name) last_layer_dimension = self.sentence_transformer.get_sentence_embedding_dimension() # New linear layer with 16 output dimensions self.linear = torch.nn.Linear(last_layer_dimension, 16) def forward(self, x): # Pass the input through the sentence transformer x = self.sentence_transformer.encode(x, convert_to_numpy=False).unsqueeze(0) # Pass through the linear layer x = self.linear(x) return x This can than be used similarly to a simple sentence transformer. In this example I loaded the all-mpnet-base-v2 model as the base sentence transformer. The input of ""Hello world"" is passed through the sentence transformer and then the linear layer, resulting in a 16 dimensional vector. model = SentenceTransformerWithLinearLayer(""all-mpnet-base-v2"") output = model.forward(""Hello world"") This vector can then be used in a loss function e.g. a MSELoss loss_function = torch.nn.MSELoss() ... expected = ... loss = loss_function(output, expected) loss.backward() ...",2024-03-02 06:20:46,2024-03-02 17:02:21,529,https://stackoverflow.com/questions/78091661/how-to-apply-a-linear-layer-atop-a-sentence-transformer,"How to apply a linear layer atop a sentence transformer Hey there I am trying to create a basic Sentence Transformer model for few shot learning, however while fitting I observed that the changes made to the model are miniscule because the model has been trained on 1B+ pairs whereas I train it on around 40 pairs per epochs, to deal with this problem I decided to apply a linear layer on top of the sentence transformer in order to learn the embeddings corresponding to a specific data set. However there seems to be no forward function for the sentence transformers. Their is an alternative with the model.encode() method but it does not change the model parameters. So summarizing I want to create a network that does a forward pass on the sentence transformer, then on the linear layer and then finally get a loss which can be used across the model. Any help would be useful. Thank you.",apply linear layer atop sentence transformer hey try create basic sentence transformer model shot learning  however fitting observe change make model miniscule model train 1b pair whereas train around 40 pair per epoch  deal problem decide apply linear layer top sentence transformer order learn embedding correspond specific datum set  however seem forward function sentence transformer  alternative modelencode   method change model parameter  summarize want create network forward pass sentence transformer  linear layer finally get loss use across model  help would useful  thank ,simple code snippet add one simple linear layer top sentence transformer  import torch sentencetransformer import sentencetransformer class sentencetransformerwithlinearlayer  torchnn  module   def   init    self  transformermodelname   super  sentencetransformerwithlinearlayer  self  init      load sentence transformer model selfsentencetransformer  sentencetransformer  transformermodelname  lastlayerdimension  selfsentencetransformergetsentenceembeddingdimension    new linear layer 16 output dimension selflinear  torchnn  linear  lastlayerdimension  16  def forward  self  x    pass input sentence transformer x  selfsentencetransformerencode  x  converttonumpy  false  unsqueeze  0   pass linear layer x  selflinear  x  return x use similarly simple sentence transformer  example load all  mpnet  base  v2 model base sentence transformer  input   hello world  pass sentence transformer linear layer  result 16 dimensional vector  model  sentencetransformerwithlinearlayer    all  mpnet  base  v2   output  modelforward    hello world   vector use loss function eg  mseloss lossfunction  torchnn  mseloss    expect   loss  lossfunction  output  expect  lossbackward   ,apply linear layer atop sentence transformer hey try create basic sentence transformer model shot learning  however fitting observe change make model miniscule model train 1b pair whereas train around 40 pair per epoch  deal problem decide apply linear layer top sentence transformer order learn embedding correspond specific datum set  however seem forward function sentence transformer  alternative modelencode   method change model parameter  summarize want create network forward pass sentence transformer  linear layer finally get loss use across model  help would useful  thank  simple code snippet add one simple linear layer top sentence transformer  import torch sentencetransformer import sentencetransformer class sentencetransformerwithlinearlayer  torchnn  module   def   init    self  transformermodelname   super  sentencetransformerwithlinearlayer  self  init      load sentence transformer model selfsentencetransformer  sentencetransformer  transformermodelname  lastlayerdimension  selfsentencetransformergetsentenceembeddingdimension    new linear layer 16 output dimension selflinear  torchnn  linear  lastlayerdimension  16  def forward  self  x    pass input sentence transformer x  selfsentencetransformerencode  x  converttonumpy  false  unsqueeze  0   pass linear layer x  selflinear  x  return x use similarly simple sentence transformer  example load all  mpnet  base  v2 model base sentence transformer  input   hello world  pass sentence transformer linear layer  result 16 dimensional vector  model  sentencetransformerwithlinearlayer    all  mpnet  base  v2   output  modelforward    hello world   vector use loss function eg  mseloss lossfunction  torchnn  mseloss    expect   loss  lossfunction  output  expect  lossbackward   ,Basic Understanding
How to get the SHAP value per class?,"I want to get shap value per class. I have checked tutorial and I found below example how to do this. However, the code do not work because of shap_value.shape is (10,None,6). 10 is your the number of samples, 4 is class. import datasets import pandas as pd import transformers import shap dataset = datasets.load_dataset(""emotion"", split=""train"") data = pd.DataFrame({""text"": dataset[""text""], ""emotion"": dataset[""label""]}) # load the model and tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( ""nateraw/bert-base-uncased-emotion"", use_fast=True ) model = transformers.AutoModelForSequenceClassification.from_pretrained( ""nateraw/bert-base-uncased-emotion"" ).cuda() # build a pipeline object to do predictions pred = transformers.pipeline( ""text-classification"", model=model, tokenizer=tokenizer, device=0, return_all_scores=True, ) explainer = shap.Explainer(pred) shap_values = explainer(data[""text""][:3]) shap.plots.bar(shap_values[:, :, ""joy""].mean(0)) Are there any way to get bar plot for per class?","['machine-learning', 'deep-learning', 'nlp', 'shap', 'xai']",1,"After installing shap 41.0, you have to do : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1 . After that if you encounter : dtype: np.bool you can change: np.bool_ in source code. Of course you can see some warning however graph will be produced ! Edit: if you want to use current version only do this : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1",2024-02-26 12:08:54,2024-02-27 15:21:03,287,https://stackoverflow.com/questions/78060804/how-to-get-the-shap-value-per-class,"How to get the SHAP value per class? I want to get shap value per class. I have checked tutorial and I found below example how to do this. However, the code do not work because of shap_value.shape is (10,None,6). 10 is your the number of samples, 4 is class. import datasets import pandas as pd import transformers import shap dataset = datasets.load_dataset(""emotion"", split=""train"") data = pd.DataFrame({""text"": dataset[""text""], ""emotion"": dataset[""label""]}) # load the model and tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( ""nateraw/bert-base-uncased-emotion"", use_fast=True ) model = transformers.AutoModelForSequenceClassification.from_pretrained( ""nateraw/bert-base-uncased-emotion"" ).cuda() # build a pipeline object to do predictions pred = transformers.pipeline( ""text-classification"", model=model, tokenizer=tokenizer, device=0, return_all_scores=True, ) explainer = shap.Explainer(pred) shap_values = explainer(data[""text""][:3]) shap.plots.bar(shap_values[:, :, ""joy""].mean(0)) Are there any way to get bar plot for per class?",get shap value per class  want get shap value per class  check tutorial find example  however  code work shapvalueshape  10  none6   10 number sample  4 class  import dataset import panda pd import transformer import shap dataset  datasetsloaddataset    emotion   split  train   datum  pd  dataframe     text   dataset    text      emotion   dataset    label      load model tokenizer tokenizer  transformer  autotokenizerfrompretraine    nateraw  bert  base  uncase  emotion   usefast  true  model  transformer  automodelforsequenceclassificationfrompretraine    nateraw  bert  base  uncase  emotion   cuda    build pipeline object prediction pre  transformerspipeline    text  classification   model  model  tokenizer  tokenizer  device0  returnallscore  true   explainer  shap  explainer  pre  shapvalue  explainer  datum    text    3   shapplotsbar  shapvalue        joy   mean  0   way get bar plot per class ,instal shap 410    pip3 install mxnet  mkl160 numpy1231  encounter  dtype  npbool change  npbool  source code  course see warning however graph produce  edit  want use current version   pip3 install mxnet  mkl160 numpy1231,get shap value per class  want get shap value per class  check tutorial find example  however  code work shapvalueshape  10  none6   10 number sample  4 class  import dataset import panda pd import transformer import shap dataset  datasetsloaddataset    emotion   split  train   datum  pd  dataframe     text   dataset    text      emotion   dataset    label      load model tokenizer tokenizer  transformer  autotokenizerfrompretraine    nateraw  bert  base  uncase  emotion   usefast  true  model  transformer  automodelforsequenceclassificationfrompretraine    nateraw  bert  base  uncase  emotion   cuda    build pipeline object prediction pre  transformerspipeline    text  classification   model  model  tokenizer  tokenizer  device0  returnallscore  true   explainer  shap  explainer  pre  shapvalue  explainer  datum    text    3   shapplotsbar  shapvalue        joy   mean  0   way get bar plot per class  instal shap 410    pip3 install mxnet  mkl160 numpy1231  encounter  dtype  npbool change  npbool  source code  course see warning however graph produce  edit  want use current version   pip3 install mxnet  mkl160 numpy1231,Task-Specific Queries
Questions about training LLMs on large text datasets for text generation from scratch,"I made a fully custom made GPT in Jax (with Keras 3), using Tensorflow for the data pipeline. I've trained the model on the Shakespeare dataset and got good results (so no problem with the model). Now I want to train it on the Tiny-Stories dataset which is pretty big with GPT of 15M parameters. Here is the code for loading the data: def get_dataset_lists(ds_path:str): dataset = open(ds_path, ""r"", encoding=""utf-8"").read() # [...] dataset = dataset.split(""<|endoftext|>"") r.shuffle(dataset) dataset:list = spm.Encode( # llama's sentence piece encoder tf.strings.strip(dataset).numpy().tolist(), add_bos=True, add_eos=False ) # [[SOS story], ..., [SOS story]] print(""\tNumber of stories:"", len(dataset)) return dataset def tf_dataload( dataset:list, batch_size:int, maxlen:int, shift:int, ): import functools; import operator dataset = functools.reduce(operator.iconcat, dataset, []) num_tokens = len(dataset); print(""\tNumber of tokens in the dataset is"", num_tokens) unique_tok = set(dataset); print(""\tNumber of unique tokens in the dataset is"", len(unique_tok)) # [SOS story ... SOS story] dataset = tf.data.Dataset.from_tensor_slices(dataset) dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True) # [[...], [...], [...], ...] shape(m, maxlen+1) dataset = dataset.flat_map(lambda window: window.batch(maxlen+1)) dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.shuffle(batch_size*100) dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE) return dataset # (shape(m//B, B, maxlen) shape(m//B, B, maxlen)) def load_data( train_ds_path:str, val_ds_path:str, batch_size:int, maxlen:int, shift:int, ): print(""Training Dataset:"") train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True) print(""Validation Dataset:"") val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True) print(f""\n{train_ds}\n{val_ds}"") datasets = {""train"": train_ds.repeat(), ""val"":val_ds} return datasets I've certain questions regarding the value of the shift ? First I set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (I think there's no problem with the learning rate as I plotted Loss Vs Lr and selected the max learning rate possible and used cosine decay with warmup) So I looked into Karpathy's llama-2 repo and the shift was equal to maxlen. So I set it equal to maxlen and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what Karpathy got (I don't know what's the problem, as I've closely followed Karpathy's llama2 repo) What is shift generally equal to when pre-training an LLM on Language Modelling? Shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if shift is not equal to 1? But then the number of samples will be very large...? And for what number of steps to train a LLM given the number of tokens You may find the below helpful... @dataclass class GPTArgs: """"""GPT Configuration"""""" d_model:int = 288 num_layers:int = 6 num_heads:int = 6 max_context_length:int = 256 vocab_size:int = VOCAB_SIZE # 32K output_units:int = None # equal to vocab_size if None in model init assert d_model % 2 == 0 assert d_model % num_heads == 0 dropout_rate:float = 0.1 @dataclass class TArgs: # lr scheduler init_lr:float = 1e-7 max_lr:float = 6.5e-4 min_lr:float = 0.1*max_lr # The factor is usually 0.1 or 0.0 num_steps:int = 100_000 warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000 decay_steps:int = num_steps # optimizer beta1:float = 0.9 beta2:float = 0.95 weight_decay:float = 1e-1 clipvalue:float = 1e0 num_grad_accumalation_steps:int = 4 # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072 # training checkpoint:str = 'weights/GPTstories/Epoch{epoch}.weights.h5' train_ds_path:str = ""TinyStoriesDataset/TinyStories-train.txt"" val_ds_path:str = ""TinyStoriesDataset/TinyStories-valid.txt"" steps_per_epoch = eval_freq = 2000 eval_steps:int = 200 batch_size:int = 128 patience:int = 10 # early stopping with restore best weights Update 1: I thought that the model wasn't getting the training samples uniformly so I modified the data pipeline and also increased the number of steps to 200,000 . But there were no significant improvements. The training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... Got a loss of 1.67 on validation set def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str): dataset = open(dataset_path, ""r"", encoding=""utf-8"").read() # [...] dataset = dataset.split(""<|endoftext|>"") r.shuffle(dataset) dataset:list = spm.Encode( tf.strings.strip(dataset).numpy().tolist(), add_bos=True, add_eos=False ) # [[SOS story], ..., [SOS story]] print(""Dataset:"") print(""\tNumber of stories:"", len(dataset)) # flatten dataset = functools.reduce(operator.iconcat, dataset, []) num_tokens = len(dataset); print(""\tNumber of tokens in the dataset:"", num_tokens) print(""\tNumber of unique tokens in the dataset:"", len(set(dataset))) dataset = np.asarray(dataset, dtype=np.uint16) # [SOS story ... SOS story] print(""\tAvg length of story:"", num_tokens/((dataset==1).sum())) # shard and save dataset sharded_datasets_list = np.array_split(dataset, num_shards) # [[SOS story...], [...], [...], ...] filenames = [os.path.join(shard_dir, f""shard{i+1}.npy"") for i in range(num_shards)] for filename, sharded_ds in zip(filenames, sharded_datasets_list): with open(filename, ""wb"") as f: np.save(f, sharded_ds) return filenames def load_data_as_tfds( dataset:np.ndarray, maxlen:int, shift:int, ): # [SOS story ... SOS story] dataset = tf.data.Dataset.from_tensor_slices(dataset.tolist()) dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True) # [[...], [...], [...], ...] shape(m, maxlen+1) dataset = dataset.flat_map(lambda window: window.batch(maxlen+1)) dataset = dataset.shuffle(10_000*128) return dataset def batch_tfds( dataset:tf.data.Dataset, batch_size:int, ): dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.shuffle(batch_size*1000) dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.repeat().prefetch(tf.data.AUTOTUNE) return dataset def load_data( dataset_path:str, batch_size:int, maxlen:int, shift:int, num_shards:int, shard_dir:str ): if os.path.exists(shard_dir) and os.listdir(shard_dir): filenames = glob.glob(os.path.join(shard_dir, ""*.npy"")) else: os.makedirs(shard_dir) filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir) r.shuffle(filenames) to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift) num_train_shards = round(0.9651*num_shards) num_val_shards = num_shards-num_train_shards print(""Training Dataset:"") print(f""\tNumber of files taken for training: {num_train_shards}/{num_shards}"") train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]] train_ds = tf.data.Dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) train_ds = batch_tfds(train_ds, batch_size=batch_size) print(""Validation Dataset:"") print(f""\tNumber of files taken for validation: {num_val_shards}/{num_shards}"") val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]] val_ds = tf.data.Dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) val_ds = batch_tfds(val_ds, batch_size=batch_size) print(f""\n{train_ds}\n{val_ds}"") datasets = {""train"": train_ds, ""val"":val_ds} return datasets","['python', 'tensorflow', 'deep-learning', 'nlp', 'tf.data.dataset']",1,"Replaced keras's gradient accumulation argument in AdamW with a custom implementation like in karpathy's and now the loss is decreasing faster. If you are using keras's num_grad_accum , increase num_steps to num_steps *= num_grad_accum",2024-02-22 08:33:55,2024-03-09 12:42:01,470,https://stackoverflow.com/questions/78039417/questions-about-training-llms-on-large-text-datasets-for-text-generation-from-sc,"Questions about training LLMs on large text datasets for text generation from scratch I made a fully custom made GPT in Jax (with Keras 3), using Tensorflow for the data pipeline. I've trained the model on the Shakespeare dataset and got good results (so no problem with the model). Now I want to train it on the Tiny-Stories dataset which is pretty big with GPT of 15M parameters. Here is the code for loading the data: def get_dataset_lists(ds_path:str): dataset = open(ds_path, ""r"", encoding=""utf-8"").read() # [...] dataset = dataset.split(""<|endoftext|>"") r.shuffle(dataset) dataset:list = spm.Encode( # llama's sentence piece encoder tf.strings.strip(dataset).numpy().tolist(), add_bos=True, add_eos=False ) # [[SOS story], ..., [SOS story]] print(""\tNumber of stories:"", len(dataset)) return dataset def tf_dataload( dataset:list, batch_size:int, maxlen:int, shift:int, ): import functools; import operator dataset = functools.reduce(operator.iconcat, dataset, []) num_tokens = len(dataset); print(""\tNumber of tokens in the dataset is"", num_tokens) unique_tok = set(dataset); print(""\tNumber of unique tokens in the dataset is"", len(unique_tok)) # [SOS story ... SOS story] dataset = tf.data.Dataset.from_tensor_slices(dataset) dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True) # [[...], [...], [...], ...] shape(m, maxlen+1) dataset = dataset.flat_map(lambda window: window.batch(maxlen+1)) dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.shuffle(batch_size*100) dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE) return dataset # (shape(m//B, B, maxlen) shape(m//B, B, maxlen)) def load_data( train_ds_path:str, val_ds_path:str, batch_size:int, maxlen:int, shift:int, ): print(""Training Dataset:"") train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True) print(""Validation Dataset:"") val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True) print(f""\n{train_ds}\n{val_ds}"") datasets = {""train"": train_ds.repeat(), ""val"":val_ds} return datasets I've certain questions regarding the value of the shift ? First I set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (I think there's no problem with the learning rate as I plotted Loss Vs Lr and selected the max learning rate possible and used cosine decay with warmup) So I looked into Karpathy's llama-2 repo and the shift was equal to maxlen. So I set it equal to maxlen and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what Karpathy got (I don't know what's the problem, as I've closely followed Karpathy's llama2 repo) What is shift generally equal to when pre-training an LLM on Language Modelling? Shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if shift is not equal to 1? But then the number of samples will be very large...? And for what number of steps to train a LLM given the number of tokens You may find the below helpful... @dataclass class GPTArgs: """"""GPT Configuration"""""" d_model:int = 288 num_layers:int = 6 num_heads:int = 6 max_context_length:int = 256 vocab_size:int = VOCAB_SIZE # 32K output_units:int = None # equal to vocab_size if None in model init assert d_model % 2 == 0 assert d_model % num_heads == 0 dropout_rate:float = 0.1 @dataclass class TArgs: # lr scheduler init_lr:float = 1e-7 max_lr:float = 6.5e-4 min_lr:float = 0.1*max_lr # The factor is usually 0.1 or 0.0 num_steps:int = 100_000 warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000 decay_steps:int = num_steps # optimizer beta1:float = 0.9 beta2:float = 0.95 weight_decay:float = 1e-1 clipvalue:float = 1e0 num_grad_accumalation_steps:int = 4 # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072 # training checkpoint:str = 'weights/GPTstories/Epoch{epoch}.weights.h5' train_ds_path:str = ""TinyStoriesDataset/TinyStories-train.txt"" val_ds_path:str = ""TinyStoriesDataset/TinyStories-valid.txt"" steps_per_epoch = eval_freq = 2000 eval_steps:int = 200 batch_size:int = 128 patience:int = 10 # early stopping with restore best weights Update 1: I thought that the model wasn't getting the training samples uniformly so I modified the data pipeline and also increased the number of steps to 200,000 . But there were no significant improvements. The training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... Got a loss of 1.67 on validation set def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str): dataset = open(dataset_path, ""r"", encoding=""utf-8"").read() # [...] dataset = dataset.split(""<|endoftext|>"") r.shuffle(dataset) dataset:list = spm.Encode( tf.strings.strip(dataset).numpy().tolist(), add_bos=True, add_eos=False ) # [[SOS story], ..., [SOS story]] print(""Dataset:"") print(""\tNumber of stories:"", len(dataset)) # flatten dataset = functools.reduce(operator.iconcat, dataset, []) num_tokens = len(dataset); print(""\tNumber of tokens in the dataset:"", num_tokens) print(""\tNumber of unique tokens in the dataset:"", len(set(dataset))) dataset = np.asarray(dataset, dtype=np.uint16) # [SOS story ... SOS story] print(""\tAvg length of story:"", num_tokens/((dataset==1).sum())) # shard and save dataset sharded_datasets_list = np.array_split(dataset, num_shards) # [[SOS story...], [...], [...], ...] filenames = [os.path.join(shard_dir, f""shard{i+1}.npy"") for i in range(num_shards)] for filename, sharded_ds in zip(filenames, sharded_datasets_list): with open(filename, ""wb"") as f: np.save(f, sharded_ds) return filenames def load_data_as_tfds( dataset:np.ndarray, maxlen:int, shift:int, ): # [SOS story ... SOS story] dataset = tf.data.Dataset.from_tensor_slices(dataset.tolist()) dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True) # [[...], [...], [...], ...] shape(m, maxlen+1) dataset = dataset.flat_map(lambda window: window.batch(maxlen+1)) dataset = dataset.shuffle(10_000*128) return dataset def batch_tfds( dataset:tf.data.Dataset, batch_size:int, ): dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.shuffle(batch_size*1000) dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE) dataset = dataset.repeat().prefetch(tf.data.AUTOTUNE) return dataset def load_data( dataset_path:str, batch_size:int, maxlen:int, shift:int, num_shards:int, shard_dir:str ): if os.path.exists(shard_dir) and os.listdir(shard_dir): filenames = glob.glob(os.path.join(shard_dir, ""*.npy"")) else: os.makedirs(shard_dir) filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir) r.shuffle(filenames) to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift) num_train_shards = round(0.9651*num_shards) num_val_shards = num_shards-num_train_shards print(""Training Dataset:"") print(f""\tNumber of files taken for training: {num_train_shards}/{num_shards}"") train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]] train_ds = tf.data.Dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) train_ds = batch_tfds(train_ds, batch_size=batch_size) print(""Validation Dataset:"") print(f""\tNumber of files taken for validation: {num_val_shards}/{num_shards}"") val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]] val_ds = tf.data.Dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards) # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1) val_ds = batch_tfds(val_ds, batch_size=batch_size) print(f""\n{train_ds}\n{val_ds}"") datasets = {""train"": train_ds, ""val"":val_ds} return datasets",question train llms large text dataset text generation scratch make fully custom make gpt jax  keras 3   use tensorflow datum pipeline   ve train model shakespeare dataset get good result  problem model   want train tiny  stories dataset pretty big gpt 15 m parameter  code loading datum  def getdatasetlist  dspath  str   dataset  open  dspath    r   encoding  utf8   read       dataset  datasetsplit     endoftext    rshuffle  dataset  dataset  list  spm  encode   llama s sentence piece encoder tfstringsstrip  dataset  numpy   tolist    addbo  true  addeo  false     sos story      sos story   print    tnumber story    len  dataset   return dataset def tfdataload  dataset  list  batchsize  int  maxlen  int  shift  int    import functool  import operator dataset  functoolsreduce  operatoriconcat  dataset     numtoken  len  dataset   print    tnumber token dataset   numtoken  uniquetok  set  dataset   print    tnumb unique token dataset   len  uniquetok     sos story  sos story  dataset  tfdata  datasetfromtensorslice  dataset  dataset  datasetwindow  maxlen1  shift  shift  dropremainder  true                  shape   maxlen1  dataset  datasetflatmap  lambda window  windowbatch  maxlen1   dataset  datasetshuffle  10000  batchsize  reshuffleeachiteration  reshuffleeachiteration                      shape  mb  b  maxlen1  dataset  datasetbatch  batchsize  dropremainder  true  numparallelcall  tfdata  autotune  dataset  datasetshuffle  batchsize  100  dataset  datasetmap  lambda window   window     1   window    1     numparallelcall  tfdata  autotune  prefetch  tfdata  autotune  return dataset   shape  mb  b  maxlen  shape  mb  b  maxlen   def loaddata  traindspath  str  valdspath  str  batchsize  int  maxlen  int  shift  int    print    training dataset    trainds  tfdataload  getdatasetlist  traindspath   batchsize  maxlen  shift  reshuffleeachiteration  true  print    validation dataset    valds  tfdataload  getdatasetlist  valdspath   batchsize  maxlen  shift  reshuffleeachiteration  true  print  f  n  trainds  n  valds    dataset     train   traindsrepeat      val   valds  return dataset  ve certain question regard value shift  first set equal 1  training slow  even 100000 step not converge even though decrease slowly  think s problem learn rate plot loss vs lr select max learning rate possible use cosine decay warmup  look karpathy s llama2 repo shift equal maxlen  set equal maxlen train 100000 step model learn slowly  not get loss even close karpathy get  not know s problem   ve closely follow karpathy s llama2 repo  shift generally equal pre  training llm language modelling  not 1  transformer model positionally invariant  would affect model performance shift equal 1  number sample large   number step train llm give number token may find helpful   dataclass class gptargs      gpt configuration    dmodel  int  288 numlayer  int  6 numhead  int  6 maxcontextlength  int  256 vocabsize  int  vocabsize  32 k outputunit  int  none  equal vocabsize none model init assert dmodel  2   0 assert dmodel  numhead   0 dropoutrate  float  01  dataclass class targs   lr scheduler initlr  float  1e7 maxlr  float  65e4 minlr  float  01  maxlr  factor usually 01 00 numstep  int  100000 warmupstep  int  1000  1000  make training stable instead 2000 decaystep  int  numsteps  optimizer beta1  float  09 beta2  float  095 weightdecay  float  1e1 clipvalue  float  1e0 numgradaccumalationsteps  int  4  numtokperupdate  batchsize  maxlen  gradientaccumalation  128  256  4  131072  training checkpoint  str   weight  gptstories  epoch  epoch  weightsh5  traindspath  str    tinystoriesdataset  tinystories  traintxt  valdspath  str    tinystoriesdataset  tinystories  validtxt  stepsperepoch  evalfreq  2000 evalstep  int  200 batchsize  int  128 patience  int  10  early stop restore good weight update 1  thought model not get training sample uniformly modify datum pipeline also increase number step 200000  significant improvement  training still slow end loss decrease 001 every epoch  2000 step   got loss 167 validation set def pretokenizeandsavedataset  datasetpath  str  numshards  int  sharddir  str   dataset  open  datasetpath    r   encoding  utf8   read       dataset  datasetsplit     endoftext    rshuffle  dataset  dataset  list  spm  encode  tfstringsstrip  dataset  numpy   tolist    addbo  true  addeo  false     sos story      sos story   print    dataset    print    tnumber story    len  dataset    flatten dataset  functoolsreduce  operatoriconcat  dataset     numtoken  len  dataset   print    tnumber token dataset    numtoken  print    tnumb unique token dataset    len  set  dataset    dataset  npasarray  dataset  dtype  npuint16    sos story  sos story  print    tavg length story    numtokens   dataset1  sum      shard save dataset shardeddatasetslist  nparraysplit  dataset  numshards     sos story              filename   ospathjoin  sharddir  f  shard  i1  npy   range  numshards   filename  shardedds zip  filename  shardeddatasetslist   open  filename    wb   f  npsave  f  shardedds  return filename def loaddataastfds  dataset  npndarray  maxlen  int  shift  int      sos story  sos story  dataset  tfdata  datasetfromtensorslice  datasettolist    dataset  datasetwindow  maxlen1  shift  shift  dropremainder  true                  shape   maxlen1  dataset  datasetflatmap  lambda window  windowbatch  maxlen1   dataset  datasetshuffle  10000  128  return dataset def batchtfds  dataset  tfdata  dataset  batchsize  int    dataset  datasetbatch  batchsize  dropremainder  true  numparallelcall  tfdata  autotune  dataset  datasetshuffle  batchsize  1000  dataset  datasetmap  lambda window   window     1   window    1     numparallelcall  tfdata  autotune  dataset  datasetrepeat   prefetch  tfdata  autotune  return dataset def loaddata  datasetpath  str  batchsize  int  maxlen  int  shift  int  numshard  int  sharddir  str   ospathexists  sharddir  oslistdir  sharddir   filename  globglob  ospathjoin  sharddir     npy    else  osmakedirs  sharddir  filename  pretokenizeandsavedataset  datasetpath  numshard  numshard  sharddir  sharddir  rshuffle  filename  totfds  lambda dataset  loaddataastfds  dataset  maxlen  maxlen  shift  shift  numtrainshard  round  09651  numshards  numvalshard  numshards  numtrainshard print    training dataset    print  f  tnumber file take training   numtrainshard    numshards    traindatasetslist   totfds  npload  filename   filename filename   numtrainshard   trainds  tfdata  datasetsamplefromdataset  traindatasetslists  weights  1  numtrainshard   numtrainshard                      shape  mb  b  maxlen1  trainds  batchtfds  trainds  batchsize  batchsize  print    validation dataset    print  f  tnumber file take validation   numvalshard    numshards    valdatasetslist   totfds  npload  filename   filename filename  numtrainshard    valds  tfdata  datasetsamplefromdataset  valdatasetslist  weights  1  numvalshard   numvalshards                      shape  mb  b  maxlen1  valds  batchtfds  valds  batchsize  batchsize  print  f  n  trainds  n  valds    dataset     train   trainds    val   valds  return dataset,replaced keras s gradient accumulation argument adamw custom implementation like karpathy s loss decrease fast  use keras s numgradaccum  increase numstep numsteps   numgradaccum,question train llms large text dataset text generation scratch make fully custom make gpt jax  keras 3   use tensorflow datum pipeline   ve train model shakespeare dataset get good result  problem model   want train tiny  stories dataset pretty big gpt 15 m parameter  code loading datum  def getdatasetlist  dspath  str   dataset  open  dspath    r   encoding  utf8   read       dataset  datasetsplit     endoftext    rshuffle  dataset  dataset  list  spm  encode   llama s sentence piece encoder tfstringsstrip  dataset  numpy   tolist    addbo  true  addeo  false     sos story      sos story   print    tnumber story    len  dataset   return dataset def tfdataload  dataset  list  batchsize  int  maxlen  int  shift  int    import functool  import operator dataset  functoolsreduce  operatoriconcat  dataset     numtoken  len  dataset   print    tnumber token dataset   numtoken  uniquetok  set  dataset   print    tnumb unique token dataset   len  uniquetok     sos story  sos story  dataset  tfdata  datasetfromtensorslice  dataset  dataset  datasetwindow  maxlen1  shift  shift  dropremainder  true                  shape   maxlen1  dataset  datasetflatmap  lambda window  windowbatch  maxlen1   dataset  datasetshuffle  10000  batchsize  reshuffleeachiteration  reshuffleeachiteration                      shape  mb  b  maxlen1  dataset  datasetbatch  batchsize  dropremainder  true  numparallelcall  tfdata  autotune  dataset  datasetshuffle  batchsize  100  dataset  datasetmap  lambda window   window     1   window    1     numparallelcall  tfdata  autotune  prefetch  tfdata  autotune  return dataset   shape  mb  b  maxlen  shape  mb  b  maxlen   def loaddata  traindspath  str  valdspath  str  batchsize  int  maxlen  int  shift  int    print    training dataset    trainds  tfdataload  getdatasetlist  traindspath   batchsize  maxlen  shift  reshuffleeachiteration  true  print    validation dataset    valds  tfdataload  getdatasetlist  valdspath   batchsize  maxlen  shift  reshuffleeachiteration  true  print  f  n  trainds  n  valds    dataset     train   traindsrepeat      val   valds  return dataset  ve certain question regard value shift  first set equal 1  training slow  even 100000 step not converge even though decrease slowly  think s problem learn rate plot loss vs lr select max learning rate possible use cosine decay warmup  look karpathy s llama2 repo shift equal maxlen  set equal maxlen train 100000 step model learn slowly  not get loss even close karpathy get  not know s problem   ve closely follow karpathy s llama2 repo  shift generally equal pre  training llm language modelling  not 1  transformer model positionally invariant  would affect model performance shift equal 1  number sample large   number step train llm give number token may find helpful   dataclass class gptargs      gpt configuration    dmodel  int  288 numlayer  int  6 numhead  int  6 maxcontextlength  int  256 vocabsize  int  vocabsize  32 k outputunit  int  none  equal vocabsize none model init assert dmodel  2   0 assert dmodel  numhead   0 dropoutrate  float  01  dataclass class targs   lr scheduler initlr  float  1e7 maxlr  float  65e4 minlr  float  01  maxlr  factor usually 01 00 numstep  int  100000 warmupstep  int  1000  1000  make training stable instead 2000 decaystep  int  numsteps  optimizer beta1  float  09 beta2  float  095 weightdecay  float  1e1 clipvalue  float  1e0 numgradaccumalationsteps  int  4  numtokperupdate  batchsize  maxlen  gradientaccumalation  128  256  4  131072  training checkpoint  str   weight  gptstories  epoch  epoch  weightsh5  traindspath  str    tinystoriesdataset  tinystories  traintxt  valdspath  str    tinystoriesdataset  tinystories  validtxt  stepsperepoch  evalfreq  2000 evalstep  int  200 batchsize  int  128 patience  int  10  early stop restore good weight update 1  thought model not get training sample uniformly modify datum pipeline also increase number step 200000  significant improvement  training still slow end loss decrease 001 every epoch  2000 step   got loss 167 validation set def pretokenizeandsavedataset  datasetpath  str  numshards  int  sharddir  str   dataset  open  datasetpath    r   encoding  utf8   read       dataset  datasetsplit     endoftext    rshuffle  dataset  dataset  list  spm  encode  tfstringsstrip  dataset  numpy   tolist    addbo  true  addeo  false     sos story      sos story   print    dataset    print    tnumber story    len  dataset    flatten dataset  functoolsreduce  operatoriconcat  dataset     numtoken  len  dataset   print    tnumber token dataset    numtoken  print    tnumb unique token dataset    len  set  dataset    dataset  npasarray  dataset  dtype  npuint16    sos story  sos story  print    tavg length story    numtokens   dataset1  sum      shard save dataset shardeddatasetslist  nparraysplit  dataset  numshards     sos story              filename   ospathjoin  sharddir  f  shard  i1  npy   range  numshards   filename  shardedds zip  filename  shardeddatasetslist   open  filename    wb   f  npsave  f  shardedds  return filename def loaddataastfds  dataset  npndarray  maxlen  int  shift  int      sos story  sos story  dataset  tfdata  datasetfromtensorslice  datasettolist    dataset  datasetwindow  maxlen1  shift  shift  dropremainder  true                  shape   maxlen1  dataset  datasetflatmap  lambda window  windowbatch  maxlen1   dataset  datasetshuffle  10000  128  return dataset def batchtfds  dataset  tfdata  dataset  batchsize  int    dataset  datasetbatch  batchsize  dropremainder  true  numparallelcall  tfdata  autotune  dataset  datasetshuffle  batchsize  1000  dataset  datasetmap  lambda window   window     1   window    1     numparallelcall  tfdata  autotune  dataset  datasetrepeat   prefetch  tfdata  autotune  return dataset def loaddata  datasetpath  str  batchsize  int  maxlen  int  shift  int  numshard  int  sharddir  str   ospathexists  sharddir  oslistdir  sharddir   filename  globglob  ospathjoin  sharddir     npy    else  osmakedirs  sharddir  filename  pretokenizeandsavedataset  datasetpath  numshard  numshard  sharddir  sharddir  rshuffle  filename  totfds  lambda dataset  loaddataastfds  dataset  maxlen  maxlen  shift  shift  numtrainshard  round  09651  numshards  numvalshard  numshards  numtrainshard print    training dataset    print  f  tnumber file take training   numtrainshard    numshards    traindatasetslist   totfds  npload  filename   filename filename   numtrainshard   trainds  tfdata  datasetsamplefromdataset  traindatasetslists  weights  1  numtrainshard   numtrainshard                      shape  mb  b  maxlen1  trainds  batchtfds  trainds  batchsize  batchsize  print    validation dataset    print  f  tnumber file take validation   numvalshard    numshards    valdatasetslist   totfds  npload  filename   filename filename  numtrainshard    valds  tfdata  datasetsamplefromdataset  valdatasetslist  weights  1  numvalshard   numvalshards                      shape  mb  b  maxlen1  valds  batchtfds  valds  batchsize  batchsize  print  f  n  trainds  n  valds    dataset     train   trainds    val   valds  return dataset replaced keras s gradient accumulation argument adamw custom implementation like karpathy s loss decrease fast  use keras s numgradaccum  increase numstep numsteps   numgradaccum,Library/Tool-Based Queries
"How to resolve ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided [&#39;label&#39;]","I am working on sentiment analysis using the IMDb dataset and a GPT-2-based model. This is a toy project to understand PEFT and LORA as well as get some experience with the Huggingface library. This is what I've tried out: from datasets import load_dataset splits = [""train"", ""test""] ds = {split: ds for split, ds in zip(splits, load_dataset(""imdb"", split=splits))} from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""gpt2"") # GPT-2 Tokenizer doesn't have a padding token. tokenizer.pad_token = tokenizer.eos_token def preprocess_function(examples): """"""Preprocess the imdb dataset by returning tokenized examples."""""" tokens = tokenizer(examples['text'],padding='max_length',truncation=True) return tokens tokenized_ds = {} for split in splits: tokenized_ds[split] = ds[split].map(preprocess_function, batched=True) model2 = AutoModelForSequenceClassification.from_pretrained( ""gpt2"", num_labels=2, id2label={0: ""NEGATIVE"", 1: ""POSITIVE""}, # For converting predictions to strings label2id={""NEGATIVE"": 0, ""POSITIVE"":1}, ) model2.config.pad_token_id = model.config.eos_token_id from peft import LoraConfig from peft import get_peft_model lora_config = LoraConfig(""lora_gpt2"", fan_in_fan_out=True,) lora_model = get_peft_model(model2, lora_config) trainer_lora = Trainer( model=lora_model, args=TrainingArguments( output_dir=""./data/sentiment_analysis2"", learning_rate=2e-3, # Reduce the batch size if you don't have enough memory per_device_train_batch_size=4, per_device_eval_batch_size=4, num_train_epochs=5, weight_decay=0.01, evaluation_strategy=""epoch"", save_strategy=""epoch"", load_best_model_at_end=True, ), train_dataset=tokenized_ds[""train""], eval_dataset=tokenized_ds[""test""], tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer=tokenizer), compute_metrics=compute_metrics, ) trainer_lora.train() When I run this code, Im getting the following error and have had some difficulty debugging the issue: File /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3018, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose) 3016 # The model's main input name, usually `input_ids`, has be passed for padding 3017 if self.model_input_names[0] not in encoded_inputs: -> 3018 raise ValueError( 3019 ""You should supply an encoding or a list of encodings to this method "" 3020 f""that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}"" 3021 ) 3023 required_input = encoded_inputs[self.model_input_names[0]] 3025 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0): ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label'] I'm not sure how to resolve this and haven't been able to find many examples for this online and was hoping the SO community could help out.","['nlp', 'huggingface-transformers', 'huggingface-tokenizers', 'peft']",1,"Turns out the LoRA model changes the name of the column expected from label, to labels. In order to fix it, you need # LoRA takes in ""labels"", not ""label"" so we need to rename the # training and testing sets train_lora = tokenized_ds['train'].rename_column('label', 'labels') test_lora = tokenized_ds['test'].rename_column('label', 'labels') Also needed was the TaskType in the config: LoraConfig( r=8, lora_alpha=32, target_modules=['c_attn', 'c_proj'], lora_dropout=0.1, bias=""none"", fan_in_fan_out=True, task_type=TaskType.SEQ_CLS )",2024-02-21 04:11:21,2024-02-21 07:15:20,4454,https://stackoverflow.com/questions/78031519/how-to-resolve-valueerror-you-should-supply-an-encoding-or-a-list-of-encodings,"How to resolve ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided [&#39;label&#39;] I am working on sentiment analysis using the IMDb dataset and a GPT-2-based model. This is a toy project to understand PEFT and LORA as well as get some experience with the Huggingface library. This is what I've tried out: from datasets import load_dataset splits = [""train"", ""test""] ds = {split: ds for split, ds in zip(splits, load_dataset(""imdb"", split=splits))} from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""gpt2"") # GPT-2 Tokenizer doesn't have a padding token. tokenizer.pad_token = tokenizer.eos_token def preprocess_function(examples): """"""Preprocess the imdb dataset by returning tokenized examples."""""" tokens = tokenizer(examples['text'],padding='max_length',truncation=True) return tokens tokenized_ds = {} for split in splits: tokenized_ds[split] = ds[split].map(preprocess_function, batched=True) model2 = AutoModelForSequenceClassification.from_pretrained( ""gpt2"", num_labels=2, id2label={0: ""NEGATIVE"", 1: ""POSITIVE""}, # For converting predictions to strings label2id={""NEGATIVE"": 0, ""POSITIVE"":1}, ) model2.config.pad_token_id = model.config.eos_token_id from peft import LoraConfig from peft import get_peft_model lora_config = LoraConfig(""lora_gpt2"", fan_in_fan_out=True,) lora_model = get_peft_model(model2, lora_config) trainer_lora = Trainer( model=lora_model, args=TrainingArguments( output_dir=""./data/sentiment_analysis2"", learning_rate=2e-3, # Reduce the batch size if you don't have enough memory per_device_train_batch_size=4, per_device_eval_batch_size=4, num_train_epochs=5, weight_decay=0.01, evaluation_strategy=""epoch"", save_strategy=""epoch"", load_best_model_at_end=True, ), train_dataset=tokenized_ds[""train""], eval_dataset=tokenized_ds[""test""], tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer=tokenizer), compute_metrics=compute_metrics, ) trainer_lora.train() When I run this code, Im getting the following error and have had some difficulty debugging the issue: File /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3018, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose) 3016 # The model's main input name, usually `input_ids`, has be passed for padding 3017 if self.model_input_names[0] not in encoded_inputs: -> 3018 raise ValueError( 3019 ""You should supply an encoding or a list of encodings to this method "" 3020 f""that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}"" 3021 ) 3023 required_input = encoded_inputs[self.model_input_names[0]] 3025 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0): ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label'] I'm not sure how to resolve this and haven't been able to find many examples for this online and was hoping the SO community could help out.",resolve valueerror  supply encoding list encoding method include inputid  provide    39  label   39   work sentiment analysis use imdb dataset gpt2  base model  toy project understand peft lora well get experience huggingface library   ve try  dataset import loaddataset split     train     test   ds   split  ds split  ds zip  split  loaddataset    imdb   split  split    transformer import autotokenizer tokenizer  autotokenizerfrompretraine    gpt2    gpt2 tokenizer not pad token  tokenizerpadtoken  tokenizereostoken def preprocessfunction  example       preprocess imdb dataset return tokenized example     token  tokenizer  example   text    paddingmaxlength   truncation  true  return token tokenizedds    split split  tokenizedds  split   ds  split  map  preprocessfunction  batch  true  model2  automodelforsequenceclassificationfrompretraine    gpt2   numlabels2  id2label  0    negative   1    positive     convert prediction string label2id    negative   0    positive  1    model2configpadtokenid  modelconfigeostokenid peft import loraconfig peft import getpeftmodel loraconfig  loraconfig    loragpt2   faninfanout  true   loramodel  getpeftmodel  model2  loraconfig  trainerlora  trainer  model  loramodel  arg  trainingarguments  outputdir  data  sentimentanalysis2   learningrate2e3   reduce batch size not enough memory perdevicetrainbatchsize4  perdeviceevalbatchsize4  numtrainepochs5  weightdecay001  evaluationstrategy  epoch   savestrategy  epoch   loadbestmodelatend  true    traindataset  tokenizedds    train    evaldataset  tokenizedds    test    tokenizer  tokenizer  datacollator  datacollatorwithpadding  tokenizer  tokenizer   computemetric  computemetric   trainerloratrain   run code  i m getting follow error difficulty debug issue  file opt  conda  lib  python310  site  package  transformer  tokenizationutilsbasepy3018  pretrainedtokenizerbasepad  self  encodedinput  padding  maxlength  padtomultipleof  returnattentionmask  returntensor  verbose  3016  model s main input name  usually  inputids   pass pad 3017 selfmodelinputname  0  encodedinput    3018 raise valueerror  3019   supply encoding list encoding method   3020 f  include  selfmodelinputname  0    provide  list  encodedinputskeys      3021  3023 requiredinput  encodedinput  selfmodelinputname  0   3025 requiredinput none  isinstance  requiredinput  sized  len  requiredinput    0   valueerror  supply encoding list encoding method include inputid  provide   label    m sure resolve not able find many example online hope community could help ,turn lora model change name column expect label  label  order fix  need  lora take   label     label  need rename  training testing set trainlora  tokenizedds   train   renamecolumn   label    label   testlora  tokenizedds   test   renamecolumn   label    label   also need tasktype config  loraconfig  r8  loraalpha32  targetmodules   cattn    cproj    loradropout01  bias  none   faninfanout  true  tasktype  tasktype  seqcls ,resolve valueerror  supply encoding list encoding method include inputid  provide    39  label   39   work sentiment analysis use imdb dataset gpt2  base model  toy project understand peft lora well get experience huggingface library   ve try  dataset import loaddataset split     train     test   ds   split  ds split  ds zip  split  loaddataset    imdb   split  split    transformer import autotokenizer tokenizer  autotokenizerfrompretraine    gpt2    gpt2 tokenizer not pad token  tokenizerpadtoken  tokenizereostoken def preprocessfunction  example       preprocess imdb dataset return tokenized example     token  tokenizer  example   text    paddingmaxlength   truncation  true  return token tokenizedds    split split  tokenizedds  split   ds  split  map  preprocessfunction  batch  true  model2  automodelforsequenceclassificationfrompretraine    gpt2   numlabels2  id2label  0    negative   1    positive     convert prediction string label2id    negative   0    positive  1    model2configpadtokenid  modelconfigeostokenid peft import loraconfig peft import getpeftmodel loraconfig  loraconfig    loragpt2   faninfanout  true   loramodel  getpeftmodel  model2  loraconfig  trainerlora  trainer  model  loramodel  arg  trainingarguments  outputdir  data  sentimentanalysis2   learningrate2e3   reduce batch size not enough memory perdevicetrainbatchsize4  perdeviceevalbatchsize4  numtrainepochs5  weightdecay001  evaluationstrategy  epoch   savestrategy  epoch   loadbestmodelatend  true    traindataset  tokenizedds    train    evaldataset  tokenizedds    test    tokenizer  tokenizer  datacollator  datacollatorwithpadding  tokenizer  tokenizer   computemetric  computemetric   trainerloratrain   run code  i m getting follow error difficulty debug issue  file opt  conda  lib  python310  site  package  transformer  tokenizationutilsbasepy3018  pretrainedtokenizerbasepad  self  encodedinput  padding  maxlength  padtomultipleof  returnattentionmask  returntensor  verbose  3016  model s main input name  usually  inputids   pass pad 3017 selfmodelinputname  0  encodedinput    3018 raise valueerror  3019   supply encoding list encoding method   3020 f  include  selfmodelinputname  0    provide  list  encodedinputskeys      3021  3023 requiredinput  encodedinput  selfmodelinputname  0   3025 requiredinput none  isinstance  requiredinput  sized  len  requiredinput    0   valueerror  supply encoding list encoding method include inputid  provide   label    m sure resolve not able find many example online hope community could help  turn lora model change name column expect label  label  order fix  need  lora take   label     label  need rename  training testing set trainlora  tokenizedds   train   renamecolumn   label    label   testlora  tokenizedds   test   renamecolumn   label    label   also need tasktype config  loraconfig  r8  loraalpha32  targetmodules   cattn    cproj    loradropout01  bias  none   faninfanout  true  tasktype  tasktype  seqcls ,Library/Tool-Based Queries
What and how LLM is used for ranking organization job title?,"Suppose there's a context like this context = Andy is a vice manager of finance department. \n Rio is a general manager finance deparment. \n Jason is a general manager finance deparment. question = who is the leader of finance department ? what task is this called ? what model is used ? how does the model know which title is higher ? how does the model handle two same data ? (e.g., Rio and Jason) thanks","['nlp', 'bert-language-model', 'large-language-model', 'nlp-question-answering']",1,"This task is a question-answering task. Likely, your model already knows which title is higher based on the training data. However, since 'Rio' and 'Jason' both hold the same position, it would be difficult to determine who the leader of the finance department is without additional information. The model might respond by suggesting all possible answers or asking for more information.",2024-02-21 02:03:50,2024-02-21 05:19:23,64,https://stackoverflow.com/questions/78031203/what-and-how-llm-is-used-for-ranking-organization-job-title,"What and how LLM is used for ranking organization job title? Suppose there's a context like this context = Andy is a vice manager of finance department. \n Rio is a general manager finance deparment. \n Jason is a general manager finance deparment. question = who is the leader of finance department ? what task is this called ? what model is used ? how does the model know which title is higher ? how does the model handle two same data ? (e.g., Rio and Jason) thanks",llm use rank organization job title  suppose s context like context  andy vice manager finance department  n rio general manager finance deparment  n jason general manager finance deparment  question  leader finance department  task call  model use  model know title higher  model handle two datum   eg  rio jason  thank,task question  answer task  likely  model already know title higher base training datum  however  since  rio   jason  hold position  would difficult determine leader finance department without additional information  model might respond suggest possible answer ask information ,llm use rank organization job title  suppose s context like context  andy vice manager finance department  n rio general manager finance deparment  n jason general manager finance deparment  question  leader finance department  task call  model use  model know title higher  model handle two datum   eg  rio jason  thank task question  answer task  likely  model already know title higher base training datum  however  since  rio   jason  hold position  would difficult determine leader finance department without additional information  model might respond suggest possible answer ask information ,Task-Specific Queries
Shorten product title to a specific length using python nlp libraries,"I have a collection of products for which I need a specific product name shorter than 40 characters. My input product name is a string column longer than 40 characters per item, so I need to make this shorter. I could use some string methods, but in that case some product names could end up in senseless names. As an example, an input name could be 'Cut Resistant Gloves, Size 8, Grey/Black - 12 per DZ' (52). How could I get from this to, as an example, 'Resistant Size 8 Grey/Black Gloves' (34)? Thanks in advance I would like to end up with a new column in my data frame containing this new product names shorter than 40 characters.","['python', 'string', 'nlp', 'nltk', 'e-commerce']",1,"You can modify the logic implemented below as per your requirement: import pandas as pd import spacy nlp = spacy.load(""en_core_web_sm"") doc = nlp(product_name) shortened_tokens = [] noun_tokens = [] adjective_tokens = [] size_tokens = [] # Iterate over tokens and identify nouns, adjectives, and size/volume information for token in doc: if token.pos_ == ""NOUN"": noun_tokens.append(token.text) elif token.pos_ == ""ADJ"": adjective_tokens.append(token.text) elif token.pos_ == ""NUM"" and token.head.text.lower() in [""size"", ""vol"", ""volume""]: size_tokens.append(token.text) elif token.lower_ in [""size"", ""vol"", ""volume""]: size_tokens.append(token.text) # Determine the number of adjectives and nouns to include num_adjectives = min(len(adjective_tokens), Max_Adj_count) # Initialise Max_Adj_count as the max number of adjectives permissible num_nouns = min(len(noun_tokens), Max_noun_count) # Initialise Max_Noun_count as the max number of nouns permissible # Construct the shortened name using specific rules size_info = "" "".join(size_tokens[:1]) shortened_tokens.extend(adjective_tokens[:num_adjectives]) shortened_tokens.extend(size_info.split()) shortened_tokens.extend(noun_tokens[:num_nouns]) shortened_name = "" "".join(shortened_tokens) # If the shortened name is longer than 40 characters, truncate at the nearest word boundary if len(shortened_name) > 40: shortened_name = "" "".join(shortened_name.split()[:7])",2024-02-16 10:09:14,2024-02-16 10:47:23,111,https://stackoverflow.com/questions/78006552/shorten-product-title-to-a-specific-length-using-python-nlp-libraries,"Shorten product title to a specific length using python nlp libraries I have a collection of products for which I need a specific product name shorter than 40 characters. My input product name is a string column longer than 40 characters per item, so I need to make this shorter. I could use some string methods, but in that case some product names could end up in senseless names. As an example, an input name could be 'Cut Resistant Gloves, Size 8, Grey/Black - 12 per DZ' (52). How could I get from this to, as an example, 'Resistant Size 8 Grey/Black Gloves' (34)? Thanks in advance I would like to end up with a new column in my data frame containing this new product names shorter than 40 characters.",shorten product title specific length use python nlp library collection product need specific product name short 40 character  input product name string column long 40 character per item  need make short  could use string method  case product name could end senseless name  example  input name could  cut resistant gloves  size 8  grey  black  12 per dz   52   could get  example   resistant size 8 grey  black gloves   34   thank advance would like end new column datum frame contain new product name short 40 character ,modify logic implement per requirement  import panda pd import spacy nlp  spacyload    encorewebsm   doc  nlp  productname  shortenedtokens    nountokens    adjectivetoken    sizetoken     iterate token identify noun  adjective  size  volume information token doc  tokenpos      noun   nountokensappend  tokentext  elif tokenpo      adj   adjectivetokensappend  tokentext  elif tokenpo      num  tokenheadtextlow      size     vol     volume    sizetokensappend  tokentext  elif tokenlower     size     vol     volume    sizetokensappend  tokentext   determine number adjective noun include numadjectives  min  len  adjectivetokens   maxadjcount   initialise maxadjcount max number adjective permissible numnoun  min  len  nountokens   maxnouncount   initialise maxnouncount max number noun permissible  construct shorten name use specific rule sizeinfo      join  sizetoken  1   shortenedtokensextend  adjectivetoken   numadjectives   shortenedtokensextend  sizeinfosplit    shortenedtokensextend  nountokens   numnoun   shortenedname      join  shortenedtokens   shorten name long 40 character  truncate near word boundary len  shortenedname   40  shortenedname      join  shortenednamesplit     7  ,shorten product title specific length use python nlp library collection product need specific product name short 40 character  input product name string column long 40 character per item  need make short  could use string method  case product name could end senseless name  example  input name could  cut resistant gloves  size 8  grey  black  12 per dz   52   could get  example   resistant size 8 grey  black gloves   34   thank advance would like end new column datum frame contain new product name short 40 character  modify logic implement per requirement  import panda pd import spacy nlp  spacyload    encorewebsm   doc  nlp  productname  shortenedtokens    nountokens    adjectivetoken    sizetoken     iterate token identify noun  adjective  size  volume information token doc  tokenpos      noun   nountokensappend  tokentext  elif tokenpo      adj   adjectivetokensappend  tokentext  elif tokenpo      num  tokenheadtextlow      size     vol     volume    sizetokensappend  tokentext  elif tokenlower     size     vol     volume    sizetokensappend  tokentext   determine number adjective noun include numadjectives  min  len  adjectivetokens   maxadjcount   initialise maxadjcount max number adjective permissible numnoun  min  len  nountokens   maxnouncount   initialise maxnouncount max number noun permissible  construct shorten name use specific rule sizeinfo      join  sizetoken  1   shortenedtokensextend  adjectivetoken   numadjectives   shortenedtokensextend  sizeinfosplit    shortenedtokensextend  nountokens   numnoun   shortenedname      join  shortenedtokens   shorten name long 40 character  truncate near word boundary len  shortenedname   40  shortenedname      join  shortenednamesplit     7  ,Library/Tool-Based Queries
Key matrix redundant in Transformer language models?,"Simple implementations of Transformer language models such as this one define 3 matrices K,Q,V to compute keys, queries and values. However matrices K and Q are never used separately: all Transformer computations form their product Q^t K . So I wonder why not learn this product matrix directly instead of splitting it into 2 matrices K and Q. Part of the answer may come from the size of K and Q, which is d -> n , where d is the dimension of the token embeddings and n is the dimension of keys and queries. The size of Q^t K is d -> d . So learning K and Q separately means optimizing 2*n*d parameters, whereas learning the product Q^t K is d*d parameters. The only useful splitting I see is when n <= d/2 , because that's less parameters to optimize. But at the limit case n = d/2 , the rank of the product matrix Q^t K is d/2 , which is very degenerate. With the same number of parameters d^2 , we could learn an unconstrained square matrix. That might learn more flexible and subtle patterns in the training data. In the Attention is all you need paper, base model page 9, we see d = 512 and n = 64, so the product matrix Q^t K does have degenerate rank. Is reducing the number of parameters the true and unique intent here? Is there a theoretical justification that these degenerate ranks help natural language processing?","['nlp', 'transformer-model']",1,"The 2 * n * d vs. d * d structure is very similar to how a LORA works, or indeed a WALS model. So as long as 2 * n * d < d * d , this might well be a good way of saving parameters.",2024-02-08 22:14:21,2024-05-14 08:28:01,94,https://stackoverflow.com/questions/77965060/key-matrix-redundant-in-transformer-language-models,"Key matrix redundant in Transformer language models? Simple implementations of Transformer language models such as this one define 3 matrices K,Q,V to compute keys, queries and values. However matrices K and Q are never used separately: all Transformer computations form their product Q^t K . So I wonder why not learn this product matrix directly instead of splitting it into 2 matrices K and Q. Part of the answer may come from the size of K and Q, which is d -> n , where d is the dimension of the token embeddings and n is the dimension of keys and queries. The size of Q^t K is d -> d . So learning K and Q separately means optimizing 2*n*d parameters, whereas learning the product Q^t K is d*d parameters. The only useful splitting I see is when n <= d/2 , because that's less parameters to optimize. But at the limit case n = d/2 , the rank of the product matrix Q^t K is d/2 , which is very degenerate. With the same number of parameters d^2 , we could learn an unconstrained square matrix. That might learn more flexible and subtle patterns in the training data. In the Attention is all you need paper, base model page 9, we see d = 512 and n = 64, so the product matrix Q^t K does have degenerate rank. Is reducing the number of parameters the true and unique intent here? Is there a theoretical justification that these degenerate ranks help natural language processing?",key matrix redundant transformer language model  simple implementation transformer language model one define 3 matrix k  q  v compute key  query value  however matrice k q never use separately  transformer computation form product qt k  wonder learn product matrix directly instead split 2 matrix k q  part answer may come size k q    n  dimension token embedding n dimension key query  size qt k    learn k q separately mean optimize 2  n  parameter  whereas learn product qt k  parameter  useful splitting see n   d2  s less parameter optimize  limit case n  d2  rank product matrix qt k d2  degenerate  number parameter d2  could learn unconstrained square matrix  might learn flexible subtle pattern training datum  attention need paper  base model page 9  see  512 n  64  product matrix qt k degenerate rank  reduce number parameter true unique intent  theoretical justification degenerate rank help natural language processing ,2  n  vs  structure similar lora work  indeed wals model  long 2  n     might well good way save parameter ,key matrix redundant transformer language model  simple implementation transformer language model one define 3 matrix k  q  v compute key  query value  however matrice k q never use separately  transformer computation form product qt k  wonder learn product matrix directly instead split 2 matrix k q  part answer may come size k q    n  dimension token embedding n dimension key query  size qt k    learn k q separately mean optimize 2  n  parameter  whereas learn product qt k  parameter  useful splitting see n   d2  s less parameter optimize  limit case n  d2  rank product matrix qt k d2  degenerate  number parameter d2  could learn unconstrained square matrix  might learn flexible subtle pattern training datum  attention need paper  base model page 9  see  512 n  64  product matrix qt k degenerate rank  reduce number parameter true unique intent  theoretical justification degenerate rank help natural language processing  2  n  vs  structure similar lora work  indeed wals model  long 2  n     might well good way save parameter ,Implementation Issues
Are [INST] and [/INST] needed for mistral chat?,"On the page Open-weight models , they use [INST] and [/INST] in the template. (If I understood correctly, they fined tuned the basic text completion model on a dataset of instructions with the format <s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST] so that you can ""turn on/off"" the instruction mode.) However, in the guide for RAG, they do not use these special tokens, so I am confused about whether they are necessary. Is the instruct model being used or is there another model fined tuned on chat messages? I found this page Fine-tuning Mistral 7B for chat , which suggests that the instruct model can be used for chat. To show the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on HuggingFace. No tricks, no proprietary data. The resulting model, Mistral 7B Instruct, outperforms all 7B models on MT-Bench, and is comparable to 13B chat models.","['nlp', 'mistral-7b']",2,"Mistral got two different models, Mistral-7B and Mistral-7B-Instruct.The Mistral-7B-Instruct is fine-tuned for conversation and question answering. For Instruct you need to put [INST] and same as you already mentioned. But for Mistral-7B you can give any format and it will try to generate. Also, if you are using transformers library you can use apply_chat_template ( https://huggingface.co/docs/transformers/main/en/chat_templating ). for more details about mistral, you can visit https://www.promptingguide.ai/models/mistral-7b",2024-02-08 17:32:46,2024-02-17 13:00:40,3103,https://stackoverflow.com/questions/77963718/are-inst-and-inst-needed-for-mistral-chat,"Are [INST] and [/INST] needed for mistral chat? On the page Open-weight models , they use [INST] and [/INST] in the template. (If I understood correctly, they fined tuned the basic text completion model on a dataset of instructions with the format <s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST] so that you can ""turn on/off"" the instruction mode.) However, in the guide for RAG, they do not use these special tokens, so I am confused about whether they are necessary. Is the instruct model being used or is there another model fined tuned on chat messages? I found this page Fine-tuning Mistral 7B for chat , which suggests that the instruct model can be used for chat. To show the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on HuggingFace. No tricks, no proprietary data. The resulting model, Mistral 7B Instruct, outperforms all 7B models on MT-Bench, and is comparable to 13B chat models.", inst   inst  need mistral chat  page open  weight model  use  inst   inst  template   understand correctly  fine tuned basic text completion model dataset instruction format    inst  instruction  inst  model answer  s   inst  follow  up instruction  inst    turn on  off  instruction mode   however  guide rag  use special token  confuse whether necessary  instruct model use another model fine tune chat message  find page fine  tuning mistral 7b chat  suggest instruct model use chat  show generalization capability mistral 7b  fine  tune instruction dataset publicly available huggingface  trick  proprietary datum  result model  mistral 7b instruct  outperform 7b model mt  bench  comparable 13b chat model ,mistral get two different model  mistral7b mistral7b  instruct  the mistral7b  instruct fine  tune conversation question answer  instruct need put  inst  already mention  mistral7b give format try generate  also  use transformer library use applychattemplate    huggingfaceco  doc  transformer  main  en  chattemplating   detail mistral  visit   wwwpromptingguideai  model  mistral7b, inst   inst  need mistral chat  page open  weight model  use  inst   inst  template   understand correctly  fine tuned basic text completion model dataset instruction format    inst  instruction  inst  model answer  s   inst  follow  up instruction  inst    turn on  off  instruction mode   however  guide rag  use special token  confuse whether necessary  instruct model use another model fine tune chat message  find page fine  tuning mistral 7b chat  suggest instruct model use chat  show generalization capability mistral 7b  fine  tune instruction dataset publicly available huggingface  trick  proprietary datum  result model  mistral 7b instruct  outperform 7b model mt  bench  comparable 13b chat model  mistral get two different model  mistral7b mistral7b  instruct  the mistral7b  instruct fine  tune conversation question answer  instruct need put  inst  already mention  mistral7b give format try generate  also  use transformer library use applychattemplate    huggingfaceco  doc  transformer  main  en  chattemplating   detail mistral  visit   wwwpromptingguideai  model  mistral7b,Basic Understanding
Extracting and Identifying locations with NLP + Spacy,"My goal is to be able to recognize (aka identify) and identify (aka name, retrieve an ID) locations from text using NLP. I'm using Spacy specifically. There are about 1,000 possible locations, but the difficulty is that they are unlikely to be written in a fully qualified way, not to mention spelling mistakes and aliases. For example, the Mission neighborhood in San Francisco written in a fully-qualified way might be (1) Mission (2) City of San Francisco (3) San Francisco County (4) California (5) US . (The numbers are just to illustrate the separate pieces.) However, many people might write it as (1) Mission (2) City of San Francisco , or (1) Mission , or (1) Mission (2) City of San Francisco (4) California . (Not to mention that #1 might be called ""Mission District"", #2 might be called ""San Francisco"", #4 might be ""CA"", etc.) So my goal is to be able have an ID for ""Mission"" and all other neighborhoods, and ID for California and some other states, etc. If the text is like Mission, San Francisco, CA then I get the Mission ID. If the text is like San Francisco, CA then I get the San Francisco ID. It's also easy to create synthetic training data by creating aliases of the individual location pieces (e.g., (a) ""City of San Francisco"", (b) ""San Francisco"", (c) ""San Francisco City"") and permutations of the ""name chain"" (e.g, 1 + 2 + 3 + 4 + 5, 1 + 2, 1 + 2 + 5, etc) for each alias. Rough estimate is about 50 combinations of alias and name chain per location, or about O(50,000) total values. So, extraction seems to be a good job for NER. The surrounding text usually has a bit of context (e.g., ""Location: ...."" or ""Comes from ..."". However, I'm unsure about the ability to do identification. My understanding is that much of the NER identification (e.g., Spacy's EntityLinker , which I planned on using) relies on surrounding context . I expect that there will be very little surrounding context that would help disambiguate one of the O(1000) locations from others. I also understand that EntityLinker matches on the token is lookup and not statistical (in other words, the value is from disambiguating when you have multiple exact-string matches and not from disambiguating from multiple very-fuzzy matches). The KnowledgeBase / LookupDB does have a mechanism for setting aliases , so I could add each permutation as an alias. But at that point I feel like I'm not getting any value out of the EntityLinker's statistical models. If I have to create a gazetteer for the identification aspect, then maybe it makes sense to put all my effort into the gazetteer and skip the NER?","['nlp', 'spacy', 'named-entity-recognition']",2,"Thanks to Vimal for the thoughts. As I suspected, and Vishal confirmed, I needed to extract the string first, and then process it with a separate, non-NLP algorithm. I ended up solving this in two ways, and wanted to document my findings. With some testing I found that an LLM (GPT) was actually pretty effective at determining the ""administrative hierarchy"" given the extracted string. This prompt, for example: Evaluate the following place identifier and determine the most likely place. List the administrative entities from the lowest-level to the highest-level. Explain your reasoning. """"""'Castro, San Francisco, U.S."""""" returns this (plus some additional explanation): The place identifier ""Castro, San Francisco, U.S."" likely refers to a specific location within the city of San Francisco in the United States. I can't find the final version of my prompt at the moment, but I was able to tweak it to get it to provide JSON with the administrative entities in order (national, first level, second level, etc), plus I asked for any ""geographical feature"" as a catch-all. (In some cases, I found that my extracted term was something like ""Bay Area, United States"", and GPT was able to sort that out with the right prompting.) There was a little bit of hallucination in my testing, which worried me. With all that being said, I ended up on a much lower-tech approach, along the lines of my original gazetteer idea. My original plan was to use the gazetteer idea and then sort out the remaining strings with GPT. I ended up matching like 98% using this approach, and the unmatched strings were pretty objectively wrong and not worth sending to GPT. (Like a city with an incorrect country.) To create the gazetteer: This approach used a dictionary which I compiled from the wikidata API search API. I did a first-pass and sent all the strings through Wikidata search to get the top 10 matching entities for each search string. E.g., https://www.wikidata.org/w/api.php?action=query&list=search&srsearch=castro%20san%20francisco%20california&srwhat=text&srlimit=10&srprop=titlesnippet|categorysnippet&srsort=incoming_links_desc I did some pre-filtering to exclude any entity that wasn't an instance of or subclass of geographical features or administrative regions (using lists that I manually searched for and downloaded manually). Then I ran all those entities through a method that stored the data (including name, aliases, lat/lng, etc) and walked up through the ""administrative entity"" and ""country"" paths to collect the family tree. To search for place names: Compiled a dictionary where the keys were entity names and aliases. I took the list of places (castro, san francisco, california) and started with the lowest-level string (castro) and did a fuzzy search against the dictionary keys to look for a match. Anything that matched over, e.g., 85% was chosen as a candidate. Then I created a list of all the candidate's parent (+grandparent/etc) names and aliases, and I looped through the next place names to try to match every place against a name in the parents list, and got those scores. Added the scores up. Some other operations to divide by the number of places that I was looking at, bias toward places with fewer parents (so Castro Valley, California would score higher than Castro, San Francisco, California), etc. All in all, this was surprisingly effective.",2024-02-06 22:34:12,2024-02-19 19:45:11,1110,https://stackoverflow.com/questions/77951208/extracting-and-identifying-locations-with-nlp-spacy,"Extracting and Identifying locations with NLP + Spacy My goal is to be able to recognize (aka identify) and identify (aka name, retrieve an ID) locations from text using NLP. I'm using Spacy specifically. There are about 1,000 possible locations, but the difficulty is that they are unlikely to be written in a fully qualified way, not to mention spelling mistakes and aliases. For example, the Mission neighborhood in San Francisco written in a fully-qualified way might be (1) Mission (2) City of San Francisco (3) San Francisco County (4) California (5) US . (The numbers are just to illustrate the separate pieces.) However, many people might write it as (1) Mission (2) City of San Francisco , or (1) Mission , or (1) Mission (2) City of San Francisco (4) California . (Not to mention that #1 might be called ""Mission District"", #2 might be called ""San Francisco"", #4 might be ""CA"", etc.) So my goal is to be able have an ID for ""Mission"" and all other neighborhoods, and ID for California and some other states, etc. If the text is like Mission, San Francisco, CA then I get the Mission ID. If the text is like San Francisco, CA then I get the San Francisco ID. It's also easy to create synthetic training data by creating aliases of the individual location pieces (e.g., (a) ""City of San Francisco"", (b) ""San Francisco"", (c) ""San Francisco City"") and permutations of the ""name chain"" (e.g, 1 + 2 + 3 + 4 + 5, 1 + 2, 1 + 2 + 5, etc) for each alias. Rough estimate is about 50 combinations of alias and name chain per location, or about O(50,000) total values. So, extraction seems to be a good job for NER. The surrounding text usually has a bit of context (e.g., ""Location: ...."" or ""Comes from ..."". However, I'm unsure about the ability to do identification. My understanding is that much of the NER identification (e.g., Spacy's EntityLinker , which I planned on using) relies on surrounding context . I expect that there will be very little surrounding context that would help disambiguate one of the O(1000) locations from others. I also understand that EntityLinker matches on the token is lookup and not statistical (in other words, the value is from disambiguating when you have multiple exact-string matches and not from disambiguating from multiple very-fuzzy matches). The KnowledgeBase / LookupDB does have a mechanism for setting aliases , so I could add each permutation as an alias. But at that point I feel like I'm not getting any value out of the EntityLinker's statistical models. If I have to create a gazetteer for the identification aspect, then maybe it makes sense to put all my effort into the gazetteer and skip the NER?",extract identify location nlp  spacy goal able recognize  aka identify  identify  aka name  retrieve id  location text use nlp   m use spacy specifically  1000 possible location  difficulty unlikely write fully qualified way  mention spelling mistake alias  example  mission neighborhood san francisco write fully  qualified way might  1  mission  2  city san francisco  3  san francisco county  4  california  5  us   number illustrate separate piece   however  many people might write  1  mission  2  city san francisco   1  mission   1  mission  2  city san francisco  4  california   mention  1 might call   mission district    2 might call   san francisco    4 might   ca   etc   goal able id   mission  neighborhood  id california state  etc  text like mission  san francisco  ca get mission id  text like san francisco  ca get san francisco id  be also easy create synthetic training datum create alias individual location piece  eg      city san francisco    b    san francisco    c    san francisco city   permutation   name chain   eg  1  2  3  4  5  1  2  1  2  5  etc  alia  rough estimate 50 combination alias name chain per location   50000  total value   extraction seem good job ner  surround text usually bit context  eg    location      come    however   m unsure ability identification  understand much ner identification  eg  spacy s entitylinker  plan use  relie surround context  expect little surround context would help disambiguate one  1000  location other  also understand entitylinker match token lookup statistical  word  value disambiguate multiple exact  string match disambiguate multiple very  fuzzy match   knowledgebase  lookupdb mechanism set alias  could add permutation alia  point feel like  m get value entitylinker s statistical model  create gazetteer identification aspect  maybe make sense put effort gazetteer skip ner ,thank vimal thought  suspect  vishal confirm  need extract string first  process separate  non  nlp algorithm  end solve two way  want document finding  testing find llm  gpt  actually pretty effective determine   administrative hierarchy  give extract string  prompt  example  evaluate follow place identifier determine likely place  list administrative entity low  level high  level  explain reasoning        castro  san francisco  us    return  plus additional explanation   place identifier   castro  san francisco  us  likely refer specific location within city san francisco united states  can not find final version prompt moment  able tweak get provide json administrative entity order  national  first level  second level  etc   plus ask   geographical feature  catch  all   case  find extract term something like   bay area  united states   gpt able sort right prompt   little bit hallucination testing  worry  say  end much low  tech approach  along line original gazetteer idea  original plan use gazetteer idea sort remain string gpt  end match like 98  use approach  unmatched string pretty objectively wrong worth send gpt   like city incorrect country   create gazetteer  approach use dictionary compile wikidata api search api  first  pass send string wikidata search get top 10 matching entity search string  eg    wwwwikidataorg  w  apiphp  action  query  list  search  srsearch  castro  20san  20francisco  20california  srwhat  text  srlimit10  srprop  titlesnippetcategorysnippet  srsort  incominglinksdesc pre  filtering exclude entity not instance subclass geographical feature administrative region  use list manually search download manually   run entity method store datum  include name  alias  lat  lng  etc  walk   administrative entity    country  path collect family tree  search place name  compile dictionary key entity name alias  take list place  castro  san francisco  california  start low  level string  castro  fuzzy search dictionary key look match  anything match  eg  85  choose candidate  create list candidate s parent   grandparent  etc  name alias  loop next place name try match every place name parent list  get score  add score  operation divide number place look  bias toward place few parent  castro valley  california would score high castro  san francisco  california   etc   surprisingly effective ,extract identify location nlp  spacy goal able recognize  aka identify  identify  aka name  retrieve id  location text use nlp   m use spacy specifically  1000 possible location  difficulty unlikely write fully qualified way  mention spelling mistake alias  example  mission neighborhood san francisco write fully  qualified way might  1  mission  2  city san francisco  3  san francisco county  4  california  5  us   number illustrate separate piece   however  many people might write  1  mission  2  city san francisco   1  mission   1  mission  2  city san francisco  4  california   mention  1 might call   mission district    2 might call   san francisco    4 might   ca   etc   goal able id   mission  neighborhood  id california state  etc  text like mission  san francisco  ca get mission id  text like san francisco  ca get san francisco id  be also easy create synthetic training datum create alias individual location piece  eg      city san francisco    b    san francisco    c    san francisco city   permutation   name chain   eg  1  2  3  4  5  1  2  1  2  5  etc  alia  rough estimate 50 combination alias name chain per location   50000  total value   extraction seem good job ner  surround text usually bit context  eg    location      come    however   m unsure ability identification  understand much ner identification  eg  spacy s entitylinker  plan use  relie surround context  expect little surround context would help disambiguate one  1000  location other  also understand entitylinker match token lookup statistical  word  value disambiguate multiple exact  string match disambiguate multiple very  fuzzy match   knowledgebase  lookupdb mechanism set alias  could add permutation alia  point feel like  m get value entitylinker s statistical model  create gazetteer identification aspect  maybe make sense put effort gazetteer skip ner  thank vimal thought  suspect  vishal confirm  need extract string first  process separate  non  nlp algorithm  end solve two way  want document finding  testing find llm  gpt  actually pretty effective determine   administrative hierarchy  give extract string  prompt  example  evaluate follow place identifier determine likely place  list administrative entity low  level high  level  explain reasoning        castro  san francisco  us    return  plus additional explanation   place identifier   castro  san francisco  us  likely refer specific location within city san francisco united states  can not find final version prompt moment  able tweak get provide json administrative entity order  national  first level  second level  etc   plus ask   geographical feature  catch  all   case  find extract term something like   bay area  united states   gpt able sort right prompt   little bit hallucination testing  worry  say  end much low  tech approach  along line original gazetteer idea  original plan use gazetteer idea sort remain string gpt  end match like 98  use approach  unmatched string pretty objectively wrong worth send gpt   like city incorrect country   create gazetteer  approach use dictionary compile wikidata api search api  first  pass send string wikidata search get top 10 matching entity search string  eg    wwwwikidataorg  w  apiphp  action  query  list  search  srsearch  castro  20san  20francisco  20california  srwhat  text  srlimit10  srprop  titlesnippetcategorysnippet  srsort  incominglinksdesc pre  filtering exclude entity not instance subclass geographical feature administrative region  use list manually search download manually   run entity method store datum  include name  alias  lat  lng  etc  walk   administrative entity    country  path collect family tree  search place name  compile dictionary key entity name alias  take list place  castro  san francisco  california  start low  level string  castro  fuzzy search dictionary key look match  anything match  eg  85  choose candidate  create list candidate s parent   grandparent  etc  name alias  loop next place name try match every place name parent list  get score  add score  operation divide number place look  bias toward place few parent  castro valley  california would score high castro  san francisco  california   etc   surprisingly effective ,Implementation Issues
Google semantic retriever example error &#39;Credentials&#39; object has no attribute &#39;universe_domain&#39;,"Here is the code example and steps to create service account and enable API: https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/docs/semantic_retriever.ipynb Error: AttributeError Traceback (most recent call last) <ipython-input-20-ee7d7add68db> in <cell line: 8>() 6 7 # Make the request ----> 8 create_corpus_response = retriever_service_client.create_corpus(create_corpus_request) 9 10 # Set the `corpus_resource_name` for subsequent sections. 2 frames /usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/retriever_service/client.py in _compare_universes(client_universe, credentials) 514 """""" 515 if credentials: --> 516 credentials_universe = credentials.universe_domain 517 if client_universe != credentials_universe: 518 default_universe = RetrieverServiceClient._DEFAULT_UNIVERSE AttributeError: 'Credentials' object has no attribute 'universe_domain'","['google-cloud-platform', 'nlp', 'artificial-intelligence', 'large-language-model', 'google-ai-platform']",1,Can you try upgrading your Google Auth it appears that it has some issues with the some of the versions: pip install --upgrade google-auth Reference Issue: https://github.com/googleapis/google-cloud-python/issues/12254,2024-02-05 12:20:31,2024-02-05 15:14:55,116,https://stackoverflow.com/questions/77940890/google-semantic-retriever-example-error-credentials-object-has-no-attribute-u,"Google semantic retriever example error &#39;Credentials&#39; object has no attribute &#39;universe_domain&#39; Here is the code example and steps to create service account and enable API: https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/docs/semantic_retriever.ipynb Error: AttributeError Traceback (most recent call last) <ipython-input-20-ee7d7add68db> in <cell line: 8>() 6 7 # Make the request ----> 8 create_corpus_response = retriever_service_client.create_corpus(create_corpus_request) 9 10 # Set the `corpus_resource_name` for subsequent sections. 2 frames /usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/retriever_service/client.py in _compare_universes(client_universe, credentials) 514 """""" 515 if credentials: --> 516 credentials_universe = credentials.universe_domain 517 if client_universe != credentials_universe: 518 default_universe = RetrieverServiceClient._DEFAULT_UNIVERSE AttributeError: 'Credentials' object has no attribute 'universe_domain'",google semantic retriever example error   39  credential   39  object attribute   39  universedomain   39  code example step create service account enable api    colabresearchgooglecom  github  google  generative  ai  docs  blob  main  site  en  docs  semanticretrieveripynb error  attributeerror traceback  recent call last   ipython  input20  ee7d7add68db   cell line  8    6 7  make request    8 createcorpusresponse  retrieverserviceclientcreatecorpu  createcorpusrequest  9 10  set  corpusresourcename  subsequent section  2 frame usr  local  lib  python310  dist  package  google  ai  generativelanguagev1beta  service  retrieverservice  clientpy  compareuniverse  clientuniverse  credential  514     515 credential    516 credentialsuniverse  credentialsuniversedomain 517 clientuniverse   credentialsuniverse  518 defaultuniverse  retrieverserviceclientdefaultuniverse attributeerror   credential  object attribute  universedomain ,try upgrade google auth appear issue version  pip install  upgrade google  auth reference issue    githubcom  googleapis  google  cloud  python  issues12254,google semantic retriever example error   39  credential   39  object attribute   39  universedomain   39  code example step create service account enable api    colabresearchgooglecom  github  google  generative  ai  docs  blob  main  site  en  docs  semanticretrieveripynb error  attributeerror traceback  recent call last   ipython  input20  ee7d7add68db   cell line  8    6 7  make request    8 createcorpusresponse  retrieverserviceclientcreatecorpu  createcorpusrequest  9 10  set  corpusresourcename  subsequent section  2 frame usr  local  lib  python310  dist  package  google  ai  generativelanguagev1beta  service  retrieverservice  clientpy  compareuniverse  clientuniverse  credential  514     515 credential    516 credentialsuniverse  credentialsuniversedomain 517 clientuniverse   credentialsuniverse  518 defaultuniverse  retrieverserviceclientdefaultuniverse attributeerror   credential  object attribute  universedomain  try upgrade google auth appear issue version  pip install  upgrade google  auth reference issue    githubcom  googleapis  google  cloud  python  issues12254,Library/Tool-Based Queries
What is the TREC 2006 Spam Track Public Corpora Format?,"link to original dataset I have downloaded this dataset The TREC 2006 Public Corpus -- 75MB (trec06p.tgz) . Here is the folder structure: . └── trec 06p/ ├── data ├── data-delay ├── full ├── full-delay ├── ham25 ├── ham25-delay ├── ham50 ├── ham50-delay ├── spam25 ├── spam25-delay ├── spam50 └── spam50-delay Some questions: What is the delay for? (e.g. data-delay , full-delay ) What does full mean in this case? (is it just the labels?) What is the difference between HAM and ham in the full-delay subfolder? Why is the data-delay folder empty? Is there any special way to parse the contents in the data folder?","['nlp', 'spam-prevention']",1,"Disclaimer Before reading the answer, please note that since I had not participated in the TREC06 task nor am I the data creator/provider, I can do only some educated guess to the questions you have on the dataset. Educated Guessed Answers First, reading the task paper helps https://trec.nist.gov/pubs/trec16/papers/SPAM.OVERVIEW16.pdf =) Next, the right download link for future readers would be https://plg.uwaterloo.ca/~gvcormac/treccorpus06/ And now, some summary: TREC 2006 Spam Track dataset is "" a set of chronologically ordered email messages a spam filter for classification "" Four different forms of user feedback are modeled with immediate feedback the gold standard for each message is communicated to the filter immediately following classification; delayed feedback the gold standard is communicated to the filter sometime later (or potentially never), so as to model a user reading email from time to time and perhaps not diligently reporting the filter’s errors; partial feedback the gold standard for only a subset of email recipients is transmitted to the filter, so as to model the case of some users never reporting filter errors; active on-line learning the filter is allowed to request immediate feedback for a certain quota of messages which is considerably smaller than the total number. Q: How are the above forms of feedback represented by the files in the dataset? A: All the actual textual data are actually found in the trec06/data/**/* files /trec06p /data /000 /000 ... /299 ... /126 /000 /021 And for the rest of the directories, they are just a indices pointing to the subsets to emulate the different forms of evaluations. Q: What does full mean in this case? (is it just the labels?) trec06p/full/index : The index of email lists that points to all the data points in trec06p/data/**/* Q: What is the delay for? (e.g. data-delay, full-delay) trec06p/full-delay/index : The indices that points to the delayed feedback evaluation trec06p/ham*-delay/index : The indices that points to only the non-spam labelled emails in the delayed feedback evaluation trec06p/spam*-delay/index : The indices that points to only the spam labelled emails in the delayed feedback evaluation So essentially, all the unique list of trec06p/ham*-delay/index + trec06p/spam*-delay/index = trec06p/full-delay/index Q: Why is the data-delay folder empty? For this, I don't have an answer... Got to ask the data provider/creator. Q: Is there any special way to parse the contents in the data folder? Now that's the fun coding part =) Lets step back a little and think what we have essentially: A list of emails in trec06/data/**/* The spam/ham labels of each email in trec06/full/index The Spam/SPAM/Ham/HAM labels of a subset of emails in trec06/full-delay/index So... import pandas as pd from tqdm import tqdm from lazyme import find_files data_rows = {} # Assuming you're on `trec06p` directory. # P/S: you can use any other file path list function, # I just use lazyme.find_files because I find it convenient. for fn in tqdm(find_files('./data/**/*')): if fn.endswith('.DS_Store'): continue # Note that not all files are in utf8/ascii charset # so you'll have to read them in binary to store them. # Also note: THIS CAN BE DANGEROUS IF THERE'S EXCUTABLES IN THE DATA!!! # Assuming that there isn't. with open(fn, 'rb') as fin: data_id = tuple(fn.split('/')[-2:]) data_rows[data_id] = fin.read() full_labels = {} with open('./full/index') as fin: for line in tqdm(fin): label, fn = line.strip().split() data_id = tuple(fn.split('/')[-2:]) full_labels[data_id] = label full_delay_labels = {} with open('./full-delay/index') as fin: for line in tqdm(fin): label, fn = line.strip().split() data_id = tuple(fn.split('/')[-2:]) # You'll realize that the labels repeated per data point. # but they are exactly the same.... -_- if data_id in full_delay_labels: assert label.lower() == full_delay_labels[data_id].lower() full_delay_labels[data_id] = label.lower() Q: What is the difference between HAM, Ham, SPAM and Spam labels in the trec06p/*-delay/index If we look carefully at the if data_id in full_delay_labels: assert label.lower() == full_delay_labels[data_id].lower() line, we see that all the caps and the non-caps labels are the same. Q: So why is there a difference? A: Not sure, best to ask data provider/creator Q: Is there a difference between the labels from trec06p/full-delay/index and trec06p/full/index ? Don't seem like there's any. >>> any(full_labels[data_id] != full_delay_labels[data_id] for data_id in full_labels) False Q: How do I just read it into a pandas dataframe? Given what we know above: import pandas as pd from tqdm import tqdm from lazyme import find_files data_rows = {} for fn in tqdm(find_files('./data/**/*')): if fn.endswith('.DS_Store'): continue with open(fn, 'rb') as fin: data_id = tuple(fn.split('/')[-2:]) data_rows[data_id] = fin.read() full_labels = {} with open('./full/index') as fin: for line in tqdm(fin): label, fn = line.strip().split() data_id = tuple(fn.split('/')[-2:]) full_labels[data_id] = label df = pd.DataFrame({'binary':pd.Series(data_rows),'label':full_labels}) Q: But the input columns are still binaries, can I somehow guess the encoding? Not really, it's pretty hard / messy to guess the encoding of a binary file but you can try this (though not all file specify charset=... in the content) import re, mmap def find_charset(fn): with open(fn, 'rb') as f: view = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) return re.split("";|,|\n"", next( re.finditer(br'charset\=([!-~\s]{%i,})\n' % 5, view)).group(1).decode('utf8') )[0].strip('""').strip(""'"")",2024-02-04 10:58:44,2024-02-07 18:21:29,129,https://stackoverflow.com/questions/77935492/what-is-the-trec-2006-spam-track-public-corpora-format,"What is the TREC 2006 Spam Track Public Corpora Format? link to original dataset I have downloaded this dataset The TREC 2006 Public Corpus -- 75MB (trec06p.tgz) . Here is the folder structure: . └── trec 06p/ ├── data ├── data-delay ├── full ├── full-delay ├── ham25 ├── ham25-delay ├── ham50 ├── ham50-delay ├── spam25 ├── spam25-delay ├── spam50 └── spam50-delay Some questions: What is the delay for? (e.g. data-delay , full-delay ) What does full mean in this case? (is it just the labels?) What is the difference between HAM and ham in the full-delay subfolder? Why is the data-delay folder empty? Is there any special way to parse the contents in the data folder?",trec 2006 spam track public corpora format  link original dataset download dataset trec 2006 public corpus  75 mb  trec06ptgz   folder structure      trec 06p    data    data  delay    full    full  delay    ham25    ham25  delay    ham50    ham50  delay    spam25    spam25  delay    spam50    spam50  delay question  delay   eg  data  delay  full  delay  full mean case   label   difference ham ham full  delay subfolder  data  delay folder empty  special way parse content datum folder ,disclaimer reading answer  please note since participate trec06 task data creator  provider  educate guess question dataset  educated guessed answers first  read task paper help   trecnistgov  pub  trec16  paper  spamoverview16pdf   next  right download link future reader would   plguwaterloocagvcormac  treccorpus06  summary  trec 2006 spam track dataset   set chronologically order email message spam filter classification   four different form user feedback model immediate feedback gold standard message communicate filter immediately follow classification  delay feedback gold standard communicate filter sometime later  potentially never   model user read email time time perhaps diligently report filter  error  partial feedback gold standard subset email recipient transmit filter  model case user never report filter error  active on  line learning filter allow request immediate feedback certain quota message considerably small total number  q  form feedback represent file dataset   actual textual datum actually find trec06  data     file trec06p data 000 000  299  126 000 021 rest directory  index point subset emulate different form evaluation  q  full mean case   label   trec06p  full  index  index email list point datum point trec06p  data     q  delay   eg  data  delay  full  delay  trec06p  full  delay  index  index point delay feedback evaluation trec06p  ham  delay  index  index point non  spam label email delay feedback evaluation trec06p  spam  delay  index  index point spam label email delay feedback evaluation essentially  unique list trec06p  ham  delay  index  trec06p  spam  delay  index  trec06p  full  delay  index q  data  delay folder empty   not answer  got ask data provider  creator  q  special way parse content datum folder  s fun code part   let step back little think essentially  list email trec06  data     spam  ham label email trec06  full  index spam  spam  ham  ham label subset email trec06  full  delay  index  import panda pd tqdm import tqdm lazyme import findfiles datarow     assume be  trec06p  directory   p  s  use file path list function   use lazymefindfile find convenient  fn tqdm  findfile   data         fnendswith   dsstore    continue  note file utf8  ascii charset  will read binary store   also note  dangerous s excutables data     assume not  open  fn   rb   fin  dataid  tuple  fnsplit       2    datarow  dataid   finread   fulllabels    open   full  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2    fulllabels  dataid   label fulldelaylabels    open   full  delay  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2     will realize label repeat per datum point   exactly   dataid fulldelaylabel  assert labellower     fulldelaylabel  dataid  lower   fulldelaylabel  dataid   labellower   q  difference ham  ham  spam spam label trec06p  delay  index look carefully dataid fulldelaylabel  assert labellower     fulldelaylabel  dataid  lower   line  see cap non  caps label  q  difference   sure  well ask data provider  creator q  difference label trec06p  full  delay  index trec06p  full  index  not seem like s      fulllabels  dataid    fulldelaylabels  dataid  dataid fulllabels  false q  read panda dataframe  give know  import panda pd tqdm import tqdm lazyme import findfiles datarow    fn tqdm  findfile   data         fnendswith   dsstore    continue open  fn   rb   fin  dataid  tuple  fnsplit       2    datarow  dataid   finread   fulllabels    open   full  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2    fulllabels  dataid   label df  pd  dataframe    binary   pd  series  datarows    label   fulllabels   q  input column still binarie  somehow guess encode  really  be pretty hard  messy guess encode binary file try  though file specify charset  content  import  mmap def findcharset  fn   open  fn   rb   f  view  mmapmmap  ffileno    0  access  mmap  accessread  return resplit       n   next  refinditer  brcharset    s       n   5  view   group  1  decode   utf8     0  strip      strip      ,trec 2006 spam track public corpora format  link original dataset download dataset trec 2006 public corpus  75 mb  trec06ptgz   folder structure      trec 06p    data    data  delay    full    full  delay    ham25    ham25  delay    ham50    ham50  delay    spam25    spam25  delay    spam50    spam50  delay question  delay   eg  data  delay  full  delay  full mean case   label   difference ham ham full  delay subfolder  data  delay folder empty  special way parse content datum folder  disclaimer reading answer  please note since participate trec06 task data creator  provider  educate guess question dataset  educated guessed answers first  read task paper help   trecnistgov  pub  trec16  paper  spamoverview16pdf   next  right download link future reader would   plguwaterloocagvcormac  treccorpus06  summary  trec 2006 spam track dataset   set chronologically order email message spam filter classification   four different form user feedback model immediate feedback gold standard message communicate filter immediately follow classification  delay feedback gold standard communicate filter sometime later  potentially never   model user read email time time perhaps diligently report filter  error  partial feedback gold standard subset email recipient transmit filter  model case user never report filter error  active on  line learning filter allow request immediate feedback certain quota message considerably small total number  q  form feedback represent file dataset   actual textual datum actually find trec06  data     file trec06p data 000 000  299  126 000 021 rest directory  index point subset emulate different form evaluation  q  full mean case   label   trec06p  full  index  index email list point datum point trec06p  data     q  delay   eg  data  delay  full  delay  trec06p  full  delay  index  index point delay feedback evaluation trec06p  ham  delay  index  index point non  spam label email delay feedback evaluation trec06p  spam  delay  index  index point spam label email delay feedback evaluation essentially  unique list trec06p  ham  delay  index  trec06p  spam  delay  index  trec06p  full  delay  index q  data  delay folder empty   not answer  got ask data provider  creator  q  special way parse content datum folder  s fun code part   let step back little think essentially  list email trec06  data     spam  ham label email trec06  full  index spam  spam  ham  ham label subset email trec06  full  delay  index  import panda pd tqdm import tqdm lazyme import findfiles datarow     assume be  trec06p  directory   p  s  use file path list function   use lazymefindfile find convenient  fn tqdm  findfile   data         fnendswith   dsstore    continue  note file utf8  ascii charset  will read binary store   also note  dangerous s excutables data     assume not  open  fn   rb   fin  dataid  tuple  fnsplit       2    datarow  dataid   finread   fulllabels    open   full  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2    fulllabels  dataid   label fulldelaylabels    open   full  delay  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2     will realize label repeat per datum point   exactly   dataid fulldelaylabel  assert labellower     fulldelaylabel  dataid  lower   fulldelaylabel  dataid   labellower   q  difference ham  ham  spam spam label trec06p  delay  index look carefully dataid fulldelaylabel  assert labellower     fulldelaylabel  dataid  lower   line  see cap non  caps label  q  difference   sure  well ask data provider  creator q  difference label trec06p  full  delay  index trec06p  full  index  not seem like s      fulllabels  dataid    fulldelaylabels  dataid  dataid fulllabels  false q  read panda dataframe  give know  import panda pd tqdm import tqdm lazyme import findfiles datarow    fn tqdm  findfile   data         fnendswith   dsstore    continue open  fn   rb   fin  dataid  tuple  fnsplit       2    datarow  dataid   finread   fulllabels    open   full  index   fin  line tqdm  fin   label  fn  linestrip   split   dataid  tuple  fnsplit       2    fulllabels  dataid   label df  pd  dataframe    binary   pd  series  datarows    label   fulllabels   q  input column still binarie  somehow guess encode  really  be pretty hard  messy guess encode binary file try  though file specify charset  content  import  mmap def findcharset  fn   open  fn   rb   f  view  mmapmmap  ffileno    0  access  mmap  accessread  return resplit       n   next  refinditer  brcharset    s       n   5  view   group  1  decode   utf8     0  strip      strip      ,Implementation Issues
How to calculate the weighted sum of last 4 hidden layers using Roberta?,"The table from this paper that explains various approaches to obtain the embedding, I think these approaches are also applicable to Roberta too: I'm trying to calculate the weighted sum of last 4 hidden layers using Roberta to obtain token embedding, but I don't know if this is the correct way to do, this is the code I have tried: from transformers import RobertaTokenizer, RobertaModel import torch tokenizer = RobertaTokenizer.from_pretrained('roberta-base') model = RobertaModel.from_pretrained('roberta-base') caption = ['this is a yellow bird', 'example caption'] tokens = tokenizer(caption, return_tensors='pt', padding=True) input_ids = tokens['input_ids'] attention_mask = tokens['attention_mask'] output = model(input_ids, attention_mask, output_hidden_states=True) states = output.hidden_states token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()","['python', 'machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",1,"First, lets do some digging from the OG BERT code, https://github.com/google-research/bert If we just do a quick search for ""sum"" on the github repo, we find this https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L814 # The Transformer performs sum residuals on all layers so the input needs # to be the same as the hidden size. if input_width != hidden_size: raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" % (input_width, hidden_size)) Then a quick search on Stackoverflow reveals How to get intermediate layers' output of pre-trained BERT model in HuggingFace Transformers library? Now, lets validate if your code logic works by working backwards a little: from transformers import RobertaTokenizer, RobertaModel import torch tokenizer = RobertaTokenizer.from_pretrained('roberta-base') model = RobertaModel.from_pretrained('roberta-base') caption = ['this is a yellow bird', 'example caption'] tokens = tokenizer(caption, return_tensors='pt', padding=True) input_ids = tokens['input_ids'] attention_mask = tokens['attention_mask'] output = model(input_ids, attention_mask, output_hidden_states=True) Then: >>>len(output.hidden_states) Out: 13 Why 13? 12 Encoder (hidden) layer output + final pooler output import torchinfo torchinfo.summary(model) [out]: ================================================================================ Layer (type:depth-idx) Param # ================================================================================ RobertaModel -- ├─RobertaEmbeddings: 1-1 -- │ └─Embedding: 2-1 38,603,520 │ └─Embedding: 2-2 394,752 │ └─Embedding: 2-3 768 │ └─LayerNorm: 2-4 1,536 │ └─Dropout: 2-5 -- ├─RobertaEncoder: 1-2 -- │ └─ModuleList: 2-6 -- │ │ └─RobertaLayer: 3-1 7,087,872 │ │ └─RobertaLayer: 3-2 7,087,872 │ │ └─RobertaLayer: 3-3 7,087,872 │ │ └─RobertaLayer: 3-4 7,087,872 │ │ └─RobertaLayer: 3-5 7,087,872 │ │ └─RobertaLayer: 3-6 7,087,872 │ │ └─RobertaLayer: 3-7 7,087,872 │ │ └─RobertaLayer: 3-8 7,087,872 │ │ └─RobertaLayer: 3-9 7,087,872 │ │ └─RobertaLayer: 3-10 7,087,872 │ │ └─RobertaLayer: 3-11 7,087,872 │ │ └─RobertaLayer: 3-12 7,087,872 ├─RobertaPooler: 1-3 -- │ └─Linear: 2-7 590,592 │ └─Tanh: 2-8 -- ================================================================================ Total params: 124,645,632 Trainable params: 124,645,632 Non-trainable params: 0 ================================================================================ To validate that the last layer output is the last layer in the hidden_states: assert( True for x in torch.flatten( output[0] == output.hidden_states[-1] ) ) Lets check if the size for each layer's output matches: first_hidden_shape = output.hidden_states[0].shape for x in output.hidden_states: assert x.shape == first_hidden_shape Checks out! >>>first_hidden_shape [out]: torch.Size([2, 7, 768]) Why [2, 7, 768] ? It's (batch_size, sequence_length, hidden_size) 2 sentences = batch size of 2 7 longest sequence length = no. of tokens (i.e. 5 in the case of your longest example sentence + <s> and </s> from len(input_ids[0]) ) 768 outputs = fixed for all hidden layers output Bíddu aðeins! (Wait a minute!), does that mean I've sequence_length * 768 outputs for each batch? And if my batches are not equal lengths, the output size are different? Yes that is correct! And to get some sense of ""equality"" for all inputs, it'll be good to pad/truncate all outputs to a fixed length if you're still going to use the feature-based BERT approaches . Soooo, is my torch.stack approach right? Yes, it seems so, but it depends on whether you consider the pooler output to be last or second to last. If second to last: torch.stack(output.hidden_states[-5:-1]).sum(0) if you consider the pooler to be the last: torch.stack(output.hidden_states[-4:]).sum(0) Minor nitpicking, you can access the output.hidden_states through slices because it's a tuple object. Next you won't need to squeeze the stacked output because the the outer most layer tensor is non-empty. This is a special case for stack, in NLP where the 1st dimension is batch size and 2nd is token length, so summing the hidden dimensions up ends up the same when you're not explicitly stating which dimension you stack. To be a little more explicit: # 2nd dimension is where our hidden states are # and that's where we want to do our sum too. torch.stack(output.hidden_states[-4:], dim=2).sum(2) But in practice, you can do this to comfort yourself: assert( True for x in torch.flatten( torch.stack(output.hidden_states[-4:]).sum(0) == torch.stack(output.hidden_states[-4:], dim=2).sum(2) ) ) Interesting, what about ""concat last four hidden""? In the case of concat you'll need to be explicit when in the dimensions >>> torch.cat(output.hidden_states[-4:], dim=2).shape [out]: torch.Size([2, 7, 3072]) but note, you still ends with sequence_length * hidden_size * 4 , which makes batches with unequal lengths a pain. Since you've covered almost everything on the table, what about the ""embeddings"" output? This is the interesting part, it's actually not accessible through the model(inputs_ids) directly, you'll need to do this: model.embeddings(input_ids) Finally, why didn't you just answer ""yes, you are right""? If I did, would that convince you more than you proving the above for yourself?",2024-02-03 20:34:36,2024-02-07 04:09:30,328,https://stackoverflow.com/questions/77933640/how-to-calculate-the-weighted-sum-of-last-4-hidden-layers-using-roberta,"How to calculate the weighted sum of last 4 hidden layers using Roberta? The table from this paper that explains various approaches to obtain the embedding, I think these approaches are also applicable to Roberta too: I'm trying to calculate the weighted sum of last 4 hidden layers using Roberta to obtain token embedding, but I don't know if this is the correct way to do, this is the code I have tried: from transformers import RobertaTokenizer, RobertaModel import torch tokenizer = RobertaTokenizer.from_pretrained('roberta-base') model = RobertaModel.from_pretrained('roberta-base') caption = ['this is a yellow bird', 'example caption'] tokens = tokenizer(caption, return_tensors='pt', padding=True) input_ids = tokens['input_ids'] attention_mask = tokens['attention_mask'] output = model(input_ids, attention_mask, output_hidden_states=True) states = output.hidden_states token_emb = torch.stack([states[i] for i in [-4, -3, -2, -1]]).sum(0).squeeze()",calculate weight sum last 4 hide layer use roberta  table paper explain various approach obtain embed  think approach also applicable roberta   m try calculate weight sum last 4 hide layer use roberta obtain token embed  not know correct way  code try  transformer import robertatokenizer  robertamodel import torch tokenizer  robertatokenizerfrompretraine   roberta  base   model  robertamodelfrompretraine   roberta  base   caption    this yellow bird    example caption   token  tokenizer  caption  returntensorspt   pad  true  inputids  token   inputids   attentionmask  token   attentionmask   output  model  inputids  attentionmask  outputhiddenstate  true  state  outputhiddenstate tokenemb  torchstack   state    4  3  2  1    sum  0  squeeze  ,first  let dig og bert code    githubcom  google  research  bert quick search   sum  github repo  find   githubcom  google  research  bert  blob  eedf5716ce1268e56f0a50264a88cafad334ac61  modelingpy  l814  transformer perform sum residual layer input need  hide size  inputwidth   hiddensize  raise valueerror    width input tensor      hide size       inputwidth  hiddensize   quick search stackoverflow reveal get intermediate layer  output pre  train bert model huggingface transformers library   let validate code logic work work backwards little  transformer import robertatokenizer  robertamodel import torch tokenizer  robertatokenizerfrompretraine   roberta  base   model  robertamodelfrompretraine   roberta  base   caption    this yellow bird    example caption   token  tokenizer  caption  returntensorspt   pad  true  inputids  token   inputids   attentionmask  token   attentionmask   output  model  inputids  attentionmask  outputhiddenstate  true      len  outputhiddenstates   13 13  12 encoder  hide  layer output  final pooler output import torchinfo torchinfosummary  model                                                                                     layer  type  depth  idx  param                                                                                  robertamodel    robertaembeddings  1  1     embed  2  1 38603520    embed  2  2 394752    embed  2  3 768    layernorm  2  4 1536    dropout  2  5    robertaencoder  1  2     modulelist  2  6      robertalayer  3  1 7087872     robertalayer  3  2 7087872     robertalayer  3  3 7087872     robertalayer  3  4 7087872     robertalayer  3  5 7087872     robertalayer  3  6 7087872     robertalayer  3  7 7087872     robertalayer  3  8 7087872     robertalayer  3  9 7087872     robertalayer  3  10 7087872     robertalayer  3  11 7087872     robertalayer  3  12 7087872   robertapooler  1  3     linear  2  7 590592    tanh  2  8                                                                                  total param  124645632 trainable param  124645632 non  trainable param  0                                                                                 validate last layer output last layer hiddenstate  assert  true x torchflatten  output  0    outputhiddenstates  1    let check size layer s output match  firsthiddenshape  outputhiddenstate  0  shape x outputhiddenstate  assert xshape   firsthiddenshape check     firsthiddenshape    torch  size   2  7  768    2  7  768   s  batchsize  sequencelength  hiddensize  2 sentence  batch size 2 7 long sequence length   token  ie  5 case long example sentence     s  len  inputid  0    768 output  fix hidden layer output bddu aein   wait minute    mean  ve sequencelength  768 output batch  batch equal length  output size different  yes correct  get sense   equality  input  will good pad  truncate output fix length be still go use feature  base bert approach  soooo  torchstack approach right  yes  seem  depend whether consider pooler output last second last  second last  torchstack  outputhiddenstate  5  1   sum  0  consider pooler last  torchstack  outputhiddenstate  4    sum  0  minor nitpicking  access outputhiddenstates slice s tuple object  next will not need squeeze stack output outer layer tensor non  empty  special case stack  nlp 1st dimension batch size 2nd token length  sum hide dimension end be explicitly state dimension stack  little explicit   2nd dimension hidden state  s want sum  torchstack  outputhiddenstate  4    dim2  sum  2  practice  comfort  assert  true x torchflatten  torchstack  outputhiddenstate  4    sum  0    torchstack  outputhiddenstate  4    dim2  sum  2    interesting    concat last four hide   case concat will need explicit dimension    torchcat  outputhiddenstate  4    dim2  shape    torch  size   2  7  3072   note  still end sequencelength  hiddensize  4  make batch unequal length pain  since  ve cover almost everything table    embedding  output  interesting part  be actually accessible model  inputsid  directly  will need  modelembedding  inputids  finally  not answer   yes  right    would convince prove ,calculate weight sum last 4 hide layer use roberta  table paper explain various approach obtain embed  think approach also applicable roberta   m try calculate weight sum last 4 hide layer use roberta obtain token embed  not know correct way  code try  transformer import robertatokenizer  robertamodel import torch tokenizer  robertatokenizerfrompretraine   roberta  base   model  robertamodelfrompretraine   roberta  base   caption    this yellow bird    example caption   token  tokenizer  caption  returntensorspt   pad  true  inputids  token   inputids   attentionmask  token   attentionmask   output  model  inputids  attentionmask  outputhiddenstate  true  state  outputhiddenstate tokenemb  torchstack   state    4  3  2  1    sum  0  squeeze   first  let dig og bert code    githubcom  google  research  bert quick search   sum  github repo  find   githubcom  google  research  bert  blob  eedf5716ce1268e56f0a50264a88cafad334ac61  modelingpy  l814  transformer perform sum residual layer input need  hide size  inputwidth   hiddensize  raise valueerror    width input tensor      hide size       inputwidth  hiddensize   quick search stackoverflow reveal get intermediate layer  output pre  train bert model huggingface transformers library   let validate code logic work work backwards little  transformer import robertatokenizer  robertamodel import torch tokenizer  robertatokenizerfrompretraine   roberta  base   model  robertamodelfrompretraine   roberta  base   caption    this yellow bird    example caption   token  tokenizer  caption  returntensorspt   pad  true  inputids  token   inputids   attentionmask  token   attentionmask   output  model  inputids  attentionmask  outputhiddenstate  true      len  outputhiddenstates   13 13  12 encoder  hide  layer output  final pooler output import torchinfo torchinfosummary  model                                                                                     layer  type  depth  idx  param                                                                                  robertamodel    robertaembeddings  1  1     embed  2  1 38603520    embed  2  2 394752    embed  2  3 768    layernorm  2  4 1536    dropout  2  5    robertaencoder  1  2     modulelist  2  6      robertalayer  3  1 7087872     robertalayer  3  2 7087872     robertalayer  3  3 7087872     robertalayer  3  4 7087872     robertalayer  3  5 7087872     robertalayer  3  6 7087872     robertalayer  3  7 7087872     robertalayer  3  8 7087872     robertalayer  3  9 7087872     robertalayer  3  10 7087872     robertalayer  3  11 7087872     robertalayer  3  12 7087872   robertapooler  1  3     linear  2  7 590592    tanh  2  8                                                                                  total param  124645632 trainable param  124645632 non  trainable param  0                                                                                 validate last layer output last layer hiddenstate  assert  true x torchflatten  output  0    outputhiddenstates  1    let check size layer s output match  firsthiddenshape  outputhiddenstate  0  shape x outputhiddenstate  assert xshape   firsthiddenshape check     firsthiddenshape    torch  size   2  7  768    2  7  768   s  batchsize  sequencelength  hiddensize  2 sentence  batch size 2 7 long sequence length   token  ie  5 case long example sentence     s  len  inputid  0    768 output  fix hidden layer output bddu aein   wait minute    mean  ve sequencelength  768 output batch  batch equal length  output size different  yes correct  get sense   equality  input  will good pad  truncate output fix length be still go use feature  base bert approach  soooo  torchstack approach right  yes  seem  depend whether consider pooler output last second last  second last  torchstack  outputhiddenstate  5  1   sum  0  consider pooler last  torchstack  outputhiddenstate  4    sum  0  minor nitpicking  access outputhiddenstates slice s tuple object  next will not need squeeze stack output outer layer tensor non  empty  special case stack  nlp 1st dimension batch size 2nd token length  sum hide dimension end be explicitly state dimension stack  little explicit   2nd dimension hidden state  s want sum  torchstack  outputhiddenstate  4    dim2  sum  2  practice  comfort  assert  true x torchflatten  torchstack  outputhiddenstate  4    sum  0    torchstack  outputhiddenstate  4    dim2  sum  2    interesting    concat last four hide   case concat will need explicit dimension    torchcat  outputhiddenstate  4    dim2  shape    torch  size   2  7  3072   note  still end sequencelength  hiddensize  4  make batch unequal length pain  since  ve cover almost everything table    embedding  output  interesting part  be actually accessible model  inputsid  directly  will need  modelembedding  inputids  finally  not answer   yes  right    would convince prove ,Library/Tool-Based Queries
Why token embedding different from the embedding by the BartForConditionalGeneration model,"Why both the embeddings are different even when i generate them using same BartForConditionalGenration model? First embedding is generated by combining token embedding and positional embedding from embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids) inputs_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids) The Second embedding by the model via output = modelBART(input_ids.input_ids) print(""\n\n output: \n\n"",output.encoder_last_hidden_state) Shouldn't the embedding by first and second be same? What to do so that difference of the embedding from first and second be zero?","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'bart']",1,The first embeddings (input + position) are the first layer of the model. These embeddings are used to map tokens to vectors. The second set of embeddings ( encoder_last_hidden_state ) are the outputs of the final layer in the model's encoder. These embeddings are supposed to be different.,2024-01-30 13:18:34,2024-01-30 20:13:06,127,https://stackoverflow.com/questions/77906649/why-token-embedding-different-from-the-embedding-by-the-bartforconditionalgenera,"Why token embedding different from the embedding by the BartForConditionalGeneration model Why both the embeddings are different even when i generate them using same BartForConditionalGenration model? First embedding is generated by combining token embedding and positional embedding from embed_pos = modelBART.model.encoder.embed_positions(input_ids.input_ids) inputs_embeds = modelBART.model.encoder.embed_tokens(input_ids.input_ids) The Second embedding by the model via output = modelBART(input_ids.input_ids) print(""\n\n output: \n\n"",output.encoder_last_hidden_state) Shouldn't the embedding by first and second be same? What to do so that difference of the embedding from first and second be zero?",token embed different embed bartforconditionalgeneration model embedding different even generate use bartforconditionalgenration model  first embed generate combine token embed positional embed embedpos  modelbartmodelencoderembedposition  inputidsinputid  inputsembed  modelbartmodelencoderembedtoken  inputidsinputids  second embed model via output  modelbart  inputidsinputids  print    nn output  nn   outputencoderlasthiddenstate  not embed first second  difference embed first second zero ,first embedding  input  position  first layer model  embedding use map token vector  second set embedding  encoderlasthiddenstate  output final layer model s encoder  embedding suppose different ,token embed different embed bartforconditionalgeneration model embedding different even generate use bartforconditionalgenration model  first embed generate combine token embed positional embed embedpos  modelbartmodelencoderembedposition  inputidsinputid  inputsembed  modelbartmodelencoderembedtoken  inputidsinputids  second embed model via output  modelbart  inputidsinputids  print    nn output  nn   outputencoderlasthiddenstate  not embed first second  difference embed first second zero  first embedding  input  position  first layer model  embedding use map token vector  second set embedding  encoderlasthiddenstate  output final layer model s encoder  embedding suppose different ,Implementation Issues
Finding embedding dimentions of the HuggingFace model,"I try to figure out how to use faiss Vectore Store with LlamaIndex . Instruction says, that I must indicate vector dimensions in advance. Here is the code: import faiss # dimensions of text-ada-embedding-002 d = 1536 faiss_index = faiss.IndexFlatL2(d) So, dimensions of text-ada-embedding-002 model is 1536 . I want to use BAAI/bge-small-en-v1.5 model. What does embedding vector dimensions it outputs? How do I find the output vector dimensions of other transformer models? Do I need to run the models and measure the results, or is there a simpler way?","['nlp', 'huggingface-transformers', 'llama-index', 'faiss']",1,"The dimension for bge-small-en-v1.5 is 384. You can find it on the model page https://huggingface.co/BAAI/bge-small-en-v1.5 , you will find a table with dimension, sequence length and scores. Also, when loading a model via transformers.AutoModel you can more details on the loaded model using model.eval() likes input dimensions, layers, output, etc.",2024-01-29 22:58:06,2024-02-06 11:26:28,2583,https://stackoverflow.com/questions/77903080/finding-embedding-dimentions-of-the-huggingface-model,"Finding embedding dimentions of the HuggingFace model I try to figure out how to use faiss Vectore Store with LlamaIndex . Instruction says, that I must indicate vector dimensions in advance. Here is the code: import faiss # dimensions of text-ada-embedding-002 d = 1536 faiss_index = faiss.IndexFlatL2(d) So, dimensions of text-ada-embedding-002 model is 1536 . I want to use BAAI/bge-small-en-v1.5 model. What does embedding vector dimensions it outputs? How do I find the output vector dimensions of other transformer models? Do I need to run the models and measure the results, or is there a simpler way?",find embed dimention huggingface model try figure use faiss vectore store llamaindex  instruction say  must indicate vector dimension advance  code  import faiss  dimension text  ada  embedding002  1536 faissindex  faiss  indexflatl2    dimension text  ada  embedding002 model 1536  want use baai  bge  small  en  v15 model  embed vector dimension output  find output vector dimension transformer model  need run model measure result  simple way ,dimension bge  small  en  v15 384  find model page   huggingfaceco  baai  bge  small  en  v15  find table dimension  sequence length score  also  load model via transformer  automodel detail load model use modeleval   like input dimension  layer  output  etc ,find embed dimention huggingface model try figure use faiss vectore store llamaindex  instruction say  must indicate vector dimension advance  code  import faiss  dimension text  ada  embedding002  1536 faissindex  faiss  indexflatl2    dimension text  ada  embedding002 model 1536  want use baai  bge  small  en  v15 model  embed vector dimension output  find output vector dimension transformer model  need run model measure result  simple way  dimension bge  small  en  v15 384  find model page   huggingfaceco  baai  bge  small  en  v15  find table dimension  sequence length score  also  load model via transformer  automodel detail load model use modeleval   like input dimension  layer  output  etc ,Task-Specific Queries
I am unable to produce rearch results from google/youtube in my speech recognition code,"I am trying to build a chatbot which can interact with people and help them with quick updates.Below is the code that I am using to get the search results from youtube/google. Please tell me where the issue is lying? maya_google_search.py code: import speech_recognition import pyttsx3 import pywhatkit from wikipedia import wikipedia import wikipedia as googleScrap import webbrowser engine = pyttsx3.init(""sapi5"") voices = engine.getProperty(""voices"") engine.setProperty(""voice"", voices[1].id) engine.setProperty(""rate"", 150) def speak(audio): engine.say(audio) engine.runAndWait() def takeCommand(): r = speech_recognition.Recognizer() with speech_recognition.Microphone() as source: print(""listening............."") r.pause_threshold = 1 r.energy_threshold = 300 audio = r.listen(source,0,4) try: print(""Understanding............"") query = r.recognize_google(audio, language='en-in') print(f""You said: {query}\n"") except Exception as e: print(""Say that again"") speak(""Say that again"") return ""None"" return query query = takeCommand().lower() def Googlesearch(query): if ""google"" in query: query = query.replace(""Maya"", """") query = query.replace(""google search"", """") query = query.replace(""google"", """") speak(""This is what I found on Google....."") try: pywhatkit.search(query) result = googleScrap.summary(query,sentences=2) speak(""According to Google.........."") speak(result) except: speak(""No speakable output available"") def Youtubesearch(query): if ""youtube"" in query: query = query.replace(""Maya"", """") query = query.replace(""youtube search"", """") query = query.replace(""youtube"", """") speak(""This is what I found for your search!"") web = ""https://www.youtube.com/results?search_query="" + query webbrowser.open(web) pywhatkit.playonyt(query) speak(""Done, sir"") maya_ai.py code: import pyttsx3 import speech_recognition engine = pyttsx3.init(""sapi5"") voices = engine.getProperty(""voices"") engine.setProperty(""voice"", voices[1].id) engine.setProperty(""rate"", 150) def speak(audio): engine.say(audio) engine.runAndWait() def takeCommand(): r = speech_recognition.Recognizer() with speech_recognition.Microphone() as source: print(""listening............."") r.pause_threshold = 1 r.energy_threshold = 300 audio = r.listen(source,0,4) try: print(""Understanding............"") query = r.recognize_google(audio, language='en-in') print(f""You said: {query}\n"") # speak(query) except Exception as e: print(""Say that again"") return ""None"" return query if __name__ == ""__main__"": while True: query = takeCommand().lower() if ""wake up"" in query: from maya_greeting import greetMe greetMe() while True: query = takeCommand().lower() if ""go to sleep"" in query: speak(""Ok sir, You can call me anytime..."") break elif ""hello"" in query: speak(""Hello Sir, how are you?"") elif ""i am fine"" in query: speak(""That's really great to know sir...."") elif ""how are you"": speak(""i am perfectly alright sir."") elif ""thank you"" in query: speak(""you're welcome sir"") elif ""google"" in query: from maya_google_search import Googlesearch Googlesearch(query) elif ""youtube"" in query: from maya_google_search import Youtubesearch Youtubesearch(query) elif ""wikipedia"" in query: from maya_google_search import Wikisearch Wikisearch(query) if I say google Sundar Pichai it will just print what i said and say i am perfectly alright sir or nothing. Please help me with this.","['python', 'nlp', 'artificial-intelligence', 'speech-recognition', 'google-text-to-speech']",1,"Change elif ""how are you"": for elif ""how are you"" in query: Then you need to add a final else statement in case none of the previous conditions trigger",2024-01-26 13:01:50,2024-01-26 14:21:16,40,https://stackoverflow.com/questions/77886445/i-am-unable-to-produce-rearch-results-from-google-youtube-in-my-speech-recogniti,"I am unable to produce rearch results from google/youtube in my speech recognition code I am trying to build a chatbot which can interact with people and help them with quick updates.Below is the code that I am using to get the search results from youtube/google. Please tell me where the issue is lying? maya_google_search.py code: import speech_recognition import pyttsx3 import pywhatkit from wikipedia import wikipedia import wikipedia as googleScrap import webbrowser engine = pyttsx3.init(""sapi5"") voices = engine.getProperty(""voices"") engine.setProperty(""voice"", voices[1].id) engine.setProperty(""rate"", 150) def speak(audio): engine.say(audio) engine.runAndWait() def takeCommand(): r = speech_recognition.Recognizer() with speech_recognition.Microphone() as source: print(""listening............."") r.pause_threshold = 1 r.energy_threshold = 300 audio = r.listen(source,0,4) try: print(""Understanding............"") query = r.recognize_google(audio, language='en-in') print(f""You said: {query}\n"") except Exception as e: print(""Say that again"") speak(""Say that again"") return ""None"" return query query = takeCommand().lower() def Googlesearch(query): if ""google"" in query: query = query.replace(""Maya"", """") query = query.replace(""google search"", """") query = query.replace(""google"", """") speak(""This is what I found on Google....."") try: pywhatkit.search(query) result = googleScrap.summary(query,sentences=2) speak(""According to Google.........."") speak(result) except: speak(""No speakable output available"") def Youtubesearch(query): if ""youtube"" in query: query = query.replace(""Maya"", """") query = query.replace(""youtube search"", """") query = query.replace(""youtube"", """") speak(""This is what I found for your search!"") web = ""https://www.youtube.com/results?search_query="" + query webbrowser.open(web) pywhatkit.playonyt(query) speak(""Done, sir"") maya_ai.py code: import pyttsx3 import speech_recognition engine = pyttsx3.init(""sapi5"") voices = engine.getProperty(""voices"") engine.setProperty(""voice"", voices[1].id) engine.setProperty(""rate"", 150) def speak(audio): engine.say(audio) engine.runAndWait() def takeCommand(): r = speech_recognition.Recognizer() with speech_recognition.Microphone() as source: print(""listening............."") r.pause_threshold = 1 r.energy_threshold = 300 audio = r.listen(source,0,4) try: print(""Understanding............"") query = r.recognize_google(audio, language='en-in') print(f""You said: {query}\n"") # speak(query) except Exception as e: print(""Say that again"") return ""None"" return query if __name__ == ""__main__"": while True: query = takeCommand().lower() if ""wake up"" in query: from maya_greeting import greetMe greetMe() while True: query = takeCommand().lower() if ""go to sleep"" in query: speak(""Ok sir, You can call me anytime..."") break elif ""hello"" in query: speak(""Hello Sir, how are you?"") elif ""i am fine"" in query: speak(""That's really great to know sir...."") elif ""how are you"": speak(""i am perfectly alright sir."") elif ""thank you"" in query: speak(""you're welcome sir"") elif ""google"" in query: from maya_google_search import Googlesearch Googlesearch(query) elif ""youtube"" in query: from maya_google_search import Youtubesearch Youtubesearch(query) elif ""wikipedia"" in query: from maya_google_search import Wikisearch Wikisearch(query) if I say google Sundar Pichai it will just print what i said and say i am perfectly alright sir or nothing. Please help me with this.",unable produce rearch result google  youtube speech recognition code try build chatbot interact people help quick update  below code use get search result youtube  google  please tell issue lie  mayagooglesearchpy code  import speechrecognition import pyttsx3 import pywhatkit wikipedia import wikipedia import wikipedia googlescrap import webbrowser engine  pyttsx3init    sapi5   voice  enginegetproperty    voice   enginesetproperty    voice   voice  1  id  enginesetproperty    rate   150  def speak  audio   enginesay  audio  enginerunandwait   def takecommand    r  speechrecognition  recognizer   speechrecognition  microphone   source  print    listen    rpausethreshold  1 renergythreshold  300 audio  rlisten  source04  try  print    understanding    query  rrecognizegoogle  audio  languageen  in   print  f  say   query  n   except exception e  print    say   speak    say   return   none  return query query  takecommand   lower   def googlesearch  query     google  query  query  queryreplace    maya       query  queryreplace    google search       query  queryreplace    google       speak    find google    try  pywhatkitsearch  query  result  googlescrapsummary  query  sentences2  speak    accord google    speak  result  except  speak    speakable output available   def youtubesearch  query     youtube  query  query  queryreplace    maya       query  queryreplace    youtube search       query  queryreplace    youtube       speak    find search    web      wwwyoutubecom  result  searchquery   query webbrowseropen  web  pywhatkitplayonyt  query  speak    do  sir   mayaaipy code  import pyttsx3 import speechrecognition engine  pyttsx3init    sapi5   voice  enginegetproperty    voice   enginesetproperty    voice   voice  1  id  enginesetproperty    rate   150  def speak  audio   enginesay  audio  enginerunandwait   def takecommand    r  speechrecognition  recognizer   speechrecognition  microphone   source  print    listen    rpausethreshold  1 renergythreshold  300 audio  rlisten  source04  try  print    understanding    query  rrecognizegoogle  audio  languageen  in   print  f  say   query  n    speak  query  except exception e  print    say   return   none  return query   name         main     true  query  takecommand   lower     wake  query  mayagreeting import greetme greetme   true  query  takecommand   lower     go sleep  query  speak    ok sir  call anytime    break elif   hello  query  speak    hello sir     elif   fine  query  speak    be really great know sir    elif     speak    perfectly alright sir    elif   thank  query  speak    be welcome sir   elif   google  query  mayagooglesearch import googlesearch googlesearch  query  elif   youtube  query  mayagooglesearch import youtubesearch youtubesearch  query  elif   wikipedia  query  mayagooglesearch import wikisearch wikisearch  query  say google sundar pichai print say say perfectly alright sir nothing  please help ,change elif     elif    query  need add final else statement case none previous condition trigger,unable produce rearch result google  youtube speech recognition code try build chatbot interact people help quick update  below code use get search result youtube  google  please tell issue lie  mayagooglesearchpy code  import speechrecognition import pyttsx3 import pywhatkit wikipedia import wikipedia import wikipedia googlescrap import webbrowser engine  pyttsx3init    sapi5   voice  enginegetproperty    voice   enginesetproperty    voice   voice  1  id  enginesetproperty    rate   150  def speak  audio   enginesay  audio  enginerunandwait   def takecommand    r  speechrecognition  recognizer   speechrecognition  microphone   source  print    listen    rpausethreshold  1 renergythreshold  300 audio  rlisten  source04  try  print    understanding    query  rrecognizegoogle  audio  languageen  in   print  f  say   query  n   except exception e  print    say   speak    say   return   none  return query query  takecommand   lower   def googlesearch  query     google  query  query  queryreplace    maya       query  queryreplace    google search       query  queryreplace    google       speak    find google    try  pywhatkitsearch  query  result  googlescrapsummary  query  sentences2  speak    accord google    speak  result  except  speak    speakable output available   def youtubesearch  query     youtube  query  query  queryreplace    maya       query  queryreplace    youtube search       query  queryreplace    youtube       speak    find search    web      wwwyoutubecom  result  searchquery   query webbrowseropen  web  pywhatkitplayonyt  query  speak    do  sir   mayaaipy code  import pyttsx3 import speechrecognition engine  pyttsx3init    sapi5   voice  enginegetproperty    voice   enginesetproperty    voice   voice  1  id  enginesetproperty    rate   150  def speak  audio   enginesay  audio  enginerunandwait   def takecommand    r  speechrecognition  recognizer   speechrecognition  microphone   source  print    listen    rpausethreshold  1 renergythreshold  300 audio  rlisten  source04  try  print    understanding    query  rrecognizegoogle  audio  languageen  in   print  f  say   query  n    speak  query  except exception e  print    say   return   none  return query   name         main     true  query  takecommand   lower     wake  query  mayagreeting import greetme greetme   true  query  takecommand   lower     go sleep  query  speak    ok sir  call anytime    break elif   hello  query  speak    hello sir     elif   fine  query  speak    be really great know sir    elif     speak    perfectly alright sir    elif   thank  query  speak    be welcome sir   elif   google  query  mayagooglesearch import googlesearch googlesearch  query  elif   youtube  query  mayagooglesearch import youtubesearch youtubesearch  query  elif   wikipedia  query  mayagooglesearch import wikisearch wikisearch  query  say google sundar pichai print say say perfectly alright sir nothing  please help  change elif     elif    query  need add final else statement case none previous condition trigger,Implementation Issues
combining falcon 40b instruct with langchain,"I want to create a local LLM using falcon 40b instruct model and combine it with lanchain so I can give it a pdf or some resource to learn from so I can query it ask it questions, learn from it and ultimately be able to derive insights from the pdf report from an Excel sheet. For now, I just want to load a pdf using langchain and have the falcon-40b-instruct model as the agent. I want to build an llm where I can make it interact with my own data using langchain. Here is my attempt so far: from langchain_community.llms import HuggingFaceHub llm = HuggingFaceHub( repo_id=model_name, task=""text-generation"", model_kwargs={ ""max_new_tokens"": 512, ""top_k"": 30, ""temperature"": 0.1, ""repetition_penalty"": 1.03 }, huggingfacehub_api_token=""hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"" ) I reached the following stage: from langchain_community.chat_models.huggingface import ChatHuggingFace llm = ChatHuggingFace(llm=llm) yet I get this error: HfHubHTTPError: 401 Client Error: Unauthorized for url I am doing do this to be able to run the following: qa_chain = RetrievalQA.from_chain_type( llm=llm, retriever=vector_db.as_retriever() ) What am I missing and is there a way to be able to do this fully local like doing the falcon model and pass it to ChatHuggingFace?","['nlp', 'chatbot', 'langchain', 'large-language-model', 'falcon']",1,"The message of HfHubHTTPError: 401 Client Error: Unauthorized for url indicates that you don't have access to endpoint service from HuggingFace > https://huggingface.co/inference-endpoints . As you want to run everything locally, here's an example, using HuggingFacePipeline function from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # model_id = ""tiiuae/falcon-7b-instruct"" # this model is too large to run on Nvidia 4090 with 16G ram model_id = ""gpt2"" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id) pipeline = pipeline( ""text-generation"", model=model, tokenizer=tokenizer, max_new_tokens=200, ) from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline llm = HuggingFacePipeline(pipeline=pipeline) from langchain.prompts import PromptTemplate template = """"""Question: {question} Answer: Let's think step by step."""""" prompt = PromptTemplate.from_template(template) chain = prompt | llm question = ""Tell me about Italy"" print(chain.invoke({""question"": question}))",2024-01-23 17:43:17,2024-01-24 03:54:30,327,https://stackoverflow.com/questions/77868284/combining-falcon-40b-instruct-with-langchain,"combining falcon 40b instruct with langchain I want to create a local LLM using falcon 40b instruct model and combine it with lanchain so I can give it a pdf or some resource to learn from so I can query it ask it questions, learn from it and ultimately be able to derive insights from the pdf report from an Excel sheet. For now, I just want to load a pdf using langchain and have the falcon-40b-instruct model as the agent. I want to build an llm where I can make it interact with my own data using langchain. Here is my attempt so far: from langchain_community.llms import HuggingFaceHub llm = HuggingFaceHub( repo_id=model_name, task=""text-generation"", model_kwargs={ ""max_new_tokens"": 512, ""top_k"": 30, ""temperature"": 0.1, ""repetition_penalty"": 1.03 }, huggingfacehub_api_token=""hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"" ) I reached the following stage: from langchain_community.chat_models.huggingface import ChatHuggingFace llm = ChatHuggingFace(llm=llm) yet I get this error: HfHubHTTPError: 401 Client Error: Unauthorized for url I am doing do this to be able to run the following: qa_chain = RetrievalQA.from_chain_type( llm=llm, retriever=vector_db.as_retriever() ) What am I missing and is there a way to be able to do this fully local like doing the falcon model and pass it to ChatHuggingFace?",combine falcon 40b instruct langchain want create local llm use falcon 40b instruct model combine lanchain give pdf resource learn query ask question  learn ultimately able derive insight pdf report excel sheet   want load pdf use langchain falcon40b  instruct model agent  want build llm make interact datum use langchain  attempt far  langchaincommunityllms import huggingfacehub llm  huggingfacehub  repoid  modelname  task  text  generation   modelkwargs    maxnewtoken   512    topk   30    temperature   01    repetitionpenalty   103   huggingfacehubapitoken  hfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   reach follow stage  langchaincommunitychatmodelshuggingface import chathuggingface llm  chathuggingface  llm  llm  yet get error  hfhub  401 client error  unauthorize url able run follow  qachain  retrievalqafromchaintype  llm  llm  retriever  vectordbasretriever    miss way able fully local like falcon model pass chathuggingface ,message hfhub  401 client error  unauthorize url indicate not access endpoint service huggingface    huggingfaceco  inference  endpoint  want run everything locally  s example  use huggingfacepipeline function transformer import autotokenizer  automodelforcausallm  pipeline  modelid    tiiuae  falcon7b  instruct   model large run nvidia 4090 16 g ram modelid    gpt2  tokenizer  autotokenizerfrompretraine  modelid  model  automodelforcausallmfrompretraine  modelid  pipeline  pipeline    text  generation   model  model  tokenizer  tokenizer  maxnewtokens200   langchaincommunityllmshuggingfacepipeline import huggingfacepipeline llm  huggingfacepipeline  pipeline  pipeline  langchainprompt import prompttemplate template      question   question  answer  let us think step step     prompt  prompttemplatefromtemplate  template  chain  prompt  llm question    tell italy  print  chaininvoke     question   question   ,combine falcon 40b instruct langchain want create local llm use falcon 40b instruct model combine lanchain give pdf resource learn query ask question  learn ultimately able derive insight pdf report excel sheet   want load pdf use langchain falcon40b  instruct model agent  want build llm make interact datum use langchain  attempt far  langchaincommunityllms import huggingfacehub llm  huggingfacehub  repoid  modelname  task  text  generation   modelkwargs    maxnewtoken   512    topk   30    temperature   01    repetitionpenalty   103   huggingfacehubapitoken  hfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   reach follow stage  langchaincommunitychatmodelshuggingface import chathuggingface llm  chathuggingface  llm  llm  yet get error  hfhub  401 client error  unauthorize url able run follow  qachain  retrievalqafromchaintype  llm  llm  retriever  vectordbasretriever    miss way able fully local like falcon model pass chathuggingface  message hfhub  401 client error  unauthorize url indicate not access endpoint service huggingface    huggingfaceco  inference  endpoint  want run everything locally  s example  use huggingfacepipeline function transformer import autotokenizer  automodelforcausallm  pipeline  modelid    tiiuae  falcon7b  instruct   model large run nvidia 4090 16 g ram modelid    gpt2  tokenizer  autotokenizerfrompretraine  modelid  model  automodelforcausallmfrompretraine  modelid  pipeline  pipeline    text  generation   model  model  tokenizer  tokenizer  maxnewtokens200   langchaincommunityllmshuggingfacepipeline import huggingfacepipeline llm  huggingfacepipeline  pipeline  pipeline  langchainprompt import prompttemplate template      question   question  answer  let us think step step     prompt  prompttemplatefromtemplate  template  chain  prompt  llm question    tell italy  print  chaininvoke     question   question   ,Library/Tool-Based Queries
BERTopic: &quot;Make sure that the iterable only contains strings&quot;,"I'm still fairly new to Python so this might be easier than it appears to me, but I'm stuck. I'm trying to use BERTopic and visualize the results with PyLDAVis. I want to compare the results with the ones I got using LDA. This is my code, where ""data_words"" is the same object that I previously used with LDA Topic Modeling: import pyLDAvis import numpy as np from bertopic import BERTopic # Train Model bert_model = BERTopic(verbose=True, calculate_probabilities=True) topics, probs = bert_model.fit_transform(data_words) # Prepare data for PyLDAVis top_n = 5 topic_term_dists = bert_model.c_tf_idf.toarray()[:top_n+1, ] new_probs = probs[:, :top_n] outlier = np.array(1 - new_probs.sum(axis=1)).reshape(-1, 1) doc_topic_dists = np.hstack((new_probs, outlier)) doc_lengths = [len(doc) for doc in docs] vocab = [word for word in bert_model.vectorizer_model.vocabulary_.keys()] term_frequency = [bert_model.vectorizer_model.vocabulary_[word] for word in vocab] data = {'topic_term_dists': topic_term_dists, 'doc_topic_dists': doc_topic_dists, 'doc_lengths': doc_lengths, 'vocab': vocab, 'term_frequency': term_frequency} # Visualize using pyLDAvis vis_data= pyLDAvis.prepare(**data, mds='mmds') pyLDAvis.display(vis_data) I keep getting the following error and I don't understand how to fix the problem: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[9], line 4 1 from bertopic import BERTopic 3 bert_model = BERTopic() ----> 4 topics, probs = bert_model.fit_transform(data_words) 6 bert_model.get_topic_freq() File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bertopic/_bertopic.py:373, in BERTopic.fit_transform(self, documents, embeddings, images, y) 325 """""" Fit the models on a collection of documents, generate topics, 326 and return the probabilities and topic per document. 327 (...) 370 ``` 371 """""" 372 if documents is not None: --> 373 check_documents_type(documents) 374 check_embeddings_shape(embeddings, documents) 376 doc_ids = range(len(documents)) if documents is not None else range(len(images)) File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bertopic/_utils.py:43, in check_documents_type(documents) 41 elif isinstance(documents, Iterable) and not isinstance(documents, str): 42 if not any([isinstance(doc, str) for doc in documents]): ---> 43 raise TypeError(""Make sure that the iterable only contains strings."") 44 else: 45 raise TypeError(""Make sure that the documents variable is an iterable containing strings only."") TypeError: Make sure that the iterable only contains strings. Edit: So, I'm assuming that the data that I'm to trying to analyze aren't formatted the way in which BERTopic expects them to be. My dataset is structured like this: { ""TFU_1881_00102"": { ""magazine"": ""edited out"", ""country"": ""United Kingdom"", ""year"": ""1881"", ""tokens"": [ ""word1"", ""word2"" ], ""bigramFreqs"": { ""word1 word2"": 1 }, ""tokenFreqs"": { ""word1"": 1, ""word2"": 1 } }, ""TFU_1881_00103"": { ""magazine"": ""edited out"", ""country"": ""United Kingdom"", ""year"": ""1881"", ""tokens"": [ ""word3"", ""word4"" ], ""bigramFreqs"": { ""word3 word4"": 1 }, ""tokenFreqs"": { ""word3"": 1, ""word4"": 1 } } } I then create the ""data_words"" object with this code: with open(""Data/5_json/output_final.json"", ""r"") as file: data = json.load(file) data_words = [] counter = 0 for key in data: counter += 1 sub_list = data[key][""tokens""] data_words.append(sub_list) print(counter) Edit2: What worked So, after Goku suggested to flatten my list of lists, I initially tried this solution: flat_data_words = [] for list in data_words: for lists in list: flat_data_words.append(lists) It apparently worked, but the code resulted in a new error. I tried to search a bit more and I found a similar topic that made me understand that BERTopic is expecting each string in the list to be a document. That wasn't my case, because the code I used to flatten my list of lists simply results in a list of single tokens. I think that that's why I was getting the new error. Then I tried this and now it seemingly works: flat_data_words = [] for list_of_strings in data_words: sentence = ' '.join(list_of_strings) flat_data_words.append(sentence)","['python', 'python-3.x', 'nlp', 'topic-modeling']",1,data_words is a nested list. It contains lists and strings . bert_model.fit_transform(data_words) .fit() is expecting an iterable with only strings . You can try flattening data_words so that it only contains strings and then use : bert_model.fit_transform(data_words) A related issue: https://github.com/meghutch/tracking_pasc/blob/main/BERTopic%20Preprocessing%20Test%20using%20120%2C000%20test%20tweets.ipynb,2024-01-19 13:36:59,2024-01-19 17:16:05,939,https://stackoverflow.com/questions/77846486/bertopic-make-sure-that-the-iterable-only-contains-strings,"BERTopic: &quot;Make sure that the iterable only contains strings&quot; I'm still fairly new to Python so this might be easier than it appears to me, but I'm stuck. I'm trying to use BERTopic and visualize the results with PyLDAVis. I want to compare the results with the ones I got using LDA. This is my code, where ""data_words"" is the same object that I previously used with LDA Topic Modeling: import pyLDAvis import numpy as np from bertopic import BERTopic # Train Model bert_model = BERTopic(verbose=True, calculate_probabilities=True) topics, probs = bert_model.fit_transform(data_words) # Prepare data for PyLDAVis top_n = 5 topic_term_dists = bert_model.c_tf_idf.toarray()[:top_n+1, ] new_probs = probs[:, :top_n] outlier = np.array(1 - new_probs.sum(axis=1)).reshape(-1, 1) doc_topic_dists = np.hstack((new_probs, outlier)) doc_lengths = [len(doc) for doc in docs] vocab = [word for word in bert_model.vectorizer_model.vocabulary_.keys()] term_frequency = [bert_model.vectorizer_model.vocabulary_[word] for word in vocab] data = {'topic_term_dists': topic_term_dists, 'doc_topic_dists': doc_topic_dists, 'doc_lengths': doc_lengths, 'vocab': vocab, 'term_frequency': term_frequency} # Visualize using pyLDAvis vis_data= pyLDAvis.prepare(**data, mds='mmds') pyLDAvis.display(vis_data) I keep getting the following error and I don't understand how to fix the problem: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[9], line 4 1 from bertopic import BERTopic 3 bert_model = BERTopic() ----> 4 topics, probs = bert_model.fit_transform(data_words) 6 bert_model.get_topic_freq() File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bertopic/_bertopic.py:373, in BERTopic.fit_transform(self, documents, embeddings, images, y) 325 """""" Fit the models on a collection of documents, generate topics, 326 and return the probabilities and topic per document. 327 (...) 370 ``` 371 """""" 372 if documents is not None: --> 373 check_documents_type(documents) 374 check_embeddings_shape(embeddings, documents) 376 doc_ids = range(len(documents)) if documents is not None else range(len(images)) File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bertopic/_utils.py:43, in check_documents_type(documents) 41 elif isinstance(documents, Iterable) and not isinstance(documents, str): 42 if not any([isinstance(doc, str) for doc in documents]): ---> 43 raise TypeError(""Make sure that the iterable only contains strings."") 44 else: 45 raise TypeError(""Make sure that the documents variable is an iterable containing strings only."") TypeError: Make sure that the iterable only contains strings. Edit: So, I'm assuming that the data that I'm to trying to analyze aren't formatted the way in which BERTopic expects them to be. My dataset is structured like this: { ""TFU_1881_00102"": { ""magazine"": ""edited out"", ""country"": ""United Kingdom"", ""year"": ""1881"", ""tokens"": [ ""word1"", ""word2"" ], ""bigramFreqs"": { ""word1 word2"": 1 }, ""tokenFreqs"": { ""word1"": 1, ""word2"": 1 } }, ""TFU_1881_00103"": { ""magazine"": ""edited out"", ""country"": ""United Kingdom"", ""year"": ""1881"", ""tokens"": [ ""word3"", ""word4"" ], ""bigramFreqs"": { ""word3 word4"": 1 }, ""tokenFreqs"": { ""word3"": 1, ""word4"": 1 } } } I then create the ""data_words"" object with this code: with open(""Data/5_json/output_final.json"", ""r"") as file: data = json.load(file) data_words = [] counter = 0 for key in data: counter += 1 sub_list = data[key][""tokens""] data_words.append(sub_list) print(counter) Edit2: What worked So, after Goku suggested to flatten my list of lists, I initially tried this solution: flat_data_words = [] for list in data_words: for lists in list: flat_data_words.append(lists) It apparently worked, but the code resulted in a new error. I tried to search a bit more and I found a similar topic that made me understand that BERTopic is expecting each string in the list to be a document. That wasn't my case, because the code I used to flatten my list of lists simply results in a list of single tokens. I think that that's why I was getting the new error. Then I tried this and now it seemingly works: flat_data_words = [] for list_of_strings in data_words: sentence = ' '.join(list_of_strings) flat_data_words.append(sentence)",bertopic   quot  make sure iterable contain string  quot   m still fairly new python might easier appear   m stuck   m try use bertopic visualize result pyldavis  want compare result one get use lda  code    datawords  object previously use lda topic modeling  import pyldavis import numpy np bertopic import bertopic  train model bertmodel  bertopic  verbose  true  calculateprobabilitie  true  topic  prob  bertmodelfittransform  datawords   prepare datum pyldavis topn  5 topictermdist  bertmodelctfidftoarray     topn1   newprob  prob     topn  outlier  nparray  1  newprobssum  axis1   reshape  1  1  doctopicdist  nphstack   newprobs  outli   doclength   len  doc  doc docs  vocab   word word bertmodelvectorizermodelvocabularykeys    termfrequency   bertmodelvectorizermodelvocabulary   word  word vocab  datum    topictermdist   topictermdist   doctopicdist   doctopicdist   doclength   doclength   vocab   vocab   termfrequency   termfrequency   visualize use pyldavis visdata pyldavisprepare    datum  mdsmmds   pyldavisdisplay  visdata  keep get follow error not understand fix problem  library  frameworks  pythonframework  versions311  lib  python311  site  package  tqdm  autopy21  tqdmwarning  iprogress find  please update jupyter ipywidget  see   ipywidgetsreadthedocsio  en  stable  userinstallhtml autonotebook import tqdm notebooktqdm                                       typeerror traceback  recent call last  cell  9   line 4 1 bertopic import bertopic 3 bertmodel  bertopic      4 topic  prob  bertmodelfittransform  datawords  6 bertmodelgettopicfreq   file library  frameworks  pythonframework  versions311  lib  python311  site  package  bertopicbertopicpy373  bertopicfittransform  self  document  embedding  image   325     fit model collection document  generate topic  326 return probability topic per document  327    370    371     372 document none    373 checkdocumentstype  document  374 checkembeddingsshape  embedding  document  376 docid  range  len  document   document none else range  len  image   file library  frameworks  pythonframework  versions311  lib  python311  site  package  bertopicutilspy43  checkdocumentstype  document  41 elif isinstance  document  iterable  isinstance  document  str   42   isinstance  doc  str  doc document       43 raise typeerror    make sure iterable contain string    44 else  45 raise typeerror    make sure document variable iterable contain string    typeerror  make sure iterable contain string  edit    m assume datum  m try analyze not format way bertopic expect  dataset structure like     tfu188100102      magazine     edit     country     united kingdom     year     1881     token      word1     word2      bigramfreq      word1 word2   1     tokenfreqs      word1   1    word2   1      tfu188100103      magazine     edit     country     united kingdom     year     1881     token      word3     word4      bigramfreq      word3 word4   1     tokenfreqs      word3   1    word4   1    create   datawords  object code  open    data5json  outputfinaljson     r   file  datum  jsonload  file  dataword    counter  0 key datum  counter   1 sublist  datum  key     token   datawordsappend  sublist  print  counter  edit2  work  goku suggest flatten list list  initially try solution  flatdatawords    list datawords  list list  flatdatawordsappend  list  apparently work  code result new error  try search bit find similar topic make understand bertopic expect string list document  not case  code use flatten list list simply results list single token  think be get new error  try seemingly work  flatdatawords    listofstring datawords  sentence    join  listofstrings  flatdatawordsappend  sentence ,datawords nest list  contain list string  bertmodelfittransform  datawords  fit   expect iterable string  try flatten datawords contain string use  bertmodelfittransform  datawords  relate issue    githubcom  meghutch  trackingpasc  blob  main  bertopic  20preprocessing  20test  20using  20120  2c000  20test  20tweetsipynb,bertopic   quot  make sure iterable contain string  quot   m still fairly new python might easier appear   m stuck   m try use bertopic visualize result pyldavis  want compare result one get use lda  code    datawords  object previously use lda topic modeling  import pyldavis import numpy np bertopic import bertopic  train model bertmodel  bertopic  verbose  true  calculateprobabilitie  true  topic  prob  bertmodelfittransform  datawords   prepare datum pyldavis topn  5 topictermdist  bertmodelctfidftoarray     topn1   newprob  prob     topn  outlier  nparray  1  newprobssum  axis1   reshape  1  1  doctopicdist  nphstack   newprobs  outli   doclength   len  doc  doc docs  vocab   word word bertmodelvectorizermodelvocabularykeys    termfrequency   bertmodelvectorizermodelvocabulary   word  word vocab  datum    topictermdist   topictermdist   doctopicdist   doctopicdist   doclength   doclength   vocab   vocab   termfrequency   termfrequency   visualize use pyldavis visdata pyldavisprepare    datum  mdsmmds   pyldavisdisplay  visdata  keep get follow error not understand fix problem  library  frameworks  pythonframework  versions311  lib  python311  site  package  tqdm  autopy21  tqdmwarning  iprogress find  please update jupyter ipywidget  see   ipywidgetsreadthedocsio  en  stable  userinstallhtml autonotebook import tqdm notebooktqdm                                       typeerror traceback  recent call last  cell  9   line 4 1 bertopic import bertopic 3 bertmodel  bertopic      4 topic  prob  bertmodelfittransform  datawords  6 bertmodelgettopicfreq   file library  frameworks  pythonframework  versions311  lib  python311  site  package  bertopicbertopicpy373  bertopicfittransform  self  document  embedding  image   325     fit model collection document  generate topic  326 return probability topic per document  327    370    371     372 document none    373 checkdocumentstype  document  374 checkembeddingsshape  embedding  document  376 docid  range  len  document   document none else range  len  image   file library  frameworks  pythonframework  versions311  lib  python311  site  package  bertopicutilspy43  checkdocumentstype  document  41 elif isinstance  document  iterable  isinstance  document  str   42   isinstance  doc  str  doc document       43 raise typeerror    make sure iterable contain string    44 else  45 raise typeerror    make sure document variable iterable contain string    typeerror  make sure iterable contain string  edit    m assume datum  m try analyze not format way bertopic expect  dataset structure like     tfu188100102      magazine     edit     country     united kingdom     year     1881     token      word1     word2      bigramfreq      word1 word2   1     tokenfreqs      word1   1    word2   1      tfu188100103      magazine     edit     country     united kingdom     year     1881     token      word3     word4      bigramfreq      word3 word4   1     tokenfreqs      word3   1    word4   1    create   datawords  object code  open    data5json  outputfinaljson     r   file  datum  jsonload  file  dataword    counter  0 key datum  counter   1 sublist  datum  key     token   datawordsappend  sublist  print  counter  edit2  work  goku suggest flatten list list  initially try solution  flatdatawords    list datawords  list list  flatdatawordsappend  list  apparently work  code result new error  try search bit find similar topic make understand bertopic expect string list document  not case  code use flatten list list simply results list single token  think be get new error  try seemingly work  flatdatawords    listofstring datawords  sentence    join  listofstrings  flatdatawordsappend  sentence  datawords nest list  contain list string  bertmodelfittransform  datawords  fit   expect iterable string  try flatten datawords contain string use  bertmodelfittransform  datawords  relate issue    githubcom  meghutch  trackingpasc  blob  main  bertopic  20preprocessing  20test  20using  20120  2c000  20test  20tweetsipynb,Library/Tool-Based Queries
Loading en_core_web_sm results in AttributeError: module &#39;transformers&#39; has no attribute &#39;BertTokenizerFast&#39;,"I (a beginner at programming and anything pc related) am clueless how to solve the following problem: I had spacy 3.7.2 installed, including en_core_web_sm. Running the code nlp = spacy.load(""en_core_web_sm"") resulted in the traceback mentioned in the title question. I tried downgrading spacy to version 3.6.1 (via pip install), then ran the code again and got the traceback: RegistryError: [E892] Unknown function registry: 'vectors'. Available names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers Same happend with spacy version 3.6.0. Some other person ran my code with spacy 3.4.3 and that worked. So tried downgrading spacy to that, but got the AttributeError again. I noticed, that on my PC (windows) under ...anaconda3\Lib\site-packages there were still folders with en_core_web_sm version 3.7.2 (despite the downgrading). I randomly deleted those folders. Then installed spacy again, and also tried to manually install en_core_web_sm with python -m spacy download en_core_web_sm-3.4.3 But got the AttributeError in cmd. Tried installing different older/newer versions of spacy as well as en_core_web_sm but can't successfully install the latter. So now I can't even load en_core_web_sm since it's not installed. Does anyone have an idea what else I could try to install en_core_web_sm?","['python', 'pip', 'nlp', 'anaconda', 'spacy']",1,"I think that there is older version of the transformers in your global environment that cause the problem. To avoid version conflict, create a new virtual environment using conda : conda create -n myenv Activate myenv : conda activate myenv install scipy check the instalation page: conda install -c conda-forge spacy Download en_core_web_sm : python -m spacy download en_core_web_sm Now you can run your code.",2024-01-18 13:04:42,2024-01-18 13:31:18,284,https://stackoverflow.com/questions/77839628/loading-en-core-web-sm-results-in-attributeerror-module-transformers-has-no-a,"Loading en_core_web_sm results in AttributeError: module &#39;transformers&#39; has no attribute &#39;BertTokenizerFast&#39; I (a beginner at programming and anything pc related) am clueless how to solve the following problem: I had spacy 3.7.2 installed, including en_core_web_sm. Running the code nlp = spacy.load(""en_core_web_sm"") resulted in the traceback mentioned in the title question. I tried downgrading spacy to version 3.6.1 (via pip install), then ran the code again and got the traceback: RegistryError: [E892] Unknown function registry: 'vectors'. Available names: architectures, augmenters, batchers, callbacks, cli, datasets, displacy_colors, factories, initializers, languages, layers, lemmatizers, loggers, lookups, losses, misc, models, ops, optimizers, readers, schedules, scorers, tokenizers Same happend with spacy version 3.6.0. Some other person ran my code with spacy 3.4.3 and that worked. So tried downgrading spacy to that, but got the AttributeError again. I noticed, that on my PC (windows) under ...anaconda3\Lib\site-packages there were still folders with en_core_web_sm version 3.7.2 (despite the downgrading). I randomly deleted those folders. Then installed spacy again, and also tried to manually install en_core_web_sm with python -m spacy download en_core_web_sm-3.4.3 But got the AttributeError in cmd. Tried installing different older/newer versions of spacy as well as en_core_web_sm but can't successfully install the latter. So now I can't even load en_core_web_sm since it's not installed. Does anyone have an idea what else I could try to install en_core_web_sm?",load encorewebsm result attributeerror  module   39  transformer   39  attribute   39  berttokenizerfast   39   beginner programming anything pc relate  clueless solve follow problem  spacy 372 instal  include encorewebsm  run code nlp  spacyload    encorewebsm   result traceback mention title question  try downgrade spacy version 361  via pip install   run code get traceback  registryerror   e892  unknown function registry   vector   available name  architecture  augmenter  batcher  callback  cli  dataset  displacycolor  factory  initializer  language  layer  lemmatizer  logger  lookup  loss  misc  model  op  optimizer  reader  schedule  scorer  tokenizer happend spacy version 360  person run code spacy 343 work  try downgrade spacy  get attributeerror  notice  pc  window   anaconda3libsite  package still folder encorewebsm version 372  despite downgrade   randomly delete folder  instal spacy  also try manually install encorewebsm python m spacy download encorewebsm343 get attributeerror cmd  try instal different old  new version spacy well encorewebsm can not successfully install latter  can not even load encorewebsm since be instal  anyone idea else could try install encorewebsm ,think old version transformer global environment cause problem  avoid version conflict  create new virtual environment use conda  conda create n myenv activate myenv  conda activate myenv install scipy check instalation page  conda install c conda  forge spacy download encorewebsm  python m spacy download encorewebsm run code ,load encorewebsm result attributeerror  module   39  transformer   39  attribute   39  berttokenizerfast   39   beginner programming anything pc relate  clueless solve follow problem  spacy 372 instal  include encorewebsm  run code nlp  spacyload    encorewebsm   result traceback mention title question  try downgrade spacy version 361  via pip install   run code get traceback  registryerror   e892  unknown function registry   vector   available name  architecture  augmenter  batcher  callback  cli  dataset  displacycolor  factory  initializer  language  layer  lemmatizer  logger  lookup  loss  misc  model  op  optimizer  reader  schedule  scorer  tokenizer happend spacy version 360  person run code spacy 343 work  try downgrade spacy  get attributeerror  notice  pc  window   anaconda3libsite  package still folder encorewebsm version 372  despite downgrade   randomly delete folder  instal spacy  also try manually install encorewebsm python m spacy download encorewebsm343 get attributeerror cmd  try instal different old  new version spacy well encorewebsm can not successfully install latter  can not even load encorewebsm since be instal  anyone idea else could try install encorewebsm  think old version transformer global environment cause problem  avoid version conflict  create new virtual environment use conda  conda create n myenv activate myenv  conda activate myenv install scipy check instalation page  conda install c conda  forge spacy download encorewebsm  python m spacy download encorewebsm run code ,Basic Understanding
Truncate texts in the middle for Bert,"I am learning about Bert, which only deals with texts with fewer than 512 tokens, and came across this answer which says that truncating text in the middle (as opposed to at the start or at the end) may work well for Bert. I wonder whether there is any library to do that type of truncation because as far as I understand, one word can consist of multiple Bert token so I cannot simply get the middle 512 words. Thanks in advance","['nlp', 'token', 'tokenize', 'bert-language-model']",1,"The post references a paper which says that the first 128 tokens and the last 382 tokens (not including the CLS and SEP tokens) should be kept. For tokenization, you can use the Bert Tokenizer from HuggingFace's Transformers library to tokenize the full String and then trim out everything besides the first 129 and last 383 tokens. 129 because we include the initial CLS token and 383 because we include the ending SEP token. Code: # Generate sample text from string import ascii_lowercase sample_text = """" for c1 in ascii_lowercase: for c2 in ascii_lowercase: sample_text += f""{c1}{c2} "" # Get tokenizer from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"") # Perform tokenization tokenized = tokenizer(sample_text) # Trim tokens if len(tokenized[""input_ids""]) > 512: for k,v in tokenized.items(): tokenized[k] = v[:129] + v[-383:] # Verify result print(tokenizer.decode(tokenized[""input_ids""])) print(len(tokenized[""input_ids""]))",2024-01-17 21:05:10,2024-01-18 01:56:01,245,https://stackoverflow.com/questions/77835506/truncate-texts-in-the-middle-for-bert,"Truncate texts in the middle for Bert I am learning about Bert, which only deals with texts with fewer than 512 tokens, and came across this answer which says that truncating text in the middle (as opposed to at the start or at the end) may work well for Bert. I wonder whether there is any library to do that type of truncation because as far as I understand, one word can consist of multiple Bert token so I cannot simply get the middle 512 words. Thanks in advance",truncate text middle bert learn bert  deal texts few 512 token  come across answer say truncating text middle  oppose start end  may work well bert  wonder whether library type truncation far understand  one word consist multiple bert token simply get middle 512 word  thank advance,post reference paper say first 128 token last 382 token  include cls sep token  keep  tokenization  use bert tokenizer huggingface s transformers library tokenize full string trim everything besides first 129 last 383 token  129 include initial cls token 383 include end sep token  code   generate sample text string import asciilowercase sampletext     c1 asciilowercase  c2 asciilowercase  sampletext   f   c1   c2     get tokenizer transformer import berttokenizer tokenizer  berttokenizerfrompretraine    bert  base  uncased    perform tokenization tokenized  tokenizer  sampletext   trim token len  tokenized    inputids     512  k  v tokenizeditem    tokenize  k   v   129   v  383    verify result print  tokenizerdecode  tokenized    inputids     print  len  tokenized    inputids    ,truncate text middle bert learn bert  deal texts few 512 token  come across answer say truncating text middle  oppose start end  may work well bert  wonder whether library type truncation far understand  one word consist multiple bert token simply get middle 512 word  thank advance post reference paper say first 128 token last 382 token  include cls sep token  keep  tokenization  use bert tokenizer huggingface s transformers library tokenize full string trim everything besides first 129 last 383 token  129 include initial cls token 383 include end sep token  code   generate sample text string import asciilowercase sampletext     c1 asciilowercase  c2 asciilowercase  sampletext   f   c1   c2     get tokenizer transformer import berttokenizer tokenizer  berttokenizerfrompretraine    bert  base  uncased    perform tokenization tokenized  tokenizer  sampletext   trim token len  tokenized    inputids     512  k  v tokenizeditem    tokenize  k   v   129   v  383    verify result print  tokenizerdecode  tokenized    inputids     print  len  tokenized    inputids    ,Basic Understanding
Spacy import error: cannot import name &#39;COMBINING_DIACRITICS&#39; from &#39;spacy.lang.char_classes&#39;,"When I try importing the NLP library 'Spacy' using import spacy I'm getting the following error: ImportError: cannot import name 'COMBINING_DIACRITICS' from 'spacy.lang.char_classes' Here are my versions: spacy==3.7.2 spacy-legacy==3.0.12 thinc==8.2.2 pydantic==1.8.2 pydantic_core==2.14.6 python version - 3.9.18 I tried installing, uninstalling Spacy, Upgrading the library & all the other common checks. Where is the issue? Complete error: ImportError Traceback (most recent call last) ~\AppData\Local\Temp\ipykernel_33180\572880994.py in ----> 1 import spacy c:\Users\anaconda3\lib\site-packages\spacy\__init__.py in 11 from thinc.api import Config, prefer_gpu, require_cpu, require_gpu # noqa: F401 12 ---> 13 from . import pipeline # noqa: F401 14 from . import util 15 from .about import __version__ # noqa: F401 c:\Users\anaconda3\lib\site-packages\spacy\pipeline\__init__.py in ----> 1 from .attributeruler import AttributeRuler 2 from .dep_parser import DependencyParser 3 from .edit_tree_lemmatizer import EditTreeLemmatizer 4 from .entity_linker import EntityLinker 5 from .entityruler import EntityRuler c:\Users\anaconda3\lib\site-packages\spacy\pipeline\attributeruler.py in 6 from .. import util 7 from ..errors import Errors ----> 8 from ..language import Language 9 from ..matcher import Matcher 10 from ..scorer import Scorer ... 3 ALPHA_LOWER, 4 ALPHA_UPPER, 5 COMBINING_DIACRITICS, ImportError: cannot import name 'COMBINING_DIACRITICS' from 'spacy.lang.char_classes' (c:\Users\anaconda3\lib\site-packages\spacy\lang\char_classes.py)","['python', 'pip', 'nlp', 'spacy', 'pydantic']",1,"Create a new enviroments and install scipy , you can use the following command: conda create -n myenv Activate myenv : conda activate myenv install scipy check the instalation page: conda install -c conda-forge spacy Then you can use scipy .",2024-01-17 07:03:54,2024-01-17 19:38:39,233,https://stackoverflow.com/questions/77830490/spacy-import-error-cannot-import-name-combining-diacritics-from-spacy-lang-c,"Spacy import error: cannot import name &#39;COMBINING_DIACRITICS&#39; from &#39;spacy.lang.char_classes&#39; When I try importing the NLP library 'Spacy' using import spacy I'm getting the following error: ImportError: cannot import name 'COMBINING_DIACRITICS' from 'spacy.lang.char_classes' Here are my versions: spacy==3.7.2 spacy-legacy==3.0.12 thinc==8.2.2 pydantic==1.8.2 pydantic_core==2.14.6 python version - 3.9.18 I tried installing, uninstalling Spacy, Upgrading the library & all the other common checks. Where is the issue? Complete error: ImportError Traceback (most recent call last) ~\AppData\Local\Temp\ipykernel_33180\572880994.py in ----> 1 import spacy c:\Users\anaconda3\lib\site-packages\spacy\__init__.py in 11 from thinc.api import Config, prefer_gpu, require_cpu, require_gpu # noqa: F401 12 ---> 13 from . import pipeline # noqa: F401 14 from . import util 15 from .about import __version__ # noqa: F401 c:\Users\anaconda3\lib\site-packages\spacy\pipeline\__init__.py in ----> 1 from .attributeruler import AttributeRuler 2 from .dep_parser import DependencyParser 3 from .edit_tree_lemmatizer import EditTreeLemmatizer 4 from .entity_linker import EntityLinker 5 from .entityruler import EntityRuler c:\Users\anaconda3\lib\site-packages\spacy\pipeline\attributeruler.py in 6 from .. import util 7 from ..errors import Errors ----> 8 from ..language import Language 9 from ..matcher import Matcher 10 from ..scorer import Scorer ... 3 ALPHA_LOWER, 4 ALPHA_UPPER, 5 COMBINING_DIACRITICS, ImportError: cannot import name 'COMBINING_DIACRITICS' from 'spacy.lang.char_classes' (c:\Users\anaconda3\lib\site-packages\spacy\lang\char_classes.py)",spacy import error  import name   39  combiningdiacritics   39    39  spacylangcharclasse   39  try import nlp library  spacy  use import spacy  m get follow error  importerror  import name  combiningdiacritics   spacylangcharclasse  version  spacy372 spacy  legacy3012 thinc822 pydantic182 pydanticcore2146 python version  3918 try instal  uninstalle spacy  upgrade library  common check  issue  complete error  importerror traceback  recent call last  appdatalocaltempipykernel33180572880994py    1 import spacy c  usersanaconda3libsite  packagesspacyinitpy 11 thincapi import config  prefergpu  requirecpu  requiregpu  noqa  f401 12    13  import pipeline  noqa  f401 14  import util 15 about import   version    noqa  f401 c  usersanaconda3libsite  packagesspacypipelineinitpy    1 attributeruler import attributeruler 2 depparser import dependencyparser 3 edittreelemmatizer import edittreelemmatizer 4 entitylinker import entitylinker 5 entityruler import entityruler c  usersanaconda3libsite  packagesspacypipelineattributerulerpy 6  import util 7  error import errors    8  language import language 9  matcher import matcher 10  scorer import scorer  3 alphalower  4 alphaupper  5 combiningdiacritics  importerror  import name  combiningdiacritics   spacylangcharclasse   c  usersanaconda3libsite  packagesspacylangcharclassespy ,create new enviroment install scipy  use follow command  conda create n myenv activate myenv  conda activate myenv install scipy check instalation page  conda install c conda  forge spacy use scipy ,spacy import error  import name   39  combiningdiacritics   39    39  spacylangcharclasse   39  try import nlp library  spacy  use import spacy  m get follow error  importerror  import name  combiningdiacritics   spacylangcharclasse  version  spacy372 spacy  legacy3012 thinc822 pydantic182 pydanticcore2146 python version  3918 try instal  uninstalle spacy  upgrade library  common check  issue  complete error  importerror traceback  recent call last  appdatalocaltempipykernel33180572880994py    1 import spacy c  usersanaconda3libsite  packagesspacyinitpy 11 thincapi import config  prefergpu  requirecpu  requiregpu  noqa  f401 12    13  import pipeline  noqa  f401 14  import util 15 about import   version    noqa  f401 c  usersanaconda3libsite  packagesspacypipelineinitpy    1 attributeruler import attributeruler 2 depparser import dependencyparser 3 edittreelemmatizer import edittreelemmatizer 4 entitylinker import entitylinker 5 entityruler import entityruler c  usersanaconda3libsite  packagesspacypipelineattributerulerpy 6  import util 7  error import errors    8  language import language 9  matcher import matcher 10  scorer import scorer  3 alphalower  4 alphaupper  5 combiningdiacritics  importerror  import name  combiningdiacritics   spacylangcharclasse   c  usersanaconda3libsite  packagesspacylangcharclassespy  create new enviroment install scipy  use follow command  conda create n myenv activate myenv  conda activate myenv install scipy check instalation page  conda install c conda  forge spacy use scipy ,Implementation Issues
What is sent to the llm when using a chat model?,"I am confused by how multiple messages are combined and sent to a large language model such as ChatOpenAI . from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate.from_messages([ (""system"", ""You are a helpful AI bot. Your name is {name}.""), (""human"", ""Hello, how are you doing?""), (""ai"", ""I'm doing well, thanks!""), (""human"", ""{user_input}""), ]) messages = template.format_messages( name=""Bob"", user_input=""What is your name?"" ) messages [SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=""I'm doing well, thanks!""), HumanMessage(content='What is your name?')] Is it generating text that looks like this: System: Human: Assistant: Human: ... How can I print the final text sent to the llm?","['nlp', 'langchain']",2,"You can use format method to format the chat template into a string, here is the updated code: prompt = template.format( name=""Bob"", user_input=""What is your name?"" ) prompt Output: System: You are a helpful AI bot. Your name is Bob. Human: Hello, how are you doing? AI: I'm doing well, thanks! Human: What is your name?",2024-01-16 20:32:54,2024-01-16 21:22:25,158,https://stackoverflow.com/questions/77828572/what-is-sent-to-the-llm-when-using-a-chat-model,"What is sent to the llm when using a chat model? I am confused by how multiple messages are combined and sent to a large language model such as ChatOpenAI . from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate.from_messages([ (""system"", ""You are a helpful AI bot. Your name is {name}.""), (""human"", ""Hello, how are you doing?""), (""ai"", ""I'm doing well, thanks!""), (""human"", ""{user_input}""), ]) messages = template.format_messages( name=""Bob"", user_input=""What is your name?"" ) messages [SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=""I'm doing well, thanks!""), HumanMessage(content='What is your name?')] Is it generating text that looks like this: System: Human: Assistant: Human: ... How can I print the final text sent to the llm?",send llm use chat model  confused multiple message combine send large language model chatopenai  langchaincoreprompts import chatprompttemplate template  chatprompttemplatefrommessage      system     helpful ai bot  name  name          human     hello          ai      m well  thank         human      userinput       message  templateformatmessage  name  bob   userinput  name    message  systemmessage  contentyou helpful ai bot  name bob     humanmessage  contenthello      aimessage  content   m well  thank      humanmessage  contentwhat name     generate text look like  system  human  assistant  human   print final text send llm ,use format method format chat template string  update code  prompt  templateformat  name  bob   userinput  name    prompt output  system  helpful ai bot  name bob  human  hello   ai   m well  thank  human  name ,send llm use chat model  confused multiple message combine send large language model chatopenai  langchaincoreprompts import chatprompttemplate template  chatprompttemplatefrommessage      system     helpful ai bot  name  name          human     hello          ai      m well  thank         human      userinput       message  templateformatmessage  name  bob   userinput  name    message  systemmessage  contentyou helpful ai bot  name bob     humanmessage  contenthello      aimessage  content   m well  thank      humanmessage  contentwhat name     generate text look like  system  human  assistant  human   print final text send llm  use format method format chat template string  update code  prompt  templateformat  name  bob   userinput  name    prompt output  system  helpful ai bot  name bob  human  hello   ai   m well  thank  human  name ,Basic Understanding
I am classifying each word in a sentence (Named Entity Recognition) but I receive ...an unexpected keyword argument &#39;grouped_entities&#39;,"sentence = 'American Airlines was the first airline to fly every A380 flight perfectly when President George Bush was in Office. The Woodlands Texas is a great place to be.' ner = pipeline('text-classification', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True) ners = ner(sentence) print('\nSentence:') print(wrapper.fill(sentence)) print('\n') for n in ners: print(f""{n['word']} -> {n['entity_group']}"") I am inside google colab. I tried !pip install transformers --upgrade # The error is caused by a bug in the transformers library. The fix is to install the latest version of the library. but I received the following: /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py in _encode_plus(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs) 574 ) -> BatchEncoding: 575 batched_input = [(text, text_pair)] if text_pair else [text] --> 576 batched_output = self._batch_encode_plus( 577 batched_input, 578 is_split_into_words=is_split_into_words, TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'grouped_entities'","['python', 'nlp', 'google-cloud-colab-enterprise']",1,"There may be a confusion , the Named Entity Recognition task is a token-classification task, not a text-classification task. Please update your code: ner = pipeline( 'token-classification', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True ) # alias ""ner"" available That will raise a warning : UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=""simple""` instead. Updated code with aggregation_strategy : # Updated code with 'aggregation_strategy' ner = pipeline( 'ner', model='dbmdz/bert-large-cased-finetuned-conll03-english', aggregation_strategy='simple' )",2024-01-16 01:47:34,2024-01-16 11:01:46,173,https://stackoverflow.com/questions/77823105/i-am-classifying-each-word-in-a-sentence-named-entity-recognition-but-i-receiv,"I am classifying each word in a sentence (Named Entity Recognition) but I receive ...an unexpected keyword argument &#39;grouped_entities&#39; sentence = 'American Airlines was the first airline to fly every A380 flight perfectly when President George Bush was in Office. The Woodlands Texas is a great place to be.' ner = pipeline('text-classification', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True) ners = ner(sentence) print('\nSentence:') print(wrapper.fill(sentence)) print('\n') for n in ners: print(f""{n['word']} -> {n['entity_group']}"") I am inside google colab. I tried !pip install transformers --upgrade # The error is caused by a bug in the transformers library. The fix is to install the latest version of the library. but I received the following: /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py in _encode_plus(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs) 574 ) -> BatchEncoding: 575 batched_input = [(text, text_pair)] if text_pair else [text] --> 576 batched_output = self._batch_encode_plus( 577 batched_input, 578 is_split_into_words=is_split_into_words, TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'grouped_entities'",classify word sentence  name entity recognition  receive  unexpected keyword argument   39  groupedentitie   39  sentence   american airlines first airline fly every a380 flight perfectly president george bush office  woodland texas great place   ner  pipeline   text  classification   modeldbmdz  bert  large  case  finetune  conll03  english   groupedentitie  true  ner  ner  sentence  print   nsentence    print  wrapperfill  sentence   print   n   n ner  print  f   n   word       n   entitygroup      inside google colab  try  pip install transformer  upgrade  error cause bug transformer library  fix install late version library  receive follow  usr  local  lib  python310  dist  package  transformer  tokenizationutilsfastpy  encodeplus  self  text  textpair  addspecialtoken  paddingstrategy  truncationstrategy  maxlength  stride  issplitintoword  padtomultipleof  returntensor  returntokentypeid  returnattentionmask  returnoverflowingtokens  returnspecialtokensmask  returnoffsetsmapping  returnlength  verbose    kwargs  574    batchencoding  575 batchedinput    text  textpair   textpair else  text    576 batchedoutput  selfbatchencodeplus  577 batchedinput  578 issplitintoword  issplitintoword  typeerror  pretrainedtokenizerfastbatchencodeplus   get unexpected keyword argument  groupedentitie ,may confusion  name entity recognition task token  classification task  text  classification task  please update code  ner  pipeline   token  classification   modeldbmdz  bert  large  case  finetune  conll03  english   groupedentitie  true   alia   ner  available raise warning  userwarning   groupedentitie  deprecate remove version v500  default  aggregationstrategy  simple   instead  update code aggregationstrategy   update code  aggregationstrategy  ner  pipeline   ner   modeldbmdz  bert  large  case  finetune  conll03  english   aggregationstrategysimple  ,classify word sentence  name entity recognition  receive  unexpected keyword argument   39  groupedentitie   39  sentence   american airlines first airline fly every a380 flight perfectly president george bush office  woodland texas great place   ner  pipeline   text  classification   modeldbmdz  bert  large  case  finetune  conll03  english   groupedentitie  true  ner  ner  sentence  print   nsentence    print  wrapperfill  sentence   print   n   n ner  print  f   n   word       n   entitygroup      inside google colab  try  pip install transformer  upgrade  error cause bug transformer library  fix install late version library  receive follow  usr  local  lib  python310  dist  package  transformer  tokenizationutilsfastpy  encodeplus  self  text  textpair  addspecialtoken  paddingstrategy  truncationstrategy  maxlength  stride  issplitintoword  padtomultipleof  returntensor  returntokentypeid  returnattentionmask  returnoverflowingtokens  returnspecialtokensmask  returnoffsetsmapping  returnlength  verbose    kwargs  574    batchencoding  575 batchedinput    text  textpair   textpair else  text    576 batchedoutput  selfbatchencodeplus  577 batchedinput  578 issplitintoword  issplitintoword  typeerror  pretrainedtokenizerfastbatchencodeplus   get unexpected keyword argument  groupedentitie  may confusion  name entity recognition task token  classification task  text  classification task  please update code  ner  pipeline   token  classification   modeldbmdz  bert  large  case  finetune  conll03  english   groupedentitie  true   alia   ner  available raise warning  userwarning   groupedentitie  deprecate remove version v500  default  aggregationstrategy  simple   instead  update code aggregationstrategy   update code  aggregationstrategy  ner  pipeline   ner   modeldbmdz  bert  large  case  finetune  conll03  english   aggregationstrategysimple  ,Library/Tool-Based Queries
How to add a dense layer on top of SentenceTransformer?,"In this tutorial ( Train and Fine-Tune Sentence Transformers Models ) they go through creating a SentenceTransformer by combining a word embedding module with a pooling layer: from sentence_transformers import SentenceTransformer, models ## Step 1: use an existing language model word_embedding_model = models.Transformer('distilroberta-base') ## Step 2: use a pool function over the token embeddings pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension()) ## Join steps 1 and 2 using the modules argument model = SentenceTransformer(modules=[word_embedding_model, pooling_model]) # model.encode(""Hi there"") # => works fine And then they say: If necessary, additional layers can be added, for example, dense, bag of words, and convolutional. I tried to add a dense layer on top of the model, but I'm getting an error: from sentence_transformers import SentenceTransformer, models ## Step 1: use an existing language model word_embedding_model = models.Transformer('distilroberta-base') ## Step 2: use a pool function over the token embeddings pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension()) ## My Dense Layer dense_layer = torch.nn.Linear(pooling_model.get_sentence_embedding_dimension(), 128) ## Join steps 1 and 2 using the modules argument model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_layer]) And when I run model.encode(""hi there"") I get: TypeError: linear(): argument 'input' (position 1) must be Tensor, not dict I found the same error here but using BertModel.from_pretrained , not models.Transformer . The suggested answer (passing the argument return_dict=False ) doesn't work: word_embedding_model = models.Transformer('distilroberta-base', return_dict=False) TypeError: Transformer. init () got an unexpected keyword argument 'return_dict' Any ideas how to add a dense layer correctly?","['python', 'nlp', 'huggingface-transformers', 'sentence-transformers']",1,"According to the documentation , replace this line: dense_layer = torch.nn.Linear(pooling_model.get_sentence_embedding_dimension(), 128) with the following: dense_layer = models.Dense(pooling_model.get_sentence_embedding_dimension(), 128)",2024-01-15 20:29:40,2024-01-15 20:43:19,488,https://stackoverflow.com/questions/77822202/how-to-add-a-dense-layer-on-top-of-sentencetransformer,"How to add a dense layer on top of SentenceTransformer? In this tutorial ( Train and Fine-Tune Sentence Transformers Models ) they go through creating a SentenceTransformer by combining a word embedding module with a pooling layer: from sentence_transformers import SentenceTransformer, models ## Step 1: use an existing language model word_embedding_model = models.Transformer('distilroberta-base') ## Step 2: use a pool function over the token embeddings pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension()) ## Join steps 1 and 2 using the modules argument model = SentenceTransformer(modules=[word_embedding_model, pooling_model]) # model.encode(""Hi there"") # => works fine And then they say: If necessary, additional layers can be added, for example, dense, bag of words, and convolutional. I tried to add a dense layer on top of the model, but I'm getting an error: from sentence_transformers import SentenceTransformer, models ## Step 1: use an existing language model word_embedding_model = models.Transformer('distilroberta-base') ## Step 2: use a pool function over the token embeddings pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension()) ## My Dense Layer dense_layer = torch.nn.Linear(pooling_model.get_sentence_embedding_dimension(), 128) ## Join steps 1 and 2 using the modules argument model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_layer]) And when I run model.encode(""hi there"") I get: TypeError: linear(): argument 'input' (position 1) must be Tensor, not dict I found the same error here but using BertModel.from_pretrained , not models.Transformer . The suggested answer (passing the argument return_dict=False ) doesn't work: word_embedding_model = models.Transformer('distilroberta-base', return_dict=False) TypeError: Transformer. init () got an unexpected keyword argument 'return_dict' Any ideas how to add a dense layer correctly?",add dense layer top sentencetransformer  tutorial  train fine  tune sentence transformers models  go create sentencetransformer combine word embed module pool layer  sentencetransformer import sentencetransformer  model   step 1  use exist language model wordembeddingmodel  model  transformer   distilroberta  base     step 2  use pool function token embedding poolingmodel  model  pooling  wordembeddingmodelgetwordembeddingdimension      join step 1 2 use module argument model  sentencetransformer  modules  wordembeddingmodel  poolingmodel    modelencode    hi      work fine say  necessary  additional layer add  example  dense  bag word  convolutional  try add dense layer top model   m get error  sentencetransformer import sentencetransformer  model   step 1  use exist language model wordembeddingmodel  model  transformer   distilroberta  base     step 2  use pool function token embedding poolingmodel  model  pooling  wordembeddingmodelgetwordembeddingdimension      dense layer denselayer  torchnn  linear  poolingmodelgetsentenceembeddingdimension    128    join step 1 2 use module argument model  sentencetransformer  modules  wordembeddingmodel  poolingmodel  denselayer   run modelencode    hi   get  typeerror  linear    argument  input   position 1  must tensor  dict find error use bertmodelfrompretrained  model  transformer  suggest answer  pass argument returndict  false  not work  wordembeddingmodel  model  transformer   distilroberta  base   returndict  false  typeerror  transformer  init   get unexpected keyword argument  returndict  idea add dense layer correctly ,accord documentation  replace line  denselayer  torchnn  linear  poolingmodelgetsentenceembeddingdimension    128  follow  denselayer  model  dense  poolingmodelgetsentenceembeddingdimension    128 ,add dense layer top sentencetransformer  tutorial  train fine  tune sentence transformers models  go create sentencetransformer combine word embed module pool layer  sentencetransformer import sentencetransformer  model   step 1  use exist language model wordembeddingmodel  model  transformer   distilroberta  base     step 2  use pool function token embedding poolingmodel  model  pooling  wordembeddingmodelgetwordembeddingdimension      join step 1 2 use module argument model  sentencetransformer  modules  wordembeddingmodel  poolingmodel    modelencode    hi      work fine say  necessary  additional layer add  example  dense  bag word  convolutional  try add dense layer top model   m get error  sentencetransformer import sentencetransformer  model   step 1  use exist language model wordembeddingmodel  model  transformer   distilroberta  base     step 2  use pool function token embedding poolingmodel  model  pooling  wordembeddingmodelgetwordembeddingdimension      dense layer denselayer  torchnn  linear  poolingmodelgetsentenceembeddingdimension    128    join step 1 2 use module argument model  sentencetransformer  modules  wordembeddingmodel  poolingmodel  denselayer   run modelencode    hi   get  typeerror  linear    argument  input   position 1  must tensor  dict find error use bertmodelfrompretrained  model  transformer  suggest answer  pass argument returndict  false  not work  wordembeddingmodel  model  transformer   distilroberta  base   returndict  false  typeerror  transformer  init   get unexpected keyword argument  returndict  idea add dense layer correctly  accord documentation  replace line  denselayer  torchnn  linear  poolingmodelgetsentenceembeddingdimension    128  follow  denselayer  model  dense  poolingmodelgetsentenceembeddingdimension    128 ,Implementation Issues
How to calculate word and sentence embedding using Roberta?,"I'm trying to calculate word and sentence embeddings using Roberta, for word embeddings, I extract the last hidden state outputs[0] from the RobertaModel class, but I'm not sure if this is the correct way to calculate. As for sentence embeddings, I don't know how to calculate them, this is the code I have tried: from transformers import RobertaModel, RobertaTokenizer import torch model = RobertaModel.from_pretrained('roberta-base') tokenizer = RobertaTokenizer.from_pretrained('roberta-base') captions = [""example caption"", ""lorem ipsum"", ""this bird is yellow has red wings"", ""hi"", ""example""] encoded_captions = [tokenizer.encode(caption) for caption in captions] # Pad sequences to the same length with 0s max_len = max(len(seq) for seq in encoded_captions) padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions] # Convert to a PyTorch tensor with batch size 5 input_ids = torch.tensor(padded_captions) outputs = model(input_ids) word_embedding = outputs[0].contiguous() sentence_embedding = ????? How to calculate word and sentence embeddings using Roberta?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'transformer-model']",3,"Warning: This answer only shows ways to retrieve word and sentence embeddings from a technical perspective as requested by OP In the comments. The respective embeddings will not be useful from a performance perspective to for example calculate the similarity between two sentences or words. Compare this SO answer for further information. Word embeddings It is important to note, that RoBERTa was trained with a byte-level BPE tokenizer. This is a so-called subword tokenizer which means that one word of your input string can be split into several tokens. For example your second caption lorem ipsum : from transformers import RobertaModel, RobertaTokenizerFast import torch m = RobertaModel.from_pretrained('roberta-base') t = RobertaTokenizerFast.from_pretrained('roberta-base') captions = [""example caption"", ""lorem ipsum"", ""this bird is yellow has red wings"", ""hi"", ""example""] print(t(captions[1]).input_ids) Output: [0, 462, 43375, 1437, 7418, 783, 2] As you can see the two words were mapped to 5 tokens ( 0 and 2 are special tokens). That means to retrieve the actual word embeddings and not the token embeddings, you need to apply some kind of aggregation. A common approach is applying mean pooling (compare this SO answer ). Using the respective fast tokenizer of the model helps you here because it returns a BatchEncoding object that can be used to map the tokens back to the respective words: # no need to pad manually, the tokenizer can do that for you tokenized_captions = t(captions, return_tensors='pt', padding='longest') with torch.inference_mode(): model_inference_output = m(**tokenized_captions) contextualized_token_embeddings = model_inference_output.last_hidden_state #properly padded print(contextualized_token_embeddings.shape) def fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings): word_embeddings = {} # fetching word_ids, each id is a word in the original sentence word_ids = {i for i in tokenized_captions[idx].word_ids if i is not None} for word_id in word_ids: token_start, token_end = tokenized_captions[idx].word_to_tokens(word_id) word_start, word_end = tokenized_captions[idx].word_to_chars(word_id) word=sentence[word_start:word_end] word_embeddings[word] = contextualized_token_embeddings[idx][token_start:token_end].mean(dim=0) return word_embeddings result = [] for idx, sentence in enumerate(captions): word_embeddings = fetch_word_embeddings(idx, sentence, tokenized_captions, contextualized_token_embeddings) result.append({""sentence"": sentence, ""word_embeddings"":word_embeddings}) # contextualized word embedding of the word `ipsum` of the second caption print(result[1]['word_embeddings']['ipsum'].shape) Output: torch.Size([5, 9, 768]) torch.Size([768]) Sentence embeddings Sentence embeddings represent the whole sentence in a vector. There are different strategies to retrieve them. Commonly used are mean or cls -pooling, with mean-pooling delivering better results as shown in this paper section 6 . The ""only"" challenge from a technical perspective (compare warning preamble) is, that you want to exclude the padding tokens: # has 1 for none-padding-tokens and 0 for padding-tokens attention_mask = tokenized_captions.attention_mask.unsqueeze(-1) # mutiply the contextualized embeddings with the attention mask to # set the padding token weights to zero sum_embeddings = torch.sum(contextualized_token_embeddings * attention_mask,1) print(sum_embeddings.shape) num_none_padding_tokens = attention_mask.sum(1) print(num_none_padding_tokens) sentence_embeddings = sum_embeddings / num_none_padding_tokens print(sentence_embeddings.shape) Output: torch.Size([5, 768]) tensor([[4], [7], [9], [3], [3]]) torch.Size([5, 768]) You also wanted to know in the comments if you could use the pooler_output of roberta-base directly to retrieve the sentence embeddings. Yes, you can do that. The pooler_output is retrieved via a form of cls-pooling ( code ). Please note in addition to the warning preamble that the layers used for to generate the pooler_output are randomly initialized (i.e. untrained) for the roberta-base weights you load. That means they are even less meaningful! Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']",2024-01-12 10:05:01,2024-01-13 16:46:35,2468,https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta,"How to calculate word and sentence embedding using Roberta? I'm trying to calculate word and sentence embeddings using Roberta, for word embeddings, I extract the last hidden state outputs[0] from the RobertaModel class, but I'm not sure if this is the correct way to calculate. As for sentence embeddings, I don't know how to calculate them, this is the code I have tried: from transformers import RobertaModel, RobertaTokenizer import torch model = RobertaModel.from_pretrained('roberta-base') tokenizer = RobertaTokenizer.from_pretrained('roberta-base') captions = [""example caption"", ""lorem ipsum"", ""this bird is yellow has red wings"", ""hi"", ""example""] encoded_captions = [tokenizer.encode(caption) for caption in captions] # Pad sequences to the same length with 0s max_len = max(len(seq) for seq in encoded_captions) padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions] # Convert to a PyTorch tensor with batch size 5 input_ids = torch.tensor(padded_captions) outputs = model(input_ids) word_embedding = outputs[0].contiguous() sentence_embedding = ????? How to calculate word and sentence embeddings using Roberta?",calculate word sentence embed use roberta   m try calculate word sentence embedding use roberta  word embedding  extract last hide state output  0  robertamodel class   m sure correct way calculate  sentence embedding  not know calculate  code try  transformer import robertamodel  robertatokenizer import torch model  robertamodelfrompretraine   roberta  base   tokenizer  robertatokenizerfrompretraine   roberta  base   caption     example caption     lorem ipsum     bird yellow red wing     hi     example   encodedcaption   tokenizerencode  caption  caption caption   pad sequence length 0s maxlen  max  len  seq  seq encodedcaptions  paddedcaption   seq   0    maxlen  len  seq   seq encodedcaptions   convert pytorch tensor batch size 5 inputids  torchtensor  paddedcaption  output  model  inputids  wordembedde  output  0  contiguous   sentenceembedde       calculate word sentence embedding use roberta ,warning  answer show way retrieve word sentence embedding technical perspective request op comment  respective embedding useful performance perspective example calculate similarity two sentence word  compare answer information  word embedding important note  roberta train byte  level bpe tokenizer  so  call subword tokenizer mean one word input string split several token  example second caption lorem ipsum  transformer import robertamodel  robertatokenizerfast import torch  robertamodelfrompretraine   roberta  base    robertatokenizerfastfrompretraine   roberta  base   caption     example caption     lorem ipsum     bird yellow red wing     hi     example   print   caption  1   inputids  output   0  462  43375  1437  7418  783  2  see two word map 5 token  0 2 special token   mean retrieve actual word embedding token embedding  need apply kind aggregation  common approach apply mean pooling  compare answer   use respective fast tokenizer model help return batchencoding object use map token back respective word   need pad manually  tokenizer tokenizedcaption   caption  returntensorspt   paddinglong   torchinferencemode    modelinferenceoutput     tokenizedcaption  contextualizedtokenembedding  modelinferenceoutputlasthiddenstate  properly pad print  contextualizedtokenembeddingsshape  def fetchwordembedding  idx  sentence  tokenizedcaption  contextualizedtokenembeddings   wordembedding     fetch wordids  i d word original sentence wordid   tokenizedcaption  idx  wordids none  wordid wordids  tokenstart  tokenend  tokenizedcaptions  idx  wordtotokens  wordid  wordstart  wordend  tokenizedcaption  idx  wordtochar  wordid  word  sentence  wordstart  wordend  wordembedding  word   contextualizedtokenembeddings  idx   tokenstart  tokenend  mean  dim0  return wordembedding result    idx  sentence enumerate  caption   wordembedding  fetchwordembeddings  idx  sentence  tokenizedcaption  contextualizedtokenembeddings  resultappend     sentence   sentence    wordembedding   wordembeddings    contextualize word embed word  ipsum  second caption print  result  1    wordembedding     ipsum   shape  output  torch  size   5  9  768   torch  size   768   sentence embedding sentence embedding represent whole sentence vector  different strategy retrieve  commonly use mean cls pooling  mean  pool deliver well result show paper section 6     challenge technical perspective  compare warning preamble   want exclude padding token   1 none  padding  tokens 0 padding  token attentionmask  tokenizedcaptionsattentionmaskunsqueeze  1   mutiply contextualize embedding attention mask  set pad token weight zero sumembedding  torchsum  contextualizedtokenembedding  attentionmask1  print  sumembeddingsshape  numnonepaddingtoken  attentionmasksum  1  print  numnonepaddingtokens  sentenceembedding  sumembeddings  numnonepaddingtoken print  sentenceembeddingsshape  output  torch  size   5  768   tensor    4    7    9    3    3    torch  size   5  768   also want know comment could use pooleroutput roberta  base directly retrieve sentence embedding  yes   pooleroutput retrieve via form cls  pool  code   please note addition warn preamble layer use generate pooleroutput randomly initialize  ie  untraine  roberta  base weight load  mean even less meaningful  weight robertamodel initialize model checkpoint roberta  base newly initialize    robertapoolerdenseweight    robertapoolerdensebia  ,calculate word sentence embed use roberta   m try calculate word sentence embedding use roberta  word embedding  extract last hide state output  0  robertamodel class   m sure correct way calculate  sentence embedding  not know calculate  code try  transformer import robertamodel  robertatokenizer import torch model  robertamodelfrompretraine   roberta  base   tokenizer  robertatokenizerfrompretraine   roberta  base   caption     example caption     lorem ipsum     bird yellow red wing     hi     example   encodedcaption   tokenizerencode  caption  caption caption   pad sequence length 0s maxlen  max  len  seq  seq encodedcaptions  paddedcaption   seq   0    maxlen  len  seq   seq encodedcaptions   convert pytorch tensor batch size 5 inputids  torchtensor  paddedcaption  output  model  inputids  wordembedde  output  0  contiguous   sentenceembedde       calculate word sentence embedding use roberta  warning  answer show way retrieve word sentence embedding technical perspective request op comment  respective embedding useful performance perspective example calculate similarity two sentence word  compare answer information  word embedding important note  roberta train byte  level bpe tokenizer  so  call subword tokenizer mean one word input string split several token  example second caption lorem ipsum  transformer import robertamodel  robertatokenizerfast import torch  robertamodelfrompretraine   roberta  base    robertatokenizerfastfrompretraine   roberta  base   caption     example caption     lorem ipsum     bird yellow red wing     hi     example   print   caption  1   inputids  output   0  462  43375  1437  7418  783  2  see two word map 5 token  0 2 special token   mean retrieve actual word embedding token embedding  need apply kind aggregation  common approach apply mean pooling  compare answer   use respective fast tokenizer model help return batchencoding object use map token back respective word   need pad manually  tokenizer tokenizedcaption   caption  returntensorspt   paddinglong   torchinferencemode    modelinferenceoutput     tokenizedcaption  contextualizedtokenembedding  modelinferenceoutputlasthiddenstate  properly pad print  contextualizedtokenembeddingsshape  def fetchwordembedding  idx  sentence  tokenizedcaption  contextualizedtokenembeddings   wordembedding     fetch wordids  i d word original sentence wordid   tokenizedcaption  idx  wordids none  wordid wordids  tokenstart  tokenend  tokenizedcaptions  idx  wordtotokens  wordid  wordstart  wordend  tokenizedcaption  idx  wordtochar  wordid  word  sentence  wordstart  wordend  wordembedding  word   contextualizedtokenembeddings  idx   tokenstart  tokenend  mean  dim0  return wordembedding result    idx  sentence enumerate  caption   wordembedding  fetchwordembeddings  idx  sentence  tokenizedcaption  contextualizedtokenembeddings  resultappend     sentence   sentence    wordembedding   wordembeddings    contextualize word embed word  ipsum  second caption print  result  1    wordembedding     ipsum   shape  output  torch  size   5  9  768   torch  size   768   sentence embedding sentence embedding represent whole sentence vector  different strategy retrieve  commonly use mean cls pooling  mean  pool deliver well result show paper section 6     challenge technical perspective  compare warning preamble   want exclude padding token   1 none  padding  tokens 0 padding  token attentionmask  tokenizedcaptionsattentionmaskunsqueeze  1   mutiply contextualize embedding attention mask  set pad token weight zero sumembedding  torchsum  contextualizedtokenembedding  attentionmask1  print  sumembeddingsshape  numnonepaddingtoken  attentionmasksum  1  print  numnonepaddingtokens  sentenceembedding  sumembeddings  numnonepaddingtoken print  sentenceembeddingsshape  output  torch  size   5  768   tensor    4    7    9    3    3    torch  size   5  768   also want know comment could use pooleroutput roberta  base directly retrieve sentence embedding  yes   pooleroutput retrieve via form cls  pool  code   please note addition warn preamble layer use generate pooleroutput randomly initialize  ie  untraine  roberta  base weight load  mean even less meaningful  weight robertamodel initialize model checkpoint roberta  base newly initialize    robertapoolerdenseweight    robertapoolerdensebia  ,Basic Understanding
How to Find Positional embeddings from BARTTokenizer?,"The objective is to add token embeddings (customized- obtained using different model) and the positional Embeddings. Is there a Way I can find out positonal embedding along with the token embeddings for an article(length 500-1000 words) using BART model. tokenized_sequence = tokenizer(sentence, padding='max_length', truncation=True, max_length=512, return_tensors=""pt"") the output is input_ids and attention_mask but not parameter to return position_ids like in BERT model. bert.embeddings.position_embeddings('YOUR_POSITIONS_IDS') Or the only way to obtain Positional Embedding is using sinusoidal positional encoding?","['pytorch', 'nlp', 'huggingface-transformers', 'summarization', 'bart']",1,"The tokenizer is not responsible for the embeddings. It only generates the ids to be fed into the embedding layer. Barts embeddings are learned, i.e. the embedding come from their own embedding layer. You can retrieve both types of embeddings like this. Here bart is a BartModel . The encoding is (roughly) done like this: embed_pos = bart.encoder.embed_positions(input_ids) inputs_embeds = bart.encoder.embed_tokens(input_ids) hidden_states = inputs_embeds + embed_pos Full working code: from transformers import BartForConditionalGeneration, BartTokenizer bart = BartForConditionalGeneration.from_pretrained(""facebook/bart-base"", forced_bos_token_id=0) tok = BartTokenizer.from_pretrained(""facebook/bart-base"") example_english_phrase = ""UN Chief Says There Is No <mask> in Syria"" input_ids = tok(example_english_phrase, return_tensors=""pt"").input_ids embed_pos = bart.model.encoder.embed_positions(input_ids) * bart.model.encoder.embed_scale # by default the scale is 1.0 inputs_embeds = bart.model.encoder.embed_tokens(input_ids) hidden_states = inputs_embeds + embed_pos Note that embed_pos is invariant to the actual token ids. Only their position matters. ""New"" embeddings are added if the input grows larger without changing the embeddings of the earlier positions: These cases yield the same embeddings: embed_positions([0, 1]) == embed_positions([123, 241]) == embed_positions([444, 3453, 9344, 3453])[:2]",2024-01-11 13:13:15,2024-01-11 13:51:58,549,https://stackoverflow.com/questions/77800331/how-to-find-positional-embeddings-from-barttokenizer,"How to Find Positional embeddings from BARTTokenizer? The objective is to add token embeddings (customized- obtained using different model) and the positional Embeddings. Is there a Way I can find out positonal embedding along with the token embeddings for an article(length 500-1000 words) using BART model. tokenized_sequence = tokenizer(sentence, padding='max_length', truncation=True, max_length=512, return_tensors=""pt"") the output is input_ids and attention_mask but not parameter to return position_ids like in BERT model. bert.embeddings.position_embeddings('YOUR_POSITIONS_IDS') Or the only way to obtain Positional Embedding is using sinusoidal positional encoding?",find positional embedding barttokenizer  objective add token embedding  customized obtain use different model  positional embedding  way find positonal embed along token embedding article  length 500  1000 word  use bart model  tokenizedsequence  tokenizer  sentence  paddingmaxlength   truncation  true  maxlength512  returntensors  pt   output inputid attentionmask parameter return positionid like bert model  bertembeddingspositionembedding   yourpositionsids   way obtain positional embed use sinusoidal positional encoding ,tokenizer responsible embedding  generate ids feed embed layer  bart embedding learn  ie  embed come embed layer  retrieve type embedding like  bart bartmodel  encode  roughly  do like  embedpos  bartencoderembedposition  inputids  inputsembed  bartencoderembedtoken  inputids  hiddenstate  inputsembeds  embedpos full work code  transformer import bartforconditionalgeneration  barttokenizer bart  bartforconditionalgenerationfrompretrained    facebook  bart  base   forcedbostokenid0  tok  barttokenizerfrompretraine    facebook  bart  base   exampleenglishphrase    un chief say  mask  syria  inputids  tok  exampleenglishphrase  returntensors  pt   inputids embedpos  bartmodelencoderembedposition  inputids   bartmodelencoderembedscale  default scale 10 inputsembed  bartmodelencoderembedtoken  inputids  hiddenstate  inputsembeds  embedpos note embedpos invariant actual token id  position matter    new  embedding add input grow large without change embedding early position  case yield embedding  embedposition   0  1     embedposition   123  241     embedposition   444  3453  9344  3453     2 ,find positional embedding barttokenizer  objective add token embedding  customized obtain use different model  positional embedding  way find positonal embed along token embedding article  length 500  1000 word  use bart model  tokenizedsequence  tokenizer  sentence  paddingmaxlength   truncation  true  maxlength512  returntensors  pt   output inputid attentionmask parameter return positionid like bert model  bertembeddingspositionembedding   yourpositionsids   way obtain positional embed use sinusoidal positional encoding  tokenizer responsible embedding  generate ids feed embed layer  bart embedding learn  ie  embed come embed layer  retrieve type embedding like  bart bartmodel  encode  roughly  do like  embedpos  bartencoderembedposition  inputids  inputsembed  bartencoderembedtoken  inputids  hiddenstate  inputsembeds  embedpos full work code  transformer import bartforconditionalgeneration  barttokenizer bart  bartforconditionalgenerationfrompretrained    facebook  bart  base   forcedbostokenid0  tok  barttokenizerfrompretraine    facebook  bart  base   exampleenglishphrase    un chief say  mask  syria  inputids  tok  exampleenglishphrase  returntensors  pt   inputids embedpos  bartmodelencoderembedposition  inputids   bartmodelencoderembedscale  default scale 10 inputsembed  bartmodelencoderembedtoken  inputids  hiddenstate  inputsembeds  embedpos note embedpos invariant actual token id  position matter    new  embedding add input grow large without change embedding early position  case yield embedding  embedposition   0  1     embedposition   123  241     embedposition   444  3453  9344  3453     2 ,Task-Specific Queries
How to find similar sounding words?,"I'm writing a specialized (in food realm) multi-lingual search engine. I use python and nltk libraries. I have quite a big database of recipes for all cultures I want to support. I'm asking if and how it is possible to be able to find in my indexed words corpus a wrong spelled word... For example, in Italian, to look for ""couscous"" word, many users would say/write ""cus cus"", or ""cuscus""... In synthesis, this is an example of how I tokenize my index of lexemes for search: import re import nltk import string corpus = 'italian' stemmer = nltk.stem.snowball.ItalianStemmer() stopWords = nltk.corpus.stopwords.words(corpus) # tokenize the sentence(s) wordTokenizedList = nltk.tokenize.word_tokenize(text) # remove punctuation and everything lower case wordTokenizedListNoPunct = [ word.lower() for word in wordTokenizedList if word not in string.punctuation ] # remove stop words wordTokenizedListNoPunctNoStopWords = [ word for word in wordTokenizedListNoPunct if word not in stopWords ] # snowball stemmer wordTokenizedListNoPunctNoStopWordsStems = [ stemmer.stem(i) for i in wordTokenizedListNoPunctNoStopWords ] return wordTokenizedListNoPunctNoStopWordsStems Should I prepare my index someway differently to reach my goal? Any additional remark about a more complete flow in the text analysis for tokenization should be welcome, of course... :-)","['python', 'nlp', 'nltk']",2,"Well, I'd just use a database with elastic search capabilities. PROS: They already solve these kind of issues WAY faster Safer And well, a long etcetera as you can imagine. Is really easy to connect Python with SQLite and the FTS5 (Full Text Search) module works great! I'll highly recommend to watch the following video for you to get an idea if this will suit your solution :) video",2024-01-09 06:57:38,2024-01-09 07:18:04,211,https://stackoverflow.com/questions/77784846/how-to-find-similar-sounding-words,"How to find similar sounding words? I'm writing a specialized (in food realm) multi-lingual search engine. I use python and nltk libraries. I have quite a big database of recipes for all cultures I want to support. I'm asking if and how it is possible to be able to find in my indexed words corpus a wrong spelled word... For example, in Italian, to look for ""couscous"" word, many users would say/write ""cus cus"", or ""cuscus""... In synthesis, this is an example of how I tokenize my index of lexemes for search: import re import nltk import string corpus = 'italian' stemmer = nltk.stem.snowball.ItalianStemmer() stopWords = nltk.corpus.stopwords.words(corpus) # tokenize the sentence(s) wordTokenizedList = nltk.tokenize.word_tokenize(text) # remove punctuation and everything lower case wordTokenizedListNoPunct = [ word.lower() for word in wordTokenizedList if word not in string.punctuation ] # remove stop words wordTokenizedListNoPunctNoStopWords = [ word for word in wordTokenizedListNoPunct if word not in stopWords ] # snowball stemmer wordTokenizedListNoPunctNoStopWordsStems = [ stemmer.stem(i) for i in wordTokenizedListNoPunctNoStopWords ] return wordTokenizedListNoPunctNoStopWordsStems Should I prepare my index someway differently to reach my goal? Any additional remark about a more complete flow in the text analysis for tokenization should be welcome, of course... :-)",find similar sounding word   m write specialized  food realm  multi  lingual search engine  use python nltk library  quite big database recipe culture want support   m ask possible able find index word corpus wrong spell word  example  italian  look   couscous  word  many user would say  write   cus cus     cuscus   synthesis  example tokenize index lexeme search  import import nltk import string corpus   italian  stemmer  nltkstemsnowball  italianstemmer   stopword  nltkcorpusstopwordswords  corpus   tokenize sentence   wordtokenizedlist  nltktokenizewordtokenize  text   remove punctuation everything low case wordtokenizedlistnopunct   wordlower   word wordtokenizedlist word stringpunctuation   remove stop word wordtokenizedlistnopunctnostopword   word word wordtokenizedlistnopunct word stopwords   snowball stemmer wordtokenizedlistnopunctnostopwordsstem   stemmerstem   wordtokenizedlistnopunctnostopword  return wordtokenizedlistnopunctnostopwordsstems prepare index someway differently reach goal  additional remark complete flow text analysis tokenization welcome  course    ,well  would use database elastic search capability  pros  already solve kind issue way fast safer well  long etcetera imagine  really easy connect python sqlite fts5  full text search  module work great  will highly recommend watch follow video get idea suit solution   video,find similar sounding word   m write specialized  food realm  multi  lingual search engine  use python nltk library  quite big database recipe culture want support   m ask possible able find index word corpus wrong spell word  example  italian  look   couscous  word  many user would say  write   cus cus     cuscus   synthesis  example tokenize index lexeme search  import import nltk import string corpus   italian  stemmer  nltkstemsnowball  italianstemmer   stopword  nltkcorpusstopwordswords  corpus   tokenize sentence   wordtokenizedlist  nltktokenizewordtokenize  text   remove punctuation everything low case wordtokenizedlistnopunct   wordlower   word wordtokenizedlist word stringpunctuation   remove stop word wordtokenizedlistnopunctnostopword   word word wordtokenizedlistnopunct word stopwords   snowball stemmer wordtokenizedlistnopunctnostopwordsstem   stemmerstem   wordtokenizedlistnopunctnostopword  return wordtokenizedlistnopunctnostopwordsstems prepare index someway differently reach goal  additional remark complete flow text analysis tokenization welcome  course     well  would use database elastic search capability  pros  already solve kind issue way fast safer well  long etcetera imagine  really easy connect python sqlite fts5  full text search  module work great  will highly recommend watch follow video get idea suit solution   video,Library/Tool-Based Queries
How to save Keras TextVectorization layer configuration with custom standardization function into a pickle file and reload it?,"I have a Keras TextVectorization layer which uses a custom standardization function. def custom_standardization(input_string, preserve=['[', ']'], add=['¿']): strip_chars = string.punctuation for item in add: strip_chars += item for item in preserve: strip_chars = strip_chars.replace(item, '') lowercase = tf.strings.lower(input_string) output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '') return output target_vectorization = keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length + 1, standardize=custom_standardization) target_vectorization.adapt(train_spanish_texts) I want to save the adapted configuration for an inference model to make use of. One way, as described here , is to save the weights and config separately as a pickle file and reload them. However, target_vectorization.get_config() returns {'name': 'text_vectorization_5', 'trainable': True, ... 'standardize': <function __main__.custom_standardization(input_string, preserve=['[', ']'], add=['¿'])>, ... 'vocabulary_size': 15000} which is being saved into the pickle file. Trying to load this config using keras.layers.TextVectorization.from_config(pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb'))['config']) results in TypeError: Could not parse config: <function custom_standardization at 0x2a1973a60> , because the file does not have any information about this custom standardization function. What is a good way to save the TextVectorization weights and configuration for an inference model to make use of, in this scenario?","['python', 'tensorflow', 'keras', 'nlp']",2,"The solution here was to define a wrapper around the TextVectorization object and use the custom standardizer as a method. Moreover, we needed to exclude callable objects while saving configuration to the pickle file. Here's the fixed code: @keras.utils.register_keras_serializable(package='custom_layers', name='TextVectorizer') class TextVectorizer(layers.Layer): '''English - Spanish Text Vectorizer''' def __init__(self, max_tokens=None, output_mode='int', output_sequence_length=None, standardize='lower_and_strip_punctuation', vocabulary=None, config=None): super().__init__() if config: self.vectorization = layers.TextVectorization.from_config(config) else: self.max_tokens = max_tokens self.output_mode = output_mode self.output_sequence_length = output_sequence_length self.vocabulary = vocabulary if standardize != 'lower_and_strip_punctuation': self.vectorization = layers.TextVectorization(max_tokens=self.max_tokens, output_mode=self.output_mode, output_sequence_length=self.output_sequence_length, vocabulary=self.vocabulary, standardize=self.standardize) else: self.vectorization = layers.TextVectorization(max_tokens=self.max_tokens, output_mode=self.output_mode, output_sequence_length=self.output_sequence_length, vocabulary=self.vocabulary) def standardize(self, input_string, preserve=['[', ']'], add=['¿']) -> str: strip_chars = string.punctuation for item in add: strip_chars += item for item in preserve: strip_chars = strip_chars.replace(item, '') lowercase = tf.strings.lower(input_string) output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '') return output def __call__(self, *args, **kwargs): return self.vectorization.__call__(*args, **kwargs) def get_config(self): return {key: value if not callable(value) else None for key, value in self.vectorization.get_config().items()} def from_config(config): return TextVectorizer(config=config) def set_weights(self, weights): self.vectorization.set_weights(weights) def adapt(self, dataset): self.vectorization.adapt(dataset) def get_vocabulary(self): return self.vectorization.get_vocabulary() To adapt and save weights [Training Phase]: vocab_size = 15000 sequence_length = 20 source_vectorization = TextVectorizer(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length) target_vectorization = TextVectorizer(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length + 1, standardize='spanish') train_english_texts = [pair[0] for pair in train_pairs] train_spanish_texts = [pair[1] for pair in train_pairs] source_vectorization.adapt(train_english_texts) target_vectorization.adapt(train_spanish_texts) pickle.dump({'config': source_vectorization.get_config(), 'weights': source_vectorization.get_weights()}, open('ckpts/english_vectorization.pkl', 'wb')) pickle.dump({'config': target_vectorization.get_config(), 'weights': target_vectorization.get_weights()}, open('ckpts/spanish_vectorization.pkl', 'wb')) To load and use them [Inference Phase]: vectorization_data = pickle.load(open('ckpts/english_vectorization.pkl', 'rb')) source_vectorization = TextVectorizer.from_config(vectorization_data['config']) source_vectorization.set_weights(vectorization_data['weights']) vectorization_data = pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb')) target_vectorization = TextVectorizer.from_config(vectorization_data['config']) target_vectorization.set_weights(vectorization_data['weights'])",2024-01-07 19:21:49,2024-01-09 17:16:53,464,https://stackoverflow.com/questions/77774499/how-to-save-keras-textvectorization-layer-configuration-with-custom-standardizat,"How to save Keras TextVectorization layer configuration with custom standardization function into a pickle file and reload it? I have a Keras TextVectorization layer which uses a custom standardization function. def custom_standardization(input_string, preserve=['[', ']'], add=['¿']): strip_chars = string.punctuation for item in add: strip_chars += item for item in preserve: strip_chars = strip_chars.replace(item, '') lowercase = tf.strings.lower(input_string) output = tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '') return output target_vectorization = keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length + 1, standardize=custom_standardization) target_vectorization.adapt(train_spanish_texts) I want to save the adapted configuration for an inference model to make use of. One way, as described here , is to save the weights and config separately as a pickle file and reload them. However, target_vectorization.get_config() returns {'name': 'text_vectorization_5', 'trainable': True, ... 'standardize': <function __main__.custom_standardization(input_string, preserve=['[', ']'], add=['¿'])>, ... 'vocabulary_size': 15000} which is being saved into the pickle file. Trying to load this config using keras.layers.TextVectorization.from_config(pickle.load(open('ckpts/spanish_vectorization.pkl', 'rb'))['config']) results in TypeError: Could not parse config: <function custom_standardization at 0x2a1973a60> , because the file does not have any information about this custom standardization function. What is a good way to save the TextVectorization weights and configuration for an inference model to make use of, in this scenario?",save keras textvectorization layer configuration custom standardization function pickle file reload  keras textvectorization layer use custom standardization function  def customstandardization  inputstring  preserve           add        stripchar  stringpunctuation item add  stripchar   item item preserve  stripchar  stripcharsreplace  item     lowercase  tfstringslower  inputstre  output  tfstringsregexreplace  lowercase  f    reescape  stripchar         return output targetvectorization  keraslayer  textvectorization  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  1  standardize  customstandardization  targetvectorizationadapt  trainspanishtexts  want save adapt configuration inference model make use  one way  describe  save weight config separately pickle file reload  however  targetvectorizationgetconfig   return   name    textvectorization5    trainable   true    standardize    function   maincustomstandardization  inputstring  preserve           add           vocabularysize   15000  save pickle file  try load config use keraslayer  textvectorizationfromconfig  pickleload  open   ckpt  spanishvectorizationpkl    rb      config    result typeerror  could parse config   function customstandardization 0x2a1973a60   file information custom standardization function  good way save textvectorization weight configuration inference model make use  scenario ,solution define wrapper around textvectorization object use custom standardizer method  moreover  needed exclude callable object save configuration pickle file  s fix code   kerasutilsregisterkerasserializable  packagecustomlayer   nametextvectorizer   class textvectorizer  layer  layer      english  spanish text vectorizer   def   init    self  maxtoken  none  outputmodeint   outputsequencelength  none  standardizelowerandstrippunctuation   vocabulary  none  config  none   super   init     config  selfvectorization  layer  textvectorizationfromconfig  config  else  selfmaxtoken  maxtokens selfoutputmode  outputmode selfoutputsequencelength  outputsequencelength selfvocabulary  vocabulary standardize    lowerandstrippunctuation   selfvectorization  layer  textvectorization  maxtoken  selfmaxtoken  outputmode  selfoutputmode  outputsequencelength  selfoutputsequencelength  vocabulary  selfvocabulary  standardize  selfstandardize  else  selfvectorization  layer  textvectorization  maxtoken  selfmaxtoken  outputmode  selfoutputmode  outputsequencelength  selfoutputsequencelength  vocabulary  selfvocabulary  def standardize  self  inputstring  preserve           add         str  stripchar  stringpunctuation item add  stripchar   item item preserve  stripchar  stripcharsreplace  item     lowercase  tfstringslower  inputstre  output  tfstringsregexreplace  lowercase  f    reescape  stripchar         return output def   call    self   args    kwargs   return selfvectorizationcall     args    kwargs  def getconfig  self   return  key  value callable  value  else none key  value selfvectorizationgetconfig   items    def fromconfig  config   return textvectorizer  config  config  def setweight  self  weight   selfvectorizationsetweight  weight  def adapt  self  dataset   selfvectorizationadapt  dataset  def getvocabulary  self   return selfvectorizationgetvocabulary   adapt save weight  training phase   vocabsize  15000 sequencelength  20 sourcevectorization  textvectorizer  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  targetvectorization  textvectorizer  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  1  standardizespanish   trainenglishtexts   pair  0  pair trainpair  trainspanishtexts   pair  1  pair trainpair  sourcevectorizationadapt  trainenglishtexts  targetvectorizationadapt  trainspanishtexts  pickledump    config   sourcevectorizationgetconfig     weight   sourcevectorizationgetweight     open   ckpt  englishvectorizationpkl    wb    pickledump    config   targetvectorizationgetconfig     weight   targetvectorizationgetweight     open   ckpt  spanishvectorizationpkl    wb    load use  inference phase   vectorizationdata  pickleload  open   ckpt  englishvectorizationpkl    rb    sourcevectorization  textvectorizerfromconfig  vectorizationdata   config    sourcevectorizationsetweight  vectorizationdata   weight    vectorizationdata  pickleload  open   ckpt  spanishvectorizationpkl    rb    targetvectorization  textvectorizerfromconfig  vectorizationdata   config    targetvectorizationsetweight  vectorizationdata   weight   ,save keras textvectorization layer configuration custom standardization function pickle file reload  keras textvectorization layer use custom standardization function  def customstandardization  inputstring  preserve           add        stripchar  stringpunctuation item add  stripchar   item item preserve  stripchar  stripcharsreplace  item     lowercase  tfstringslower  inputstre  output  tfstringsregexreplace  lowercase  f    reescape  stripchar         return output targetvectorization  keraslayer  textvectorization  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  1  standardize  customstandardization  targetvectorizationadapt  trainspanishtexts  want save adapt configuration inference model make use  one way  describe  save weight config separately pickle file reload  however  targetvectorizationgetconfig   return   name    textvectorization5    trainable   true    standardize    function   maincustomstandardization  inputstring  preserve           add           vocabularysize   15000  save pickle file  try load config use keraslayer  textvectorizationfromconfig  pickleload  open   ckpt  spanishvectorizationpkl    rb      config    result typeerror  could parse config   function customstandardization 0x2a1973a60   file information custom standardization function  good way save textvectorization weight configuration inference model make use  scenario  solution define wrapper around textvectorization object use custom standardizer method  moreover  needed exclude callable object save configuration pickle file  s fix code   kerasutilsregisterkerasserializable  packagecustomlayer   nametextvectorizer   class textvectorizer  layer  layer      english  spanish text vectorizer   def   init    self  maxtoken  none  outputmodeint   outputsequencelength  none  standardizelowerandstrippunctuation   vocabulary  none  config  none   super   init     config  selfvectorization  layer  textvectorizationfromconfig  config  else  selfmaxtoken  maxtokens selfoutputmode  outputmode selfoutputsequencelength  outputsequencelength selfvocabulary  vocabulary standardize    lowerandstrippunctuation   selfvectorization  layer  textvectorization  maxtoken  selfmaxtoken  outputmode  selfoutputmode  outputsequencelength  selfoutputsequencelength  vocabulary  selfvocabulary  standardize  selfstandardize  else  selfvectorization  layer  textvectorization  maxtoken  selfmaxtoken  outputmode  selfoutputmode  outputsequencelength  selfoutputsequencelength  vocabulary  selfvocabulary  def standardize  self  inputstring  preserve           add         str  stripchar  stringpunctuation item add  stripchar   item item preserve  stripchar  stripcharsreplace  item     lowercase  tfstringslower  inputstre  output  tfstringsregexreplace  lowercase  f    reescape  stripchar         return output def   call    self   args    kwargs   return selfvectorizationcall     args    kwargs  def getconfig  self   return  key  value callable  value  else none key  value selfvectorizationgetconfig   items    def fromconfig  config   return textvectorizer  config  config  def setweight  self  weight   selfvectorizationsetweight  weight  def adapt  self  dataset   selfvectorizationadapt  dataset  def getvocabulary  self   return selfvectorizationgetvocabulary   adapt save weight  training phase   vocabsize  15000 sequencelength  20 sourcevectorization  textvectorizer  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  targetvectorization  textvectorizer  maxtokens  vocabsize  outputmodeint   outputsequencelength  sequencelength  1  standardizespanish   trainenglishtexts   pair  0  pair trainpair  trainspanishtexts   pair  1  pair trainpair  sourcevectorizationadapt  trainenglishtexts  targetvectorizationadapt  trainspanishtexts  pickledump    config   sourcevectorizationgetconfig     weight   sourcevectorizationgetweight     open   ckpt  englishvectorizationpkl    wb    pickledump    config   targetvectorizationgetconfig     weight   targetvectorizationgetweight     open   ckpt  spanishvectorizationpkl    wb    load use  inference phase   vectorizationdata  pickleload  open   ckpt  englishvectorizationpkl    rb    sourcevectorization  textvectorizerfromconfig  vectorizationdata   config    sourcevectorizationsetweight  vectorizationdata   weight    vectorizationdata  pickleload  open   ckpt  spanishvectorizationpkl    rb    targetvectorization  textvectorizerfromconfig  vectorizationdata   config    targetvectorizationsetweight  vectorizationdata   weight   ,Implementation Issues
How to calculate word and sentence embedding using GPT-2?,"I'm working on a program that calculates word and sentence embeddings using GPT-2, specifically the GPT2Model class. For word embedding, I extract the last hidden state outputs[0] after forwarding the input_ids , that has a shape of batch size x seq len , to the GPT2Model class. As for sentence embedding, I extract the hidden state of the word at the end of sequence. This is the code I have tried: from transformers import GPT2Tokenizer, GPT2Model import torch tokenizer = GPT2Tokenizer.from_pretrained('gpt2') model = GPT2Model.from_pretrained('gpt2') captions = [""example caption"", ""example bird"", ""the bird is yellow has red wings"", ""hi"", ""very good""] encoded_captions = [tokenizer.encode(caption) for caption in captions] # Pad sequences to the same length with 0s max_len = max(len(seq) for seq in encoded_captions) padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions] # Convert to a PyTorch tensor with batch size 5 input_ids = torch.tensor(padded_captions) outputs = model(input_ids) word_embedding = outputs[0].contiguous() sentence_embedding = word_embedding[ :, -1, : ].contiguous() I'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?","['python', 'machine-learning', 'nlp', 'huggingface-transformers', 'transformer-model']",1,"Here is your modified code to compute sentence and word embeddings: from transformers import GPT2Tokenizer, GPT2Model import torch tokenizer = GPT2Tokenizer.from_pretrained('gpt2') tokenizer.pad_token = tokenizer.eos_token model = GPT2Model.from_pretrained('gpt2') captions = [ ""example caption"", ""example bird"", ""the bird is yellow has red wings"", ""hi"", ""very good"" ] # Tokenize and pad sequences encoded_captions = tokenizer( captions, return_tensors='pt', padding=True, truncation=True ) input_ids = encoded_captions['input_ids'] # Forward pass to get embeddings with torch.no_grad(): outputs = model(input_ids) # Extract embeddings word_embeddings = outputs.last_hidden_state # Mask to ignore padding tokens masked_word_embeddings = word_embeddings * encoded_captions.attention_mask.unsqueeze(-1).float() # Sum pooling considering only non-padding tokens sentence_embeddings = masked_word_embeddings.sum(dim=1) # Normalize by the count of non-padding tokens sentence_embeddings /= encoded_captions.attention_mask.sum(dim=1, keepdim=True).float() Some relevant facts: As you said, word embeddings are the last hidden output. If you print the out put you see 5 vectors (number of sentences) of length 7 (maximum number of tokens in the list of sentences) and shape 768 (model dimension). word_embeddings.shape >> torch.Size([5, 7, 768]) It means that some sentences have embeddings for non existent tokens, so we need to mask the output to consider only existent tokens Mask consists on multiplying by zero (or whatever special value but zero is the more accepted and useful, as it nulls values) on non existent token places of the word vector. The attention mask is crucial for handling variable-length sequences and ensuring that padding tokens do not contribute to the embeddings. print(masked_word_embeddings) >> tensor([[[-0.2835, -0.0469, -0.5029, ..., -0.0525, -0.0089, -0.1395], [-0.2636, -0.1355, -0.4277, ..., -0.3552, 0.0437, -0.2479], [ 0.0000, -0.0000, 0.0000, ..., 0.0000, -0.0000, -0.0000], ..., [ 0.0000, -0.0000, 0.0000, ..., 0.0000, -0.0000, -0.0000], [ 0.0000, -0.0000, 0.0000, ..., 0.0000, -0.0000, -0.0000], [ 0.0000, -0.0000, 0.0000, ..., 0.0000, -0.0000, -0.0000]], ... Usually, sentence embeddings are computed as the sum, mean or max of the masked word embeddings. It depends on your use case. Mean is more suitable to variable length: sentence_embeddings = masked_word_embeddings.mean(dim=1) Sum is intended to force importance on relevant parts: sentence_embeddings = masked_word_embeddings.max(dim=1) It exist a lot of techniques and it depends on how embeddings perform for your task. I would choose a method that maximices the cosine similarity between vectors I consider similar for my task. Ex: If the sum gets more similarity than mean, it may be more suitable. Additionally I suggest you to normalize values by the number of tokens in the sentence. So that, with that normalization, larger sentences tend to have lower vector values. It is to embed information on the number of tokens in the sentence. It prevents to get high similarity scores between a sentence of 4 tokens with a whole book, that its meaningless.",2024-01-02 21:55:52,2024-01-03 11:41:45,1681,https://stackoverflow.com/questions/77748737/how-to-calculate-word-and-sentence-embedding-using-gpt-2,"How to calculate word and sentence embedding using GPT-2? I'm working on a program that calculates word and sentence embeddings using GPT-2, specifically the GPT2Model class. For word embedding, I extract the last hidden state outputs[0] after forwarding the input_ids , that has a shape of batch size x seq len , to the GPT2Model class. As for sentence embedding, I extract the hidden state of the word at the end of sequence. This is the code I have tried: from transformers import GPT2Tokenizer, GPT2Model import torch tokenizer = GPT2Tokenizer.from_pretrained('gpt2') model = GPT2Model.from_pretrained('gpt2') captions = [""example caption"", ""example bird"", ""the bird is yellow has red wings"", ""hi"", ""very good""] encoded_captions = [tokenizer.encode(caption) for caption in captions] # Pad sequences to the same length with 0s max_len = max(len(seq) for seq in encoded_captions) padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions] # Convert to a PyTorch tensor with batch size 5 input_ids = torch.tensor(padded_captions) outputs = model(input_ids) word_embedding = outputs[0].contiguous() sentence_embedding = word_embedding[ :, -1, : ].contiguous() I'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?",calculate word sentence embed use gpt2   m work program calculate word sentence embedding use gpt2  specifically gpt2model class  word embed  extract last hide state output  0  forward inputids  shape batch size x seq len  gpt2model class  sentence embed  extract hide state word end sequence  code try  transformer import gpt2tokenizer  gpt2model import torch tokenizer  gpt2tokenizerfrompretraine   gpt2   model  gpt2modelfrompretraine   gpt2   caption     example caption     example bird     bird yellow red wing     hi     good   encodedcaption   tokenizerencode  caption  caption caption   pad sequence length 0s maxlen  max  len  seq  seq encodedcaptions  paddedcaption   seq   0    maxlen  len  seq   seq encodedcaptions   convert pytorch tensor batch size 5 inputids  torchtensor  paddedcaption  output  model  inputids  wordembedde  output  0  contiguous   sentenceembedding  wordembedding    1    contiguous    m sure calculation word sentence embed correct  anyone help confirm ,modify code compute sentence word embedding  transformer import gpt2tokenizer  gpt2model import torch tokenizer  gpt2tokenizerfrompretraine   gpt2   tokenizerpadtoken  tokenizereostoken model  gpt2modelfrompretraine   gpt2   caption     example caption     example bird     bird yellow red wing     hi     good    tokenize pad sequence encodedcaption  tokenizer  caption  returntensorspt   pad  true  truncation  true  inputids  encodedcaption   inputids    forward pass get embedding torchnograd    output  model  inputids   extract embedding wordembedding  outputslasthiddenstate  mask ignore pad token maskedwordembeddings  wordembeddings  encodedcaptionsattentionmaskunsqueeze  1  float    sum pooling consider non  padding token sentenceembedding  maskedwordembeddingssum  dim1   normalize count non  padding token sentenceembeddings  encodedcaptionsattentionmasksum  dim1  keepdim  true  float   relevant fact  say  word embedding last hide output  print put see 5 vector  number sentence  length 7  maximum number token list sentence  shape 768  model dimension   wordembeddingsshape   torch  size   5  7  768   mean sentence embedding non existent token  need mask output consider existent token mask consist multiplying zero  whatever special value zero accept useful  null value  non existent token place word vector  attention mask crucial handling variable  length sequence ensure pad token contribute embedding  print  maskedwordembeddings    tensor     02835  00469  05029    00525  00089  01395    02636  01355  04277    03552  00437  02479    00000  00000  00000    00000  00000  00000      00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000     usually  sentence embedding compute sum  mean max mask word embedding  depend use case  mean suitable variable length  sentenceembedding  maskedwordembeddingsmean  dim1  sum intend force importance relevant part  sentenceembedding  maskedwordembeddingsmax  dim1  exist lot technique depend embedding perform task  would choose method maximice cosine similarity vector consider similar task  ex  sum get similarity mean  may suitable  additionally suggest normalize value number token sentence   normalization  large sentence tend low vector value  embe information number tokens sentence  prevent get high similarity score sentence 4 token whole book  meaningless ,calculate word sentence embed use gpt2   m work program calculate word sentence embedding use gpt2  specifically gpt2model class  word embed  extract last hide state output  0  forward inputids  shape batch size x seq len  gpt2model class  sentence embed  extract hide state word end sequence  code try  transformer import gpt2tokenizer  gpt2model import torch tokenizer  gpt2tokenizerfrompretraine   gpt2   model  gpt2modelfrompretraine   gpt2   caption     example caption     example bird     bird yellow red wing     hi     good   encodedcaption   tokenizerencode  caption  caption caption   pad sequence length 0s maxlen  max  len  seq  seq encodedcaptions  paddedcaption   seq   0    maxlen  len  seq   seq encodedcaptions   convert pytorch tensor batch size 5 inputids  torchtensor  paddedcaption  output  model  inputids  wordembedde  output  0  contiguous   sentenceembedding  wordembedding    1    contiguous    m sure calculation word sentence embed correct  anyone help confirm  modify code compute sentence word embedding  transformer import gpt2tokenizer  gpt2model import torch tokenizer  gpt2tokenizerfrompretraine   gpt2   tokenizerpadtoken  tokenizereostoken model  gpt2modelfrompretraine   gpt2   caption     example caption     example bird     bird yellow red wing     hi     good    tokenize pad sequence encodedcaption  tokenizer  caption  returntensorspt   pad  true  truncation  true  inputids  encodedcaption   inputids    forward pass get embedding torchnograd    output  model  inputids   extract embedding wordembedding  outputslasthiddenstate  mask ignore pad token maskedwordembeddings  wordembeddings  encodedcaptionsattentionmaskunsqueeze  1  float    sum pooling consider non  padding token sentenceembedding  maskedwordembeddingssum  dim1   normalize count non  padding token sentenceembeddings  encodedcaptionsattentionmasksum  dim1  keepdim  true  float   relevant fact  say  word embedding last hide output  print put see 5 vector  number sentence  length 7  maximum number token list sentence  shape 768  model dimension   wordembeddingsshape   torch  size   5  7  768   mean sentence embedding non existent token  need mask output consider existent token mask consist multiplying zero  whatever special value zero accept useful  null value  non existent token place word vector  attention mask crucial handling variable  length sequence ensure pad token contribute embedding  print  maskedwordembeddings    tensor     02835  00469  05029    00525  00089  01395    02636  01355  04277    03552  00437  02479    00000  00000  00000    00000  00000  00000      00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000    00000  00000  00000     usually  sentence embedding compute sum  mean max mask word embedding  depend use case  mean suitable variable length  sentenceembedding  maskedwordembeddingsmean  dim1  sum intend force importance relevant part  sentenceembedding  maskedwordembeddingsmax  dim1  exist lot technique depend embedding perform task  would choose method maximice cosine similarity vector consider similar task  ex  sum get similarity mean  may suitable  additionally suggest normalize value number token sentence   normalization  large sentence tend low vector value  embe information number tokens sentence  prevent get high similarity score sentence 4 token whole book  meaningless ,Task-Specific Queries
Find vocab word from vector for flexible comparisons,"Is it possible to find a vocab word from a vector so that I can do more flexible comparisons? Something like this: queen = nlp.vocab[""king""].vector - nlp.vocab[""man""].vector + nlp.vocab[""woman""].vector king = nlp.vocab[""queen""].vector - nlp.vocab[""woman""].vector + nlp.vocab[""man""].vector queen.similarity(king) I realize in this example I could just check the similarity of king and queen directly, but my use-case is that I'd like to compare sentence/doc similarity and I read that in order to get sentence vector I can simply add up the words in a given sentence and I'm assuming that means I can compare them as well.","['nlp', 'spacy']",1,"Better then summing the embeddings of the words forming your sentence, you can use a sentence embedding model as sentence-transformer . This will give you : from sentence_transformers import SentenceTransformer from sklearn.metrics.pairwise import cosine_similarity model = SentenceTransformer('paraphrase-MiniLM-L6-v2') e1 = model.encode(['This is your first sentence']) e2 = model.encode(['This is your 2nd sentence']) cosine_similarity(e1, e2)",2024-01-02 13:37:11,2024-01-02 15:59:26,30,https://stackoverflow.com/questions/77746423/find-vocab-word-from-vector-for-flexible-comparisons,"Find vocab word from vector for flexible comparisons Is it possible to find a vocab word from a vector so that I can do more flexible comparisons? Something like this: queen = nlp.vocab[""king""].vector - nlp.vocab[""man""].vector + nlp.vocab[""woman""].vector king = nlp.vocab[""queen""].vector - nlp.vocab[""woman""].vector + nlp.vocab[""man""].vector queen.similarity(king) I realize in this example I could just check the similarity of king and queen directly, but my use-case is that I'd like to compare sentence/doc similarity and I read that in order to get sentence vector I can simply add up the words in a given sentence and I'm assuming that means I can compare them as well.",find vocab word vector flexible comparison possible find vocab word vector flexible comparison  something like  queen  nlpvocab    king   vector  nlpvocab    man   vector  nlpvocab    woman   vector king  nlpvocab    queen   vector  nlpvocab    woman   vector  nlpvocab    man   vector queensimilarity  king  realize example could check similarity king queen directly  use  case would like compare sentence  doc similarity read order get sentence vector simply add word give sentence  m assume mean compare well ,well sum embedding word form sentence  use sentence embed model sentence  transformer  give  sentencetransformer import sentencetransformer sklearnmetricspairwise import cosinesimilarity model  sentencetransformer   paraphrase  minilm  l6  v2   e1  modelencode    this first sentence    e2  modelencode    this 2nd sentence    cosinesimilarity  e1  e2 ,find vocab word vector flexible comparison possible find vocab word vector flexible comparison  something like  queen  nlpvocab    king   vector  nlpvocab    man   vector  nlpvocab    woman   vector king  nlpvocab    queen   vector  nlpvocab    woman   vector  nlpvocab    man   vector queensimilarity  king  realize example could check similarity king queen directly  use  case would like compare sentence  doc similarity read order get sentence vector simply add word give sentence  m assume mean compare well  well sum embedding word form sentence  use sentence embed model sentence  transformer  give  sentencetransformer import sentencetransformer sklearnmetricspairwise import cosinesimilarity model  sentencetransformer   paraphrase  minilm  l6  v2   e1  modelencode    this first sentence    e2  modelencode    this 2nd sentence    cosinesimilarity  e1  e2 ,Library/Tool-Based Queries
GooglePalm(). NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases,"this code was working fine before, and now it raise this error when calling GooglePalm with langchain. The error: ----> 8 llm = GooglePalm(). NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases. My code: import google.generativeai as palm from langchain.embeddings import GooglePalmEmbeddings from langchain.llms import GooglePalm palm.configure(api_key=GOOGLE_API_KEY) llm = GooglePalm()","['nlp', 'langchain', 'large-language-model', 'palm-pre']",3,I solved it by upgradown langchain version: !pip uninstall langchain !pip install langchain==0.0.339,2023-12-19 11:53:38,2023-12-20 22:34:34,470,https://stackoverflow.com/questions/77684999/googlepalm-notimplementederror-need-to-determine-which-default-deprecation-s,"GooglePalm(). NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases this code was working fine before, and now it raise this error when calling GooglePalm with langchain. The error: ----> 8 llm = GooglePalm(). NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases. My code: import google.generativeai as palm from langchain.embeddings import GooglePalmEmbeddings from langchain.llms import GooglePalm palm.configure(api_key=GOOGLE_API_KEY) llm = GooglePalm()",googlepalm    notimplementederror  need determine default deprecation schedule use  within   minor release code work fine  raise error call googlepalm langchain  error     8 llm  googlepalm    notimplementederror  need determine default deprecation schedule use  within   minor release  code  import googlegenerativeai palm langchainembedding import googlepalmembedding langchainllm import googlepalm palmconfigure  apikey  googleapikey  llm  googlepalm  ,solve upgradown langchain version   pip uninstall langchain  pip install langchain00339,googlepalm    notimplementederror  need determine default deprecation schedule use  within   minor release code work fine  raise error call googlepalm langchain  error     8 llm  googlepalm    notimplementederror  need determine default deprecation schedule use  within   minor release  code  import googlegenerativeai palm langchainembedding import googlepalmembedding langchainllm import googlepalm palmconfigure  apikey  googleapikey  llm  googlepalm   solve upgradown langchain version   pip uninstall langchain  pip install langchain00339,Implementation Issues
Unable to tag the POS of the text file,"I want to tag the parts of speech of a sentence. For this task I am using pos-english-fast model. If there was one sentence the model identified the tags for the pos. I created a data file where I kept all my sentences. The name of the data file is 'data1.txt'. Now if I try to tag the sentences on the data file it does not work. My code from flair.models import SequenceTagger model = SequenceTagger.load(""flair/pos-english"") #Read the data from the data.txt with open('data1.txt') as f: data = f.read().splitlines() #Create a list of sentences from the data sentences = [sentence.split() for sentence in data] #Tag each sentence using the model tagged_sentences = [] for sentence in sentences: tagged_sentences.append(model.predict(sentence)) for sentence in tagged_sentences: print(sentence) The error I received AttributeError Traceback (most recent call last) <ipython-input-16-03268ee0d9c9> in <cell line: 10>() 9 tagged_sentences = [] 10 for sentence in sentences: ---> 11 tagged_sentences.append(model.predict(sentence)) 12 for sentence in tagged_sentences: 13 print(sentence) 1 frames /usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences) 1116 previous_sentence = None 1117 for sentence in sentences: -> 1118 if sentence.is_context_set(): 1119 continue 1120 sentence._previous_sentence = previous_sentence AttributeError: 'str' object has no attribute 'is_context_set' The snapshot of the errors How could I resolve it?","['python', 'nlp', 'tagging', 'huggingface', 'flair']",1,"Let's say this is your data: ['Not My Responsibility is a 2020 American short film written and produced by singer-songwriter Billie Eilish.', ""A commentary on body shaming and double standards placed upon young women's appearances, it features a monologue from Eilish about the media scrutiny surrounding her body."", 'The film is spoken-word and stars Eilish in a dark room, where she gradually undresses before submerging herself in a black substance.'] This is what you need to do to do part-of-speech tagging in Flair: from flair.data import Sentence from flair.models import SequenceTagger sentences = list(map(Sentence, data)) _ = model.predict(sentences) Now all sentences are correctly tagged. If you want to visualize, for example, the tags for the first sentence, just use print(sentences[0]) . This is the output: Sentence[17]: ""Not My Responsibility is a 2020 American short film written and produced by singer-songwriter Billie Eilish."" → [""Not""/RB, ""My""/PRP$, ""Responsibility""/NN, ""is""/VBZ, ""a""/DT, ""2020""/CD, ""American""/JJ, ""short""/JJ, ""film""/NN, ""written""/VBN, ""and""/CC, ""produced""/VBN, ""by""/IN, ""singer-songwriter""/NN, ""Billie""/NNP, ""Eilish""/NNP, "".""/.] ``",2023-12-18 20:02:17,2023-12-18 21:20:44,37,https://stackoverflow.com/questions/77681471/unable-to-tag-the-pos-of-the-text-file,"Unable to tag the POS of the text file I want to tag the parts of speech of a sentence. For this task I am using pos-english-fast model. If there was one sentence the model identified the tags for the pos. I created a data file where I kept all my sentences. The name of the data file is 'data1.txt'. Now if I try to tag the sentences on the data file it does not work. My code from flair.models import SequenceTagger model = SequenceTagger.load(""flair/pos-english"") #Read the data from the data.txt with open('data1.txt') as f: data = f.read().splitlines() #Create a list of sentences from the data sentences = [sentence.split() for sentence in data] #Tag each sentence using the model tagged_sentences = [] for sentence in sentences: tagged_sentences.append(model.predict(sentence)) for sentence in tagged_sentences: print(sentence) The error I received AttributeError Traceback (most recent call last) <ipython-input-16-03268ee0d9c9> in <cell line: 10>() 9 tagged_sentences = [] 10 for sentence in sentences: ---> 11 tagged_sentences.append(model.predict(sentence)) 12 for sentence in tagged_sentences: 13 print(sentence) 1 frames /usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences) 1116 previous_sentence = None 1117 for sentence in sentences: -> 1118 if sentence.is_context_set(): 1119 continue 1120 sentence._previous_sentence = previous_sentence AttributeError: 'str' object has no attribute 'is_context_set' The snapshot of the errors How could I resolve it?",unable tag pos text file want tag part speech sentence  task use pos  english  fast model  one sentence model identify tag pos  create data file keep sentence  name datum file  data1txt   try tag sentence datum file work  code flairmodel import sequencetagger model  sequencetaggerload    flair  pos  english    read datum datatxt open   data1txt   f  datum  fread   splitlines    create list sentence datum sentence   sentencesplit   sentence datum   tag sentence use model taggedsentence    sentence sentence  taggedsentencesappend  modelpredict  sentence   sentence taggedsentence  print  sentence  error receive attributeerror traceback  recent call last   ipython  input16  03268ee0d9c9   cell line  10    9 taggedsentence    10 sentence sentence     11 taggedsentencesappend  modelpredict  sentence   12 sentence taggedsentence  13 print  sentence  1 frame usr  local  lib  python310  dist  package  flair  datapy setcontextforsentence  cls  sentence  1116 previoussentence  none 1117 sentence sentence    1118 sentenceiscontextset    1119 continue 1120 sentenceprevioussentence  previoussentence attributeerror   str  object attribute  iscontextset  snapshot error could resolve ,let us say datum    not responsibility 2020 american short film write produce singer  songwriter billie eilish      commentary body shame double standard place upon young woman s appearance  feature monologue eilish medium scrutiny surround body      the film speak  word star eilish dark room  gradually undress submerge black substance    need part  of  speech tagging flair  flairdata import sentence flairmodel import sequencetagger sentence  list  map  sentence  datum     modelpredict  sentence  sentence correctly tag  want visualize  example  tag first sentence  use print  sentence  0    output  sentence  17     responsibility 2020 american short film write produce singer  songwriter billie eilish        rb     prp     responsibility  nn     vbz     dt    2020  cd    american  jj    short  jj    film  nn    write  vbn     cc    produce  vbn     in    singer  songwriter  nn    billie  nnp    eilish  nnp          ,unable tag pos text file want tag part speech sentence  task use pos  english  fast model  one sentence model identify tag pos  create data file keep sentence  name datum file  data1txt   try tag sentence datum file work  code flairmodel import sequencetagger model  sequencetaggerload    flair  pos  english    read datum datatxt open   data1txt   f  datum  fread   splitlines    create list sentence datum sentence   sentencesplit   sentence datum   tag sentence use model taggedsentence    sentence sentence  taggedsentencesappend  modelpredict  sentence   sentence taggedsentence  print  sentence  error receive attributeerror traceback  recent call last   ipython  input16  03268ee0d9c9   cell line  10    9 taggedsentence    10 sentence sentence     11 taggedsentencesappend  modelpredict  sentence   12 sentence taggedsentence  13 print  sentence  1 frame usr  local  lib  python310  dist  package  flair  datapy setcontextforsentence  cls  sentence  1116 previoussentence  none 1117 sentence sentence    1118 sentenceiscontextset    1119 continue 1120 sentenceprevioussentence  previoussentence attributeerror   str  object attribute  iscontextset  snapshot error could resolve  let us say datum    not responsibility 2020 american short film write produce singer  songwriter billie eilish      commentary body shame double standard place upon young woman s appearance  feature monologue eilish medium scrutiny surround body      the film speak  word star eilish dark room  gradually undress submerge black substance    need part  of  speech tagging flair  flairdata import sentence flairmodel import sequencetagger sentence  list  map  sentence  datum     modelpredict  sentence  sentence correctly tag  want visualize  example  tag first sentence  use print  sentence  0    output  sentence  17     responsibility 2020 american short film write produce singer  songwriter billie eilish        rb     prp     responsibility  nn     vbz     dt    2020  cd    american  jj    short  jj    film  nn    write  vbn     cc    produce  vbn     in    singer  songwriter  nn    billie  nnp    eilish  nnp          ,Task-Specific Queries
Google Colab unable to Hugging Face model,"I like to tag parts of speech using the BERT model. I used the Hugging face library for this purpose. When I run the model on Hugging face API I got the output However, when I run the code on Google Colab I got errors. My code from transformers import AutoModelWithHeads from transformers import pipeline from transformers import AutoTokenizer model = AutoModelWithHeads.from_pretrained(""bert-base-uncased"") adapter_name = model.load_adapter(""AdapterHub/bert-base-uncased-pf-ud_pos"", source=""hf"") model.active_adapters = adapter_name tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer, aggregation_strategy=""NONE"") res = token_classification(""Take out the trash bag from the bin and replace it."") print(res) The error is The model 'BertModelWithHeads' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'YosoForTokenClassification', 'XLMRobertaAdapterModel', 'RobertaAdapterModel', 'AlbertAdapterModel', 'BeitAdapterModel', 'BertAdapterModel', 'BertGenerationAdapterModel', 'DistilBertAdapterModel', 'DebertaV2AdapterModel', 'DebertaAdapterModel', 'BartAdapterModel', 'MBartAdapterModel', 'GPT2AdapterModel', 'GPTJAdapterModel', 'T5AdapterModel', 'ViTAdapterModel']. --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-18-79b43720402e> in <cell line: 12>() 10 tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") 11 token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer, aggregation_strategy=""NONE"") ---> 12 res = token_classification(""Take out the trash bag from the bin and replace it."") 13 print(res) 4 frames /usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py in aggregate(self, pre_entities, aggregation_strategy) 346 score = pre_entity[""scores""][entity_idx] 347 entity = { --> 348 ""entity"": self.model.config.id2label[entity_idx], 349 ""score"": score, 350 ""index"": pre_entity[""index""], KeyError: 16 I don't understand if the model ran ok in the Hugging Face API then why it was unable to run on Google Colab? Thank you in advance.","['python', 'nlp', 'google-colaboratory', 'huggingface-transformers', 'bert-language-model']",2,"Here is how you can do it: from transformers import AutoModelWithHeads, AutoTokenizer, pipeline tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") model = AutoModelWithHeads.from_pretrained(""bert-base-uncased"") model.load_adapter( ""AdapterHub/bert-base-uncased-pf-ud_pos"", source=""hf"", set_active=True, ) token_classification = pipeline( ""token-classification"", model=model, tokenizer=tokenizer, ) This pipeline creation part is going to show a warning, i.e. The model 'BertModelWithHeads' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'YosoForTokenClassification', 'XLMRobertaAdapterModel', 'RobertaAdapterModel', 'AlbertAdapterModel', 'BeitAdapterModel', 'BertAdapterModel', 'BertGenerationAdapterModel', 'DistilBertAdapterModel', 'DebertaV2AdapterModel', 'DebertaAdapterModel', 'BartAdapterModel', 'MBartAdapterModel', 'GPT2AdapterModel', 'GPTJAdapterModel', 'T5AdapterModel', 'ViTAdapterModel']. You can just ignore it, the pipeline will still work. Here is an example: >>> token_classification(""Take out the trash bag from the bin and replace it"") [{'entity': 'VERB','score': 0.99986637, 'index': 1, 'word': 'take', 'start': 0, 'end': 4}, {'entity': 'ADP', 'score': 0.9829973, 'index': 2, 'word': 'out', 'start': 5, 'end': 8}, {'entity': 'DET', 'score': 0.9998791, 'index': 3, 'word': 'the', 'start': 9, 'end': 12}, {'entity': 'NOUN', 'score': 0.9958676, 'index': 4, 'word': 'trash', 'start': 13, 'end': 18}, {'entity': 'NOUN', 'score': 0.99657273, 'index': 5, 'word': 'bag', 'start': 19, 'end': 22}, {'entity': 'ADP', 'score': 0.99989176, 'index': 6, 'word': 'from', 'start': 23, 'end': 27}, {'entity': 'DET', 'score': 0.99982834, 'index': 7, 'word': 'the', 'start': 28, 'end': 31}, {'entity': 'NOUN', 'score': 0.99584526, 'index': 8, 'word': 'bin', 'start': 32, 'end': 35}, {'entity': 'CCONJ', 'score': 0.99962616, 'index': 9, 'word': 'and', 'start': 36, 'end': 39}, {'entity': 'VERB', 'score': 0.99976975, 'index': 10, 'word': 'replace', 'start': 40, 'end': 47}, {'entity': 'PRON', 'score': 0.9989698, 'index': 11, 'word': 'it', 'start': 48, 'end': 50}]",2023-12-18 03:07:35,2023-12-19 16:45:58,1184,https://stackoverflow.com/questions/77676747/google-colab-unable-to-hugging-face-model,"Google Colab unable to Hugging Face model I like to tag parts of speech using the BERT model. I used the Hugging face library for this purpose. When I run the model on Hugging face API I got the output However, when I run the code on Google Colab I got errors. My code from transformers import AutoModelWithHeads from transformers import pipeline from transformers import AutoTokenizer model = AutoModelWithHeads.from_pretrained(""bert-base-uncased"") adapter_name = model.load_adapter(""AdapterHub/bert-base-uncased-pf-ud_pos"", source=""hf"") model.active_adapters = adapter_name tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer, aggregation_strategy=""NONE"") res = token_classification(""Take out the trash bag from the bin and replace it."") print(res) The error is The model 'BertModelWithHeads' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'YosoForTokenClassification', 'XLMRobertaAdapterModel', 'RobertaAdapterModel', 'AlbertAdapterModel', 'BeitAdapterModel', 'BertAdapterModel', 'BertGenerationAdapterModel', 'DistilBertAdapterModel', 'DebertaV2AdapterModel', 'DebertaAdapterModel', 'BartAdapterModel', 'MBartAdapterModel', 'GPT2AdapterModel', 'GPTJAdapterModel', 'T5AdapterModel', 'ViTAdapterModel']. --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-18-79b43720402e> in <cell line: 12>() 10 tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") 11 token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer, aggregation_strategy=""NONE"") ---> 12 res = token_classification(""Take out the trash bag from the bin and replace it."") 13 print(res) 4 frames /usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py in aggregate(self, pre_entities, aggregation_strategy) 346 score = pre_entity[""scores""][entity_idx] 347 entity = { --> 348 ""entity"": self.model.config.id2label[entity_idx], 349 ""score"": score, 350 ""index"": pre_entity[""index""], KeyError: 16 I don't understand if the model ran ok in the Hugging Face API then why it was unable to run on Google Colab? Thank you in advance.",google colab unable hug face model like tag part speech use bert model  use hugging face library purpose  run model hugging face api get output however  run code google colab get error  code transformer import automodelwithhead transformer import pipeline transformer import autotokenizer model  automodelwithheadsfrompretraine    bert  base  uncased   adaptername  modelloadadapt    adapterhub  bert  base  uncase  pf  udpos   source  hf   modelactiveadapter  adaptername tokenizer  autotokenizerfrompretraine    bert  base  uncased   tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  none   re  tokenclassification    take trash bag bin replace    print  re  error model  bertmodelwithhead  support token  classification  support model   albertfortokenclassification    bertfortokenclassification    bigbirdfortokenclassification    bloomfortokenclassification    camembertfortokenclassification    caninefortokenclassification    convbertfortokenclassification    data2vectextfortokenclassification    debertafortokenclassification    debertav2fortokenclassification    distilbertfortokenclassification    electrafortokenclassification    erniefortokenclassification    esmfortokenclassification    flaubertfortokenclassification    fnetfortokenclassification    funnelfortokenclassification    gpt2fortokenclassification    gpt2fortokenclassification    ibertfortokenclassification    layoutlmfortokenclassification    layoutlmv2fortokenclassification    layoutlmv3fortokenclassification    liltfortokenclassification    longformerfortokenclassification    lukefortokenclassification    markuplmfortokenclassification    megatronbertfortokenclassification    mobilebertfortokenclassification    mpnetfortokenclassification    nezhafortokenclassification    nystromformerfortokenclassification    qdqbertfortokenclassification    rembertfortokenclassification    robertafortokenclassification    robertaprelayernormfortokenclassification    rocbertfortokenclassification    roformerfortokenclassification    squeezebertfortokenclassification    xlmfortokenclassification    xlmrobertafortokenclassification    xlmrobertaxlfortokenclassification    xlnetfortokenclassification    yosofortokenclassification    xlmrobertaadaptermodel    robertaadaptermodel    albertadaptermodel    beitadaptermodel    bertadaptermodel    bertgenerationadaptermodel    distilbertadaptermodel    debertav2adaptermodel    debertaadaptermodel    bartadaptermodel    mbartadaptermodel    gpt2adaptermodel    gptjadaptermodel    t5adaptermodel    vitadaptermodel                                          keyerror traceback  recent call last   ipython  input18  79b43720402e   cell line  12    10 tokenizer  autotokenizerfrompretraine    bert  base  uncased   11 tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  none      12 re  tokenclassification    take trash bag bin replace    13 print  re  4 frame usr  local  lib  python310  dist  package  transformer  pipeline  tokenclassificationpy aggregate  self  preentitie  aggregationstrategy  346 score  preentity    score    entityidx  347 entity     348   entity   selfmodelconfigid2label  entityidx   349   score   score  350   index   preentity    index    keyerror  16 not understand model run ok hugging face api unable run google colab  thank advance , transformer import automodelwithhead  autotokenizer  pipeline tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  automodelwithheadsfrompretraine    bert  base  uncased   modelloadadapter    adapterhub  bert  base  uncase  pf  udpos   source  hf   setactive  true   tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer   pipeline creation part go show warning  ie  model  bertmodelwithheads  support token  classification  support model   albertfortokenclassification    bertfortokenclassification    bigbirdfortokenclassification    bloomfortokenclassification    camembertfortokenclassification    caninefortokenclassification    convbertfortokenclassification    data2vectextfortokenclassification    debertafortokenclassification    debertav2fortokenclassification    distilbertfortokenclassification    electrafortokenclassification    erniefortokenclassification    esmfortokenclassification    flaubertfortokenclassification    fnetfortokenclassification    funnelfortokenclassification    gpt2fortokenclassification    gpt2fortokenclassification    ibertfortokenclassification    layoutlmfortokenclassification    layoutlmv2fortokenclassification    layoutlmv3fortokenclassification    liltfortokenclassification    longformerfortokenclassification    lukefortokenclassification    markuplmfortokenclassification    megatronbertfortokenclassification    mobilebertfortokenclassification    mpnetfortokenclassification    nezhafortokenclassification    nystromformerfortokenclassification    qdqbertfortokenclassification    rembertfortokenclassification    robertafortokenclassification    robertaprelayernormfortokenclassification    rocbertfortokenclassification    roformerfortokenclassification    squeezebertfortokenclassification    xlmfortokenclassification    xlmrobertafortokenclassification    xlmrobertaxlfortokenclassification    xlnetfortokenclassification    yosofortokenclassification    xlmrobertaadaptermodel    robertaadaptermodel    albertadaptermodel    beitadaptermodel    bertadaptermodel    bertgenerationadaptermodel    distilbertadaptermodel    debertav2adaptermodel    debertaadaptermodel    bartadaptermodel    mbartadaptermodel    gpt2adaptermodel    gptjadaptermodel    t5adaptermodel    vitadaptermodel    ignore  pipeline still work  example     tokenclassification    take trash bag bin replace      entity    verb    score   099986637   index   1   word    take    start   0   end   4     entity    adp    score   09829973   index   2   word    out    start   5   end   8     entity    det    score   09998791   index   3   word    the    start   9   end   12     entity    noun    score   09958676   index   4   word    trash    start   13   end   18     entity    noun    score   099657273   index   5   word    bag    start   19   end   22     entity    adp    score   099989176   index   6   word    from    start   23   end   27     entity    det    score   099982834   index   7   word    the    start   28   end   31     entity    noun    score   099584526   index   8   word    bin    start   32   end   35     entity    cconj    score   099962616   index   9   word    and    start   36   end   39     entity    verb    score   099976975   index   10   word    replace    start   40   end   47     entity    pron    score   09989698   index   11   word    it    start   48   end   50  ,google colab unable hug face model like tag part speech use bert model  use hugging face library purpose  run model hugging face api get output however  run code google colab get error  code transformer import automodelwithhead transformer import pipeline transformer import autotokenizer model  automodelwithheadsfrompretraine    bert  base  uncased   adaptername  modelloadadapt    adapterhub  bert  base  uncase  pf  udpos   source  hf   modelactiveadapter  adaptername tokenizer  autotokenizerfrompretraine    bert  base  uncased   tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  none   re  tokenclassification    take trash bag bin replace    print  re  error model  bertmodelwithhead  support token  classification  support model   albertfortokenclassification    bertfortokenclassification    bigbirdfortokenclassification    bloomfortokenclassification    camembertfortokenclassification    caninefortokenclassification    convbertfortokenclassification    data2vectextfortokenclassification    debertafortokenclassification    debertav2fortokenclassification    distilbertfortokenclassification    electrafortokenclassification    erniefortokenclassification    esmfortokenclassification    flaubertfortokenclassification    fnetfortokenclassification    funnelfortokenclassification    gpt2fortokenclassification    gpt2fortokenclassification    ibertfortokenclassification    layoutlmfortokenclassification    layoutlmv2fortokenclassification    layoutlmv3fortokenclassification    liltfortokenclassification    longformerfortokenclassification    lukefortokenclassification    markuplmfortokenclassification    megatronbertfortokenclassification    mobilebertfortokenclassification    mpnetfortokenclassification    nezhafortokenclassification    nystromformerfortokenclassification    qdqbertfortokenclassification    rembertfortokenclassification    robertafortokenclassification    robertaprelayernormfortokenclassification    rocbertfortokenclassification    roformerfortokenclassification    squeezebertfortokenclassification    xlmfortokenclassification    xlmrobertafortokenclassification    xlmrobertaxlfortokenclassification    xlnetfortokenclassification    yosofortokenclassification    xlmrobertaadaptermodel    robertaadaptermodel    albertadaptermodel    beitadaptermodel    bertadaptermodel    bertgenerationadaptermodel    distilbertadaptermodel    debertav2adaptermodel    debertaadaptermodel    bartadaptermodel    mbartadaptermodel    gpt2adaptermodel    gptjadaptermodel    t5adaptermodel    vitadaptermodel                                          keyerror traceback  recent call last   ipython  input18  79b43720402e   cell line  12    10 tokenizer  autotokenizerfrompretraine    bert  base  uncased   11 tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  none      12 re  tokenclassification    take trash bag bin replace    13 print  re  4 frame usr  local  lib  python310  dist  package  transformer  pipeline  tokenclassificationpy aggregate  self  preentitie  aggregationstrategy  346 score  preentity    score    entityidx  347 entity     348   entity   selfmodelconfigid2label  entityidx   349   score   score  350   index   preentity    index    keyerror  16 not understand model run ok hugging face api unable run google colab  thank advance   transformer import automodelwithhead  autotokenizer  pipeline tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  automodelwithheadsfrompretraine    bert  base  uncased   modelloadadapter    adapterhub  bert  base  uncase  pf  udpos   source  hf   setactive  true   tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer   pipeline creation part go show warning  ie  model  bertmodelwithheads  support token  classification  support model   albertfortokenclassification    bertfortokenclassification    bigbirdfortokenclassification    bloomfortokenclassification    camembertfortokenclassification    caninefortokenclassification    convbertfortokenclassification    data2vectextfortokenclassification    debertafortokenclassification    debertav2fortokenclassification    distilbertfortokenclassification    electrafortokenclassification    erniefortokenclassification    esmfortokenclassification    flaubertfortokenclassification    fnetfortokenclassification    funnelfortokenclassification    gpt2fortokenclassification    gpt2fortokenclassification    ibertfortokenclassification    layoutlmfortokenclassification    layoutlmv2fortokenclassification    layoutlmv3fortokenclassification    liltfortokenclassification    longformerfortokenclassification    lukefortokenclassification    markuplmfortokenclassification    megatronbertfortokenclassification    mobilebertfortokenclassification    mpnetfortokenclassification    nezhafortokenclassification    nystromformerfortokenclassification    qdqbertfortokenclassification    rembertfortokenclassification    robertafortokenclassification    robertaprelayernormfortokenclassification    rocbertfortokenclassification    roformerfortokenclassification    squeezebertfortokenclassification    xlmfortokenclassification    xlmrobertafortokenclassification    xlmrobertaxlfortokenclassification    xlnetfortokenclassification    yosofortokenclassification    xlmrobertaadaptermodel    robertaadaptermodel    albertadaptermodel    beitadaptermodel    bertadaptermodel    bertgenerationadaptermodel    distilbertadaptermodel    debertav2adaptermodel    debertaadaptermodel    bartadaptermodel    mbartadaptermodel    gpt2adaptermodel    gptjadaptermodel    t5adaptermodel    vitadaptermodel    ignore  pipeline still work  example     tokenclassification    take trash bag bin replace      entity    verb    score   099986637   index   1   word    take    start   0   end   4     entity    adp    score   09829973   index   2   word    out    start   5   end   8     entity    det    score   09998791   index   3   word    the    start   9   end   12     entity    noun    score   09958676   index   4   word    trash    start   13   end   18     entity    noun    score   099657273   index   5   word    bag    start   19   end   22     entity    adp    score   099989176   index   6   word    from    start   23   end   27     entity    det    score   099982834   index   7   word    the    start   28   end   31     entity    noun    score   099584526   index   8   word    bin    start   32   end   35     entity    cconj    score   099962616   index   9   word    and    start   36   end   39     entity    verb    score   099976975   index   10   word    replace    start   40   end   47     entity    pron    score   09989698   index   11   word    it    start   48   end   50  ,Implementation Issues
How to use adapter transformers with a Huggingface Pipeline,"I tried to run the model ""AdapterHub/bert-base-uncased-pf-conll2003"" ( Model description here ) for token classification in NLP. First I tried to install the adapter transformers pip install -U adapter-transformers The output of the above command was Collecting adapter-transformers [... see edit history for skipped lines ...] Installing collected packages: tokenizers, huggingface-hub, adapter-transformers Attempting uninstall: tokenizers Found existing installation: tokenizers 0.15.0 Uninstalling tokenizers-0.15.0: Successfully uninstalled tokenizers-0.15.0 Attempting uninstall: huggingface-hub Found existing installation: huggingface-hub 0.19.4 Uninstalling huggingface-hub-0.19.4: Successfully uninstalled huggingface-hub-0.19.4 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.13.4 which is incompatible. transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.13.3 which is incompatible. Successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3 I tried to load the model like this into the pipeline: from transformers import AutoModelWithHeads from transformers import pipeline token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"") res = token_classification(""Take out the trash bag from the bin and replace it."") print(res) I received the errors EntryNotFoundError: 404 Client Error. (Request ID: Root=1-657e793c-0ce0c1936aff5e5741676650) Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/resolve/main/config.json. During handling of the above exception, another exception occurred: OSError Traceback (most recent call last) <ipython-input-3-030dfe0e128d> in <cell line: 3>() 1 from transformers import AutoModelWithHeads 2 from transformers import pipeline ----> 3 token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"") 4 res = token_classification(""Take out the trash bag from the bin and replace it."") 5 print(res) /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs) 673 hub_kwargs[""_commit_hash""] = config._commit_hash 674 elif config is None and isinstance(model, str): --> 675 config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs) 676 hub_kwargs[""_commit_hash""] = config._commit_hash 677 [... see edit history for skipped lines ...] /usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs) 624 try: 625 # Load from local folder or from cache or download from model Hub and cache --> 626 resolved_config_file = cached_file( 627 pretrained_model_name_or_path, 628 configuration_file, /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash) 452 if revision is None: 453 revision = ""main"" --> 454 raise EnvironmentError( 455 f""{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout "" 456 f""'https://huggingface.co/{path_or_repo_id}/{revision}' for available files."" OSError: AdapterHub/bert-base-uncased-pf-conll2003 does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/main' for available files. How do I correctly load this adapter model?","['python', 'machine-learning', 'nlp', 'huggingface-transformers']",1,"# be sure you have the dependencies (NEW) $ pip install adapters The old & legacy package is pip install -U adapter-transformers Create the model outside of the pipeline from transformers import AutoModelWithHeads from transformers import pipeline from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") model = AutoModelWithHeads.from_pretrained(""bert-base-uncased"") adapter_name = model.load_adapter(""AdapterHub/bert-base-uncased-pf-conll2003"", source=""hf"") model.active_adapters = adapter_name token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer) res = token_classification(""Take out the trash bag from the bin and replace it."") print(res)",2023-12-17 04:49:28,2023-12-17 19:57:10,899,https://stackoverflow.com/questions/77673353/how-to-use-adapter-transformers-with-a-huggingface-pipeline,"How to use adapter transformers with a Huggingface Pipeline I tried to run the model ""AdapterHub/bert-base-uncased-pf-conll2003"" ( Model description here ) for token classification in NLP. First I tried to install the adapter transformers pip install -U adapter-transformers The output of the above command was Collecting adapter-transformers [... see edit history for skipped lines ...] Installing collected packages: tokenizers, huggingface-hub, adapter-transformers Attempting uninstall: tokenizers Found existing installation: tokenizers 0.15.0 Uninstalling tokenizers-0.15.0: Successfully uninstalled tokenizers-0.15.0 Attempting uninstall: huggingface-hub Found existing installation: huggingface-hub 0.19.4 Uninstalling huggingface-hub-0.19.4: Successfully uninstalled huggingface-hub-0.19.4 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.13.4 which is incompatible. transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.13.3 which is incompatible. Successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3 I tried to load the model like this into the pipeline: from transformers import AutoModelWithHeads from transformers import pipeline token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"") res = token_classification(""Take out the trash bag from the bin and replace it."") print(res) I received the errors EntryNotFoundError: 404 Client Error. (Request ID: Root=1-657e793c-0ce0c1936aff5e5741676650) Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/resolve/main/config.json. During handling of the above exception, another exception occurred: OSError Traceback (most recent call last) <ipython-input-3-030dfe0e128d> in <cell line: 3>() 1 from transformers import AutoModelWithHeads 2 from transformers import pipeline ----> 3 token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"") 4 res = token_classification(""Take out the trash bag from the bin and replace it."") 5 print(res) /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs) 673 hub_kwargs[""_commit_hash""] = config._commit_hash 674 elif config is None and isinstance(model, str): --> 675 config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs) 676 hub_kwargs[""_commit_hash""] = config._commit_hash 677 [... see edit history for skipped lines ...] /usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs) 624 try: 625 # Load from local folder or from cache or download from model Hub and cache --> 626 resolved_config_file = cached_file( 627 pretrained_model_name_or_path, 628 configuration_file, /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash) 452 if revision is None: 453 revision = ""main"" --> 454 raise EnvironmentError( 455 f""{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout "" 456 f""'https://huggingface.co/{path_or_repo_id}/{revision}' for available files."" OSError: AdapterHub/bert-base-uncased-pf-conll2003 does not appear to have a file named config.json. Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/main' for available files. How do I correctly load this adapter model?",use adapter transformer huggingface pipeline try run model   adapterhub  bert  base  uncase  pf  conll2003   model description  token classification nlp  first try install adapter transformer pip install u adapter  transformer output command collect adapter  transformer   see edit history skip line   instal collect package  tokenizer  huggingface  hub  adapter  transformer attempt uninstall  tokenizer find exist installation  tokenizer 0150 uninstalling tokenizers0150  successfully uninstalled tokenizers0150 attempt uninstall  huggingface  hub find exist installation  huggingface  hub 0194 uninstalling huggingface  hub0194  successfully uninstalled huggingface  hub0194 error  pip s dependency resolver currently take account package instal  behaviour source follow dependency conflict  transformer 4352 require huggingface  hub  10    0164  huggingface  hub 0134 incompatible  transformer 4352 require tokenizer  019    014  tokenizer 0133 incompatible  successfully instal adapter  transformers321post0 huggingface  hub0134 tokenizers0133 try load model like pipeline  transformer import automodelwithhead transformer import pipeline tokenclassification  pipeline    token  classification   model    adapterhub  bert  base  uncase  pf  conll2003   re  tokenclassification    take trash bag bin replace    print  re  receive error entrynotfounderror  404 client error   request id  root1  657e793c0ce0c1936aff5e5741676650  entry find url    huggingfaceco  adapterhub  bert  base  uncase  pf  conll2003  resolve  main  configjson  handle exception  another exception occur  oserror traceback  recent call last   ipython  input3  030dfe0e128d   cell line  3    1 transformer import automodelwithhead 2 transformer import pipeline    3 tokenclassification  pipeline    token  classification   model    adapterhub  bert  base  uncase  pf  conll2003   4 re  tokenclassification    take trash bag bin replace    5 print  re  usr  local  lib  python310  dist  package  transformer  pipelinesinitpy pipeline  task  model  config  tokenizer  featureextractor  framework  revision  usefast  useauthtoken  device  devicemap  torchdtype  trustremotecode  modelkwarg  pipelineclass    kwargs  673 hubkwarg     commithash    configcommithash 674 elif config none isinstance  model  str     675 config  autoconfigfrompretrained  model   frompipeline  task    hubkwargs    modelkwarg  676 hubkwarg     commithash    configcommithash 677   see edit history skip line   usr  local  lib  python310  dist  package  transformer  configurationutilspy  getconfigdict  cls  pretrainedmodelnameorpath    kwargs  624 try  625  load local folder cache download model hub cache   626 resolvedconfigfile  cachedfile  627 pretrainedmodelnameorpath  628 configurationfile  usr  local  lib  python310  dist  package  transformer  util  hubpy cachedfile  pathorrepoid  filename  cachedir  forcedownload  resumedownload  proxy  useauthtoken  revision  localfilesonly  subfolder  useragent   raiseexceptionsformissingentrie   raiseexceptionsforconnectionerror   commithash  452 revision none  453 revision    main    454 raise environmenterror  455 f   pathorrepoid  appear file name  fullfilename   checkout   456 f     huggingfaceco  pathorrepoid    revision   available file   oserror  adapterhub  bert  base  uncase  pf  conll2003 appear file name configjson  checkout    huggingfaceco  adapterhub  bert  base  uncase  pf  conll2003  main  available file  correctly load adapter model , sure dependencie  new   pip install adapter old  legacy package pip install u adapter  transformer create model outside pipeline transformer import automodelwithhead transformer import pipeline transformer import autotokenizer tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  automodelwithheadsfrompretraine    bert  base  uncased   adaptername  modelloadadapt    adapterhub  bert  base  uncase  pf  conll2003   source  hf   modelactiveadapter  adaptername tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  re  tokenclassification    take trash bag bin replace    print  re ,use adapter transformer huggingface pipeline try run model   adapterhub  bert  base  uncase  pf  conll2003   model description  token classification nlp  first try install adapter transformer pip install u adapter  transformer output command collect adapter  transformer   see edit history skip line   instal collect package  tokenizer  huggingface  hub  adapter  transformer attempt uninstall  tokenizer find exist installation  tokenizer 0150 uninstalling tokenizers0150  successfully uninstalled tokenizers0150 attempt uninstall  huggingface  hub find exist installation  huggingface  hub 0194 uninstalling huggingface  hub0194  successfully uninstalled huggingface  hub0194 error  pip s dependency resolver currently take account package instal  behaviour source follow dependency conflict  transformer 4352 require huggingface  hub  10    0164  huggingface  hub 0134 incompatible  transformer 4352 require tokenizer  019    014  tokenizer 0133 incompatible  successfully instal adapter  transformers321post0 huggingface  hub0134 tokenizers0133 try load model like pipeline  transformer import automodelwithhead transformer import pipeline tokenclassification  pipeline    token  classification   model    adapterhub  bert  base  uncase  pf  conll2003   re  tokenclassification    take trash bag bin replace    print  re  receive error entrynotfounderror  404 client error   request id  root1  657e793c0ce0c1936aff5e5741676650  entry find url    huggingfaceco  adapterhub  bert  base  uncase  pf  conll2003  resolve  main  configjson  handle exception  another exception occur  oserror traceback  recent call last   ipython  input3  030dfe0e128d   cell line  3    1 transformer import automodelwithhead 2 transformer import pipeline    3 tokenclassification  pipeline    token  classification   model    adapterhub  bert  base  uncase  pf  conll2003   4 re  tokenclassification    take trash bag bin replace    5 print  re  usr  local  lib  python310  dist  package  transformer  pipelinesinitpy pipeline  task  model  config  tokenizer  featureextractor  framework  revision  usefast  useauthtoken  device  devicemap  torchdtype  trustremotecode  modelkwarg  pipelineclass    kwargs  673 hubkwarg     commithash    configcommithash 674 elif config none isinstance  model  str     675 config  autoconfigfrompretrained  model   frompipeline  task    hubkwargs    modelkwarg  676 hubkwarg     commithash    configcommithash 677   see edit history skip line   usr  local  lib  python310  dist  package  transformer  configurationutilspy  getconfigdict  cls  pretrainedmodelnameorpath    kwargs  624 try  625  load local folder cache download model hub cache   626 resolvedconfigfile  cachedfile  627 pretrainedmodelnameorpath  628 configurationfile  usr  local  lib  python310  dist  package  transformer  util  hubpy cachedfile  pathorrepoid  filename  cachedir  forcedownload  resumedownload  proxy  useauthtoken  revision  localfilesonly  subfolder  useragent   raiseexceptionsformissingentrie   raiseexceptionsforconnectionerror   commithash  452 revision none  453 revision    main    454 raise environmenterror  455 f   pathorrepoid  appear file name  fullfilename   checkout   456 f     huggingfaceco  pathorrepoid    revision   available file   oserror  adapterhub  bert  base  uncase  pf  conll2003 appear file name configjson  checkout    huggingfaceco  adapterhub  bert  base  uncase  pf  conll2003  main  available file  correctly load adapter model   sure dependencie  new   pip install adapter old  legacy package pip install u adapter  transformer create model outside pipeline transformer import automodelwithhead transformer import pipeline transformer import autotokenizer tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  automodelwithheadsfrompretraine    bert  base  uncased   adaptername  modelloadadapt    adapterhub  bert  base  uncase  pf  conll2003   source  hf   modelactiveadapter  adaptername tokenclassification  pipeline    token  classification   model  model  tokenizer  tokenizer  re  tokenclassification    take trash bag bin replace    print  re ,Task-Specific Queries
Sentence Similarity between a phrase with 2-3 words and documents with multiple sentences,"What I want to achieve: I have thousands of documents (descriptions of incidents) and I would like to find the documents which match a phrase or are similar to the words in the phrase. An example, for an input phrase, ""electric vehicle"", I would like to find all the documents that has any discussion related to anything happening with any type of electric vehicle or conveyance, the documents in the corpus might not have the word ""vehicle"", but may have the specific vehicle type mentioned, like ""scooter"", ""bicycle"", ""hoverboard"" etc,. and document may have the word ""electrical"" or even something like ""lithium battery of a "". So, from an input phrase like ""an electric vehicle"" or ""an electric automobile"" or ""vehicle powered by a lithium-ion battery"", I need to find out all the documents that has related mentions to that term. But, I don't want to capture the documents with ""automobile"", ""scooter"" that doesn't have any mention of ""electric"" or ""lithium-ion"". So, from a phrase with 1 to 4 words, I must find matching documents containing anywhere from 2 to 100 words used for 1 to 7 sentences in each document. And the list of input phrases (that are used to find matching documents) will vary, hence something like Siamese-networks or even training a classification model can't be done I suppose. And the count of documents will also keep increasing by day and each of the document is independent of each other. Here's what I have done till now: I have used sentence-transformers (tried the pre-trained models , multi-qa-mpnet-base-dot-v1 , all-MiniLM-L12-v2 , all-MiniLM-L16-v2 and all-mpnet-base-v2 ), to get normalized embeddings for all the documents, then my input phrase. and then computed cosine-similarity between my input phrase's embeddings with all the documents, then get the top 20 sentences with highest values. The matched documents were barely relevant. For ex, for input phrase ""an electrical vehicle"" matches documents, with highest cosine-similarity, containing nothing but the word ""electrical"", followed by documents with only ""vehicle"", then documents with only ""electrical vehicle"" or a bit more words or the same 2 words in different forms, followed by documents just a bit more words but having mentions only of ""vehicle"" without ""electrical"" and vice-versa. I presume, because of the less count of words in the input phrase. How do I counter this and find documents that actually mention all the words in my input phrase instead of just using one word to find the matching documents?","['nlp', 'word-embedding', 'sentence-similarity', 'sentence-transformers']",1,In general your approach so far seem sensible and you should see more relevant search results. I suggest these improvements: Use models for asymmetric semantic search . Have a look at this part of Sentence Transformer's documentation . Here you'll find a selection of MS Marco models specifically trained for short queries (e.g. few words) and search for larger text passages (e.g. your documents). At the moment you seem to use suboptimal pretrained models. Have a look at hybrid search . Combining lexical with semantic search might yield better results for your use case. Weaviate has a nice implementation that you quickly can whip up and try out. You could also provide a minimal reproducible example. This might help to give more detailed recommendations.,2023-12-14 15:09:37,2023-12-15 14:20:03,306,https://stackoverflow.com/questions/77661152/sentence-similarity-between-a-phrase-with-2-3-words-and-documents-with-multiple,"Sentence Similarity between a phrase with 2-3 words and documents with multiple sentences What I want to achieve: I have thousands of documents (descriptions of incidents) and I would like to find the documents which match a phrase or are similar to the words in the phrase. An example, for an input phrase, ""electric vehicle"", I would like to find all the documents that has any discussion related to anything happening with any type of electric vehicle or conveyance, the documents in the corpus might not have the word ""vehicle"", but may have the specific vehicle type mentioned, like ""scooter"", ""bicycle"", ""hoverboard"" etc,. and document may have the word ""electrical"" or even something like ""lithium battery of a "". So, from an input phrase like ""an electric vehicle"" or ""an electric automobile"" or ""vehicle powered by a lithium-ion battery"", I need to find out all the documents that has related mentions to that term. But, I don't want to capture the documents with ""automobile"", ""scooter"" that doesn't have any mention of ""electric"" or ""lithium-ion"". So, from a phrase with 1 to 4 words, I must find matching documents containing anywhere from 2 to 100 words used for 1 to 7 sentences in each document. And the list of input phrases (that are used to find matching documents) will vary, hence something like Siamese-networks or even training a classification model can't be done I suppose. And the count of documents will also keep increasing by day and each of the document is independent of each other. Here's what I have done till now: I have used sentence-transformers (tried the pre-trained models , multi-qa-mpnet-base-dot-v1 , all-MiniLM-L12-v2 , all-MiniLM-L16-v2 and all-mpnet-base-v2 ), to get normalized embeddings for all the documents, then my input phrase. and then computed cosine-similarity between my input phrase's embeddings with all the documents, then get the top 20 sentences with highest values. The matched documents were barely relevant. For ex, for input phrase ""an electrical vehicle"" matches documents, with highest cosine-similarity, containing nothing but the word ""electrical"", followed by documents with only ""vehicle"", then documents with only ""electrical vehicle"" or a bit more words or the same 2 words in different forms, followed by documents just a bit more words but having mentions only of ""vehicle"" without ""electrical"" and vice-versa. I presume, because of the less count of words in the input phrase. How do I counter this and find documents that actually mention all the words in my input phrase instead of just using one word to find the matching documents?",sentence similarity phrase 2  3 word document multiple sentence want achieve  thousand document  description incident  would like find document match phrase similar word phrase  example  input phrase    electric vehicle   would like find document discussion relate anything happen type electric vehicle conveyance  document corpus might word   vehicle   may specific vehicle type mention  like   scooter     bicycle     hoverboard  etc   document may word   electrical  even something like   lithium battery     input phrase like   electric vehicle    electric automobile    vehicle power lithium  ion battery   need find document relate mention term   not want capture document   automobile     scooter  not mention   electric    lithium  ion    phrase 1 4 word  must find matching document contain anywhere 2 100 word use 1 7 sentence document  list input phrase  use find matching document  vary  hence something like siamese  network even train classification model can not do suppose  count document also keep increase day document independent  be do till  use sentence  transformer  try pre  train model  multi  qa  mpnet  base  dot  v1  all  minilm  l12  v2  all  minilm  l16  v2 all  mpnet  base  v2   get normalize embedding document  input phrase  compute cosine  similarity input phrase s embedding document  get top 20 sentence high value  match document barely relevant  ex  input phrase   electrical vehicle  match document  high cosine  similarity  contain nothing word   electrical   follow document   vehicle   document   electrical vehicle  bit word 2 word different form  follow document bit word mention   vehicle  without   electrical  vice  versa  presume  less count word input phrase  counter find document actually mention word input phrase instead use one word find matching document ,general approach far seem sensible see relevant search result  suggest improvement  use model asymmetric semantic search  look part sentence transformer s documentation  will find selection ms marco model specifically train short query  eg  word  search large text passage  eg  document   moment seem use suboptimal pretraine model  look hybrid search  combine lexical semantic search might yield well result use case  weaviate nice implementation quickly whip try  could also provide minimal reproducible example  might help give detailed recommendation ,sentence similarity phrase 2  3 word document multiple sentence want achieve  thousand document  description incident  would like find document match phrase similar word phrase  example  input phrase    electric vehicle   would like find document discussion relate anything happen type electric vehicle conveyance  document corpus might word   vehicle   may specific vehicle type mention  like   scooter     bicycle     hoverboard  etc   document may word   electrical  even something like   lithium battery     input phrase like   electric vehicle    electric automobile    vehicle power lithium  ion battery   need find document relate mention term   not want capture document   automobile     scooter  not mention   electric    lithium  ion    phrase 1 4 word  must find matching document contain anywhere 2 100 word use 1 7 sentence document  list input phrase  use find matching document  vary  hence something like siamese  network even train classification model can not do suppose  count document also keep increase day document independent  be do till  use sentence  transformer  try pre  train model  multi  qa  mpnet  base  dot  v1  all  minilm  l12  v2  all  minilm  l16  v2 all  mpnet  base  v2   get normalize embedding document  input phrase  compute cosine  similarity input phrase s embedding document  get top 20 sentence high value  match document barely relevant  ex  input phrase   electrical vehicle  match document  high cosine  similarity  contain nothing word   electrical   follow document   vehicle   document   electrical vehicle  bit word 2 word different form  follow document bit word mention   vehicle  without   electrical  vice  versa  presume  less count word input phrase  counter find document actually mention word input phrase instead use one word find matching document  general approach far seem sensible see relevant search result  suggest improvement  use model asymmetric semantic search  look part sentence transformer s documentation  will find selection ms marco model specifically train short query  eg  word  search large text passage  eg  document   moment seem use suboptimal pretraine model  look hybrid search  combine lexical semantic search might yield well result use case  weaviate nice implementation quickly whip try  could also provide minimal reproducible example  might help give detailed recommendation ,Implementation Issues
Import error in training arguments in Colaboratory,"I am using Google Colaboratory for my NLP project. I installed transformers and other libraries, but I got an error. from transformers import Trainer, TrainingArguments batch_size = 64 logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size model_name = f""{model_ckpt}-finetuned-stationary-update"" training_args = TrainingArguments(output_dir=model_name, num_train_epochs=10, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01, evaluation_strategy=""epoch"", disable_tqdm=False, logging_steps=logging_steps, push_to_hub=False, log_level=""error"") In the training_args line, I got an import error. It’s showing as: ImportError Traceback (most recent call last) <ipython-input-98-839907b16fa0> in <cell line: 6>() 4 logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size 5 model_name = f""{model_ckpt}-finetuned-stationary-update"" ----> 6 training_args = TrainingArguments(output_dir=model_name, 7 num_train_epochs=10, 8 learning_rate=2e-5, 4 frames /usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self) 1785 def __str__(self): 1786 self_as_dict = asdict(self) -> 1787 1788 # Remove deprecated arguments. That code should be removed once 1789 # those deprecated arguments are removed from TrainingArguments. (TODO: v5) ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U I tried to reinstall transformers , use pip install accelerate -U and also used PyTorch accelerate >= 0.20.1. How can I resolve this issue?","['python', 'nlp', 'huggingface']",1,There might be an issue with your transformers library installation. Uninstall the current transformers library. Reinstall the transformers library. Install the accelerate library. Ensure that you have the correct version of PyTorch. !pip uninstall -y transformers !pip install transformers !pip install accelerate -U Check your PyTorch version with the following command: import torch print(torch.__version__) The message suggests that you need to install the accelerate library with version 0.20.1 or higher. You can install it using the following command: !pip install accelerate -U you might want to try installing the transformers library with the torch extra. This can be done with the following command: !pip install transformers[torch],2023-12-13 12:41:06,2023-12-13 12:49:42,768,https://stackoverflow.com/questions/77653666/import-error-in-training-arguments-in-colaboratory,"Import error in training arguments in Colaboratory I am using Google Colaboratory for my NLP project. I installed transformers and other libraries, but I got an error. from transformers import Trainer, TrainingArguments batch_size = 64 logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size model_name = f""{model_ckpt}-finetuned-stationary-update"" training_args = TrainingArguments(output_dir=model_name, num_train_epochs=10, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01, evaluation_strategy=""epoch"", disable_tqdm=False, logging_steps=logging_steps, push_to_hub=False, log_level=""error"") In the training_args line, I got an import error. It’s showing as: ImportError Traceback (most recent call last) <ipython-input-98-839907b16fa0> in <cell line: 6>() 4 logging_steps = len(stationary_dataset_encoded[""train""]) // batch_size 5 model_name = f""{model_ckpt}-finetuned-stationary-update"" ----> 6 training_args = TrainingArguments(output_dir=model_name, 7 num_train_epochs=10, 8 learning_rate=2e-5, 4 frames /usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self) 1785 def __str__(self): 1786 self_as_dict = asdict(self) -> 1787 1788 # Remove deprecated arguments. That code should be removed once 1789 # those deprecated arguments are removed from TrainingArguments. (TODO: v5) ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U I tried to reinstall transformers , use pip install accelerate -U and also used PyTorch accelerate >= 0.20.1. How can I resolve this issue?",import error training argument colaboratory use google colaboratory nlp project  instal transformer library  get error  transformer import trainer  trainingarguments batchsize  64 loggingstep  len  stationarydatasetencode    train     batchsize modelname  f   modelckpt  finetune  stationary  update  trainingargs  trainingarguments  outputdir  modelname  numtrainepochs10  learningrate2e5  perdevicetrainbatchsize  batchsize  perdeviceevalbatchsize  batchsize  weightdecay001  evaluationstrategy  epoch   disabletqdm  false  loggingstep  loggingsteps  pushtohub  false  loglevel  error   trainingargs line  get import error   show  importerror traceback  recent call last   ipython  input98  839907b16fa0   cell line  6    4 loggingstep  len  stationarydatasetencode    train     batchsize 5 modelname  f   modelckpt  finetune  stationary  update     6 trainingargs  trainingarguments  outputdir  modelname  7 numtrainepochs10  8 learningrate2e5  4 frame usr  local  lib  python310  dist  package  transformer  trainingargspy  setupdevice  self  1785 def   str    self   1786 selfasdict  asdict  self    1787 1788  remove deprecate argument  code remove 1789  deprecate argument remove trainingarguments   todo  v5  importerror  use  trainer   pytorch  require  accelerate   0201   please run  pip install transformer  torch    pip install accelerate u try reinstall transformer  use pip install accelerate u also use pytorch accelerate   0201  resolve issue ,might issue transformer library installation  uninstall current transformer library  reinstall transformer library  install accelerate library  ensure correct version pytorch   pip uninstall y transformer  pip install transformer  pip install accelerate u check pytorch version follow command  import torch print  torchversion    message suggest need install accelerate library version 0201 high  install use follow command   pip install accelerate u might want try instal transformer library torch extra  do follow command   pip install transformer  torch ,import error training argument colaboratory use google colaboratory nlp project  instal transformer library  get error  transformer import trainer  trainingarguments batchsize  64 loggingstep  len  stationarydatasetencode    train     batchsize modelname  f   modelckpt  finetune  stationary  update  trainingargs  trainingarguments  outputdir  modelname  numtrainepochs10  learningrate2e5  perdevicetrainbatchsize  batchsize  perdeviceevalbatchsize  batchsize  weightdecay001  evaluationstrategy  epoch   disabletqdm  false  loggingstep  loggingsteps  pushtohub  false  loglevel  error   trainingargs line  get import error   show  importerror traceback  recent call last   ipython  input98  839907b16fa0   cell line  6    4 loggingstep  len  stationarydatasetencode    train     batchsize 5 modelname  f   modelckpt  finetune  stationary  update     6 trainingargs  trainingarguments  outputdir  modelname  7 numtrainepochs10  8 learningrate2e5  4 frame usr  local  lib  python310  dist  package  transformer  trainingargspy  setupdevice  self  1785 def   str    self   1786 selfasdict  asdict  self    1787 1788  remove deprecate argument  code remove 1789  deprecate argument remove trainingarguments   todo  v5  importerror  use  trainer   pytorch  require  accelerate   0201   please run  pip install transformer  torch    pip install accelerate u try reinstall transformer  use pip install accelerate u also use pytorch accelerate   0201  resolve issue  might issue transformer library installation  uninstall current transformer library  reinstall transformer library  install accelerate library  ensure correct version pytorch   pip uninstall y transformer  pip install transformer  pip install accelerate u check pytorch version follow command  import torch print  torchversion    message suggest need install accelerate library version 0201 high  install use follow command   pip install accelerate u might want try instal transformer library torch extra  do follow command   pip install transformer  torch ,Library/Tool-Based Queries
How to Load the pre-trained word embeddings in .npy files,"I'm trying to use the word embeddings pre-trained in HistWords Project by the Stanford NLP team. But when I ran the document example.py from the GitHub website , there was an error: ModuleNotFoundError: No module named 'representations.sequentialembedding' . How can I solve this problem? I've installed the ""representations"" module, but it doesn't work. The pre-trained word embeddings are of "".npy"" format, is there any Python-based method for uploading them?","['python', 'nlp', 'word-embedding']",1,To set it up you can do the following in your shell: cd <path you want to store your project> git clone https://github.com/williamleif/histwords.git cd histwords # ----- Set up Python2.7 ----- ## Python2.7 via conda is quite easy conda create -y -n <env_name> python=2.7 conda activate <env_name> ## Else install/locate a python 2 version ### Check python -V (maybe its python2.7) ### Maybe you have python2 or python2.7 executables ### On linux you could look for ls /usr/bin/python* python2 -m venv <env_name> source <env_name>/bin/activate # if you use linux bin/Scripts/activate # on Windows # ---- Now with Python2.7 ---- pip install -r requirements.txt # ---- Download & Move your embedding from https://nlp.stanford.edu/projects/histwords/ # to embeddings/<category> subfolders python examples.py # Outputs the similarities,2023-12-13 07:34:20,2023-12-13 13:37:38,367,https://stackoverflow.com/questions/77651770/how-to-load-the-pre-trained-word-embeddings-in-npy-files,"How to Load the pre-trained word embeddings in .npy files I'm trying to use the word embeddings pre-trained in HistWords Project by the Stanford NLP team. But when I ran the document example.py from the GitHub website , there was an error: ModuleNotFoundError: No module named 'representations.sequentialembedding' . How can I solve this problem? I've installed the ""representations"" module, but it doesn't work. The pre-trained word embeddings are of "".npy"" format, is there any Python-based method for uploading them?",load pre  train word embedding npy file  m try use word embedding pre  train histwords project stanford nlp team  run document examplepy github website  error  modulenotfounderror  module name  representationssequentialembedde   solve problem   ve instal   representation  module  not work  pre  train word embedding   npy  format  python  base method upload ,set follow shell  cd  path want store project  git clone   githubcom  williamleif  histwordsgit cd histword     set python27      python27 via conda quite easy conda create y n  envname  python27 conda activate  envname    else install  locate python 2 version    check python v  maybe python27     maybe python2 python27 executable    linux could look ls usr  bin  python  python2 m venv  envname  source  envname  bin  activate  use linux bin  scripts  activate  windows    python27   pip install r requirementstxt    download  move embed   nlpstanfordedu  project  histwords  embeddings  category  subfolder python examplespy  output similarity,load pre  train word embedding npy file  m try use word embedding pre  train histwords project stanford nlp team  run document examplepy github website  error  modulenotfounderror  module name  representationssequentialembedde   solve problem   ve instal   representation  module  not work  pre  train word embedding   npy  format  python  base method upload  set follow shell  cd  path want store project  git clone   githubcom  williamleif  histwordsgit cd histword     set python27      python27 via conda quite easy conda create y n  envname  python27 conda activate  envname    else install  locate python 2 version    check python v  maybe python27     maybe python2 python27 executable    linux could look ls usr  bin  python  python2 m venv  envname  source  envname  bin  activate  use linux bin  scripts  activate  windows    python27   pip install r requirementstxt    download  move embed   nlpstanfordedu  project  histwords  embeddings  category  subfolder python examplespy  output similarity,Implementation Issues
Logistic Regression gradually tends to predict all 0s while training on mini batches,"I am using mini-batches to train my model which is as follows SimpleModel( (embed): Embedding(vocab_size, embedding_size, max_norm=2) (model): Sequential( (0): Flatten(start_dim=1, end_dim=-1) (1): Linear(in_features=in_features, out_features=1, bias=True) ) (sig): Sigmoid() ) these are the specifics of the model and upon training through mini-batches, after 2-3 minibatches all the outputs become 0. training function looks like this while the training loop is usual def trainer(train_loader, model, optimizer, criterion): model.train() it_loss = 0 counter = 0 for data in train_loader: optimizer.zero_grad() msgs = data['msg'] targets = data['target'] out = model(msgs) print(out) loss = criterion(out, targets) loss.backward() optimizer.step() it_loss+=loss.item()*msgs.shape[0] counter+=msgs.shape[0] return it_loss/counter I have tried using various optimizer and all those things, data is not imbalanced as shown 0 3900 1 1896 Name: count, dtype: int64 what could be the possible reason and how can I solve it Edit : The output of first mini batch looks like tensor([[0.4578], [0.4569], [0.4686], . . . [0.4602], [0.4674], [0.4398]], grad_fn=<SigmoidBackward0>) while output of 4th or 5th mini batch looks like tensor([[0.0057], [0.0058], [0.0058], . . . [0.0058], [0.0057], [0.0059]], grad_fn=<SigmoidBackward0>) And furthermore it gradually becomes exact 0.","['regression', 'classification', 'binary-data', 'nlp']",2,Changed the optimizer and Loss function Used the RAdam optimizer and changed loss function to BCELoss and it worked,2023-12-11 13:50:10,2023-12-12 06:41:19,58,https://stackoverflow.com/questions/77640950/logistic-regression-gradually-tends-to-predict-all-0s-while-training-on-mini-bat,"Logistic Regression gradually tends to predict all 0s while training on mini batches I am using mini-batches to train my model which is as follows SimpleModel( (embed): Embedding(vocab_size, embedding_size, max_norm=2) (model): Sequential( (0): Flatten(start_dim=1, end_dim=-1) (1): Linear(in_features=in_features, out_features=1, bias=True) ) (sig): Sigmoid() ) these are the specifics of the model and upon training through mini-batches, after 2-3 minibatches all the outputs become 0. training function looks like this while the training loop is usual def trainer(train_loader, model, optimizer, criterion): model.train() it_loss = 0 counter = 0 for data in train_loader: optimizer.zero_grad() msgs = data['msg'] targets = data['target'] out = model(msgs) print(out) loss = criterion(out, targets) loss.backward() optimizer.step() it_loss+=loss.item()*msgs.shape[0] counter+=msgs.shape[0] return it_loss/counter I have tried using various optimizer and all those things, data is not imbalanced as shown 0 3900 1 1896 Name: count, dtype: int64 what could be the possible reason and how can I solve it Edit : The output of first mini batch looks like tensor([[0.4578], [0.4569], [0.4686], . . . [0.4602], [0.4674], [0.4398]], grad_fn=<SigmoidBackward0>) while output of 4th or 5th mini batch looks like tensor([[0.0057], [0.0058], [0.0058], . . . [0.0058], [0.0057], [0.0059]], grad_fn=<SigmoidBackward0>) And furthermore it gradually becomes exact 0.",logistic regression gradually tends predict 0s training mini batch use mini  batches train model follow simplemodel   embed   embed  vocabsize  embeddingsize  maxnorm2   model   sequential   0   flatten  startdim1  enddim1   1   linear  infeature  infeatures  outfeatures1  bias  true    sig   sigmoid    specific model upon train mini  batch  2  3 minibatche output become 0  training function look like training loop usual def trainer  trainload  model  optimizer  criterion   modeltrain   itloss  0 counter  0 data trainloader  optimizerzerograd   msg  datum   msg   target  datum   target    model  msgs  print   loss  criterion   target  lossbackward   optimizerstep   itlosslossitem    msgsshape  0  countermsgsshape  0  return itloss  counter try use various optimizer thing  datum imbalance show 0 3900 1 1896 name  count  dtype  int64 could possible reason solve edit  output first mini batch look like tensor    04578    04569    04686       04602    04674    04398    gradfn  sigmoidbackward0   output 4th 5th mini batch look like tensor    00057    00058    00058       00058    00057    00059    gradfn  sigmoidbackward0   furthermore gradually become exact 0 ,change optimizer loss function use radam optimizer change loss function bceloss work,logistic regression gradually tends predict 0s training mini batch use mini  batches train model follow simplemodel   embed   embed  vocabsize  embeddingsize  maxnorm2   model   sequential   0   flatten  startdim1  enddim1   1   linear  infeature  infeatures  outfeatures1  bias  true    sig   sigmoid    specific model upon train mini  batch  2  3 minibatche output become 0  training function look like training loop usual def trainer  trainload  model  optimizer  criterion   modeltrain   itloss  0 counter  0 data trainloader  optimizerzerograd   msg  datum   msg   target  datum   target    model  msgs  print   loss  criterion   target  lossbackward   optimizerstep   itlosslossitem    msgsshape  0  countermsgsshape  0  return itloss  counter try use various optimizer thing  datum imbalance show 0 3900 1 1896 name  count  dtype  int64 could possible reason solve edit  output first mini batch look like tensor    04578    04569    04686       04602    04674    04398    gradfn  sigmoidbackward0   output 4th 5th mini batch look like tensor    00057    00058    00058       00058    00057    00059    gradfn  sigmoidbackward0   furthermore gradually become exact 0  change optimizer loss function use radam optimizer change loss function bceloss work,Implementation Issues
Big Query - Case Statements and Apostrophe&#39;s,"Afternoon, was wondering if anyone is able to assist. Whilst writing case statements to flag particular phrases in netezza, if we came across verbatim which contained an apostrophe I would get this round this with the below example. However im struggling to recreate this on BQ and wondered if anyone has a solution? verbatim example : Doesn't have an account netezza case statement example : when lower(a.case_detail) like lower('%doesn''t have an account%') Thanks in advance for any helpful suggestions :) currently not found a solution","['sql', 'google-cloud-platform', 'google-bigquery', 'nlp']",1,"You have at least two options: escape the ' using backslash \ : '%doesn\'t have an account%' use double quotes to start and end the string literal: ""%doesn't have an account%""",2023-12-11 12:48:43,2023-12-11 13:30:12,106,https://stackoverflow.com/questions/77639714/big-query-case-statements-and-apostrophes,"Big Query - Case Statements and Apostrophe&#39;s Afternoon, was wondering if anyone is able to assist. Whilst writing case statements to flag particular phrases in netezza, if we came across verbatim which contained an apostrophe I would get this round this with the below example. However im struggling to recreate this on BQ and wondered if anyone has a solution? verbatim example : Doesn't have an account netezza case statement example : when lower(a.case_detail) like lower('%doesn''t have an account%') Thanks in advance for any helpful suggestions :) currently not found a solution",big query  case statement apostrophe   39  afternoon  wonder anyone able assist  whilst write case statement flag particular phrase netezza  come across verbatim contain apostrophe would get round example  however i m struggle recreate bq wonder anyone solution  verbatim example  not account netezza case statement example  low  acasedetail  like low     account    thank advance helpful suggestion   currently find solution,least two option  escape  use backslash     doesnt account   use double quote start end string literal     not account  ,big query  case statement apostrophe   39  afternoon  wonder anyone able assist  whilst write case statement flag particular phrase netezza  come across verbatim contain apostrophe would get round example  however i m struggle recreate bq wonder anyone solution  verbatim example  not account netezza case statement example  low  acasedetail  like low     account    thank advance helpful suggestion   currently find solution least two option  escape  use backslash     doesnt account   use double quote start end string literal     not account  ,Task-Specific Queries
Insert dots/points in messy string for textual analysis in python,"I am given with a long, messy string that lacks sentence structures , i.e., the string does not consistently contain dots/points . Therefore, I am currently unable to break-down the long string into sentences, which is required for my textual analysis . The following example best describes what I am given with and what I would need as output. example_string = ""Football is the world's most popular sport Played on rectangular fields, two teams of eleven players each compete to score goals One of the most famous teams is Real Madrid."" output_string = ""Football is the world's most popular sport. Played on rectangular fields, two teams of eleven players each compete to score goals. One of the most famous teams is Real Madrid."" I was first thinking of putting a dot/point whenever there is none between a lower-case word and a capitalized word. However, given certain words and especially names may start with a capital letter, I would incorrectly add the dot/point (e.g., in the example, I would add a dot/point before ""Real Madrid"") Any help is appreciated. Thank you!","['python', 'nlp', 'sentence']",2,"How about leveraging an LLM (via an API) for that? Quick test run with GPT-4: Prompt Separate the following string into sentences. List each sentence with a bullet point: ""Football is the world's most popular sport Played on rectangular fields, two teams of eleven players each compete to score goals One of the most famous teams is Real Madrid."" Output - Football is the world's most popular sport. - Played on rectangular fields, two teams of eleven players each compete to score goals. - One of the most famous teams is Real Madrid.",2023-12-07 14:57:47,2023-12-07 15:21:31,56,https://stackoverflow.com/questions/77621022/insert-dots-points-in-messy-string-for-textual-analysis-in-python,"Insert dots/points in messy string for textual analysis in python I am given with a long, messy string that lacks sentence structures , i.e., the string does not consistently contain dots/points . Therefore, I am currently unable to break-down the long string into sentences, which is required for my textual analysis . The following example best describes what I am given with and what I would need as output. example_string = ""Football is the world's most popular sport Played on rectangular fields, two teams of eleven players each compete to score goals One of the most famous teams is Real Madrid."" output_string = ""Football is the world's most popular sport. Played on rectangular fields, two teams of eleven players each compete to score goals. One of the most famous teams is Real Madrid."" I was first thinking of putting a dot/point whenever there is none between a lower-case word and a capitalized word. However, given certain words and especially names may start with a capital letter, I would incorrectly add the dot/point (e.g., in the example, I would add a dot/point before ""Real Madrid"") Any help is appreciated. Thank you!",insert dot  point messy string textual analysis python give long  messy string lack sentence structure  ie  string consistently contain dot  point  therefore  currently unable break  down long string sentence  require textual analysis  follow example good describe give would need output  examplestre    football world s popular sport play rectangular field  two team eleven player compete score goal one famous team real madrid   outputstring    football world s popular sport  play rectangular field  two team eleven player compete score goal  one famous team real madrid   first think put dot  point whenever none low  case word capitalize word  however  give certain word especially name may start capital letter  would incorrectly add dot  point  eg  example  would add dot  point   real madrid   help appreciate  thank ,leverage llm  via api   quick test run gpt4  prompt separate follow string sentence  list sentence bullet point    football world s popular sport play rectangular field  two team eleven player compete score goal one famous team real madrid   output  football world s popular sport   play rectangular field  two team eleven player compete score goal   one famous team real madrid ,insert dot  point messy string textual analysis python give long  messy string lack sentence structure  ie  string consistently contain dot  point  therefore  currently unable break  down long string sentence  require textual analysis  follow example good describe give would need output  examplestre    football world s popular sport play rectangular field  two team eleven player compete score goal one famous team real madrid   outputstring    football world s popular sport  play rectangular field  two team eleven player compete score goal  one famous team real madrid   first think put dot  point whenever none low  case word capitalize word  however  give certain word especially name may start capital letter  would incorrectly add dot  point  eg  example  would add dot  point   real madrid   help appreciate  thank  leverage llm  via api   quick test run gpt4  prompt separate follow string sentence  list sentence bullet point    football world s popular sport play rectangular field  two team eleven player compete score goal one famous team real madrid   output  football world s popular sport   play rectangular field  two team eleven player compete score goal   one famous team real madrid ,Library/Tool-Based Queries
Why does OpenNLP CLI output &quot;SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot; on Windows?,"Based on Apache OpenNLP documentation, I downloaded binary version of OpenNLP, then set JAVA_HOME and OPENNLP_HOME. When I run opennlp command it faced to below exception: SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. I used windows operating system and tried this command on different systems with different versions of Java, but always this exception is shown. It is too odd, because I've searched and no one faced this issued before.","['java', 'nlp', 'slf4j', 'opennlp']",1,"You found a bug. Congratulation! I created OPENNLP-1527 for it. Meanwhile, you can override the content of the opennlp.bat file with @ECHO off REM # Licensed to the Apache Software Foundation (ASF) under one REM # or more contributor license agreements. See the NOTICE file REM # distributed with this work for additional information REM # regarding copyright ownership. The ASF licenses this file REM # to you under the Apache License, Version 2.0 (the REM # ""License""); you may not use this file except in compliance REM # with the License. You may obtain a copy of the License at REM # REM # http://www.apache.org/licenses/LICENSE-2.0 REM # REM # Unless required by applicable law or agreed to in writing, REM # software distributed under the License is distributed on an REM # # ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY REM # KIND, either express or implied. See the License for the REM # specific language governing permissions and limitations REM # under the License. REM # Note: Do not output anything in this script file, any output REM # may be inadvertantly placed in any output files if REM # output redirection is used. SETLOCAL IF ""%JAVA_CMD%"" == """" ( IF ""%JAVA_HOME%"" == """" ( SET JAVA_CMD=java ) ELSE ( REM # Keep JAVA_HOME to short-name without spaces FOR %%A IN (""%JAVA_HOME%"") DO SET JAVA_CMD=%%~sfA\bin\java ) ) REM remove HEAP variable SET HEAP= IF not ""%JAVA_HEAP%"" == """" ( SET HEAP=""-Xmx%JAVA_HEAP%"" ) REM # Should work with Windows XP and greater. If not, specify the path to where it is installed. IF ""%OPENNLP_HOME%"" == """" ( SET OPENNLP_HOME=%~sp0.. ) ELSE ( REM # Keep OPENNLP_HOME to short-name without spaces FOR %%A IN (""%OPENNLP_HOME%"") DO SET OPENNLP_HOME=%%~sfA ) echo Environment echo JAVA_HOME=%JAVA_HOME% echo OPENNLP_HOME=%OPENNLP_HOME% REM Add lib directory to the classpath SET CLASSPATH=""%OPENNLP_HOME%\lib\*"" echo CLASSPATH=%CLASSPATH% %JAVA_CMD% %HEAP% ""-Dlog4j.configurationFile=%OPENNLP_HOME%\conf\log4j2.xml"" -cp %CLASSPATH% opennlp.tools.cmdline.CLI %* ENDLOCAL This will correctly append the classes contained in the lib folder to the classpath and the cli will work as expected.",2023-12-06 17:47:17,2023-12-09 07:47:14,173,https://stackoverflow.com/questions/77615264/why-does-opennlp-cli-output-slf4j-failed-to-load-class-org-slf4j-impl-staticl,"Why does OpenNLP CLI output &quot;SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot; on Windows? Based on Apache OpenNLP documentation, I downloaded binary version of OpenNLP, then set JAVA_HOME and OPENNLP_HOME. When I run opennlp command it faced to below exception: SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. I used windows operating system and tried this command on different systems with different versions of Java, but always this exception is shown. It is too odd, because I've searched and no one faced this issued before.",opennlp cli output  quot  slf4j  fail load class  quot  orgslf4jimpl  staticloggerbinder  quot  windows  base apache opennlp documentation  download binary version opennlp  set javahome opennlphome  run opennlp command face exception  slf4j  fail load class   orgslf4jimpl  staticloggerbinder   slf4j  default no  operation  nop  logger implementation slf4j  see http  wwwslf4jorg  codeshtml  staticloggerbinder detail  use window operating system try command different system different version java  always exception show  odd   ve search one face issue ,find bug  congratulation  create opennlp1527  meanwhile  override content opennlpbat file  echo rem  license apache software foundation  asf  one rem  contributor license agreement  see notice file rem  distribute work additional information rem  regard copyright ownership  asf license file rem  apache license  version 20  rem    license    may use file except compliance rem  license  may obtain copy license rem  rem  http  wwwapacheorg  license  license20 rem  rem  unless require applicable law agree writing  rem  software distribute license distribute rem      basis  without warranty condition rem  kind  either express imply  see license rem  specific language govern permission limitation rem  license  rem  note  output anything script file  output rem  may inadvertantly place output file rem  output redirection use  setlocal    javacmd            javahome         set javacmd  java  else  rem  keep javahome short  name without space       javahome    set javacmd   sfabinjava   rem remove heap variable set heap    javaheap         set heap  xmx  javaheap    rem  work windows xp great   specify path instal     opennlphome         set opennlphome  sp0   else  rem  keep opennlphome short  name without space       opennlphome    set opennlphome   sfa  echo environment echo javahome  javahome  echo opennlphome  opennlphome  rem add lib directory classpath set classpath   opennlphome  lib   echo classpath  classpath   javacmd   heap    dlog4jconfigurationfile  opennlphome  conflog4j2xml  cp  classpath  opennlptoolscmdline  cli   endlocal correctly append class contain lib folder classpath cli work expect ,opennlp cli output  quot  slf4j  fail load class  quot  orgslf4jimpl  staticloggerbinder  quot  windows  base apache opennlp documentation  download binary version opennlp  set javahome opennlphome  run opennlp command face exception  slf4j  fail load class   orgslf4jimpl  staticloggerbinder   slf4j  default no  operation  nop  logger implementation slf4j  see http  wwwslf4jorg  codeshtml  staticloggerbinder detail  use window operating system try command different system different version java  always exception show  odd   ve search one face issue  find bug  congratulation  create opennlp1527  meanwhile  override content opennlpbat file  echo rem  license apache software foundation  asf  one rem  contributor license agreement  see notice file rem  distribute work additional information rem  regard copyright ownership  asf license file rem  apache license  version 20  rem    license    may use file except compliance rem  license  may obtain copy license rem  rem  http  wwwapacheorg  license  license20 rem  rem  unless require applicable law agree writing  rem  software distribute license distribute rem      basis  without warranty condition rem  kind  either express imply  see license rem  specific language govern permission limitation rem  license  rem  note  output anything script file  output rem  may inadvertantly place output file rem  output redirection use  setlocal    javacmd            javahome         set javacmd  java  else  rem  keep javahome short  name without space       javahome    set javacmd   sfabinjava   rem remove heap variable set heap    javaheap         set heap  xmx  javaheap    rem  work windows xp great   specify path instal     opennlphome         set opennlphome  sp0   else  rem  keep opennlphome short  name without space       opennlphome    set opennlphome   sfa  echo environment echo javahome  javahome  echo opennlphome  opennlphome  rem add lib directory classpath set classpath   opennlphome  lib   echo classpath  classpath   javacmd   heap    dlog4jconfigurationfile  opennlphome  conflog4j2xml  cp  classpath  opennlptoolscmdline  cli   endlocal correctly append class contain lib folder classpath cli work expect ,Implementation Issues
How to make stanza lemmatizer to return just the lemma instead of a dictionary?,"I'm implementing stanza's lemmatizer because it works well with spanish texts but the lemmatizer retuns a whole dictionary with ID and other characteristics I don't care about for the time being. I checked the ""processors"" in the pipeline but I don't seem to find and example where I just get the sence with the lemmatized text instead of the dictionary. This is what I have: stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False) stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True) stNLP('me hubiera gustado mas “sincronia” con la primaria') Output: [ [ { ""id"": 1, ""text"": ""me"", ""lemma"": ""yo"", ""upos"": ""PRON"", ""xpos"": ""pp1cs000"", ""feats"": ""Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs"", ""start_char"": 0, ""end_char"": 2 }, .... Of course when I try to lemmatize my document it returns a lot of text I don't need at the moment, how can I just obtain the lemma? I'm aware I could possibly extract the word from the dictionary but it takes a lot of time as it is, what I want to avoid is giving the fuction extra work. Thank you in advance.","['python', 'nlp', 'stanford-nlp', 'lemmatization']",1,"I'm not entirely sure yet, but from what I've seen, it appears that Stanza's pipeline generates a nested structure in which each sentence is a list of tokens, and each token is akin to a dictionary containing various attributes such as ID, text, lemma, and so on. It is easy to extract the lemmas by navigating this nested structure. Here's how I've done it. stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False) stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True) doc = stNLP('me hubiera gustado mas “sincronia” con la primaria') lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words] Note: As of the time of writing, the version of Stanza being used is stanza==1.7.0",2023-12-05 23:30:30,2023-12-06 00:09:19,432,https://stackoverflow.com/questions/77609784/how-to-make-stanza-lemmatizer-to-return-just-the-lemma-instead-of-a-dictionary,"How to make stanza lemmatizer to return just the lemma instead of a dictionary? I'm implementing stanza's lemmatizer because it works well with spanish texts but the lemmatizer retuns a whole dictionary with ID and other characteristics I don't care about for the time being. I checked the ""processors"" in the pipeline but I don't seem to find and example where I just get the sence with the lemmatized text instead of the dictionary. This is what I have: stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False) stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True) stNLP('me hubiera gustado mas “sincronia” con la primaria') Output: [ [ { ""id"": 1, ""text"": ""me"", ""lemma"": ""yo"", ""upos"": ""PRON"", ""xpos"": ""pp1cs000"", ""feats"": ""Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs"", ""start_char"": 0, ""end_char"": 2 }, .... Of course when I try to lemmatize my document it returns a lot of text I don't need at the moment, how can I just obtain the lemma? I'm aware I could possibly extract the word from the dictionary but it takes a lot of time as it is, what I want to avoid is giving the fuction extra work. Thank you in advance.",make stanza lemmatizer return lemma instead dictionary   m implement stanza s lemmatizer work well spanish text lemmatizer retun whole dictionary id characteristic not care time  check   processor  pipeline not seem find example get sence lemmatize text instead dictionary   stanzadownload   es   packageancora   processorstokenize  mwt  pos  lemma   verbose  false  stnlp  stanza  pipeline  processorstokenize  mwt  pos  lemma   langes   usegpu  true  stnlp   i hubiera gustado mas  sincronia  con la primaria   output       i d   1    text         lemma     yo     upos     pron     xpos     pp1cs000     feat     case  datnumber  singperson1prepcase  nprprontype  prs     startchar   0    endchar   2    course try lemmatize document return lot text not need moment  obtain lemma   m aware could possibly extract word dictionary take lot time  want avoid give fuction extra work  thank advance , m entirely sure yet   ve see  appear stanza s pipeline generate nest structure sentence list token  token akin dictionary contain various attribute id  text  lemma   easy extract lemmas navigate nest structure  s  ve do  stanzadownload   es   packageancora   processorstokenize  mwt  pos  lemma   verbose  false  stnlp  stanza  pipeline  processorstokenize  mwt  pos  lemma   langes   usegpu  true  doc  stnlp   i hubiera gustado mas  sincronia  con la primaria   lemma   wordlemma docitertoken   word tword  note  time writing  version stanza use stanza170,make stanza lemmatizer return lemma instead dictionary   m implement stanza s lemmatizer work well spanish text lemmatizer retun whole dictionary id characteristic not care time  check   processor  pipeline not seem find example get sence lemmatize text instead dictionary   stanzadownload   es   packageancora   processorstokenize  mwt  pos  lemma   verbose  false  stnlp  stanza  pipeline  processorstokenize  mwt  pos  lemma   langes   usegpu  true  stnlp   i hubiera gustado mas  sincronia  con la primaria   output       i d   1    text         lemma     yo     upos     pron     xpos     pp1cs000     feat     case  datnumber  singperson1prepcase  nprprontype  prs     startchar   0    endchar   2    course try lemmatize document return lot text not need moment  obtain lemma   m aware could possibly extract word dictionary take lot time  want avoid give fuction extra work  thank advance   m entirely sure yet   ve see  appear stanza s pipeline generate nest structure sentence list token  token akin dictionary contain various attribute id  text  lemma   easy extract lemmas navigate nest structure  s  ve do  stanzadownload   es   packageancora   processorstokenize  mwt  pos  lemma   verbose  false  stnlp  stanza  pipeline  processorstokenize  mwt  pos  lemma   langes   usegpu  true  doc  stnlp   i hubiera gustado mas  sincronia  con la primaria   lemma   wordlemma docitertoken   word tword  note  time writing  version stanza use stanza170,Implementation Issues
How to run a NLP+Transformers LLM on low memory GPUs?,"I am trying to load an AI pre-trained model, from intel on hugging face, I have used Colab its resources exceeded, used Kaggle resources increased, used paperspace, which showing me an error: The kernel for Text_Generation.ipynb appears to have died. It will restart automatically. this is the model load script: import transformers model_name = 'Intel/neural-chat-7b-v3-1' model = transformers.AutoModelForCausalLM.from_pretrained(model_name) tokenizer = transformers.AutoTokenizer.from_pretrained(model_name) def generate_response(system_input, user_input): # Format the input using the provided template prompt = f""### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"" # Tokenize and encode the prompt inputs = tokenizer.encode(prompt, return_tensors=""pt"", add_special_tokens=False) # Generate a response outputs = model.generate(inputs, max_length=1000, num_return_sequences=1) response = tokenizer.decode(outputs[0], skip_special_tokens=True) # Extract only the assistant's response return response.split(""### Assistant:\n"")[-1] # Example usage system_input = ""You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer."" user_input = ""calculate 100 + 520 + 60"" response = generate_response(system_input, user_input) print(response) # expected response """""" To calculate the sum of 100, 520, and 60, we will follow these steps: 1. Add the first two numbers: 100 + 520 2. Add the result from step 1 to the third number: (100 + 520) + 60 Step 1: Add 100 and 520 100 + 520 = 620 Step 2: Add the result from step 1 to the third number (60) (620) + 60 = 680 So, the sum of 100, 520, and 60 is 680. """""" My purpose is to load this pretrained model, I have done some research on my end I have find some solutions but not working with me, download packages using cuda instead of pip","['python', 'nlp', 'gpu', 'huggingface-transformers', 'huggingface-tokenizers']",1,"I would recommend looking into model quantization as this is one of the approaches which specifically addresses this type of problem, of loading a large model for inference. TheBloke has provided a quantized version of this model which is available here: neural-chat-7B-v3-1-AWQ . To use this, you'll need to use AutoAWQ, and as per Hugging Face in this notebook , for Colab you need to install an earlier version given Colab's CUDA version. You should also make sure your model is using GPU, not CPU, by adding .cuda() to the input tensor after it is generated: !pip install -q transformers accelerate !pip install -q -U https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.6/autoawq-0.1.6+cu118-cp310-cp310-linux_x86_64.whl import torch from awq import AutoAWQForCausalLM from transformers import AutoTokenizer model_name = 'TheBloke/neural-chat-7B-v3-1-AWQ' ### Use AutoAWQ and from quantized instead of transformers here model = AutoAWQForCausalLM.from_quantized(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) def generate_response(system_input, user_input): # Format the input using the provided template prompt = f""### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"" ### ADD .cuda() inputs = tokenizer.encode(prompt, return_tensors=""pt"", add_special_tokens=False).cuda() # Generate a response outputs = model.generate(inputs, max_length=1000, num_return_sequences=1) response = tokenizer.decode(outputs[0], skip_special_tokens=True) # Extract only the assistant's response return response.split(""### Assistant:\n"")[-1]",2023-12-03 11:32:25,2023-12-04 14:42:42,1078,https://stackoverflow.com/questions/77594086/how-to-run-a-nlptransformers-llm-on-low-memory-gpus,"How to run a NLP+Transformers LLM on low memory GPUs? I am trying to load an AI pre-trained model, from intel on hugging face, I have used Colab its resources exceeded, used Kaggle resources increased, used paperspace, which showing me an error: The kernel for Text_Generation.ipynb appears to have died. It will restart automatically. this is the model load script: import transformers model_name = 'Intel/neural-chat-7b-v3-1' model = transformers.AutoModelForCausalLM.from_pretrained(model_name) tokenizer = transformers.AutoTokenizer.from_pretrained(model_name) def generate_response(system_input, user_input): # Format the input using the provided template prompt = f""### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"" # Tokenize and encode the prompt inputs = tokenizer.encode(prompt, return_tensors=""pt"", add_special_tokens=False) # Generate a response outputs = model.generate(inputs, max_length=1000, num_return_sequences=1) response = tokenizer.decode(outputs[0], skip_special_tokens=True) # Extract only the assistant's response return response.split(""### Assistant:\n"")[-1] # Example usage system_input = ""You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer."" user_input = ""calculate 100 + 520 + 60"" response = generate_response(system_input, user_input) print(response) # expected response """""" To calculate the sum of 100, 520, and 60, we will follow these steps: 1. Add the first two numbers: 100 + 520 2. Add the result from step 1 to the third number: (100 + 520) + 60 Step 1: Add 100 and 520 100 + 520 = 620 Step 2: Add the result from step 1 to the third number (60) (620) + 60 = 680 So, the sum of 100, 520, and 60 is 680. """""" My purpose is to load this pretrained model, I have done some research on my end I have find some solutions but not working with me, download packages using cuda instead of pip",run nlptransformer llm low memory gpu  try load ai pre  trained model  intel hug face  use colab resource exceed  use kaggle resource increase  use paperspace  show error  kernel textgenerationipynb appear die  restart automatically  model load script  import transformer modelname   intel  neural  chat7b  v3  1  model  transformer  automodelforcausallmfrompretraine  modelname  tokenizer  transformer  autotokenizerfrompretraine  modelname  def generateresponse  systeminput  userinput    format input use provide template prompt  f     system  n  systeminput  n    user  n  userinput  n    assistant  n   tokenize encode prompt input  tokenizerencode  prompt  returntensors  pt   addspecialtoken  false   generate response output  modelgenerate  input  maxlength1000  numreturnsequences1  response  tokenizerdecode  output  0   skipspecialtoken  true   extract assistant s response return responsesplit       assistant  n    1   example usage systeminput    math expert assistant  mission help user understand solve various math problem  provide step  by  step solution  explain reasoning give correct answer   userinput    calculate 100  520  60  response  generateresponse  systeminput  userinput  print  response   expect response     calculate sum 100  520  60  follow step  1  add first two number  100  520 2  add result step 1 third number   100  520   60 step 1  add 100 520 100  520  620 step 2  add result step 1 third number  60   620   60  680  sum 100  520  60 680      purpose load pretraine model  do research end find solution work  download package use cuda instead pip,would recommend look model quantization one approach specifically addresses type problem  load large model inference  thebloke provide quantize version model available  neural  chat7b  v3  1  awq  use  will need use autoawq  per hugging face notebook  colab need install early version give colab s cuda version  also make sure model use gpu  cpu  add cuda   input tensor generate   pip install q transformer accelerate  pip install q u   githubcom  casper  hansen  autoawq  release  download  v016  autoawq016cu118  cp310  cp310  linuxx8664whl import torch awq import autoawqforcausallm transformer import autotokenizer modelname   thebloke  neural  chat7b  v3  1  awq     use autoawq quantize instead transformer model  autoawqforcausallmfromquantize  modelname  tokenizer  autotokenizerfrompretraine  modelname  def generateresponse  systeminput  userinput    format input use provide template prompt  f     system  n  systeminput  n    user  n  userinput  n    assistant  n     add cuda   input  tokenizerencode  prompt  returntensors  pt   addspecialtoken  false  cuda    generate response output  modelgenerate  input  maxlength1000  numreturnsequences1  response  tokenizerdecode  output  0   skipspecialtoken  true   extract assistant s response return responsesplit       assistant  n    1 ,run nlptransformer llm low memory gpu  try load ai pre  trained model  intel hug face  use colab resource exceed  use kaggle resource increase  use paperspace  show error  kernel textgenerationipynb appear die  restart automatically  model load script  import transformer modelname   intel  neural  chat7b  v3  1  model  transformer  automodelforcausallmfrompretraine  modelname  tokenizer  transformer  autotokenizerfrompretraine  modelname  def generateresponse  systeminput  userinput    format input use provide template prompt  f     system  n  systeminput  n    user  n  userinput  n    assistant  n   tokenize encode prompt input  tokenizerencode  prompt  returntensors  pt   addspecialtoken  false   generate response output  modelgenerate  input  maxlength1000  numreturnsequences1  response  tokenizerdecode  output  0   skipspecialtoken  true   extract assistant s response return responsesplit       assistant  n    1   example usage systeminput    math expert assistant  mission help user understand solve various math problem  provide step  by  step solution  explain reasoning give correct answer   userinput    calculate 100  520  60  response  generateresponse  systeminput  userinput  print  response   expect response     calculate sum 100  520  60  follow step  1  add first two number  100  520 2  add result step 1 third number   100  520   60 step 1  add 100 520 100  520  620 step 2  add result step 1 third number  60   620   60  680  sum 100  520  60 680      purpose load pretraine model  do research end find solution work  download package use cuda instead pip would recommend look model quantization one approach specifically addresses type problem  load large model inference  thebloke provide quantize version model available  neural  chat7b  v3  1  awq  use  will need use autoawq  per hugging face notebook  colab need install early version give colab s cuda version  also make sure model use gpu  cpu  add cuda   input tensor generate   pip install q transformer accelerate  pip install q u   githubcom  casper  hansen  autoawq  release  download  v016  autoawq016cu118  cp310  cp310  linuxx8664whl import torch awq import autoawqforcausallm transformer import autotokenizer modelname   thebloke  neural  chat7b  v3  1  awq     use autoawq quantize instead transformer model  autoawqforcausallmfromquantize  modelname  tokenizer  autotokenizerfrompretraine  modelname  def generateresponse  systeminput  userinput    format input use provide template prompt  f     system  n  systeminput  n    user  n  userinput  n    assistant  n     add cuda   input  tokenizerencode  prompt  returntensors  pt   addspecialtoken  false  cuda    generate response output  modelgenerate  input  maxlength1000  numreturnsequences1  response  tokenizerdecode  output  0   skipspecialtoken  true   extract assistant s response return responsesplit       assistant  n    1 ,Implementation Issues
Attention Layer changing Batch Size at inference,"I have trained seq-to-seq model using Encoder-Decoder architecture. I'm trying to produce an output sequence given an input context, and I am trying to do that on a batch of input context vectors. I have a Self Attention layer before the final output in the Decoder and it seems to be changing batch shape or not getting the shape correctly, and throwing an error. It works fine if I just infer on individual sample one or a batch size of 1 but practically it will take a long in production and infer on thousands of input context vectors. So, I need your help in debugging the error and to implement a better way of producing the output sequence that is computationally optimized. Below is my implementation: ### Define Inference Encoder def define_inference_encoder(input_shape): encoder_input = Input(shape=input_shape, name='en_input_layer') ### First Bidirectional GRU Layer bidi_gru1 = Bidirectional(GRU(160, return_sequences=True), name='en_bidirect_gru1') gru1_out = bidi_gru1(encoder_input) gru1_out = Dropout(0.46598303573163413, name='bidirect_gru1_dropout')(gru1_out) ### Second GRU Layer # hp_units_2 = hp.Int('enc_lstm2', min_value=32, max_value=800, step=32) gru2 = GRU(hsize, return_sequences=True, return_state=True, name='en_gru2_layer') gru2_out, gru2_states = gru2(gru1_out) encoder_model = Model(inputs=encoder_input, outputs=[gru2_out, gru2_states]) return encoder_model ### Define Inference Decoder def define_inf_decoder(context_vec, input_shape): decoder_input = Input(shape=input_shape) decoder_state_input = Input(shape=(hsize,)) de_gru1 = GRU(hsize, return_sequences=True, return_state=True, name='de_gru1_layer') de_gru1_out, de_state_out = de_gru1(decoder_input, initial_state=decoder_state_input) attn_layer = Attention(use_scale=True, name='attn_layer') attn_out = attn_layer([de_gru1_out, context_vec]) attn_added = Concatenate(name='attn_source_concat_layer')([de_gru1_out, attn_out]) attn_dense_layer = Dense(736, name='tanh_dense_layer', activation='tanh') h_hat = attn_dense_layer(attn_added) ### Output Layer preds = Dense(1, name='output_layer')(h_hat) decoder_model = Model(inputs=[decoder_input, decoder_state_input], outputs=[preds, de_state_out]) return decoder_model def set_weights(untrained_model, trained_model): trained_layers = [l.name for l in trained_model.layers] print(f""No. of trained layers: {len(trained_layers)}"") for l in untrained_model.layers: if l.name in trained_layers: trained_wts = trained_model.get_layer(l.name).get_weights() if len(trained_wts)>0: untrained_model.get_layer(l.name).set_weights(trained_wts) print(f""Layer {l.name} weight set"") return untrained_model Generate output sequence: inference_encoder = define_inference_encoder((12, 1)) inference_encoder = set_weights(inference_encoder, tuned_model) for (ex_context, ex_target_in), ex_target_out in test_ds.take(1): print(ex_context.shape, ex_target_in.shape) ### (64, 12, 1) (64, 3, 1) test_context, test_states = inference_encoder.predict(tf.reshape(ex_context, shape=(-1,seq_len, 1))) print(test_context.shape, test_states.shape) ### (64, 12, 256) (64, 256) inf_decoder = define_inf_decoder(test_context, (1,1)) inf_decoder = set_weights(inf_decoder, tuned_model) dec_inp = tf.reshape(ex_context[:,-1], shape=(-1,1,1)) dec_inp.shape ### (64,1,1) test_inf_decoder_out = inf_decoder.predict([dec_inp, test_states]) Error: ValueError: Exception encountered when calling layer 'attn_layer' (type Attention). Dimensions must be equal, but are 32 and 64 for '{{node model_7/attn_layer/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=true](model_7/de_gru1_layer/PartitionedCall:1, model_7/15181)' with input shapes: [32,1,256], [64,12,256]. Call arguments received by layer 'attn_layer' (type Attention): • inputs=['tf.Tensor(shape=(32, 1, 256), dtype=float32)', 'tf.Tensor(shape=(64, 12, 256), >dtype=float32)'] • mask=None • training=False • return_attention_scores=False • use_causal_mask=False What I don't understand is how attn_layer is getting a batch size of 32 when I'm passing the inputs with the batch size of 64. It works fine when I work with the batch size of 1. What am I doing wrong?","['tensorflow', 'keras', 'deep-learning', 'nlp', 'sequence']",2,"I solved it by replacing .predict with predict_on_batch . I also found out that predict method treat each sample individually so when passing more than one sample it creates problem for the attention layer as it gets more than one sample's context vector from the encoder to calculate the attention weights for a single sample and would be defaulting to the batch_size of 32 which might be causing the error when the context vector's batch_size is 64. So, basically changing it to predict_on_batch for more than one sample works like a charm.",2023-12-01 16:56:34,2023-12-18 01:21:11,299,https://stackoverflow.com/questions/77586902/attention-layer-changing-batch-size-at-inference,"Attention Layer changing Batch Size at inference I have trained seq-to-seq model using Encoder-Decoder architecture. I'm trying to produce an output sequence given an input context, and I am trying to do that on a batch of input context vectors. I have a Self Attention layer before the final output in the Decoder and it seems to be changing batch shape or not getting the shape correctly, and throwing an error. It works fine if I just infer on individual sample one or a batch size of 1 but practically it will take a long in production and infer on thousands of input context vectors. So, I need your help in debugging the error and to implement a better way of producing the output sequence that is computationally optimized. Below is my implementation: ### Define Inference Encoder def define_inference_encoder(input_shape): encoder_input = Input(shape=input_shape, name='en_input_layer') ### First Bidirectional GRU Layer bidi_gru1 = Bidirectional(GRU(160, return_sequences=True), name='en_bidirect_gru1') gru1_out = bidi_gru1(encoder_input) gru1_out = Dropout(0.46598303573163413, name='bidirect_gru1_dropout')(gru1_out) ### Second GRU Layer # hp_units_2 = hp.Int('enc_lstm2', min_value=32, max_value=800, step=32) gru2 = GRU(hsize, return_sequences=True, return_state=True, name='en_gru2_layer') gru2_out, gru2_states = gru2(gru1_out) encoder_model = Model(inputs=encoder_input, outputs=[gru2_out, gru2_states]) return encoder_model ### Define Inference Decoder def define_inf_decoder(context_vec, input_shape): decoder_input = Input(shape=input_shape) decoder_state_input = Input(shape=(hsize,)) de_gru1 = GRU(hsize, return_sequences=True, return_state=True, name='de_gru1_layer') de_gru1_out, de_state_out = de_gru1(decoder_input, initial_state=decoder_state_input) attn_layer = Attention(use_scale=True, name='attn_layer') attn_out = attn_layer([de_gru1_out, context_vec]) attn_added = Concatenate(name='attn_source_concat_layer')([de_gru1_out, attn_out]) attn_dense_layer = Dense(736, name='tanh_dense_layer', activation='tanh') h_hat = attn_dense_layer(attn_added) ### Output Layer preds = Dense(1, name='output_layer')(h_hat) decoder_model = Model(inputs=[decoder_input, decoder_state_input], outputs=[preds, de_state_out]) return decoder_model def set_weights(untrained_model, trained_model): trained_layers = [l.name for l in trained_model.layers] print(f""No. of trained layers: {len(trained_layers)}"") for l in untrained_model.layers: if l.name in trained_layers: trained_wts = trained_model.get_layer(l.name).get_weights() if len(trained_wts)>0: untrained_model.get_layer(l.name).set_weights(trained_wts) print(f""Layer {l.name} weight set"") return untrained_model Generate output sequence: inference_encoder = define_inference_encoder((12, 1)) inference_encoder = set_weights(inference_encoder, tuned_model) for (ex_context, ex_target_in), ex_target_out in test_ds.take(1): print(ex_context.shape, ex_target_in.shape) ### (64, 12, 1) (64, 3, 1) test_context, test_states = inference_encoder.predict(tf.reshape(ex_context, shape=(-1,seq_len, 1))) print(test_context.shape, test_states.shape) ### (64, 12, 256) (64, 256) inf_decoder = define_inf_decoder(test_context, (1,1)) inf_decoder = set_weights(inf_decoder, tuned_model) dec_inp = tf.reshape(ex_context[:,-1], shape=(-1,1,1)) dec_inp.shape ### (64,1,1) test_inf_decoder_out = inf_decoder.predict([dec_inp, test_states]) Error: ValueError: Exception encountered when calling layer 'attn_layer' (type Attention). Dimensions must be equal, but are 32 and 64 for '{{node model_7/attn_layer/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=true](model_7/de_gru1_layer/PartitionedCall:1, model_7/15181)' with input shapes: [32,1,256], [64,12,256]. Call arguments received by layer 'attn_layer' (type Attention): • inputs=['tf.Tensor(shape=(32, 1, 256), dtype=float32)', 'tf.Tensor(shape=(64, 12, 256), >dtype=float32)'] • mask=None • training=False • return_attention_scores=False • use_causal_mask=False What I don't understand is how attn_layer is getting a batch size of 32 when I'm passing the inputs with the batch size of 64. It works fine when I work with the batch size of 1. What am I doing wrong?",attention layer change batch size inference train seq  to  seq model use encoder  decoder architecture   m try produce output sequence give input context  try batch input context vector  self attention layer final output decoder seem change batch shape get shape correctly  throw error  work fine infer individual sample one batch size 1 practically take long production infer thousand input context vector   need help debug error implement well way produce output sequence computationally optimize  implementation     define inference encoder def defineinferenceencoder  inputshape   encoderinput  input  shape  inputshape  nameeninputlayer      first bidirectional gru layer bidigru1  bidirectional  gru  160  returnsequence  true   nameenbidirectgru1   gru1out  bidigru1  encoderinput  gru1out  dropout  046598303573163413  namebidirectgru1dropout    gru1out     second gru layer  hpunits2  hp  int   enclstm2   minvalue32  maxvalue800  step32  gru2  gru  hsize  returnsequence  true  returnstate  true  nameengru2layer   gru2out  gru2state  gru2  gru1out  encodermodel  model  input  encoderinput  outputs  gru2out  gru2state   return encodermodel    define inference decoder def defineinfdecoder  contextvec  inputshape   decoderinput  input  shape  inputshape  decoderstateinput  input  shape  hsize    degru1  gru  hsize  returnsequence  true  returnstate  true  namedegru1layer   degru1out  destateout  degru1  decoderinput  initialstate  decoderstateinput  attnlayer  attention  usescale  true  nameattnlayer   attnout  attnlayer   degru1out  contextvec   attnadde  concatenate  nameattnsourceconcatlayer     degru1out  attnout   attndenselayer  dense  736  nametanhdenselayer   activationtanh   hhat  attndenselayer  attnadde     output layer pred  dense  1  nameoutputlayer    hhat  decodermodel  model  inputs  decoderinput  decoderstateinput   outputs  pred  destateout   return decodermodel def setweight  untrainedmodel  trainedmodel   trainedlayer   lname l trainedmodellayers  print  f   train layer   len  trainedlayer     l untrainedmodellayers  lname trainedlayer  trainedwts  trainedmodelgetlayer  lname  getweights   len  trainedwts   0  untrainedmodelgetlayer  lname  setweights  trainedwts  print  f  layer  lname  weight set   return untrainedmodel generate output sequence  inferenceencoder  defineinferenceencoder   12  1   inferenceencoder  setweight  inferenceencoder  tunedmodel   excontext  extargetin   extargetout testdstake  1   print  excontextshape  extargetinshape      64  12  1   64  3  1  testcontext  teststate  inferenceencoderpredict  tfreshape  excontext  shape  1  seqlen  1    print  testcontextshape  teststatesshape      64  12  256   64  256  infdecoder  defineinfdecoder  testcontext   11   infdecoder  setweights  infdecoder  tunedmodel  decinp  tfreshape  excontext    1   shape  111   decinpshape     6411  testinfdecoderout  infdecoderpredict   decinp  teststate   error  valueerror  exception encounter call layer  attnlayer   type attention   dimension must equal  32 64    node model7  attnlayer  matmul    batchmatmulv2  t  dtfloat  adjx  false  adjy  true   model7  degru1layer  partitionedcall1  model715181   input shape   321256    6412256   call argument receive layer  attnlayer   type attention    inputs   tf  tensor  shape  32  1  256   dtype  float32     tf  tensor  shape  64  12  256    dtype  float32     mask  none  training  false  returnattentionscore  false  usecausalmask  false not understand attnlayer get batch size 32  m pass input batch size 64  work fine work batch size 1  wrong ,solve replace predict predictonbatch  also find predict method treat sample individually pass one sample create problem attention layer get one sample s context vector encoder calculate attention weight single sample would default batchsize 32 might cause error context vector s batchsize 64   basically change predictonbatch one sample work like charm ,attention layer change batch size inference train seq  to  seq model use encoder  decoder architecture   m try produce output sequence give input context  try batch input context vector  self attention layer final output decoder seem change batch shape get shape correctly  throw error  work fine infer individual sample one batch size 1 practically take long production infer thousand input context vector   need help debug error implement well way produce output sequence computationally optimize  implementation     define inference encoder def defineinferenceencoder  inputshape   encoderinput  input  shape  inputshape  nameeninputlayer      first bidirectional gru layer bidigru1  bidirectional  gru  160  returnsequence  true   nameenbidirectgru1   gru1out  bidigru1  encoderinput  gru1out  dropout  046598303573163413  namebidirectgru1dropout    gru1out     second gru layer  hpunits2  hp  int   enclstm2   minvalue32  maxvalue800  step32  gru2  gru  hsize  returnsequence  true  returnstate  true  nameengru2layer   gru2out  gru2state  gru2  gru1out  encodermodel  model  input  encoderinput  outputs  gru2out  gru2state   return encodermodel    define inference decoder def defineinfdecoder  contextvec  inputshape   decoderinput  input  shape  inputshape  decoderstateinput  input  shape  hsize    degru1  gru  hsize  returnsequence  true  returnstate  true  namedegru1layer   degru1out  destateout  degru1  decoderinput  initialstate  decoderstateinput  attnlayer  attention  usescale  true  nameattnlayer   attnout  attnlayer   degru1out  contextvec   attnadde  concatenate  nameattnsourceconcatlayer     degru1out  attnout   attndenselayer  dense  736  nametanhdenselayer   activationtanh   hhat  attndenselayer  attnadde     output layer pred  dense  1  nameoutputlayer    hhat  decodermodel  model  inputs  decoderinput  decoderstateinput   outputs  pred  destateout   return decodermodel def setweight  untrainedmodel  trainedmodel   trainedlayer   lname l trainedmodellayers  print  f   train layer   len  trainedlayer     l untrainedmodellayers  lname trainedlayer  trainedwts  trainedmodelgetlayer  lname  getweights   len  trainedwts   0  untrainedmodelgetlayer  lname  setweights  trainedwts  print  f  layer  lname  weight set   return untrainedmodel generate output sequence  inferenceencoder  defineinferenceencoder   12  1   inferenceencoder  setweight  inferenceencoder  tunedmodel   excontext  extargetin   extargetout testdstake  1   print  excontextshape  extargetinshape      64  12  1   64  3  1  testcontext  teststate  inferenceencoderpredict  tfreshape  excontext  shape  1  seqlen  1    print  testcontextshape  teststatesshape      64  12  256   64  256  infdecoder  defineinfdecoder  testcontext   11   infdecoder  setweights  infdecoder  tunedmodel  decinp  tfreshape  excontext    1   shape  111   decinpshape     6411  testinfdecoderout  infdecoderpredict   decinp  teststate   error  valueerror  exception encounter call layer  attnlayer   type attention   dimension must equal  32 64    node model7  attnlayer  matmul    batchmatmulv2  t  dtfloat  adjx  false  adjy  true   model7  degru1layer  partitionedcall1  model715181   input shape   321256    6412256   call argument receive layer  attnlayer   type attention    inputs   tf  tensor  shape  32  1  256   dtype  float32     tf  tensor  shape  64  12  256    dtype  float32     mask  none  training  false  returnattentionscore  false  usecausalmask  false not understand attnlayer get batch size 32  m pass input batch size 64  work fine work batch size 1  wrong  solve replace predict predictonbatch  also find predict method treat sample individually pass one sample create problem attention layer get one sample s context vector encoder calculate attention weight single sample would default batchsize 32 might cause error context vector s batchsize 64   basically change predictonbatch one sample work like charm ,Implementation Issues
Pretrained model with stride doesn’t predict long text,"My objective is to annotate long documents with bioformer-8L. I have been said to use stride and truncation so I don’t have to split my documents in chunks of 512 tokens. In the training phase, I called the tokenizer like this: tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True) Then I train my model and at this stage I don't see any parameter that could help me in my task. With my trained model I do this for the predictions: model = AutoModelForTokenClassification.from_pretrained(model_path) tokenizer = AutoTokenizer.from_pretrained(model_path, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True) ner = pipeline(“token-classification”, model=model, tokenizer=tokenizer, aggregation_strategy=“first”) But it does not work, the model stops providing annotations in the middle of the text. For the test","['python', 'nlp', 'huggingface-transformers']",1,"You can't just move the __call__ parameters like stride to from_pretrained : from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline model_id = 'Davlan/distilbert-base-multilingual-cased-ner-hrl' t = AutoTokenizer.from_pretrained(model_id, stride =3, return_overflowing_tokens=True, model_max_length=10, truncation=True, is_split_into_words=True) sample = ""test ""*200 # sliding window will not be applied print(len(t(sample).input_ids)) # sliding window will be applied print(len(t(sample, max_length=10, truncation=True, stride=3, return_overflowing_tokens=True).input_ids)) Output: 202 # One batch with 202 ids 40 # 40 batches With the pipeline, you can pass the value for stride as __init__ parameter: from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline model_id = 'Davlan/distilbert-base-multilingual-cased-ner-hrl' ner = pipeline(""token-classification"", model_id, stride=128, aggregation_strategy=""first"") sample = ""Hi my name is cronoik and I live in Germany ""*3000 o = ner(sample) print(len(o)) print(o[0:5]) Output: 3000 [{'entity_group': 'LOC', 'score': 0.9997917, 'word': 'Germany', 'start': 36, 'end': 43}, {'entity_group': 'LOC', 'score': 0.9998311, 'word': 'Germany', 'start': 80, 'end': 87}, {'entity_group': 'LOC', 'score': 0.9997998, 'word': 'Germany', 'start': 124, 'end': 131}, {'entity_group': 'LOC', 'score': 0.9997831, 'word': 'Germany', 'start': 168, 'end': 175}, {'entity_group': 'LOC', 'score': 0.99981374, 'word': 'Germany', 'start': 212, 'end': 219}]",2023-11-30 15:28:13,2023-11-30 18:30:15,543,https://stackoverflow.com/questions/77579658/pretrained-model-with-stride-doesn-t-predict-long-text,"Pretrained model with stride doesn’t predict long text My objective is to annotate long documents with bioformer-8L. I have been said to use stride and truncation so I don’t have to split my documents in chunks of 512 tokens. In the training phase, I called the tokenizer like this: tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True) Then I train my model and at this stage I don't see any parameter that could help me in my task. With my trained model I do this for the predictions: model = AutoModelForTokenClassification.from_pretrained(model_path) tokenizer = AutoTokenizer.from_pretrained(model_path, stride = 128, return_overflowing_tokens=True, model_max_length=512, truncation=True, is_split_into_words=True) ner = pipeline(“token-classification”, model=model, tokenizer=tokenizer, aggregation_strategy=“first”) But it does not work, the model stops providing annotations in the middle of the text. For the test",pretrained model stride  predict long text objective annotate long document bioformer8l  say use stride truncation  split document chunk 512 token  training phase  call tokenizer like  tokenizer  autotokenizerfrompretraine  modelcheckpoint  stride  128  returnoverflowingtoken  true  modelmaxlength512  truncation  true  issplitintoword  true  train model stage not see parameter could help task  train model prediction  model  automodelfortokenclassificationfrompretraine  modelpath  tokenizer  autotokenizerfrompretraine  modelpath  stride  128  returnoverflowingtoken  true  modelmaxlength512  truncation  true  issplitintoword  true  ner  pipeline   token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  first   work  model stop provide annotation middle text  test,can not move   call   parameter like stride frompretrained  transformer import automodelfortokenclassification  autotokenizer  pipeline modelid   davlan  distilbert  base  multilingual  case  ner  hrl   autotokenizerfrompretraine  modelid  stride 3  returnoverflowingtoken  true  modelmaxlength10  truncation  true  issplitintoword  true  sample    test    200  slide window apply print  len   sample  inputids    slide window apply print  len   sample  maxlength10  truncation  true  stride3  returnoverflowingtoken  true  inputids   output  202  one batch 202 id 40  40 batch pipeline  pass value stride   init   parameter  transformer import automodelfortokenclassification  autotokenizer  pipeline modelid   davlan  distilbert  base  multilingual  case  ner  hrl  ner  pipeline    token  classification   modelid  stride128  aggregationstrategy  first   sample    hi name cronoik live germany    3000  ner  sample  print  len    print   05   output  3000    entitygroup    loc    score   09997917   word    germany    start   36   end   43     entitygroup    loc    score   09998311   word    germany    start   80   end   87     entitygroup    loc    score   09997998   word    germany    start   124   end   131     entitygroup    loc    score   09997831   word    germany    start   168   end   175     entitygroup    loc    score   099981374   word    germany    start   212   end   219  ,pretrained model stride  predict long text objective annotate long document bioformer8l  say use stride truncation  split document chunk 512 token  training phase  call tokenizer like  tokenizer  autotokenizerfrompretraine  modelcheckpoint  stride  128  returnoverflowingtoken  true  modelmaxlength512  truncation  true  issplitintoword  true  train model stage not see parameter could help task  train model prediction  model  automodelfortokenclassificationfrompretraine  modelpath  tokenizer  autotokenizerfrompretraine  modelpath  stride  128  returnoverflowingtoken  true  modelmaxlength512  truncation  true  issplitintoword  true  ner  pipeline   token  classification   model  model  tokenizer  tokenizer  aggregationstrategy  first   work  model stop provide annotation middle text  test can not move   call   parameter like stride frompretrained  transformer import automodelfortokenclassification  autotokenizer  pipeline modelid   davlan  distilbert  base  multilingual  case  ner  hrl   autotokenizerfrompretraine  modelid  stride 3  returnoverflowingtoken  true  modelmaxlength10  truncation  true  issplitintoword  true  sample    test    200  slide window apply print  len   sample  inputids    slide window apply print  len   sample  maxlength10  truncation  true  stride3  returnoverflowingtoken  true  inputids   output  202  one batch 202 id 40  40 batch pipeline  pass value stride   init   parameter  transformer import automodelfortokenclassification  autotokenizer  pipeline modelid   davlan  distilbert  base  multilingual  case  ner  hrl  ner  pipeline    token  classification   modelid  stride128  aggregationstrategy  first   sample    hi name cronoik live germany    3000  ner  sample  print  len    print   05   output  3000    entitygroup    loc    score   09997917   word    germany    start   36   end   43     entitygroup    loc    score   09998311   word    germany    start   80   end   87     entitygroup    loc    score   09997998   word    germany    start   124   end   131     entitygroup    loc    score   09997831   word    germany    start   168   end   175     entitygroup    loc    score   099981374   word    germany    start   212   end   219  ,Task-Specific Queries
Bert topic clasiffying over a quarter of documents in outlier topic -1,"I am running Bert topic with default options import pandas as pd from sentence_transformers import SentenceTransformer import time import pickle from bertopic import BERTopic llm_mod = ""all-MiniLM-L6-v2"" model = SentenceTransformer(llm_mod) embeddings = model.encode(skills_augmented, show_progress_bar=True) bertopic_model = BERTopic(verbose=True) I have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics). -1 13573 0 1593 1 1043 2 628 3 627 From the documentation: The -1 refers to all outliers and should typically be ignored. Is there a parameter I can use to get less documents in -1? Perhaps get a more even distribution across topics? Would running kmeans be better?","['python', 'nlp', 'bert-language-model', 'topic-modeling']",1,"From the documentation : The main way to reduce your outliers in BERTopic is by using the .reduce_outliers function. To make it work without too much tweaking, you will only need to pass the docs and their corresponding topics. You can pass outlier and non-outlier documents together since it will only try to reduce outlier documents and label them to a non-outlier topic. The following is a minimal example: from bertopic import BERTopic # Train your BERTopic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) # Reduce outliers new_topics = topic_model.reduce_outliers(docs, topics) You can find all the Strategies for reducing outliers in this page Outlier reduction",2023-11-28 12:32:26,2023-11-28 16:32:10,1108,https://stackoverflow.com/questions/77563897/bert-topic-clasiffying-over-a-quarter-of-documents-in-outlier-topic-1,"Bert topic clasiffying over a quarter of documents in outlier topic -1 I am running Bert topic with default options import pandas as pd from sentence_transformers import SentenceTransformer import time import pickle from bertopic import BERTopic llm_mod = ""all-MiniLM-L6-v2"" model = SentenceTransformer(llm_mod) embeddings = model.encode(skills_augmented, show_progress_bar=True) bertopic_model = BERTopic(verbose=True) I have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics). -1 13573 0 1593 1 1043 2 628 3 627 From the documentation: The -1 refers to all outliers and should typically be ignored. Is there a parameter I can use to get less documents in -1? Perhaps get a more even distribution across topics? Would running kmeans be better?",bert topic clasiffye quarter document outlier topic 1 run bert topic default option import panda pd sentencetransformer import sentencetransformer import time import pickle bertopic import bertopic llmmod    all  minilm  l6  v2  model  sentencetransformer  llmmod  embedding  modelencode  skillsaugmente  showprogressbar  true  bertopicmodel  bertopic  verbose  true  dataset 40000 document one short sentence  13573 document get place 1 topic  distribution across top 5 topic   1 13573 0 1593 1 1043 2 628 3 627 documentation  1 refer outlier typically ignore  parameter use get less document 1  perhaps get even distribution across topic  would run kmean well ,documentation  main way reduce outlier bertopic use reduceoutliers function  make work without much tweaking  need pass doc correspond topic  pass outli non  outlier document together since try reduce outli document label non  outlier topic  follow minimal example  bertopic import bertopic  train bertopic model topicmodel  bertopic   topic  prob  topicmodelfittransform  doc   reduce outlier newtopic  topicmodelreduceoutlier  doc  topic  find strategy reduce outlier page outlier reduction,bert topic clasiffye quarter document outlier topic 1 run bert topic default option import panda pd sentencetransformer import sentencetransformer import time import pickle bertopic import bertopic llmmod    all  minilm  l6  v2  model  sentencetransformer  llmmod  embedding  modelencode  skillsaugmente  showprogressbar  true  bertopicmodel  bertopic  verbose  true  dataset 40000 document one short sentence  13573 document get place 1 topic  distribution across top 5 topic   1 13573 0 1593 1 1043 2 628 3 627 documentation  1 refer outlier typically ignore  parameter use get less document 1  perhaps get even distribution across topic  would run kmean well  documentation  main way reduce outlier bertopic use reduceoutliers function  make work without much tweaking  need pass doc correspond topic  pass outli non  outlier document together since try reduce outli document label non  outlier topic  follow minimal example  bertopic import bertopic  train bertopic model topicmodel  bertopic   topic  prob  topicmodelfittransform  doc   reduce outlier newtopic  topicmodelreduceoutlier  doc  topic  find strategy reduce outlier page outlier reduction,Library/Tool-Based Queries
Modifying the sentiment of certain words in tidytext get_sentiments(),"I am trying to modify the sentiment of a few specific words in my df to make them more suitable for my context, where they were used with a negative connotation but have been classified as having a positive sentiment. The words are ""talent"" and ""prefer"". Here is my code: #Loading packages library(dplyr) library(ggplot2) require(readxl) library(tidytext) require(writexl) data example: dput(sentiment_words[1:20,c(7,8,9)]) data output: structure(list(word = c(""talent"", ""prefer"", ""lies"", ""hard"", ""worsen"", ""addicts"", ""obnoxious"", ""unbearable"", ""sickening"", ""irritating"", ""weird"", ""inconsiderate"", ""weird"", ""overwhelming"", ""issue"", ""complaints"", ""confined"", ""love"", ""confined"", ""idiots""), sentiment = c(""positive"", ""positive"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""positive"", ""negative"", ""negative""), count = c(79L, 3L, 53L, 316L, 2L, 2L, 3L, 2L, 2L, 7L, 24L, 2L, 24L, 2L, 198L, 21L, 4L, 52L, 4L, 19L)), class = c(""grouped_df"", ""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -20L), groups = structure(list( word = c(""addicts"", ""complaints"", ""confined"", ""ftw"", ""hard"", ""idiots"", ""inconsiderate"", ""irritating"", ""issue"", ""lies"", ""lost"", ""love"", ""obnoxious"", ""overwhelming"", ""sickening"", ""unbearable"", ""weird"", ""worsen""), .rows = structure(list( 6L, 16L, c(17L, 19L), 2L, 4L, 20L, 12L, 10L, 15L, 3L, 1L, 18L, 7L, 14L, 9L, 8L, c(11L, 13L), 5L), ptype = integer(0), class = c(""vctrs_list_of"", ""vctrs_vctr"", ""list""))), class = c(""tbl_df"", ""tbl"", ""data.frame"" ), row.names = c(NA, -18L), .drop = TRUE)) ###### Sentiment Analysis by Word ###### ## Using ""TIDYTEXT"" sentiment dictionary sentiment_words <- df |> tidytext::unnest_tokens(output=""word"", input=""post"") |> dplyr::anti_join(tidytext::stop_words)|> dplyr::inner_join(tidytext::get_sentiments(""bing"")) sentiment_words %>% count(word, sort = TRUE) # Check the Most common positive and negative words sentiment_words <- sentiment_words %>% group_by(word) %>% mutate(count = n()) bing_word_counts <- sentiment_words %>% dplyr::inner_join(tidytext::get_sentiments(""bing"") %>% count(word, sentiment, sort = TRUE))","['r', 'machine-learning', 'dplyr', 'nlp', 'tidytext']",1,"get_sentiments(""bing"") returns a regular tibble with 2 string columns that you can filter and wrangle as you see fit: library(tidytext) library(dplyr) library(stringr) get_sentiments(""bing"") #> # A tibble: 6,786 × 2 #> word sentiment #> <chr> <chr> #> 1 2-faces negative #> 2 abnormal negative #> 3 abolish negative #> 4 abominable negative #> 5 abominably negative #> 6 abominate negative #> 7 abomination negative #> 8 abort negative #> 9 aborted negative #> 10 aborts negative #> # ℹ 6,776 more rows # modified sentiments tibble sentiments_mod <- get_sentiments(""bing"") |> mutate(sentiment = case_when( word %in% c(""talent"", ""prefer"") ~ ""negative"", .default = sentiment)) Though there's no magic involved, so ""prefers"" and ""talents"" are still classified as positives, which may or may not be what you are after: filter(sentiments_mod, str_starts(word, ""talent|prefer"")) #> # A tibble: 10 × 2 #> word sentiment #> <chr> <chr> #> 1 prefer negative #> 2 preferable positive #> 3 preferably positive #> 4 prefered positive #> 5 preferes positive #> 6 preferring positive #> 7 prefers positive #> 8 talent negative #> 9 talented positive #> 10 talents positive When you have applied all required modification to your sentiment table, use that ( sentiments_mod ) in your workflow: df <- tibble(post = ""talent prefer lies hard worsen addicts obnoxious unbearable sickening irritating weird inconsiderate weird overwhelming issue complaints confined love confined idiots"") df |> unnest_tokens(output=""word"", input=""post"") |> anti_join(stop_words)|> inner_join(sentiments_mod) #> Joining with `by = join_by(word)` #> Joining with `by = join_by(word)` #> # A tibble: 20 × 2 #> word sentiment #> <chr> <chr> #> 1 talent negative #> 2 prefer negative #> 3 lies negative #> 4 hard negative #> 5 worsen negative #> 6 addicts negative #> 7 obnoxious negative #> 8 unbearable negative #> 9 sickening negative #> 10 irritating negative #> 11 weird negative #> 12 inconsiderate negative #> 13 weird negative #> 14 overwhelming negative #> 15 issue negative #> 16 complaints negative #> 17 confined negative #> 18 love positive #> 19 confined negative #> 20 idiots negative Created on 2023-11-28 with reprex v2.0.2",2023-11-28 11:13:39,2023-11-28 13:16:19,65,https://stackoverflow.com/questions/77563423/modifying-the-sentiment-of-certain-words-in-tidytext-get-sentiments,"Modifying the sentiment of certain words in tidytext get_sentiments() I am trying to modify the sentiment of a few specific words in my df to make them more suitable for my context, where they were used with a negative connotation but have been classified as having a positive sentiment. The words are ""talent"" and ""prefer"". Here is my code: #Loading packages library(dplyr) library(ggplot2) require(readxl) library(tidytext) require(writexl) data example: dput(sentiment_words[1:20,c(7,8,9)]) data output: structure(list(word = c(""talent"", ""prefer"", ""lies"", ""hard"", ""worsen"", ""addicts"", ""obnoxious"", ""unbearable"", ""sickening"", ""irritating"", ""weird"", ""inconsiderate"", ""weird"", ""overwhelming"", ""issue"", ""complaints"", ""confined"", ""love"", ""confined"", ""idiots""), sentiment = c(""positive"", ""positive"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""negative"", ""positive"", ""negative"", ""negative""), count = c(79L, 3L, 53L, 316L, 2L, 2L, 3L, 2L, 2L, 7L, 24L, 2L, 24L, 2L, 198L, 21L, 4L, 52L, 4L, 19L)), class = c(""grouped_df"", ""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -20L), groups = structure(list( word = c(""addicts"", ""complaints"", ""confined"", ""ftw"", ""hard"", ""idiots"", ""inconsiderate"", ""irritating"", ""issue"", ""lies"", ""lost"", ""love"", ""obnoxious"", ""overwhelming"", ""sickening"", ""unbearable"", ""weird"", ""worsen""), .rows = structure(list( 6L, 16L, c(17L, 19L), 2L, 4L, 20L, 12L, 10L, 15L, 3L, 1L, 18L, 7L, 14L, 9L, 8L, c(11L, 13L), 5L), ptype = integer(0), class = c(""vctrs_list_of"", ""vctrs_vctr"", ""list""))), class = c(""tbl_df"", ""tbl"", ""data.frame"" ), row.names = c(NA, -18L), .drop = TRUE)) ###### Sentiment Analysis by Word ###### ## Using ""TIDYTEXT"" sentiment dictionary sentiment_words <- df |> tidytext::unnest_tokens(output=""word"", input=""post"") |> dplyr::anti_join(tidytext::stop_words)|> dplyr::inner_join(tidytext::get_sentiments(""bing"")) sentiment_words %>% count(word, sort = TRUE) # Check the Most common positive and negative words sentiment_words <- sentiment_words %>% group_by(word) %>% mutate(count = n()) bing_word_counts <- sentiment_words %>% dplyr::inner_join(tidytext::get_sentiments(""bing"") %>% count(word, sentiment, sort = TRUE))",modify sentiment certain word tidytext getsentiment   try modify sentiment specific word df make suitable context  use negative connotation classify positive sentiment  word   talent    prefer   code   loading package library  dplyr  library  ggplot2  require  readxl  library  tidytext  require  writexl  datum example  dput  sentimentwords  120  c  789    data output  structure  list  word  c    talent     prefer     lie     hard     worsen     addict     obnoxious     unbearable     sicken     irritate     weird     inconsiderate     weird     overwhelming     issue     complaint     confine     love     confine     idiot    sentiment  c    positive     positive     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     positive     negative     negative    count  c  79l  3l  53l  316l  2l  2l  3l  2l  2l  7l  24l  2l  24l  2l  198l  21l  4l  52l  4l  19l    class  c    groupeddf     tbldf     tbl     dataframe    rowname  c  na  20l   group  structure  list  word  c    addict     complaint     confine     ftw     hard     idiot     inconsiderate     irritate     issue     lie     lose     love     obnoxious     overwhelming     sicken     unbearable     weird     worsen    row  structure  list  6l  16l  c  17l  19l   2l  4l  20l  12l  10l  15l  3l  1l  18l  7l  14l  9l  8l  c  11l  13l   5l   ptype  integer  0   class  c    vctrslistof     vctrsvctr     list      class  c    tbldf     tbl     dataframe    rowname  c  na  18l   drop  true         sentiment analysis word         use   tidytext  sentiment dictionary sentimentwords   df   tidytext   unnesttokens  output  word   input  post     dplyr   antijoin  tidytext   stopwords    dplyr   innerjoin  tidytext   getsentiment    bing    sentimentwords    count  word  sort  true   check common positive negative word sentimentwords   sentimentwords    groupby  word     mutate  count  n    bingwordcounts   sentimentwords    dplyr   innerjoin  tidytext   getsentiment    bing      count  word  sentiment  sort  true  ,getsentiment    bing   return regular tibble 2 string column filter wrangle see fit  library  tidytext  library  dplyr  library  stringr  getsentiment    bing      tibble  6786  2   word sentiment    chr   chr    1 2  face negative   2 abnormal negative   3 abolish negative   4 abominable negative   5 abominably negative   6 abominate negative   7 abomination negative   8 abort negative   9 abort negative   10 abort negative     6776 row  modify sentiment tibble sentimentsmod   getsentiment    bing     mutate  sentiment  casewhen  word   c    talent     prefer      negative   default  sentiment   though s magic involve    prefer    talent  still classify positive  may may  filter  sentimentsmod  strstart  word    talentprefer       tibble  10  2   word sentiment    chr   chr    1 prefer negative   2 preferable positive   3 preferably positive   4 prefer positive   5 prefer positive   6 prefer positive   7 prefer positive   8 talent negative   9 talented positive   10 talent positive apply require modification sentiment table  use  sentimentsmod  workflow  df   tibble  post    talent prefer lie hard worsen addict obnoxious unbearable sickening irritate weird inconsiderate weird overwhelming issue complaint confine love confine idiot   df   unnesttokens  output  word   input  post     antijoin  stopwords    innerjoin  sentimentsmod    join   joinby  word     join   joinby  word      tibble  20  2   word sentiment    chr   chr    1 talent negative   2 prefer negative   3 lie negative   4 hard negative   5 worsen negative   6 addict negative   7 obnoxious negative   8 unbearable negative   9 sicken negative   10 irritate negative   11 weird negative   12 inconsiderate negative   13 weird negative   14 overwhelming negative   15 issue negative   16 complaint negative   17 confine negative   18 love positive   19 confine negative   20 idiot negative create 2023  11  28 reprex v202,modify sentiment certain word tidytext getsentiment   try modify sentiment specific word df make suitable context  use negative connotation classify positive sentiment  word   talent    prefer   code   loading package library  dplyr  library  ggplot2  require  readxl  library  tidytext  require  writexl  datum example  dput  sentimentwords  120  c  789    data output  structure  list  word  c    talent     prefer     lie     hard     worsen     addict     obnoxious     unbearable     sicken     irritate     weird     inconsiderate     weird     overwhelming     issue     complaint     confine     love     confine     idiot    sentiment  c    positive     positive     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     negative     positive     negative     negative    count  c  79l  3l  53l  316l  2l  2l  3l  2l  2l  7l  24l  2l  24l  2l  198l  21l  4l  52l  4l  19l    class  c    groupeddf     tbldf     tbl     dataframe    rowname  c  na  20l   group  structure  list  word  c    addict     complaint     confine     ftw     hard     idiot     inconsiderate     irritate     issue     lie     lose     love     obnoxious     overwhelming     sicken     unbearable     weird     worsen    row  structure  list  6l  16l  c  17l  19l   2l  4l  20l  12l  10l  15l  3l  1l  18l  7l  14l  9l  8l  c  11l  13l   5l   ptype  integer  0   class  c    vctrslistof     vctrsvctr     list      class  c    tbldf     tbl     dataframe    rowname  c  na  18l   drop  true         sentiment analysis word         use   tidytext  sentiment dictionary sentimentwords   df   tidytext   unnesttokens  output  word   input  post     dplyr   antijoin  tidytext   stopwords    dplyr   innerjoin  tidytext   getsentiment    bing    sentimentwords    count  word  sort  true   check common positive negative word sentimentwords   sentimentwords    groupby  word     mutate  count  n    bingwordcounts   sentimentwords    dplyr   innerjoin  tidytext   getsentiment    bing      count  word  sentiment  sort  true   getsentiment    bing   return regular tibble 2 string column filter wrangle see fit  library  tidytext  library  dplyr  library  stringr  getsentiment    bing      tibble  6786  2   word sentiment    chr   chr    1 2  face negative   2 abnormal negative   3 abolish negative   4 abominable negative   5 abominably negative   6 abominate negative   7 abomination negative   8 abort negative   9 abort negative   10 abort negative     6776 row  modify sentiment tibble sentimentsmod   getsentiment    bing     mutate  sentiment  casewhen  word   c    talent     prefer      negative   default  sentiment   though s magic involve    prefer    talent  still classify positive  may may  filter  sentimentsmod  strstart  word    talentprefer       tibble  10  2   word sentiment    chr   chr    1 prefer negative   2 preferable positive   3 preferably positive   4 prefer positive   5 prefer positive   6 prefer positive   7 prefer positive   8 talent negative   9 talented positive   10 talent positive apply require modification sentiment table  use  sentimentsmod  workflow  df   tibble  post    talent prefer lie hard worsen addict obnoxious unbearable sickening irritate weird inconsiderate weird overwhelming issue complaint confine love confine idiot   df   unnesttokens  output  word   input  post     antijoin  stopwords    innerjoin  sentimentsmod    join   joinby  word     join   joinby  word      tibble  20  2   word sentiment    chr   chr    1 talent negative   2 prefer negative   3 lie negative   4 hard negative   5 worsen negative   6 addict negative   7 obnoxious negative   8 unbearable negative   9 sicken negative   10 irritate negative   11 weird negative   12 inconsiderate negative   13 weird negative   14 overwhelming negative   15 issue negative   16 complaint negative   17 confine negative   18 love positive   19 confine negative   20 idiot negative create 2023  11  28 reprex v202,Library/Tool-Based Queries
spaCy displacy output using anvil.works server,"I am attempting to display entities using spaCy's displacy feature. The output of my render is being shown in my Jupyter Notebook code cell with my anvil.server.wait_forever() code. Here is an example of the code cell output I'm getting. I would rather have the output appear here. I have already tried using displacy.server instead of displacy.render and tried auto_port. Here is my code: def visualize_entities_in_sentences(self, doc_id): """"""Visualize entities in the sentences of a document. :param doc_id: the id of the document to visualize :type doc_id: str """""" doc = self.get_document(doc_id) sentences = list(doc.sents) labels = displacy.render(sentences, style=""ent"", page=False, minify=True) return labels And my callable for the Anvil implementation: @anvil.server.callable def get_visualize_entities_in_sentences(doc_id): """"""""Get the document markdown for a document in my_corpus with entity labels visualized. :param doc_id: a document id :type doc_id: str :returns: markdown :rtype: str """""" return my_corpus.visualize_entities_in_sentences(doc_id)","['python', 'jupyter-notebook', 'nlp', 'spacy', 'displacy']",2,Try adding jupyter=False to displacy.render to skip the jupyter auto-detection.,2023-11-27 21:29:31,2023-11-28 09:25:09,90,https://stackoverflow.com/questions/77560044/spacy-displacy-output-using-anvil-works-server,"spaCy displacy output using anvil.works server I am attempting to display entities using spaCy's displacy feature. The output of my render is being shown in my Jupyter Notebook code cell with my anvil.server.wait_forever() code. Here is an example of the code cell output I'm getting. I would rather have the output appear here. I have already tried using displacy.server instead of displacy.render and tried auto_port. Here is my code: def visualize_entities_in_sentences(self, doc_id): """"""Visualize entities in the sentences of a document. :param doc_id: the id of the document to visualize :type doc_id: str """""" doc = self.get_document(doc_id) sentences = list(doc.sents) labels = displacy.render(sentences, style=""ent"", page=False, minify=True) return labels And my callable for the Anvil implementation: @anvil.server.callable def get_visualize_entities_in_sentences(doc_id): """"""""Get the document markdown for a document in my_corpus with entity labels visualized. :param doc_id: a document id :type doc_id: str :returns: markdown :rtype: str """""" return my_corpus.visualize_entities_in_sentences(doc_id)",spacy displacy output use anvilwork server attempt display entity use spacy s displacy feature  output render show jupyter notebook code cell anvilserverwaitforever   code  example code cell output  m get  would rather output appear  already try use displacyserver instead displacyrender try autoport  code  def visualizeentitiesinsentence  self  docid       visualize entity sentence document   param docid  i d document visualize  type docid  str     doc  selfgetdocument  docid  sentence  list  docsent  label  displacyrend  sentence  style  ent   page  false  minify  true  return label callable anvil implementation   anvilservercallable def getvisualizeentitiesinsentence  docid        get document markdown document mycorpus entity label visualize   param docid  document i d  type docid  str  return  markdown  rtype  str     return mycorpusvisualizeentitiesinsentence  docid ,try add jupyter  false displacyrend skip jupyter auto  detection ,spacy displacy output use anvilwork server attempt display entity use spacy s displacy feature  output render show jupyter notebook code cell anvilserverwaitforever   code  example code cell output  m get  would rather output appear  already try use displacyserver instead displacyrender try autoport  code  def visualizeentitiesinsentence  self  docid       visualize entity sentence document   param docid  i d document visualize  type docid  str     doc  selfgetdocument  docid  sentence  list  docsent  label  displacyrend  sentence  style  ent   page  false  minify  true  return label callable anvil implementation   anvilservercallable def getvisualizeentitiesinsentence  docid        get document markdown document mycorpus entity label visualize   param docid  document i d  type docid  str  return  markdown  rtype  str     return mycorpusvisualizeentitiesinsentence  docid  try add jupyter  false displacyrend skip jupyter auto  detection ,Library/Tool-Based Queries
How to build a model and train it with tensorflow keras sub classing,"I have written a custom Encoder and Decoder layers that implements the architecture described in the Attention Is All You Need paper. Everything works fine until I trying compiling it, I get one error. If I run it with a sample data it compiles but then when I call the fit method to train the model it throws another error. I'm going to provide the blocks that I might be implementing incorrectly and let me know if more code is needed to debug. TF Version: 2.14.0 Multi-Head Sub Layer and Positional Encoding Layer: class MhaSubLayer(Layer): def __init__(self, units, **kwargs): super().__init__() self.mha = MultiHeadAttention(key_dim=units, **kwargs) self.inner_dense = TimeDistributed(Dense(2048, activation='relu')) self.outer_dense = TimeDistributed(Dense(units)) self.layernorm_mha = LayerNormalization() self.layernorm_ff = LayerNormalization() self.add = Add() def call(self, x, context, **kwargs): ### Calculate Attention Output attn_out, attn_scores = self.mha(query=x, value=context, return_attention_scores=True, **kwargs) attn_resid_cnxt = self.add([x, attn_out]) ## Residual connection attn_layer_norm = self.layernorm_mha(attn_resid_cnxt) attn_scores = tf.reduce_mean(attn_scores, axis=1) self.last_attention_weights = attn_scores ### Pass the Attention output to the Dense Layer dense_out = self.outer_dense(self.inner_dense(attn_layer_norm)) dense_resid_cnxt = self.add([attn_layer_norm, dense_out]) ### Feed forward residual connection dense_layer_norm = self.layernorm_ff(dense_resid_cnxt) return dense_layer_norm class PositionalEncodingLayer(Layer): def __init__(self, **kwargs): super().__init__() self.add = Add() def get_positional_encodings(self, x): seq_len = x.shape[0] d = x.shape[1] P = np.zeros((seq_len, d)) for k in range(seq_len): for i in np.arange(int(d/2)): denominator = np.power(10000, 2*i/d) P[k, 2*i] = np.sin(k/denominator) P[k, 2*i+1] = np.cos(k/denominator) return P def call(self, x): # pos_enc = [] pos_enc = tf.map_fn(fn=self.get_positional_encodings, elems=x) # for n, elm in enumerate(x): # p = self.get_positional_encodings(elm) # pos_enc.append(p) # pos_enc = tf.convert_to_tensor(pos_enc) pos_embeddings = self.add([x, pos_enc]) return pos_embeddings Encoder-Decoder Block: class Encoder(Layer): def __init__(self, units, embed_input_dim, name='encoder', **kwargs): super().__init__() ### Encoder Input Embedding and Layer self.embedding = Embedding(input_dim=embed_input_dim, output_dim=units, name='en_embed_layer') self.pos_embedding = PositionalEncodingLayer(name='en_positional_embed_layer') ### Encoder Multi-Head Self Attention Sub Layer self.mha_sub_layer1 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_1') self.mha_sub_layer2 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_2') self.mha_sub_layer3 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_3') self.mha_sub_layer4 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_4') self.mha_sub_layer5 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_5') self.mha_sub_layer6 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_6') ### Encoder MHA Dropout Layer self.dropout = Dropout(rate=0.1, name='en_dropout_pos_enc') self.dropout1 = Dropout(rate=0.1, name='en_dropout_layer1') self.dropout2 = Dropout(rate=0.1, name='en_dropout_layer2') self.dropout3 = Dropout(rate=0.1, name='en_dropout_layer3') self.dropout4 = Dropout(rate=0.1, name='en_dropout_layer4') self.dropout5 = Dropout(rate=0.1, name='en_dropout_layer5') self.dropout6 = Dropout(rate=0.1, name='en_dropout_layer6') def call(self, x): embedding_output = self.embedding(x) positional_embedding = self.pos_embedding(embedding_output) postitional_embedding = self.dropout(positional_embedding) ### First MHa Sub-Layer sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding) sub_layer1_out = self.dropout1(sub_layer1_out) ### Second MHa Sub-Layer sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, sub_layer1_out) sub_layer2_out = self.dropout2(sub_layer2_out) ### Third MHa Sub-Layer sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, sub_layer2_out) sub_layer3_out = self.dropout3(sub_layer3_out) ### Fourth MHa Sub-Layer sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, sub_layer3_out) sub_layer4_out = self.dropout4(sub_layer4_out) ### Fifth MHa Sub-Layer sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, sub_layer4_out) sub_layer5_out = self.dropout5(sub_layer5_out) ### Sixth MHa Sub-Layer sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, sub_layer5_out) sub_layer6_out = self.dropout6(sub_layer6_out) return sub_layer6_out class Decoder(Layer): def __init__(self, units, embed_input_dim, name='decoder', **kwargs): super().__init__() ### Decoder Input Embedding Layer self.embedding = Embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer') self.pos_embedding = PositionalEncodingLayer(name='de_positional_embed_layer') ### Decoder Multi-Head Attention Sub Layer self.mha_sub_layer1 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_1') self.mha_sub_layer2 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_2') self.mha_sub_layer3 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_3') self.mha_sub_layer4 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_4') self.mha_sub_layer5 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_5') self.mha_sub_layer6 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_6') ### Decoder MHA Droput Layer self.dropout = Dropout(rate=0.1, name='de_dropout_pos_enc') self.dropout1 = Dropout(rate=0.1, name='de_dropout_layer1') self.dropout2 = Dropout(rate=0.1, name='de_dropout_layer2') self.dropout3 = Dropout(rate=0.1, name='de_dropout_layer3') self.dropout4 = Dropout(rate=0.1, name='de_dropout_layer4') self.dropout5 = Dropout(rate=0.1, name='de_dropout_layer5') self.dropout6 = Dropout(rate=0.1, name='de_dropout_layer6') ### Dense Output Layer self.output_dense_layer = TimeDistributed(Dense(1), name=""output_layer"") def call(self, x, en_context): embedding_output = self.embedding(x) positional_embedding = self.pos_embedding(embedding_output) postitional_embedding = self.dropout(positional_embedding) ### First MHA Sub-Layer sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding) sub_layer1_out = self.dropout1(sub_layer1_out) ### Second MHA Sub-Layer sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context) sub_layer2_out = self.dropout2(sub_layer2_out) ### Third MHA Sub-Layer sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context) sub_layer3_out = self.dropout3(sub_layer3_out) ### Fourth MHA Sub-Layer sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context) sub_layer4_out = self.dropout4(sub_layer4_out) ### Fifth MHA Sub-Layer sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context) sub_layer5_out = self.dropout5(sub_layer5_out) ### Sixth MHA Sub-Layer sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context) sub_layer6_out = self.dropout6(sub_layer6_out) ### Output Dense Layer output = self.output_dense_layer(sub_layer6_out) output = tf.round(tf.abs(output)) return output Sample Data: np.random.seed(42) trainX = np.random.randint(0, high=250, size=(5,12)) trainXt_in = np.random.randint(0, high=250, size=(5,3)) trainY = np.random.randint(0, high=250, size=(5,3,1)) Modelling Block: Training shape: ((1616304, 12), (1616304, 3), (1616304, 3, 1)) ## The Model Sub-Class class Trxster(Model): def __init__(self, units, en_embed_dim, de_embed_dim, name='Trxster', **kwargs): super().__init__() self.encoder = Encoder(units, en_embed_dim) self.decoder = Decoder(units, de_embed_dim) def call(self, inputs): context_vec, target_in = inputs context = self.encoder(context_vec) preds = self.decoder(target_in, context) return preds forecastor = Trxster(hsize, embed_dim, embed_dim) forecastor.build(((12, 1),(3, 1))) forecastor.summary() Error-1: TypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an index method, got value '(12, 1)' with type '<class 'tuple'>'. If run the model with an example: hsize = 512 embed_dim = 268 forecastor = Trxster(hsize, embed_dim, embed_dim) forecastor((trainX, trainXt_in)) Model: ""trxster_11"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= encoder_13 (Encoder) multiple 63156224 decoder_13 (Decoder) multiple 63156737 ================================================================= Total params: 126312961 (481.85 MB) Trainable params: 126312961 (481.85 MB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________ ### Fit the Model batch_size = 64 epochs = 100 steps = trainX.shape[0]//batch_size warmup_steps = steps//25 class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps): self.d_model = d_model self.warmup_steps = warmup_steps def __call__(self, step): step_num = step.numpy() self.lr = [] denom = self.d_model**(-0.5) numer = min(step_num**(-0.5), step_num*(self.warmup_steps**(-1.5))) lrate = np.divide(numer, denom) self.lr.append(lrate) return lrate opt = tf.keras.optimizers.Adam(learning_rate=MyLRSchedule(hsize, warmup_steps), beta_1=0.9, beta_2=0.98, epsilon=1e-8) ### Configure Trxster checkpoint_filepath = './training_ckpt' cb = [tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True), tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_filepath, save_weights_only=True, monitor='val_loss', mode='min', verbose=1, save_best_only=True)] loss = tf.keras.losses.MeanSquaredError() metrics = [tf.keras.metrics.Accuracy(), tf.keras.losses.MeanAbsoluteError()] forecastor.compile(optimizer=opt, loss='mean_squared_error', metrics=['acc','mean_absolute_error']) history = forecastor.fit((trainX, trainXt_in), trainY, batch_size=batch_size, steps_per_epoch=steps, epochs=1, validation_data=((valX, ValXt_in), valY), callbacks=cb) Error-2: Providing few lines of error trace: ValueError: No gradients provided for any variable: (['trxster_11/encoder_13/en_embed_layer/embeddings:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/time_distributed_314/kernel:0' Every examples I see tells me that it should work just the way I have written but it isn't so.","['python-3.x', 'tensorflow', 'keras', 'deep-learning', 'nlp']",1,"I figured it out! Gradient calculations fail when there is a tensorflow function in the graph which is the case in my network where I have applied tf.round and tf.abs in the output layer of the Decoder . That was failing the gradient calculations. I removed them and it the model trains as expected. Here is the link to the issue https://github.com/tensorflow/tensorflow/issues/1511 . Decoder: class Decoder(Layer): def __init__(self, units, embed_input_dim, name='decoder', **kwargs): super().__init__() ### Decoder Input Embedding Layer self.embedding = Embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer') self.pos_embedding = PositionalEncodingLayer(name='de_positional_embed_layer') ### Decoder Multi-Head Attention Sub Layer self.mha_sub_layer1 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_1') self.mha_sub_layer2 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_2') self.mha_sub_layer3 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_3') self.mha_sub_layer4 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_4') self.mha_sub_layer5 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_5') self.mha_sub_layer6 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_6') ### Decoder MHA Droput Layer self.dropout = Dropout(rate=0.1, name='de_dropout_pos_enc') self.dropout1 = Dropout(rate=0.1, name='de_dropout_layer1') self.dropout2 = Dropout(rate=0.1, name='de_dropout_layer2') self.dropout3 = Dropout(rate=0.1, name='de_dropout_layer3') self.dropout4 = Dropout(rate=0.1, name='de_dropout_layer4') self.dropout5 = Dropout(rate=0.1, name='de_dropout_layer5') self.dropout6 = Dropout(rate=0.1, name='de_dropout_layer6') ### Dense Output Layer self.output_dense_layer = TimeDistributed(Dense(1), name=""output_layer"") def call(self, x, en_context): embedding_output = self.embedding(x) positional_embedding = self.pos_embedding(embedding_output) postitional_embedding = self.dropout(positional_embedding) ### First MHA Sub-Layer sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding) sub_layer1_out = self.dropout1(sub_layer1_out) ### Second MHA Sub-Layer sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context) sub_layer2_out = self.dropout2(sub_layer2_out) ### Third MHA Sub-Layer sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context) sub_layer3_out = self.dropout3(sub_layer3_out) ### Fourth MHA Sub-Layer sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context) sub_layer4_out = self.dropout4(sub_layer4_out) ### Fifth MHA Sub-Layer sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context) sub_layer5_out = self.dropout5(sub_layer5_out) ### Sixth MHA Sub-Layer sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context) sub_layer6_out = self.dropout6(sub_layer6_out) ### Output Dense Layer output = self.output_dense_layer(sub_layer6_out) return output",2023-11-25 02:49:30,2023-11-27 02:46:30,63,https://stackoverflow.com/questions/77546477/how-to-build-a-model-and-train-it-with-tensorflow-keras-sub-classing,"How to build a model and train it with tensorflow keras sub classing I have written a custom Encoder and Decoder layers that implements the architecture described in the Attention Is All You Need paper. Everything works fine until I trying compiling it, I get one error. If I run it with a sample data it compiles but then when I call the fit method to train the model it throws another error. I'm going to provide the blocks that I might be implementing incorrectly and let me know if more code is needed to debug. TF Version: 2.14.0 Multi-Head Sub Layer and Positional Encoding Layer: class MhaSubLayer(Layer): def __init__(self, units, **kwargs): super().__init__() self.mha = MultiHeadAttention(key_dim=units, **kwargs) self.inner_dense = TimeDistributed(Dense(2048, activation='relu')) self.outer_dense = TimeDistributed(Dense(units)) self.layernorm_mha = LayerNormalization() self.layernorm_ff = LayerNormalization() self.add = Add() def call(self, x, context, **kwargs): ### Calculate Attention Output attn_out, attn_scores = self.mha(query=x, value=context, return_attention_scores=True, **kwargs) attn_resid_cnxt = self.add([x, attn_out]) ## Residual connection attn_layer_norm = self.layernorm_mha(attn_resid_cnxt) attn_scores = tf.reduce_mean(attn_scores, axis=1) self.last_attention_weights = attn_scores ### Pass the Attention output to the Dense Layer dense_out = self.outer_dense(self.inner_dense(attn_layer_norm)) dense_resid_cnxt = self.add([attn_layer_norm, dense_out]) ### Feed forward residual connection dense_layer_norm = self.layernorm_ff(dense_resid_cnxt) return dense_layer_norm class PositionalEncodingLayer(Layer): def __init__(self, **kwargs): super().__init__() self.add = Add() def get_positional_encodings(self, x): seq_len = x.shape[0] d = x.shape[1] P = np.zeros((seq_len, d)) for k in range(seq_len): for i in np.arange(int(d/2)): denominator = np.power(10000, 2*i/d) P[k, 2*i] = np.sin(k/denominator) P[k, 2*i+1] = np.cos(k/denominator) return P def call(self, x): # pos_enc = [] pos_enc = tf.map_fn(fn=self.get_positional_encodings, elems=x) # for n, elm in enumerate(x): # p = self.get_positional_encodings(elm) # pos_enc.append(p) # pos_enc = tf.convert_to_tensor(pos_enc) pos_embeddings = self.add([x, pos_enc]) return pos_embeddings Encoder-Decoder Block: class Encoder(Layer): def __init__(self, units, embed_input_dim, name='encoder', **kwargs): super().__init__() ### Encoder Input Embedding and Layer self.embedding = Embedding(input_dim=embed_input_dim, output_dim=units, name='en_embed_layer') self.pos_embedding = PositionalEncodingLayer(name='en_positional_embed_layer') ### Encoder Multi-Head Self Attention Sub Layer self.mha_sub_layer1 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_1') self.mha_sub_layer2 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_2') self.mha_sub_layer3 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_3') self.mha_sub_layer4 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_4') self.mha_sub_layer5 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_5') self.mha_sub_layer6 = MhaSubLayer(units, num_heads=8, name='en_mha_layer_6') ### Encoder MHA Dropout Layer self.dropout = Dropout(rate=0.1, name='en_dropout_pos_enc') self.dropout1 = Dropout(rate=0.1, name='en_dropout_layer1') self.dropout2 = Dropout(rate=0.1, name='en_dropout_layer2') self.dropout3 = Dropout(rate=0.1, name='en_dropout_layer3') self.dropout4 = Dropout(rate=0.1, name='en_dropout_layer4') self.dropout5 = Dropout(rate=0.1, name='en_dropout_layer5') self.dropout6 = Dropout(rate=0.1, name='en_dropout_layer6') def call(self, x): embedding_output = self.embedding(x) positional_embedding = self.pos_embedding(embedding_output) postitional_embedding = self.dropout(positional_embedding) ### First MHa Sub-Layer sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding) sub_layer1_out = self.dropout1(sub_layer1_out) ### Second MHa Sub-Layer sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, sub_layer1_out) sub_layer2_out = self.dropout2(sub_layer2_out) ### Third MHa Sub-Layer sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, sub_layer2_out) sub_layer3_out = self.dropout3(sub_layer3_out) ### Fourth MHa Sub-Layer sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, sub_layer3_out) sub_layer4_out = self.dropout4(sub_layer4_out) ### Fifth MHa Sub-Layer sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, sub_layer4_out) sub_layer5_out = self.dropout5(sub_layer5_out) ### Sixth MHa Sub-Layer sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, sub_layer5_out) sub_layer6_out = self.dropout6(sub_layer6_out) return sub_layer6_out class Decoder(Layer): def __init__(self, units, embed_input_dim, name='decoder', **kwargs): super().__init__() ### Decoder Input Embedding Layer self.embedding = Embedding(input_dim=embed_input_dim, output_dim=units, name='de_embed_layer') self.pos_embedding = PositionalEncodingLayer(name='de_positional_embed_layer') ### Decoder Multi-Head Attention Sub Layer self.mha_sub_layer1 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_1') self.mha_sub_layer2 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_2') self.mha_sub_layer3 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_3') self.mha_sub_layer4 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_4') self.mha_sub_layer5 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_5') self.mha_sub_layer6 = MhaSubLayer(units, num_heads=8, name='de_mha_layer_6') ### Decoder MHA Droput Layer self.dropout = Dropout(rate=0.1, name='de_dropout_pos_enc') self.dropout1 = Dropout(rate=0.1, name='de_dropout_layer1') self.dropout2 = Dropout(rate=0.1, name='de_dropout_layer2') self.dropout3 = Dropout(rate=0.1, name='de_dropout_layer3') self.dropout4 = Dropout(rate=0.1, name='de_dropout_layer4') self.dropout5 = Dropout(rate=0.1, name='de_dropout_layer5') self.dropout6 = Dropout(rate=0.1, name='de_dropout_layer6') ### Dense Output Layer self.output_dense_layer = TimeDistributed(Dense(1), name=""output_layer"") def call(self, x, en_context): embedding_output = self.embedding(x) positional_embedding = self.pos_embedding(embedding_output) postitional_embedding = self.dropout(positional_embedding) ### First MHA Sub-Layer sub_layer1_out = self.mha_sub_layer1(positional_embedding, positional_embedding) sub_layer1_out = self.dropout1(sub_layer1_out) ### Second MHA Sub-Layer sub_layer2_out = self.mha_sub_layer2(sub_layer1_out, en_context) sub_layer2_out = self.dropout2(sub_layer2_out) ### Third MHA Sub-Layer sub_layer3_out = self.mha_sub_layer3(sub_layer2_out, en_context) sub_layer3_out = self.dropout3(sub_layer3_out) ### Fourth MHA Sub-Layer sub_layer4_out = self.mha_sub_layer4(sub_layer3_out, en_context) sub_layer4_out = self.dropout4(sub_layer4_out) ### Fifth MHA Sub-Layer sub_layer5_out = self.mha_sub_layer5(sub_layer4_out, en_context) sub_layer5_out = self.dropout5(sub_layer5_out) ### Sixth MHA Sub-Layer sub_layer6_out = self.mha_sub_layer6(sub_layer5_out, en_context) sub_layer6_out = self.dropout6(sub_layer6_out) ### Output Dense Layer output = self.output_dense_layer(sub_layer6_out) output = tf.round(tf.abs(output)) return output Sample Data: np.random.seed(42) trainX = np.random.randint(0, high=250, size=(5,12)) trainXt_in = np.random.randint(0, high=250, size=(5,3)) trainY = np.random.randint(0, high=250, size=(5,3,1)) Modelling Block: Training shape: ((1616304, 12), (1616304, 3), (1616304, 3, 1)) ## The Model Sub-Class class Trxster(Model): def __init__(self, units, en_embed_dim, de_embed_dim, name='Trxster', **kwargs): super().__init__() self.encoder = Encoder(units, en_embed_dim) self.decoder = Decoder(units, de_embed_dim) def call(self, inputs): context_vec, target_in = inputs context = self.encoder(context_vec) preds = self.decoder(target_in, context) return preds forecastor = Trxster(hsize, embed_dim, embed_dim) forecastor.build(((12, 1),(3, 1))) forecastor.summary() Error-1: TypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an index method, got value '(12, 1)' with type '<class 'tuple'>'. If run the model with an example: hsize = 512 embed_dim = 268 forecastor = Trxster(hsize, embed_dim, embed_dim) forecastor((trainX, trainXt_in)) Model: ""trxster_11"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= encoder_13 (Encoder) multiple 63156224 decoder_13 (Decoder) multiple 63156737 ================================================================= Total params: 126312961 (481.85 MB) Trainable params: 126312961 (481.85 MB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________ ### Fit the Model batch_size = 64 epochs = 100 steps = trainX.shape[0]//batch_size warmup_steps = steps//25 class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps): self.d_model = d_model self.warmup_steps = warmup_steps def __call__(self, step): step_num = step.numpy() self.lr = [] denom = self.d_model**(-0.5) numer = min(step_num**(-0.5), step_num*(self.warmup_steps**(-1.5))) lrate = np.divide(numer, denom) self.lr.append(lrate) return lrate opt = tf.keras.optimizers.Adam(learning_rate=MyLRSchedule(hsize, warmup_steps), beta_1=0.9, beta_2=0.98, epsilon=1e-8) ### Configure Trxster checkpoint_filepath = './training_ckpt' cb = [tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True), tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_filepath, save_weights_only=True, monitor='val_loss', mode='min', verbose=1, save_best_only=True)] loss = tf.keras.losses.MeanSquaredError() metrics = [tf.keras.metrics.Accuracy(), tf.keras.losses.MeanAbsoluteError()] forecastor.compile(optimizer=opt, loss='mean_squared_error', metrics=['acc','mean_absolute_error']) history = forecastor.fit((trainX, trainXt_in), trainY, batch_size=batch_size, steps_per_epoch=steps, epochs=1, validation_data=((valX, ValXt_in), valY), callbacks=cb) Error-2: Providing few lines of error trace: ValueError: No gradients provided for any variable: (['trxster_11/encoder_13/en_embed_layer/embeddings:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/query/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/key/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/value/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/kernel:0', 'trxster_11/encoder_13/mha_sub_layer_157/en_mha_layer_1/attention_output/bias:0', 'trxster_11/encoder_13/mha_sub_layer_157/time_distributed_314/kernel:0' Every examples I see tells me that it should work just the way I have written but it isn't so.",build model train tensorflow keras sub class write custom encoder decoder layer implement architecture describe attention need paper  everything work fine try compile  get one error  run sample datum compile call fit method train model throw another error   m go provide block might implementing incorrectly let know code need debug  tf version  2140 multi  head sub layer positional encoding layer  class mhasublayer  layer   def   init    self  unit    kwargs   super   init     selfmha  multiheadattention  keydim  unit    kwargs  selfinnerdense  timedistributed  dense  2048  activationrelu    selfouterdense  timedistributed  dense  unit   selflayernormmha  layernormalization   selflayernormff  layernormalization   selfadd  add   def call  self  x  context    kwarg      calculate attention output attnout  attnscore  selfmha  query  x  value  context  returnattentionscore  true    kwargs  attnresidcnxt  selfadd   x  attnout     residual connection attnlayernorm  selflayernormmha  attnresidcnxt  attnscore  tfreducemean  attnscore  axis1  selflastattentionweight  attnscore    pass attention output dense layer denseout  selfouterdense  selfinnerdense  attnlayernorm   denseresidcnxt  selfadd   attnlayernorm  denseout      feed forward residual connection denselayernorm  selflayernormff  denseresidcnxt  return denselayernorm class positionalencodinglayer  layer   def   init    self    kwargs   super   init     selfadd  add   def getpositionalencoding  self  x   seqlen  xshape  0   xshape  1  p  npzeros   seqlen    k range  seqlen   nparange  int  d2    denominator  nppower  10000  2  i  d  p  k  2    npsin  k  denominator  p  k  2  i1   npcos  k  denominator  return p def call  self  x    posenc    posenc  tfmapfn  fn  selfgetpositionalencoding  elem  x   n  elm enumerate  x    p  selfgetpositionalencoding  elm   posencappend  p   posenc  tfconverttotensor  posenc  posembedding  selfadd   x  posenc   return posembedding encoder  decoder block  class encoder  layer   def   init    self  unit  embedinputdim  nameencoder     kwargs   super   init        encoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  nameenembedlayer   selfposembedde  positionalencodinglayer  nameenpositionalembedlayer      encoder multi  head self attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  nameenmhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  nameenmhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  nameenmhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  nameenmhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  nameenmhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  nameenmhalayer6      encoder mha dropout layer selfdropout  dropout  rate01  nameendropoutposenc   selfdropout1  dropout  rate01  nameendropoutlayer1   selfdropout2  dropout  rate01  nameendropoutlayer2   selfdropout3  dropout  rate01  nameendropoutlayer3   selfdropout4  dropout  rate01  nameendropoutlayer4   selfdropout5  dropout  rate01  nameendropoutlayer5   selfdropout6  dropout  rate01  nameendropoutlayer6   def call  self  x   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  sublayer1out  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  sublayer2out  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  sublayer3out  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  sublayer4out  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  sublayer5out  sublayer6out  selfdropout6  sublayer6out  return sublayer6out class decoder  layer   def   init    self  unit  embedinputdim  namedecoder     kwargs   super   init        decoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  namedeembedlayer   selfposembedde  positionalencodinglayer  namedepositionalembedlayer      decoder multi  head attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  namedemhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  namedemhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  namedemhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  namedemhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  namedemhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  namedemhalayer6      decoder mha droput layer selfdropout  dropout  rate01  namededropoutposenc   selfdropout1  dropout  rate01  namededropoutlayer1   selfdropout2  dropout  rate01  namededropoutlayer2   selfdropout3  dropout  rate01  namededropoutlayer3   selfdropout4  dropout  rate01  namededropoutlayer4   selfdropout5  dropout  rate01  namededropoutlayer5   selfdropout6  dropout  rate01  namededropoutlayer6      dense output layer selfoutputdenselayer  timedistributed  dense  1   name  outputlayer   def call  self  x  encontext   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  encontext  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  encontext  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  encontext  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  encontext  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  encontext  sublayer6out  selfdropout6  sublayer6out     output dense layer output  selfoutputdenselayer  sublayer6out  output  tfround  tfab  output   return output sample data  nprandomseed  42  trainx  nprandomrandint  0  high250  size  512   trainxtin  nprandomrandint  0  high250  size  53   trainy  nprandomrandint  0  high250  size  531   modelling block  training shape    1616304  12    1616304  3    1616304  3  1     model sub  class class trxster  model   def   init    self  unit  enembeddim  deembeddim  nametrxster     kwargs   super   init     selfencoder  encoder  unit  enembeddim  selfdecoder  decoder  unit  deembeddim  def call  self  input   contextvec  targetin  input context  selfencoder  contextvec  pred  selfdecoder  targetin  context  return pred forecastor  trxster  hsize  embeddim  embeddim  forecastorbuild    12  1    3  1    forecastorsummary   error1  typeerror  error converting shape tensorshape  dimension value must integer none index method  get value   12  1   type   class  tuple     run model example  hsize  512 embeddim  268 forecastor  trxster  hsize  embeddim  embeddim  forecastor   trainx  trainxtin   model    trxster11                                                                   layer  type  output shape param                                                                   encoder13  encoder  multiple 63156224 decoder13  decoder  multiple 63156737                                                                  total param  126312961  48185 mb  trainable param  126312961  48185 mb  non  trainable param  0  000 byte                                                                      fit model batchsize  64 epoch  100 step  trainxshape  0  batchsize warmupsteps  steps25 class mylrschedule  tfkerasoptimizersschedule  learningrateschedule   def   init    self  dmodel  warmupsteps   selfdmodel  dmodel selfwarmupstep  warmupsteps def   call    self  step   stepnum  stepnumpy   selflr    denom  selfdmodel    05  numer  min  stepnum    05   stepnum   selfwarmupstep    15    lrate  npdivide  numer  denom  selflrappend  lrate  return lrate opt  tfkerasoptimizer  adam  learningrate  mylrschedule  hsize  warmupsteps   beta109  beta2098  epsilon1e8     configure trxster checkpointfilepath   trainingckpt  cb   tfkerascallbacks  earlystopping  patience10  monitorvalloss   restorebestweight  true   tfkerascallback  modelcheckpoint  filepath  checkpointfilepath  saveweightsonly  true  monitorvalloss   modemin   verbose1  savebestonly  true   loss  tfkeraslosse  meansquarederror   metric   tfkerasmetrics  accuracy    tfkeraslosse  meanabsoluteerror    forecastorcompile  optimizer  opt  lossmeansquarederror   metrics   acc    meanabsoluteerror    history  forecastorfit   trainx  trainxtin   trainy  batchsize  batchsize  stepsperepoch  step  epochs1  validationdata   valx  valxtin   valy   callback  cb  error2  provide line error trace  valueerror  gradient provide variable     trxster11  encoder13  enembedlayer  embeddings0    trxster11  encoder13  mhasublayer157  enmhalayer1  query  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  query  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  key  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  key  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  value  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  value  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  attentionoutput  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  attentionoutput  bias0    trxster11  encoder13  mhasublayer157  timedistributed314  kernel0  every example see tell work way write not ,figure  gradient calculation fail tensorflow function graph case network apply tfround tfabs output layer decoder  fail gradient calculation  removed model train expect  link issue   githubcom  tensorflow  tensorflow  issues1511  decoder  class decoder  layer   def   init    self  unit  embedinputdim  namedecoder     kwargs   super   init        decoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  namedeembedlayer   selfposembedde  positionalencodinglayer  namedepositionalembedlayer      decoder multi  head attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  namedemhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  namedemhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  namedemhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  namedemhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  namedemhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  namedemhalayer6      decoder mha droput layer selfdropout  dropout  rate01  namededropoutposenc   selfdropout1  dropout  rate01  namededropoutlayer1   selfdropout2  dropout  rate01  namededropoutlayer2   selfdropout3  dropout  rate01  namededropoutlayer3   selfdropout4  dropout  rate01  namededropoutlayer4   selfdropout5  dropout  rate01  namededropoutlayer5   selfdropout6  dropout  rate01  namededropoutlayer6      dense output layer selfoutputdenselayer  timedistributed  dense  1   name  outputlayer   def call  self  x  encontext   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  encontext  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  encontext  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  encontext  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  encontext  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  encontext  sublayer6out  selfdropout6  sublayer6out     output dense layer output  selfoutputdenselayer  sublayer6out  return output,build model train tensorflow keras sub class write custom encoder decoder layer implement architecture describe attention need paper  everything work fine try compile  get one error  run sample datum compile call fit method train model throw another error   m go provide block might implementing incorrectly let know code need debug  tf version  2140 multi  head sub layer positional encoding layer  class mhasublayer  layer   def   init    self  unit    kwargs   super   init     selfmha  multiheadattention  keydim  unit    kwargs  selfinnerdense  timedistributed  dense  2048  activationrelu    selfouterdense  timedistributed  dense  unit   selflayernormmha  layernormalization   selflayernormff  layernormalization   selfadd  add   def call  self  x  context    kwarg      calculate attention output attnout  attnscore  selfmha  query  x  value  context  returnattentionscore  true    kwargs  attnresidcnxt  selfadd   x  attnout     residual connection attnlayernorm  selflayernormmha  attnresidcnxt  attnscore  tfreducemean  attnscore  axis1  selflastattentionweight  attnscore    pass attention output dense layer denseout  selfouterdense  selfinnerdense  attnlayernorm   denseresidcnxt  selfadd   attnlayernorm  denseout      feed forward residual connection denselayernorm  selflayernormff  denseresidcnxt  return denselayernorm class positionalencodinglayer  layer   def   init    self    kwargs   super   init     selfadd  add   def getpositionalencoding  self  x   seqlen  xshape  0   xshape  1  p  npzeros   seqlen    k range  seqlen   nparange  int  d2    denominator  nppower  10000  2  i  d  p  k  2    npsin  k  denominator  p  k  2  i1   npcos  k  denominator  return p def call  self  x    posenc    posenc  tfmapfn  fn  selfgetpositionalencoding  elem  x   n  elm enumerate  x    p  selfgetpositionalencoding  elm   posencappend  p   posenc  tfconverttotensor  posenc  posembedding  selfadd   x  posenc   return posembedding encoder  decoder block  class encoder  layer   def   init    self  unit  embedinputdim  nameencoder     kwargs   super   init        encoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  nameenembedlayer   selfposembedde  positionalencodinglayer  nameenpositionalembedlayer      encoder multi  head self attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  nameenmhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  nameenmhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  nameenmhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  nameenmhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  nameenmhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  nameenmhalayer6      encoder mha dropout layer selfdropout  dropout  rate01  nameendropoutposenc   selfdropout1  dropout  rate01  nameendropoutlayer1   selfdropout2  dropout  rate01  nameendropoutlayer2   selfdropout3  dropout  rate01  nameendropoutlayer3   selfdropout4  dropout  rate01  nameendropoutlayer4   selfdropout5  dropout  rate01  nameendropoutlayer5   selfdropout6  dropout  rate01  nameendropoutlayer6   def call  self  x   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  sublayer1out  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  sublayer2out  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  sublayer3out  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  sublayer4out  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  sublayer5out  sublayer6out  selfdropout6  sublayer6out  return sublayer6out class decoder  layer   def   init    self  unit  embedinputdim  namedecoder     kwargs   super   init        decoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  namedeembedlayer   selfposembedde  positionalencodinglayer  namedepositionalembedlayer      decoder multi  head attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  namedemhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  namedemhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  namedemhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  namedemhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  namedemhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  namedemhalayer6      decoder mha droput layer selfdropout  dropout  rate01  namededropoutposenc   selfdropout1  dropout  rate01  namededropoutlayer1   selfdropout2  dropout  rate01  namededropoutlayer2   selfdropout3  dropout  rate01  namededropoutlayer3   selfdropout4  dropout  rate01  namededropoutlayer4   selfdropout5  dropout  rate01  namededropoutlayer5   selfdropout6  dropout  rate01  namededropoutlayer6      dense output layer selfoutputdenselayer  timedistributed  dense  1   name  outputlayer   def call  self  x  encontext   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  encontext  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  encontext  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  encontext  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  encontext  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  encontext  sublayer6out  selfdropout6  sublayer6out     output dense layer output  selfoutputdenselayer  sublayer6out  output  tfround  tfab  output   return output sample data  nprandomseed  42  trainx  nprandomrandint  0  high250  size  512   trainxtin  nprandomrandint  0  high250  size  53   trainy  nprandomrandint  0  high250  size  531   modelling block  training shape    1616304  12    1616304  3    1616304  3  1     model sub  class class trxster  model   def   init    self  unit  enembeddim  deembeddim  nametrxster     kwargs   super   init     selfencoder  encoder  unit  enembeddim  selfdecoder  decoder  unit  deembeddim  def call  self  input   contextvec  targetin  input context  selfencoder  contextvec  pred  selfdecoder  targetin  context  return pred forecastor  trxster  hsize  embeddim  embeddim  forecastorbuild    12  1    3  1    forecastorsummary   error1  typeerror  error converting shape tensorshape  dimension value must integer none index method  get value   12  1   type   class  tuple     run model example  hsize  512 embeddim  268 forecastor  trxster  hsize  embeddim  embeddim  forecastor   trainx  trainxtin   model    trxster11                                                                   layer  type  output shape param                                                                   encoder13  encoder  multiple 63156224 decoder13  decoder  multiple 63156737                                                                  total param  126312961  48185 mb  trainable param  126312961  48185 mb  non  trainable param  0  000 byte                                                                      fit model batchsize  64 epoch  100 step  trainxshape  0  batchsize warmupsteps  steps25 class mylrschedule  tfkerasoptimizersschedule  learningrateschedule   def   init    self  dmodel  warmupsteps   selfdmodel  dmodel selfwarmupstep  warmupsteps def   call    self  step   stepnum  stepnumpy   selflr    denom  selfdmodel    05  numer  min  stepnum    05   stepnum   selfwarmupstep    15    lrate  npdivide  numer  denom  selflrappend  lrate  return lrate opt  tfkerasoptimizer  adam  learningrate  mylrschedule  hsize  warmupsteps   beta109  beta2098  epsilon1e8     configure trxster checkpointfilepath   trainingckpt  cb   tfkerascallbacks  earlystopping  patience10  monitorvalloss   restorebestweight  true   tfkerascallback  modelcheckpoint  filepath  checkpointfilepath  saveweightsonly  true  monitorvalloss   modemin   verbose1  savebestonly  true   loss  tfkeraslosse  meansquarederror   metric   tfkerasmetrics  accuracy    tfkeraslosse  meanabsoluteerror    forecastorcompile  optimizer  opt  lossmeansquarederror   metrics   acc    meanabsoluteerror    history  forecastorfit   trainx  trainxtin   trainy  batchsize  batchsize  stepsperepoch  step  epochs1  validationdata   valx  valxtin   valy   callback  cb  error2  provide line error trace  valueerror  gradient provide variable     trxster11  encoder13  enembedlayer  embeddings0    trxster11  encoder13  mhasublayer157  enmhalayer1  query  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  query  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  key  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  key  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  value  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  value  bias0    trxster11  encoder13  mhasublayer157  enmhalayer1  attentionoutput  kernel0    trxster11  encoder13  mhasublayer157  enmhalayer1  attentionoutput  bias0    trxster11  encoder13  mhasublayer157  timedistributed314  kernel0  every example see tell work way write not  figure  gradient calculation fail tensorflow function graph case network apply tfround tfabs output layer decoder  fail gradient calculation  removed model train expect  link issue   githubcom  tensorflow  tensorflow  issues1511  decoder  class decoder  layer   def   init    self  unit  embedinputdim  namedecoder     kwargs   super   init        decoder input embed layer selfembedde  embed  inputdim  embedinputdim  outputdim  unit  namedeembedlayer   selfposembedde  positionalencodinglayer  namedepositionalembedlayer      decoder multi  head attention sub layer selfmhasublayer1  mhasublayer  unit  numheads8  namedemhalayer1   selfmhasublayer2  mhasublayer  unit  numheads8  namedemhalayer2   selfmhasublayer3  mhasublayer  unit  numheads8  namedemhalayer3   selfmhasublayer4  mhasublayer  unit  numheads8  namedemhalayer4   selfmhasublayer5  mhasublayer  unit  numheads8  namedemhalayer5   selfmhasublayer6  mhasublayer  unit  numheads8  namedemhalayer6      decoder mha droput layer selfdropout  dropout  rate01  namededropoutposenc   selfdropout1  dropout  rate01  namededropoutlayer1   selfdropout2  dropout  rate01  namededropoutlayer2   selfdropout3  dropout  rate01  namededropoutlayer3   selfdropout4  dropout  rate01  namededropoutlayer4   selfdropout5  dropout  rate01  namededropoutlayer5   selfdropout6  dropout  rate01  namededropoutlayer6      dense output layer selfoutputdenselayer  timedistributed  dense  1   name  outputlayer   def call  self  x  encontext   embeddingoutput  selfembedding  x  positionalembedde  selfposembedde  embeddingoutput  postitionalembedding  selfdropout  positionalembedde     first mha sub  layer sublayer1out  selfmhasublayer1  positionalembedding  positionalembedde  sublayer1out  selfdropout1  sublayer1out     second mha sub  layer sublayer2out  selfmhasublayer2  sublayer1out  encontext  sublayer2out  selfdropout2  sublayer2out     third mha sub  layer sublayer3out  selfmhasublayer3  sublayer2out  encontext  sublayer3out  selfdropout3  sublayer3out     fourth mha sub  layer sublayer4out  selfmhasublayer4  sublayer3out  encontext  sublayer4out  selfdropout4  sublayer4out     fifth mha sub  layer sublayer5out  selfmhasublayer5  sublayer4out  encontext  sublayer5out  selfdropout5  sublayer5out     sixth mha sub  layer sublayer6out  selfmhasublayer6  sublayer5out  encontext  sublayer6out  selfdropout6  sublayer6out     output dense layer output  selfoutputdenselayer  sublayer6out  return output,Basic Understanding
Importing deepspeech error: Module not found,"I am trying to install DeepSearch library so that I can use the pretrained model to build the Speech to Text Project. ModuleNotFoundError Traceback (most recent call last) <ipython-input-8-a1bcd52700aa> in <cell line: 1>() ----> 1 import deepspeech 2 3 # Path to the pre-trained model files 4 model_path = ""/content/deepspeech-0.9.3-models.pbmm"" 5 scorer_path = ""/content/deepspeech-0.9.3-models.scorer"" # Optional, for improved performance ModuleNotFoundError: No module named 'deepspeech' I have got the module not found error, and I tried using version also but it gives me the error of couldn't found the suitable version. ERROR: Could not find a version that satisfies the requirement deepspeech==0.9.3 (from versions: none) ERROR: No matching distribution found for deepspeech==0.9.3 I tried using deepspeech github directory when I tried to install the setupfile and requirements.txt. I have got the error: Collecting absl-py==0.9.0 Downloading absl-py-0.9.0.tar.gz (104 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.0/104.0 kB 3.3 MB/s eta 0:00:00 error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. Preparing metadata (setup.py) ... error error: metadata-generation-failed × Encountered error while generating package metadata. ╰─> See above for output. So how to install the deepspeech library?","['python', 'deep-learning', 'nlp', 'mozilla-deepspeech']",1,"Latest release of deepspeech was 3 years back in December 2020: You are not able to install deepsearch because it only supports python <= 3.6 which is outdated. If you want to use, you can download older version of python separately and install deepsearch. https://pypi.org/project/deepspeech/#description",2023-11-24 16:48:03,2023-11-25 08:52:46,255,https://stackoverflow.com/questions/77544604/importing-deepspeech-error-module-not-found,"Importing deepspeech error: Module not found I am trying to install DeepSearch library so that I can use the pretrained model to build the Speech to Text Project. ModuleNotFoundError Traceback (most recent call last) <ipython-input-8-a1bcd52700aa> in <cell line: 1>() ----> 1 import deepspeech 2 3 # Path to the pre-trained model files 4 model_path = ""/content/deepspeech-0.9.3-models.pbmm"" 5 scorer_path = ""/content/deepspeech-0.9.3-models.scorer"" # Optional, for improved performance ModuleNotFoundError: No module named 'deepspeech' I have got the module not found error, and I tried using version also but it gives me the error of couldn't found the suitable version. ERROR: Could not find a version that satisfies the requirement deepspeech==0.9.3 (from versions: none) ERROR: No matching distribution found for deepspeech==0.9.3 I tried using deepspeech github directory when I tried to install the setupfile and requirements.txt. I have got the error: Collecting absl-py==0.9.0 Downloading absl-py-0.9.0.tar.gz (104 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.0/104.0 kB 3.3 MB/s eta 0:00:00 error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. Preparing metadata (setup.py) ... error error: metadata-generation-failed × Encountered error while generating package metadata. ╰─> See above for output. So how to install the deepspeech library?",import deepspeech error  module find try install deepsearch library use pretraine model build speech text project  modulenotfounderror traceback  recent call last   ipython  input8  a1bcd52700aa   cell line  1       1 import deepspeech 2 3  path pre  train model file 4 modelpath    content  deepspeech093  modelspbmm  5 scorerpath    content  deepspeech093  modelsscorer   optional  improved performance modulenotfounderror  module name  deepspeech  get module find error  try use version also give error could not find suitable version  error  could find version satisfie requirement deepspeech093  version  none  error  match distribution find deepspeech093 try use deepspeech github directory try install setupfile requirementstxt  get error  collect absl  py090 download abslpy090targz  104 kb                                          10401040 kb 33 mb  s eta 00000 error  subprocess  exit  with  error  python setuppy egginfo run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip  prepare metadata  setuppy   error error  metadata  generation  fail  encountered error generating package metadata     see output  install deepspeech library ,late release deepspeech 3 year back december 2020  able install deepsearch support python   36 outdate  want use  download old version python separately install deepsearch    pypiorg  project  deepspeech  description,import deepspeech error  module find try install deepsearch library use pretraine model build speech text project  modulenotfounderror traceback  recent call last   ipython  input8  a1bcd52700aa   cell line  1       1 import deepspeech 2 3  path pre  train model file 4 modelpath    content  deepspeech093  modelspbmm  5 scorerpath    content  deepspeech093  modelsscorer   optional  improved performance modulenotfounderror  module name  deepspeech  get module find error  try use version also give error could not find suitable version  error  could find version satisfie requirement deepspeech093  version  none  error  match distribution find deepspeech093 try use deepspeech github directory try install setupfile requirementstxt  get error  collect absl  py090 download abslpy090targz  104 kb                                          10401040 kb 33 mb  s eta 00000 error  subprocess  exit  with  error  python setuppy egginfo run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip  prepare metadata  setuppy   error error  metadata  generation  fail  encountered error generating package metadata     see output  install deepspeech library  late release deepspeech 3 year back december 2020  able install deepsearch support python   36 outdate  want use  download old version python separately install deepsearch    pypiorg  project  deepspeech  description,Library/Tool-Based Queries
How to Integrate Custom OpenAIModel into a AutoModelForSequenceClassification Model?,"I've developed a custom OpenAIModel module that acts like BERT models but makes an OpenAI embeddings request and returns the results when called. I want to use this module, utilizing Hugging Face's transformers library inside an AutoModelForSequenceClassification model. However, when I try to replace the base_model with my OpenAIModel using the following code, the base_model property doesn't change as expected: from transformers import AutoModelForSequenceClassification import torch num_labels = ... # Define the appropriate number of labels train_arch = ... # Define parameters specific to your training architecture model = AutoModelForSequenceClassification.from_pretrained('dbmdz/distilbert-base-turkish-cased', num_labels=num_labels) model.base_model = OpenAIModel(train_arch) model.classifier = torch.nn.Linear(model.base_model.config.dim, num_labels) model.config = OpenAIModelConfig() model.config_class = type(OpenAIModelConfig) tokenizer = OpenAITokenizer() After executing model.base_model = OpenAIModel(train_arch) , when I inspect model.base_model, it still appears to be distilbert and not the expected OpenAIModel. I can successfully change all other values of the SequenceClassification model, but the base_model remains unaltered. I'm unsure why this is happening and how to resolve it. What is the correct way to integrate my model into AutoModelForSequenceClassification? Or should I be using a different approach to change the base_model? (I tried to load my model with from_pretrained but AutoModel class doesn't support custom model structure as i see) Any help or suggestions would be greatly appreciated!","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",1,"If we are using a distilbert model, changing the model.distilbert layer, not model.base_model, was enough to do what I intended. When I did model.distilbert = OpenAIModel(**kwargs) I achieved my goal.",2023-11-24 15:26:38,2023-11-25 08:46:35,92,https://stackoverflow.com/questions/77544203/how-to-integrate-custom-openaimodel-into-a-automodelforsequenceclassification-mo,"How to Integrate Custom OpenAIModel into a AutoModelForSequenceClassification Model? I've developed a custom OpenAIModel module that acts like BERT models but makes an OpenAI embeddings request and returns the results when called. I want to use this module, utilizing Hugging Face's transformers library inside an AutoModelForSequenceClassification model. However, when I try to replace the base_model with my OpenAIModel using the following code, the base_model property doesn't change as expected: from transformers import AutoModelForSequenceClassification import torch num_labels = ... # Define the appropriate number of labels train_arch = ... # Define parameters specific to your training architecture model = AutoModelForSequenceClassification.from_pretrained('dbmdz/distilbert-base-turkish-cased', num_labels=num_labels) model.base_model = OpenAIModel(train_arch) model.classifier = torch.nn.Linear(model.base_model.config.dim, num_labels) model.config = OpenAIModelConfig() model.config_class = type(OpenAIModelConfig) tokenizer = OpenAITokenizer() After executing model.base_model = OpenAIModel(train_arch) , when I inspect model.base_model, it still appears to be distilbert and not the expected OpenAIModel. I can successfully change all other values of the SequenceClassification model, but the base_model remains unaltered. I'm unsure why this is happening and how to resolve it. What is the correct way to integrate my model into AutoModelForSequenceClassification? Or should I be using a different approach to change the base_model? (I tried to load my model with from_pretrained but AutoModel class doesn't support custom model structure as i see) Any help or suggestions would be greatly appreciated!",integrate custom openaimodel automodelforsequenceclassification model   ve develop custom openaimodel module act like bert model make openai embedding request return result call  want use module  utilize hugging face s transformer library inside automodelforsequenceclassification model  however  try replace basemodel openaimodel use follow code  basemodel property not change expect  transformer import automodelforsequenceclassification import torch numlabel    define appropriate number label trainarch    define parameter specific training architecture model  automodelforsequenceclassificationfrompretraine   dbmdz  distilbert  base  turkish  case   numlabel  numlabels  modelbasemodel  openaimodel  trainarch  modelclassifi  torchnn  linear  modelbasemodelconfigdim  numlabels  modelconfig  openaimodelconfig   modelconfigclass  type  openaimodelconfig  tokenizer  openaitokenizer   execute modelbasemodel  openaimodel  trainarch   inspect modelbasemodel  still appear distilbert expect openaimodel  successfully change value sequenceclassification model  basemodel remain unaltered   m unsure happen resolve  correct way integrate model automodelforsequenceclassification  use different approach change basemodel   try load model frompretrained automodel class not support custom model structure see  help suggestion would greatly appreciate ,use distilbert model  change modeldistilbert layer  modelbasemodel  enough intend  modeldistilbert  openaimodel    kwarg  achieve goal ,integrate custom openaimodel automodelforsequenceclassification model   ve develop custom openaimodel module act like bert model make openai embedding request return result call  want use module  utilize hugging face s transformer library inside automodelforsequenceclassification model  however  try replace basemodel openaimodel use follow code  basemodel property not change expect  transformer import automodelforsequenceclassification import torch numlabel    define appropriate number label trainarch    define parameter specific training architecture model  automodelforsequenceclassificationfrompretraine   dbmdz  distilbert  base  turkish  case   numlabel  numlabels  modelbasemodel  openaimodel  trainarch  modelclassifi  torchnn  linear  modelbasemodelconfigdim  numlabels  modelconfig  openaimodelconfig   modelconfigclass  type  openaimodelconfig  tokenizer  openaitokenizer   execute modelbasemodel  openaimodel  trainarch   inspect modelbasemodel  still appear distilbert expect openaimodel  successfully change value sequenceclassification model  basemodel remain unaltered   m unsure happen resolve  correct way integrate model automodelforsequenceclassification  use different approach change basemodel   try load model frompretrained automodel class not support custom model structure see  help suggestion would greatly appreciate  use distilbert model  change modeldistilbert layer  modelbasemodel  enough intend  modeldistilbert  openaimodel    kwarg  achieve goal ,Library/Tool-Based Queries
Clearing context window of LLM in Huggingface,"I want to use inference to ask different questions to LLMs taken from huggingface. But, I want to ask the prompts without the model having info about the previous prompts. Does the model automatically store the previous prompts in context? Or does it not save any previous information at all and we need to provide all the context in the same prompt?","['nlp', 'huggingface-transformers', 'transformer-model', 'huggingface', 'large-language-model']",1,"LLMs generally don't store your prompts or context or directly learn from it. After training LLMs stay static and the models weights don't change during inference. IF you want to build a chatbot you have to actively build something to keep the context. One solution for handling context is LangChain ( https://github.com/langchain-ai/langchain ). But for your case, you just need to prompt to the LLM or the API to the LLM directly.",2023-11-24 04:11:30,2023-11-24 06:40:49,1288,https://stackoverflow.com/questions/77540677/clearing-context-window-of-llm-in-huggingface,"Clearing context window of LLM in Huggingface I want to use inference to ask different questions to LLMs taken from huggingface. But, I want to ask the prompts without the model having info about the previous prompts. Does the model automatically store the previous prompts in context? Or does it not save any previous information at all and we need to provide all the context in the same prompt?",clear context window llm huggingface want use inference ask different question llms take huggingface   want ask prompt without model info previous prompt  model automatically store previous prompt context  save previous information need provide context prompt ,llms generally not store prompt context directly learn  train llms stay static model weights not change inference  want build chatbot actively build something keep context  one solution handle context langchain    githubcom  langchain  ai  langchain   case  need prompt llm api llm directly ,clear context window llm huggingface want use inference ask different question llms take huggingface   want ask prompt without model info previous prompt  model automatically store previous prompt context  save previous information need provide context prompt  llms generally not store prompt context directly learn  train llms stay static model weights not change inference  want build chatbot actively build something keep context  one solution handle context langchain    githubcom  langchain  ai  langchain   case  need prompt llm api llm directly ,Library/Tool-Based Queries
NLP pre-processing on two columns in data frame gives error,"I have the following data frame: gmeDateDf.head(2) title score id url comms_num body timestamp It's not about the money, it's about sending a... 55.0 l6ulcx https://v.redd.it/6j75regs72e61 6.0 NaN 2021-01-28 21:37:41 Math Professor Scott Steiner says the numbers ... 110.0 l6uibd https://v.redd.it/ah50lyny62e61 23.0 NaN 2021-01-28 21:32:10 I have the following function to pre-process the text (with the proper libraries imported and so on): def preprocess_text(text): # Tokenize words tokens = word_tokenize(text.lower()) # Remove stopwords and non-alphabetic words, and lemmatize processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words] return processed_tokens Then calling it on specific column: gmeDateDf.loc[:, 'body'] = gmeDateDf['body'].fillna('NaN').astype(str) gmeDateDfProcessed = gmeDateDf['body'].apply(preprocess_text) That works properly as expected. However, when I try to do it on two columns, like so: gmeDateDf.loc[:, 'title','body'] = gmeDateDf['title', 'body'].fillna('NaN').astype(str) gmeDateDfProcessed = gmeDateDf['title', 'body'].apply(preprocess_text) I get the following error: 3802 return self._engine.get_loc(casted_key) 3803 except KeyError as err: -> 3804 raise KeyError(key) from err 3805 except TypeError: 3806 # If we have a listlike key, _check_indexing_error will raise KeyError: ('title', 'body') I’ve looked around, asked chatGPT for some help, but I can’t figure it out. Please, bear with me as I’m still learning the basics of Python. Why can’t I give it a listlike key? And why is it a listlike key only when I have the two columns? When I call it only on gmeDateDf.loc[:, 'body'] it is some kind of listlike key, no? So why would it not work otherwise? I’m confused, and don’t even know where to look to see what I’m doing wrong now.","['python', 'pandas', 'dataframe', 'nlp', 'nltk']",1,"As the error suggests, using gmeDateDf['title', 'body'] attempts to find a column in the DataFrame under the following key: ('title', 'body') . No column in your DataFrame is called that, therefore the code fails. If you wish to select multiple columns at once, you need to provide them in a list, like so: gmeDateDf[['title', 'body']] . For more information, head to the documentation page on data selection from a DataFrame . Given your specific example, you will need to fix the data selection, and then use some string vectorisation , something like: gmeDateDfProcessed[['title', 'body']] = gmeDateDf[['title', 'body']].apply(lambda x: preprocess_text(x.str))",2023-11-22 22:54:17,2023-11-23 16:02:09,131,https://stackoverflow.com/questions/77533488/nlp-pre-processing-on-two-columns-in-data-frame-gives-error,"NLP pre-processing on two columns in data frame gives error I have the following data frame: gmeDateDf.head(2) title score id url comms_num body timestamp It's not about the money, it's about sending a... 55.0 l6ulcx https://v.redd.it/6j75regs72e61 6.0 NaN 2021-01-28 21:37:41 Math Professor Scott Steiner says the numbers ... 110.0 l6uibd https://v.redd.it/ah50lyny62e61 23.0 NaN 2021-01-28 21:32:10 I have the following function to pre-process the text (with the proper libraries imported and so on): def preprocess_text(text): # Tokenize words tokens = word_tokenize(text.lower()) # Remove stopwords and non-alphabetic words, and lemmatize processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words] return processed_tokens Then calling it on specific column: gmeDateDf.loc[:, 'body'] = gmeDateDf['body'].fillna('NaN').astype(str) gmeDateDfProcessed = gmeDateDf['body'].apply(preprocess_text) That works properly as expected. However, when I try to do it on two columns, like so: gmeDateDf.loc[:, 'title','body'] = gmeDateDf['title', 'body'].fillna('NaN').astype(str) gmeDateDfProcessed = gmeDateDf['title', 'body'].apply(preprocess_text) I get the following error: 3802 return self._engine.get_loc(casted_key) 3803 except KeyError as err: -> 3804 raise KeyError(key) from err 3805 except TypeError: 3806 # If we have a listlike key, _check_indexing_error will raise KeyError: ('title', 'body') I’ve looked around, asked chatGPT for some help, but I can’t figure it out. Please, bear with me as I’m still learning the basics of Python. Why can’t I give it a listlike key? And why is it a listlike key only when I have the two columns? When I call it only on gmeDateDf.loc[:, 'body'] it is some kind of listlike key, no? So why would it not work otherwise? I’m confused, and don’t even know where to look to see what I’m doing wrong now.",nlp pre  processing two column datum frame give error follow datum frame  gmedatedfhead  2  title score i d url commsnum body timestamp s money  be send  550 l6ulcx   vreddit6j75regs72e61 60 nan 2021  01  28 213741 math professor scott steiner say number  1100 l6uibd   vreddit  ah50lyny62e61 230 nan 2021  01  28 213210 follow function pre  process text  proper library import   def preprocesstext  text    tokenize word token  wordtokenize  textlower     remove stopword non  alphabetic word  lemmatize processedtokens   lemmatizerlemmatize  word  word tokens wordisalpha   word stopwords  return processedtokens call specific column  gmedatedfloc     body    gmedatedf   body   fillna   nan   astype  str  gmedatedfprocesse  gmedatedf   body   apply  preprocesstext  work properly expect  however  try two column  like  gmedatedfloc     title    body    gmedatedf   title    body   fillna   nan   astype  str  gmedatedfprocesse  gmedatedf   title    body   apply  preprocesstext  get follow error  3802 return selfenginegetloc  castedkey  3803 except keyerror err    3804 raise keyerror  key  err 3805 except typeerror  3806  listlike key   checkindexingerror raise keyerror    title    body    look around  ask chatgpt help   figure  please  bear  still learn basic python   give listlike key  listlike key two column  call gmedatedfloc     body   kind listlike key   would work otherwise   confused   even know look see  wrong ,error suggest  use gmedatedf   title    body   attempt find column dataframe follow key    title    body    column dataframe call  therefore code fail  wish select multiple column  need provide list  like  gmedatedf    title    body     information  head documentation page datum selection dataframe  give specific example  need fix datum selection  use string vectorisation  something like  gmedatedfprocesse    title    body     gmedatedf    title    body    apply  lambda x  preprocesstext  xstr  ,nlp pre  processing two column datum frame give error follow datum frame  gmedatedfhead  2  title score i d url commsnum body timestamp s money  be send  550 l6ulcx   vreddit6j75regs72e61 60 nan 2021  01  28 213741 math professor scott steiner say number  1100 l6uibd   vreddit  ah50lyny62e61 230 nan 2021  01  28 213210 follow function pre  process text  proper library import   def preprocesstext  text    tokenize word token  wordtokenize  textlower     remove stopword non  alphabetic word  lemmatize processedtokens   lemmatizerlemmatize  word  word tokens wordisalpha   word stopwords  return processedtokens call specific column  gmedatedfloc     body    gmedatedf   body   fillna   nan   astype  str  gmedatedfprocesse  gmedatedf   body   apply  preprocesstext  work properly expect  however  try two column  like  gmedatedfloc     title    body    gmedatedf   title    body   fillna   nan   astype  str  gmedatedfprocesse  gmedatedf   title    body   apply  preprocesstext  get follow error  3802 return selfenginegetloc  castedkey  3803 except keyerror err    3804 raise keyerror  key  err 3805 except typeerror  3806  listlike key   checkindexingerror raise keyerror    title    body    look around  ask chatgpt help   figure  please  bear  still learn basic python   give listlike key  listlike key two column  call gmedatedfloc     body   kind listlike key   would work otherwise   confused   even know look see  wrong  error suggest  use gmedatedf   title    body   attempt find column dataframe follow key    title    body    column dataframe call  therefore code fail  wish select multiple column  need provide list  like  gmedatedf    title    body     information  head documentation page datum selection dataframe  give specific example  need fix datum selection  use string vectorisation  something like  gmedatedfprocesse    title    body     gmedatedf    title    body    apply  lambda x  preprocesstext  xstr  ,Library/Tool-Based Queries
LLM slow inference even on A100 GPU,"I am planning to deploy a fine-tuned version of Open-Orca-Platypus-2. It takes around 13.5GB on the GPU. I tried using g4dn.12xlarge in AWS which has 4 GPUs, but the inference still takes around 40 seconds. I also tried it on A100 GPU provided by Colab, but still the same. What am I doing wrong? Do I still need more computational power or is anything wrong with my code? from transformers import AutoTokenizer, AutoModelForCausalLM import torch import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3"" device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # Load model model = AutoModelForCausalLM.from_pretrained( model_path, device_map=""auto"" ) # Set the model to evaluation mode model.eval() # Load tokenizer tokenizer = AutoTokenizer.from_pretrained(""Open-Orca/OpenOrca-Platypus2-13B"", trust_remote_code=True) def ask_bot(question): with torch.no_grad(): # Tokenize input question input_ids = tokenizer.encode(question, return_tensors=""pt"").cuda() # Generate output output = model.module.generate( input_ids, max_length=200, num_return_sequences=1, do_sample=True, top_k=50 ) # Decode and extract the response generated_text = tokenizer.decode(output[0], skip_special_tokens=True) response = generated_text.split(""->:"")[-1] return response","['nlp', 'gpu', 'huggingface-transformers', 'large-language-model']",2,"It does take a long time to generate an output even on powerful GPUs. My use-case was a chatbot, so I figured it would be ideal to stream the output token by token as generated by the model. This reduced the perceived time although the actual output would remain the same. from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer with torch.no_grad(): # Tokenize input question input_ids = tokenizer.encode(question, return_tensors=""pt"", truncation=True).cuda() streamer = TextIteratorStreamer( tokenizer=tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True ) def generate_and_signal_complete(): output = model.generate( input_ids, max_length=1500, num_return_sequences=1, do_sample=True, top_k=50, streamer=streamer ) t1 = Thread(target=generate_and_signal_complete) t1.start() # Decode and extract the response for new_text in streamer: yield new_text",2023-11-19 15:31:45,2023-12-03 03:16:22,1402,https://stackoverflow.com/questions/77511368/llm-slow-inference-even-on-a100-gpu,"LLM slow inference even on A100 GPU I am planning to deploy a fine-tuned version of Open-Orca-Platypus-2. It takes around 13.5GB on the GPU. I tried using g4dn.12xlarge in AWS which has 4 GPUs, but the inference still takes around 40 seconds. I also tried it on A100 GPU provided by Colab, but still the same. What am I doing wrong? Do I still need more computational power or is anything wrong with my code? from transformers import AutoTokenizer, AutoModelForCausalLM import torch import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3"" device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # Load model model = AutoModelForCausalLM.from_pretrained( model_path, device_map=""auto"" ) # Set the model to evaluation mode model.eval() # Load tokenizer tokenizer = AutoTokenizer.from_pretrained(""Open-Orca/OpenOrca-Platypus2-13B"", trust_remote_code=True) def ask_bot(question): with torch.no_grad(): # Tokenize input question input_ids = tokenizer.encode(question, return_tensors=""pt"").cuda() # Generate output output = model.module.generate( input_ids, max_length=200, num_return_sequences=1, do_sample=True, top_k=50 ) # Decode and extract the response generated_text = tokenizer.decode(output[0], skip_special_tokens=True) response = generated_text.split(""->:"")[-1] return response",llm slow inference even a100 gpu planning deploy fine  tune version open  orca  platypus2  take around 135 gb gpu  try use g4dn12xlarge aw 4 gpu  inference still take around 40 second  also try a100 gpu provide colab  still  wrong  still need computational power anything wrong code  transformer import autotokenizer  automodelforcausallm import torch import os osenviron    cudavisibledevice      0123  device  torchdevice    cuda  torchcudaisavailable   else   cpu    load model model  automodelforcausallmfrompretraine  modelpath  devicemap  auto    set model evaluation mode modeleval    load tokenizer tokenizer  autotokenizerfrompretraine    open  orca  openorca  platypus2  13b   trustremotecode  true  def askbot  question   torchnograd     tokenize input question inputids  tokenizerencode  question  returntensors  pt   cuda    generate output output  modelmodulegenerate  inputids  maxlength200  numreturnsequences1  dosample  true  topk50   decode extract response generatedtext  tokenizerdecode  output  0   skipspecialtoken  true  response  generatedtextsplit          1  return response,take long time generate output even powerful gpu  use  case chatbot  figure would ideal stream output token token generate model  reduce perceive time although actual output would remain  transformer import autotokenizer  automodelforcausallm  textiteratorstreamer torchnograd     tokenize input question inputids  tokenizerencode  question  returntensors  pt   truncation  true  cuda   streamer  textiteratorstreamer  tokenizer  tokenizer  timeout600  skipprompt  true  skipspecialtoken  true  def generateandsignalcomplete    output  modelgenerate  inputids  maxlength1500  numreturnsequences1  dosample  true  topk50  streamer  streamer  t1  thread  target  generateandsignalcomplete  t1start    decode extract response newtext streamer  yield newtext,llm slow inference even a100 gpu planning deploy fine  tune version open  orca  platypus2  take around 135 gb gpu  try use g4dn12xlarge aw 4 gpu  inference still take around 40 second  also try a100 gpu provide colab  still  wrong  still need computational power anything wrong code  transformer import autotokenizer  automodelforcausallm import torch import os osenviron    cudavisibledevice      0123  device  torchdevice    cuda  torchcudaisavailable   else   cpu    load model model  automodelforcausallmfrompretraine  modelpath  devicemap  auto    set model evaluation mode modeleval    load tokenizer tokenizer  autotokenizerfrompretraine    open  orca  openorca  platypus2  13b   trustremotecode  true  def askbot  question   torchnograd     tokenize input question inputids  tokenizerencode  question  returntensors  pt   cuda    generate output output  modelmodulegenerate  inputids  maxlength200  numreturnsequences1  dosample  true  topk50   decode extract response generatedtext  tokenizerdecode  output  0   skipspecialtoken  true  response  generatedtextsplit          1  return response take long time generate output even powerful gpu  use  case chatbot  figure would ideal stream output token token generate model  reduce perceive time although actual output would remain  transformer import autotokenizer  automodelforcausallm  textiteratorstreamer torchnograd     tokenize input question inputids  tokenizerencode  question  returntensors  pt   truncation  true  cuda   streamer  textiteratorstreamer  tokenizer  tokenizer  timeout600  skipprompt  true  skipspecialtoken  true  def generateandsignalcomplete    output  modelgenerate  inputids  maxlength1500  numreturnsequences1  dosample  true  topk50  streamer  streamer  t1  thread  target  generateandsignalcomplete  t1start    decode extract response newtext streamer  yield newtext,Implementation Issues
Is this right way to merge LoRA weights?,"I have fine-tuned RoBERTa using LoRA with HuggingFace libraries, produced multiple LoRA files. I want to merge those LoRA weights without changing original model. So I wrote code like below. from peft import ( ... PeftModel, ... ) from transformers import ( ... RobertaForSequenceClassification ... ) model = RobertaForSequenceClassification.from_pretrained(""roberta-base"") # ""lora-1"" and ""lora-2"" are local directories. model = PeftModel.from_pretrained(model, ""lora-1"") model = model.merge_and_unload() model = PeftModel.from_pretrained(model, ""lora-2"") model = model.merge_and_unload() Code is working but bit suspicious because I don't know how merge_and_unload() working exactly. Guessing by name, I thought perhaps it merge all the LoRA weights to base model's weight and make it one final single model. But that's not how LoRA working, as far as I know. To conclude, I want to produce p(phi_0 + delta phi_1(theta_1) + delta phi_2(theta_2) + ... + delta phi_n(theta_n)) , not single merged p(phi_0) (Sorry for the equation text, I don't have enough reputation to link online equation SVG!) I couldn't find any more information than using merge_and_unload() . EDIT I found there is description about it ...shame that I didn't read it. merge_and_unload() This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model as a standalone model. So basically using merge_and_unload() is definitely not what I want. Is there any other options for merging LoRA weights? Any advice would be appreciated.","['pytorch', 'nlp', 'huggingface-transformers']",1,"Perhaps this is what you are looking for add_weighted_adapter . You can refer to this documentation and this issue model.add_weighted_adapter( adapters=['lora-1', 'lora-2'], weights=[0.5, 0.5], adapter_name=""combined"", combination_type=""svd"", )",2023-11-16 02:20:26,2023-11-17 12:41:56,8802,https://stackoverflow.com/questions/77491934/is-this-right-way-to-merge-lora-weights,"Is this right way to merge LoRA weights? I have fine-tuned RoBERTa using LoRA with HuggingFace libraries, produced multiple LoRA files. I want to merge those LoRA weights without changing original model. So I wrote code like below. from peft import ( ... PeftModel, ... ) from transformers import ( ... RobertaForSequenceClassification ... ) model = RobertaForSequenceClassification.from_pretrained(""roberta-base"") # ""lora-1"" and ""lora-2"" are local directories. model = PeftModel.from_pretrained(model, ""lora-1"") model = model.merge_and_unload() model = PeftModel.from_pretrained(model, ""lora-2"") model = model.merge_and_unload() Code is working but bit suspicious because I don't know how merge_and_unload() working exactly. Guessing by name, I thought perhaps it merge all the LoRA weights to base model's weight and make it one final single model. But that's not how LoRA working, as far as I know. To conclude, I want to produce p(phi_0 + delta phi_1(theta_1) + delta phi_2(theta_2) + ... + delta phi_n(theta_n)) , not single merged p(phi_0) (Sorry for the equation text, I don't have enough reputation to link online equation SVG!) I couldn't find any more information than using merge_and_unload() . EDIT I found there is description about it ...shame that I didn't read it. merge_and_unload() This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model as a standalone model. So basically using merge_and_unload() is definitely not what I want. Is there any other options for merging LoRA weights? Any advice would be appreciated.",right way merge lora weight  fine  tune roberta use lora huggingface library  produce multiple lora file  want merge lora weight without change original model  write code like  peft import   peftmodel    transformer import   robertaforsequenceclassification   model  robertaforsequenceclassificationfrompretrained    roberta  base      lora1    lora2  local directory  model  peftmodelfrompretraine  model    lora1   model  modelmergeandunload   model  peftmodelfrompretraine  model    lora2   model  modelmergeandunload   code working bit suspicious not know mergeandunload   work exactly  guess name  think perhaps merge lora weight base model s weight make one final single model  s lora work  far know  conclude  want produce p  phi0  delta phi1  theta1   delta phi2  theta2     delta phin  thetan    single merge p  phi0   sorry equation text  not enough reputation link online equation svg   could not find information use mergeandunload    edit find description  shame not read  mergeandunload   method merge lora layer base model  need someone want use base model standalone model  basically use mergeandunload   definitely want  option merge lora weight  advice would appreciate ,perhaps look addweightedadapter  refer documentation issue modeladdweightedadapter  adapters   lora1    lora2    weights  05  05   adaptername  combine   combinationtype  svd   ,right way merge lora weight  fine  tune roberta use lora huggingface library  produce multiple lora file  want merge lora weight without change original model  write code like  peft import   peftmodel    transformer import   robertaforsequenceclassification   model  robertaforsequenceclassificationfrompretrained    roberta  base      lora1    lora2  local directory  model  peftmodelfrompretraine  model    lora1   model  modelmergeandunload   model  peftmodelfrompretraine  model    lora2   model  modelmergeandunload   code working bit suspicious not know mergeandunload   work exactly  guess name  think perhaps merge lora weight base model s weight make one final single model  s lora work  far know  conclude  want produce p  phi0  delta phi1  theta1   delta phi2  theta2     delta phin  thetan    single merge p  phi0   sorry equation text  not enough reputation link online equation svg   could not find information use mergeandunload    edit find description  shame not read  mergeandunload   method merge lora layer base model  need someone want use base model standalone model  basically use mergeandunload   definitely want  option merge lora weight  advice would appreciate  perhaps look addweightedadapter  refer documentation issue modeladdweightedadapter  adapters   lora1    lora2    weights  05  05   adaptername  combine   combinationtype  svd   ,Library/Tool-Based Queries
unable to map color onto a network graph from an additional variable using ggraph,"I am attempting to create a text network graph. I am working with pivoted survey data, and attempting to associate words from open-ended comments with associated numeric responses. I've constructed word correlations and graphed them, but am having a devil of a time associating numeric values back into the network graph. I have experience with R, but I've not had formal training/classes and I feel confident I'm missing something pretty basic right now. I was able to successfully create a plot using the following code, assuming graph is my data frame, containing variables x (raw numeric score from the survey data), row_number (to tie individual word used back to its initial open ended comment), word, n (# of times ""word"" appears in the dataset), and y (average of x per word). graph %>% group_by(word) %>% filter(n() >= 1000)%>% pairwise_cor(word, row_number, upper=FALSE) %>% filter(correlation > .09) %>% graph_from_data_frame() %>% ggraph(layout = ""fr"") + geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) + geom_node_point(color = ""lightblue"", size = 5) + scale_color_gradient(low = ""red"", high = ""green"") + geom_node_text(aes(label = name), repel = TRUE) + theme_void() The pairwise_cor function essentially reshapes the dataframe into item1, item2, and correlation, dropping all other variables, meaning my relevant color-assigning variables were dropped, so I created a correlated words dataset, and then a final_df that joins individual word average scores (y) with the correlated_words dataset: final <- cor_df %>% left_join(filt_df, by = join_by(item1 == word)) %>% left_join(filt_df, by = join_by(item2 == word)) ""final"" now contains item1 (word 1), item2 (word 2), correlation, n.1, y.1, n.2, and y.2 (where n is count of words and y is a weird stat: the average of X, the original survey numeric score associated that that word). With the ""final"" data frame, I've now attempted a multitude of ways to map either y.1 or y.2 to the color of the nodes, generally something like: as_tbl_graph(final) ggraph(final, layout = ""fr"") + geom_node_point(aes(color = y.1), size = 5) + geom_node_text(aes(label = name), repel = TRUE) + scale_color_gradient(low = ""red"", high = ""green"") + theme_void() This is the error I receive: Error in geom_node_point() : ! Problem while computing aesthetics. ℹ Error occurred in the 1st layer. Caused by error in FUN() : ! object 'y.1' not found Not sure exactly where I'm going wrong, although I have been poring through the documentation for ggraph and tidygraph. I don't have a full conceptual understanding of the various layout possibilities, which I feel is likely where my issues lie (or possibly my confusion starts in the construction of the dataframe itself via as_tbl_graph?), and would really welcome any additional resources or documentation towards understanding those algorithms/customizing layouts. (I've read https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html and all of the ggraph vignettes!) My question, boiled down, is: how can I use a numeric variable to add a color dimension to nodes in a network graph using ggraph (or more specifically, what the heck am I doing wrong)? Thanks in advance for any help!","['r', 'nlp', 'visualization', 'ggraph']",1,"The first issue with your code is that you are passing the data.frame final to ggraph() instead of the tbl_graph object as_tbl_graph(final) . The second issue is that, when converting to a tbl_graph the y.1 and y.2 columns you added via the lef_join become a columns or features of the edges data not the nodes and are thus not available to be mapped on aesthetics in geom_node_xxx . To fix this second issue you have to convert cor_df to a tbl_graph first, then join your filt_df . This way the columns are added to the nodes data. Note: I do only one left_join as a second does not make sense for the nodes data. Also I renamed the column from y to value as I encountered a warning when using y . Using some fake data based on the highschool dataset from ggraph : library(ggraph) library(tidygraph) library(dplyr, warn.conflicts = FALSE) set.seed(123) # Create example data cor_df <- highschool names(cor_df) <- c(""item1"", ""item2"", ""correlation"") filt_df <- data.frame( word = as.character(unique(cor_df$item1)), y = runif(seq(length(unique(cor_df$item1)))) ) |> rename(value = y) final_graph <- as_tbl_graph(cor_df) |> left_join( filt_df, by = join_by(name == word) ) ggraph(final_graph, layout = ""fr"") + geom_node_point(aes(color = value), size = 5) + geom_node_text(aes(label = name), repel = TRUE) + scale_color_gradient(low = ""red"", high = ""green"") + theme_void()",2023-11-14 21:59:57,2023-11-15 08:21:49,70,https://stackoverflow.com/questions/77484087/unable-to-map-color-onto-a-network-graph-from-an-additional-variable-using-ggrap,"unable to map color onto a network graph from an additional variable using ggraph I am attempting to create a text network graph. I am working with pivoted survey data, and attempting to associate words from open-ended comments with associated numeric responses. I've constructed word correlations and graphed them, but am having a devil of a time associating numeric values back into the network graph. I have experience with R, but I've not had formal training/classes and I feel confident I'm missing something pretty basic right now. I was able to successfully create a plot using the following code, assuming graph is my data frame, containing variables x (raw numeric score from the survey data), row_number (to tie individual word used back to its initial open ended comment), word, n (# of times ""word"" appears in the dataset), and y (average of x per word). graph %>% group_by(word) %>% filter(n() >= 1000)%>% pairwise_cor(word, row_number, upper=FALSE) %>% filter(correlation > .09) %>% graph_from_data_frame() %>% ggraph(layout = ""fr"") + geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) + geom_node_point(color = ""lightblue"", size = 5) + scale_color_gradient(low = ""red"", high = ""green"") + geom_node_text(aes(label = name), repel = TRUE) + theme_void() The pairwise_cor function essentially reshapes the dataframe into item1, item2, and correlation, dropping all other variables, meaning my relevant color-assigning variables were dropped, so I created a correlated words dataset, and then a final_df that joins individual word average scores (y) with the correlated_words dataset: final <- cor_df %>% left_join(filt_df, by = join_by(item1 == word)) %>% left_join(filt_df, by = join_by(item2 == word)) ""final"" now contains item1 (word 1), item2 (word 2), correlation, n.1, y.1, n.2, and y.2 (where n is count of words and y is a weird stat: the average of X, the original survey numeric score associated that that word). With the ""final"" data frame, I've now attempted a multitude of ways to map either y.1 or y.2 to the color of the nodes, generally something like: as_tbl_graph(final) ggraph(final, layout = ""fr"") + geom_node_point(aes(color = y.1), size = 5) + geom_node_text(aes(label = name), repel = TRUE) + scale_color_gradient(low = ""red"", high = ""green"") + theme_void() This is the error I receive: Error in geom_node_point() : ! Problem while computing aesthetics. ℹ Error occurred in the 1st layer. Caused by error in FUN() : ! object 'y.1' not found Not sure exactly where I'm going wrong, although I have been poring through the documentation for ggraph and tidygraph. I don't have a full conceptual understanding of the various layout possibilities, which I feel is likely where my issues lie (or possibly my confusion starts in the construction of the dataframe itself via as_tbl_graph?), and would really welcome any additional resources or documentation towards understanding those algorithms/customizing layouts. (I've read https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html and all of the ggraph vignettes!) My question, boiled down, is: how can I use a numeric variable to add a color dimension to nodes in a network graph using ggraph (or more specifically, what the heck am I doing wrong)? Thanks in advance for any help!",unable map color onto network graph additional variable use ggraph attempt create text network graph  work pivot survey datum  attempt associate word open  end comment associate numeric response   ve construct word correlation graph  devil time associate numeric value back network graph  experience r   ve formal training  class feel confident  m miss something pretty basic right  able successfully create plot use follow code  assume graph datum frame  contain variable x  raw numeric score survey datum   rownumber  tie individual word use back initial open end comment   word  n   time   word  appear dataset    average x per word   graph    groupby  word     filter  n     1000     pairwisecor  word  rownumber  upper  false     filter  correlation  09     graphfromdataframe      ggraph  layout    fr    geomedgelink  aes  edgealpha  correlation   showlegend  false   geomnodepoint  color    lightblue   size  5   scalecolorgradient  low    red   high    green    geomnodetext  aes  label  name   repel  true   themevoid   pairwisecor function essentially reshape dataframe item1  item2  correlation  dropping variable  mean relevant color  assign variable drop  create correlate word dataset  finaldf join individual word average score   correlatedwords dataset  final   cordf    leftjoin  filtdf   joinby  item1   word      leftjoin  filtdf   joinby  item2   word     final  contain item1  word 1   item2  word 2   correlation  n1  y1  n2  y2  n count word weird stat  average x  original survey numeric score associate word     final  datum frame   ve attempt multitude way map either y1 y2 color node  generally something like  astblgraph  final  ggraph  final  layout    fr    geomnodepoint  aes  color  y1   size  5   geomnodetext  aes  label  name   repel  true   scalecolorgradient  low    red   high    green    themevoid   error receive  error geomnodepoint     problem compute aesthetic   error occur 1st layer  cause error fun     object  y1  find sure exactly  m go wrong  although pore documentation ggraph tidygraph  not full conceptual understanding various layout possibility  feel likely issue lie  possibly confusion start construction dataframe via astblgraph    would really welcome additional resource documentation towards understand algorithm  customize layout    ve read   cranr  projectorg  web  package  ggraph  vignette  layoutshtml ggraph vignette   question  boil   use numeric variable add color dimension node network graph use ggraph  specifically  heck wrong   thank advance help ,first issue code pass dataframe final ggraph   instead tblgraph object astblgraph  final   second issue  convert tblgraph y1 y2 column add via lefjoin become column feature edge datum node thus available map aesthetic geomnodexxx  fix second issue convert cordf tblgraph first  join filtdf  way column add node datum  note  one leftjoin second make sense node datum  also rename column value encounter warning use  use fake datum base highschool dataset ggraph  library  ggraph  library  tidygraph  library  dplyr  warnconflict  false  setseed  123   create example datum cordf   highschool name  cordf    c    item1     item2     correlation   filtdf   dataframe  word  ascharacter  unique  cordf  item1     runif  seq  length  unique  cordf  item1        rename  value   finalgraph   astblgraph  cordf    leftjoin  filtdf   joinby  name   word   ggraph  finalgraph  layout    fr    geomnodepoint  aes  color  value   size  5   geomnodetext  aes  label  name   repel  true   scalecolorgradient  low    red   high    green    themevoid  ,unable map color onto network graph additional variable use ggraph attempt create text network graph  work pivot survey datum  attempt associate word open  end comment associate numeric response   ve construct word correlation graph  devil time associate numeric value back network graph  experience r   ve formal training  class feel confident  m miss something pretty basic right  able successfully create plot use follow code  assume graph datum frame  contain variable x  raw numeric score survey datum   rownumber  tie individual word use back initial open end comment   word  n   time   word  appear dataset    average x per word   graph    groupby  word     filter  n     1000     pairwisecor  word  rownumber  upper  false     filter  correlation  09     graphfromdataframe      ggraph  layout    fr    geomedgelink  aes  edgealpha  correlation   showlegend  false   geomnodepoint  color    lightblue   size  5   scalecolorgradient  low    red   high    green    geomnodetext  aes  label  name   repel  true   themevoid   pairwisecor function essentially reshape dataframe item1  item2  correlation  dropping variable  mean relevant color  assign variable drop  create correlate word dataset  finaldf join individual word average score   correlatedwords dataset  final   cordf    leftjoin  filtdf   joinby  item1   word      leftjoin  filtdf   joinby  item2   word     final  contain item1  word 1   item2  word 2   correlation  n1  y1  n2  y2  n count word weird stat  average x  original survey numeric score associate word     final  datum frame   ve attempt multitude way map either y1 y2 color node  generally something like  astblgraph  final  ggraph  final  layout    fr    geomnodepoint  aes  color  y1   size  5   geomnodetext  aes  label  name   repel  true   scalecolorgradient  low    red   high    green    themevoid   error receive  error geomnodepoint     problem compute aesthetic   error occur 1st layer  cause error fun     object  y1  find sure exactly  m go wrong  although pore documentation ggraph tidygraph  not full conceptual understanding various layout possibility  feel likely issue lie  possibly confusion start construction dataframe via astblgraph    would really welcome additional resource documentation towards understand algorithm  customize layout    ve read   cranr  projectorg  web  package  ggraph  vignette  layoutshtml ggraph vignette   question  boil   use numeric variable add color dimension node network graph use ggraph  specifically  heck wrong   thank advance help  first issue code pass dataframe final ggraph   instead tblgraph object astblgraph  final   second issue  convert tblgraph y1 y2 column add via lefjoin become column feature edge datum node thus available map aesthetic geomnodexxx  fix second issue convert cordf tblgraph first  join filtdf  way column add node datum  note  one leftjoin second make sense node datum  also rename column value encounter warning use  use fake datum base highschool dataset ggraph  library  ggraph  library  tidygraph  library  dplyr  warnconflict  false  setseed  123   create example datum cordf   highschool name  cordf    c    item1     item2     correlation   filtdf   dataframe  word  ascharacter  unique  cordf  item1     runif  seq  length  unique  cordf  item1        rename  value   finalgraph   astblgraph  cordf    leftjoin  filtdf   joinby  name   word   ggraph  finalgraph  layout    fr    geomnodepoint  aes  color  value   size  5   geomnodetext  aes  label  name   repel  true   scalecolorgradient  low    red   high    green    themevoid  ,Basic Understanding
R: stm + searchK fails to determine the optimal number of topics,"Please have a look at the self-contained example at the end of the post. I simplified the reprex and you can download the dfm (document-feature matrix) from https://e.pcloud.link/publink/show?code=XZmHFDZeObPiNtsGWfzuBlnVw2ryzATt1X7 A couple of things which I do not understand happen when I run stm with 9 topics, some of them appear to yield duplicated results (at least in the top 10 keywords per topic, see the plot generated in the reprex). Any idea why? when I try using the searchK() function from stm to determine the optimal number of topics, I get an error message I cannot decipher. The same happened at least to another user, see What causes 'subscript out of bounds' error in STM topic modeling with missing data? but here I give a reproducible example. Any help for 1) and 2) is appreciated! library(dplyr) #> #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #> #> filter, lag #> The following objects are masked from 'package:base': #> #> intersect, setdiff, setequal, union library(quanteda) #> Package version: 3.3.1 #> Unicode version: 15.0 #> ICU version: 72.1 #> Parallel computing: 4 of 4 threads used. #> See https://quanteda.io for tutorials and examples. library(stm) #> stm v1.3.6.1 successfully loaded. See ?stm for help. #> Papers, resources, and other materials at structuraltopicmodel.com library(RCurl) library(readtext) #> #> Attaching package: 'readtext' #> The following object is masked from 'package:quanteda': #> #> texts library(tidytext) library(ggplot2) ## Download the dfm matrix from ## https://e.pcloud.link/publink/show?code=XZmHFDZeObPiNtsGWfzuBlnVw2ryzATt1X7 dfm_mat <- readRDS(""dfm_mat.RDS"") ## see https://rstudio-pubs-static.s3.amazonaws.com/406792_9287b832dd9e413f97243628cb2f7ddb.html ## convert the dfm to a format suitable to stm. dfm2stm <- convert(dfm_mat, to = ""stm"") model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = 9, data = dfm2stm$meta, init.type = ""Spectral"") #> Beginning Spectral Initialization #> Calculating the gram matrix... #> Finding anchor words... #> ......... #> Recovering initialization... #> ........................... #> Initialization complete. #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 1 (approx. per word bound = -6.780) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 2 (approx. per word bound = -6.762, relative change = 2.715e-03) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 3 (approx. per word bound = -6.761, relative change = 4.260e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 4 (approx. per word bound = -6.761, relative change = 1.602e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 5 (approx. per word bound = -6.761, relative change = 1.024e-05) #> Topic 1: europe, can, european, new, need #> Topic 2: union, need, europe, today, us #> Topic 3: europe, union, work, european, need #> Topic 4: union, need, europe, today, us #> Topic 5: europe, can, european, new, need #> Topic 6: europe, union, work, european, need #> Topic 7: union, need, europe, today, us #> Topic 8: europe, can, european, new, need #> Topic 9: accelerate, union, need, europe, us #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Model Converged ## I make the model tidy. ## See https://juliasilge.com/blog/sherlock-holmes-stm/ stm_tidy <- tidy(model.stm) gpl <- stm_tidy |> group_by(topic) |> top_n(10, beta) |> ungroup() |> mutate(topic = paste0(""Topic "", topic), term = reorder_within(term, beta, topic)) |> ggplot(aes(term, beta, fill = as.factor(topic))) + geom_col(alpha = 0.8, show.legend = FALSE) + facet_wrap(~ topic, scales = ""free_y"") + coord_flip() + scale_x_reordered() + labs(x = NULL, y = expression(beta), title = ""Highest word probabilities for each topic"", subtitle = ""Different words are associated with different topics"") gpl ## I can fit a model by stm with a chosen number of topics to the data ### Now I try determining the optimal number of topics using the searchK function ### See https://stackoverflow.com/questions/64989642/use-dfm-in-searchk-calcuation set.seed(02138) K <- 5:15 model_search <- searchK(dfm2stm$documents, dfm2stm$vocab, K, data = dfm2stm$meta) #> Beginning Spectral Initialization #> Calculating the gram matrix... #> Finding anchor words... #> ..... #> Recovering initialization... #> ........................... #> Initialization complete. #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 1 (approx. per word bound = -6.781) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 2 (approx. per word bound = -6.761, relative change = 2.956e-03) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 3 (approx. per word bound = -6.761, relative change = 2.235e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Model Converged #> Error in missing$docs[[i]]: subscript out of bounds ## This fails but I do not understand why.... sessionInfo() #> R version 4.3.2 (2023-10-31) #> Platform: x86_64-pc-linux-gnu (64-bit) #> Running under: Debian GNU/Linux 12 (bookworm) #> #> Matrix products: default #> BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.11.0 #> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.11.0 #> #> locale: #> [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C #> [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 #> [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 #> [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C #> [9] LC_ADDRESS=C LC_TELEPHONE=C #> [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C #> #> time zone: Europe/Brussels #> tzcode source: system (glibc) #> #> attached base packages: #> [1] stats graphics grDevices utils datasets methods base #> #> other attached packages: #> [1] ggplot2_3.4.4 tidytext_0.4.1 readtext_0.90 RCurl_1.98-1.13 #> [5] stm_1.3.6.1 quanteda_3.3.1 dplyr_1.1.3 #> #> loaded via a namespace (and not attached): #> [1] janeaustenr_1.0.0 utf8_1.2.4 generics_0.1.3 slam_0.1-50 #> [5] bitops_1.0-7 stringi_1.7.12 lattice_0.22-5 digest_0.6.33 #> [9] magrittr_2.0.3 evaluate_0.23 grid_4.3.2 fastmap_1.1.1 #> [13] plyr_1.8.9 Matrix_1.6-2 httr_1.4.7 stopwords_2.3 #> [17] fansi_1.0.5 scales_1.2.1 cli_3.6.1 rlang_1.1.2 #> [21] tokenizers_0.3.0 munsell_0.5.0 reprex_2.0.2 withr_2.5.2 #> [25] yaml_2.3.7 tools_4.3.2 reshape2_1.4.4 colorspace_2.1-0 #> [29] fastmatch_1.1-4 vctrs_0.6.4 R6_2.5.1 lifecycle_1.0.4 #> [33] stringr_1.5.0 fs_1.6.3 pkgconfig_2.0.3 RcppParallel_5.1.7 #> [37] pillar_1.9.0 gtable_0.3.4 data.table_1.14.8 glue_1.6.2 #> [41] Rcpp_1.0.11 xfun_0.41 tibble_3.2.1 tidyselect_1.2.0 #> [45] knitr_1.45 farver_2.1.1 htmltools_0.5.7 SnowballC_0.7.1 #> [49] rmarkdown_2.25 labeling_0.4.3 compiler_4.3.2 Created on 2023-11-14 with reprex v2.0.2","['r', 'nlp', 'topic-modeling', 'quanteda']",1,"I think what is happening is this: With only three documents in your dfm_mat , the searchK() is trying by default to drop half of them to use for a held-out set. This is causing many features to be zero, which means they are dropped from the vocab by default in estimating the topic models used in searchK() . stm() needs only non-zero features, but searchK() considers the vocab set to be fixed, so it's breaking some code inside the function. (I did not check this in the code however.) > sum(colSums(dfm_sample(dfm_mat, size = 2)) == 0) [1] 603 > sum(colSums(dfm_sample(dfm_mat, size = 2)) == 0) [1] 583 > sum(colSums(dfm_sample(dfm_mat, size = 2)) == 0) [1] 582 These are the three sample options for dropping 1 of the 3 documents (0.50 rounded up). You would need to contact the stm package maintainers about a potential bug report. Or, for your problem, use more documents and trim those with low frequencies.",2023-11-14 12:45:30,2023-11-14 15:01:40,420,https://stackoverflow.com/questions/77480710/r-stm-searchk-fails-to-determine-the-optimal-number-of-topics,"R: stm + searchK fails to determine the optimal number of topics Please have a look at the self-contained example at the end of the post. I simplified the reprex and you can download the dfm (document-feature matrix) from https://e.pcloud.link/publink/show?code=XZmHFDZeObPiNtsGWfzuBlnVw2ryzATt1X7 A couple of things which I do not understand happen when I run stm with 9 topics, some of them appear to yield duplicated results (at least in the top 10 keywords per topic, see the plot generated in the reprex). Any idea why? when I try using the searchK() function from stm to determine the optimal number of topics, I get an error message I cannot decipher. The same happened at least to another user, see What causes 'subscript out of bounds' error in STM topic modeling with missing data? but here I give a reproducible example. Any help for 1) and 2) is appreciated! library(dplyr) #> #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #> #> filter, lag #> The following objects are masked from 'package:base': #> #> intersect, setdiff, setequal, union library(quanteda) #> Package version: 3.3.1 #> Unicode version: 15.0 #> ICU version: 72.1 #> Parallel computing: 4 of 4 threads used. #> See https://quanteda.io for tutorials and examples. library(stm) #> stm v1.3.6.1 successfully loaded. See ?stm for help. #> Papers, resources, and other materials at structuraltopicmodel.com library(RCurl) library(readtext) #> #> Attaching package: 'readtext' #> The following object is masked from 'package:quanteda': #> #> texts library(tidytext) library(ggplot2) ## Download the dfm matrix from ## https://e.pcloud.link/publink/show?code=XZmHFDZeObPiNtsGWfzuBlnVw2ryzATt1X7 dfm_mat <- readRDS(""dfm_mat.RDS"") ## see https://rstudio-pubs-static.s3.amazonaws.com/406792_9287b832dd9e413f97243628cb2f7ddb.html ## convert the dfm to a format suitable to stm. dfm2stm <- convert(dfm_mat, to = ""stm"") model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = 9, data = dfm2stm$meta, init.type = ""Spectral"") #> Beginning Spectral Initialization #> Calculating the gram matrix... #> Finding anchor words... #> ......... #> Recovering initialization... #> ........................... #> Initialization complete. #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 1 (approx. per word bound = -6.780) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 2 (approx. per word bound = -6.762, relative change = 2.715e-03) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 3 (approx. per word bound = -6.761, relative change = 4.260e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 4 (approx. per word bound = -6.761, relative change = 1.602e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 5 (approx. per word bound = -6.761, relative change = 1.024e-05) #> Topic 1: europe, can, european, new, need #> Topic 2: union, need, europe, today, us #> Topic 3: europe, union, work, european, need #> Topic 4: union, need, europe, today, us #> Topic 5: europe, can, european, new, need #> Topic 6: europe, union, work, european, need #> Topic 7: union, need, europe, today, us #> Topic 8: europe, can, european, new, need #> Topic 9: accelerate, union, need, europe, us #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Model Converged ## I make the model tidy. ## See https://juliasilge.com/blog/sherlock-holmes-stm/ stm_tidy <- tidy(model.stm) gpl <- stm_tidy |> group_by(topic) |> top_n(10, beta) |> ungroup() |> mutate(topic = paste0(""Topic "", topic), term = reorder_within(term, beta, topic)) |> ggplot(aes(term, beta, fill = as.factor(topic))) + geom_col(alpha = 0.8, show.legend = FALSE) + facet_wrap(~ topic, scales = ""free_y"") + coord_flip() + scale_x_reordered() + labs(x = NULL, y = expression(beta), title = ""Highest word probabilities for each topic"", subtitle = ""Different words are associated with different topics"") gpl ## I can fit a model by stm with a chosen number of topics to the data ### Now I try determining the optimal number of topics using the searchK function ### See https://stackoverflow.com/questions/64989642/use-dfm-in-searchk-calcuation set.seed(02138) K <- 5:15 model_search <- searchK(dfm2stm$documents, dfm2stm$vocab, K, data = dfm2stm$meta) #> Beginning Spectral Initialization #> Calculating the gram matrix... #> Finding anchor words... #> ..... #> Recovering initialization... #> ........................... #> Initialization complete. #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 1 (approx. per word bound = -6.781) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 2 (approx. per word bound = -6.761, relative change = 2.956e-03) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Completing Iteration 3 (approx. per word bound = -6.761, relative change = 2.235e-05) #> ... #> Completed E-Step (0 seconds). #> Completed M-Step. #> Model Converged #> Error in missing$docs[[i]]: subscript out of bounds ## This fails but I do not understand why.... sessionInfo() #> R version 4.3.2 (2023-10-31) #> Platform: x86_64-pc-linux-gnu (64-bit) #> Running under: Debian GNU/Linux 12 (bookworm) #> #> Matrix products: default #> BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.11.0 #> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.11.0 #> #> locale: #> [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C #> [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 #> [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 #> [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C #> [9] LC_ADDRESS=C LC_TELEPHONE=C #> [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C #> #> time zone: Europe/Brussels #> tzcode source: system (glibc) #> #> attached base packages: #> [1] stats graphics grDevices utils datasets methods base #> #> other attached packages: #> [1] ggplot2_3.4.4 tidytext_0.4.1 readtext_0.90 RCurl_1.98-1.13 #> [5] stm_1.3.6.1 quanteda_3.3.1 dplyr_1.1.3 #> #> loaded via a namespace (and not attached): #> [1] janeaustenr_1.0.0 utf8_1.2.4 generics_0.1.3 slam_0.1-50 #> [5] bitops_1.0-7 stringi_1.7.12 lattice_0.22-5 digest_0.6.33 #> [9] magrittr_2.0.3 evaluate_0.23 grid_4.3.2 fastmap_1.1.1 #> [13] plyr_1.8.9 Matrix_1.6-2 httr_1.4.7 stopwords_2.3 #> [17] fansi_1.0.5 scales_1.2.1 cli_3.6.1 rlang_1.1.2 #> [21] tokenizers_0.3.0 munsell_0.5.0 reprex_2.0.2 withr_2.5.2 #> [25] yaml_2.3.7 tools_4.3.2 reshape2_1.4.4 colorspace_2.1-0 #> [29] fastmatch_1.1-4 vctrs_0.6.4 R6_2.5.1 lifecycle_1.0.4 #> [33] stringr_1.5.0 fs_1.6.3 pkgconfig_2.0.3 RcppParallel_5.1.7 #> [37] pillar_1.9.0 gtable_0.3.4 data.table_1.14.8 glue_1.6.2 #> [41] Rcpp_1.0.11 xfun_0.41 tibble_3.2.1 tidyselect_1.2.0 #> [45] knitr_1.45 farver_2.1.1 htmltools_0.5.7 SnowballC_0.7.1 #> [49] rmarkdown_2.25 labeling_0.4.3 compiler_4.3.2 Created on 2023-11-14 with reprex v2.0.2",r  stm  searchk fails determine optimal number topic please look self  contain example end post  simplify reprex download dfm  document  feature matrix    epcloudlink  publink  show  code  xzmhfdzeobpintsgwfzublnvw2ryzatt1x7 couple thing understand happen run stm 9 topic  appear yield duplicate result  least top 10 keyword per topic  see plot generate reprex   idea  try use searchk   function stm determine optimal number topic  get error message decipher  happen least another user  see cause  subscript bound  error stm topic modeling miss datum  give reproducible example  help 1  2  appreciate  library  dplyr      attach package   dplyr    follow object mask  package  stat       filter  lag   follow object mask  package  base       intersect  setdiff  setequal  union library  quanteda    package version  331   unicode version  150   icu version  721   parallel computing  4 4 thread use    see   quantedaio tutorial example  library  stm    stm v1361 successfully load  see  stm help    papers  resource  material structuraltopicmodelcom library  rcurl  library  readtext      attach package   readtext    follow object mask  package  quanteda       text library  tidytext  library  ggplot2    download dfm matrix     epcloudlink  publink  show  code  xzmhfdzeobpintsgwfzublnvw2ryzatt1x7 dfmmat   readrds    dfmmat  rds     see   rstudio  pub  statics3amazonawscom4067929287b832dd9e413f97243628cb2f7ddbhtml   convert dfm format suitable stm  dfm2stm   convert  dfmmat     stm   modelstm   stm  dfm2stm  document  dfm2stm  vocab  k  9  datum  dfm2stm  meta  inittype    spectral     begin spectral initialization   calculate gram matrix    find anchor word       recover initialization       initialization complete       complete e  step  0 second     completed m  step    complete iteration 1  approx  per word bind  6780       complete e  step  0 second     completed m  step    complete iteration 2  approx  per word bind  6762  relative change  2715e03       complete e  step  0 second     completed m  step    complete iteration 3  approx  per word bind  6761  relative change  4260e05       complete e  step  0 second     completed m  step    complete iteration 4  approx  per word bind  6761  relative change  1602e05       complete e  step  0 second     completed m  step    complete iteration 5  approx  per word bind  6761  relative change  1024e05    topic 1  europe   european  new  need   topic 2  union  need  europe  today  us   topic 3  europe  union  work  european  need   topic 4  union  need  europe  today  us   topic 5  europe   european  new  need   topic 6  europe  union  work  european  need   topic 7  union  need  europe  today  us   topic 8  europe   european  new  need   topic 9  accelerate  union  need  europe  us      complete e  step  0 second     completed m  step    model converged   make model tidy    see   juliasilgecom  blog  sherlock  holme  stm stmtidy   tidy  modelstm  gpl   stmtidy   groupby  topic    topn  10  beta    ungroup     mutate  topic  paste0    topic    topic   term  reorderwithin  term  beta  topic     ggplot  aes  term  beta  fill  asfactor  topic     geomcol  alpha  08  showlegend  false   facetwrap   topic  scale    freey    coordflip    scalexreordere    lab  x  null   expression  beta   title    high word probability topic   subtitle    different word associate different topic   gpl   fit model stm choose number topic datum    try determine optimal number topic use searchk function    see   stackoverflowcom  questions64989642  use  dfm  in  searchk  calcuation setseed  02138  k   515 modelsearch   searchk  dfm2stm  document  dfm2stm  vocab  k  datum  dfm2stm  meta    begin spectral initialization   calculate gram matrix    find anchor word       recover initialization       initialization complete       complete e  step  0 second     completed m  step    complete iteration 1  approx  per word bind  6781       complete e  step  0 second     completed m  step    complete iteration 2  approx  per word bind  6761  relative change  2956e03       complete e  step  0 second     completed m  step    complete iteration 3  approx  per word bind  6761  relative change  2235e05       complete e  step  0 second     completed m  step    model converged   error miss  doc      subscript bound   fail understand  sessioninfo     r version 432  2023  10  31    platform  x8664  pc  linux  gnu  64  bit    run  debian gnu  linux 12  bookworm      matrix product  default   blas  usr  lib  x8664  linux  gnu  blas  libblasso3110   lapack  usr  lib  x8664  linux  gnu  lapack  liblapackso3110     locale     1  lcctype  engbutf8 lcnumeric  c    3  lctime  engbutf8 lccollate  engbutf8    5  lcmonetary  engbutf8 lcmessage  engbutf8    7  lcpaper  engbutf8 lcname  c    9  lcaddress  c lctelephone  c    11  lcmeasurement  engbutf8 lcidentification  c     time zone  europe  brussels   tzcode source  system  glibc      attach base package     1  stat graphic grdevice util dataset method base     attach package     1  ggplot2344 tidytext041 readtext090 rcurl198  113    5  stm1361 quanteda331 dplyr113     load via namespace  attach      1  janeaustenr100 utf8124 generics013 slam01  50    5  bitops10  7 stringi1712 lattice022  5 digest0633    9  magrittr203 evaluate023 grid432 fastmap111    13  plyr189 matrix16  2 httr147 stopwords23    17  fansi105 scales121 cli361 rlang112    21  tokenizers030 munsell050 reprex202 withr252    25  yaml237 tools432 reshape2144 colorspace21  0    29  fastmatch11  4 vctrs064 r6251 lifecycle104    33  stringr150 fs163 pkgconfig203 rcppparallel517    37  pillar190 gtable034 datatable1148 glue162    41  rcpp1011 xfun041 tibble321 tidyselect120    45  knitr145 farver211 htmltools057 snowballc071    49  rmarkdown225 labeling043 compiler432 create 2023  11  14 reprex v202,think happen  three document dfmmat  searchk   try default drop half use hold  out set  cause many feature zero  mean drop vocab default estimate topic model use searchk    stm   need non  zero feature  searchk   consider vocab set fix  s break code inside function   check code however    sum  colsum  dfmsample  dfmmat  size  2     0   1  603  sum  colsum  dfmsample  dfmmat  size  2     0   1  583  sum  colsum  dfmsample  dfmmat  size  2     0   1  582 three sample option drop 1 3 document  050 rounded   would need contact stm package maintainer potential bug report   problem  use document trim low frequency ,r  stm  searchk fails determine optimal number topic please look self  contain example end post  simplify reprex download dfm  document  feature matrix    epcloudlink  publink  show  code  xzmhfdzeobpintsgwfzublnvw2ryzatt1x7 couple thing understand happen run stm 9 topic  appear yield duplicate result  least top 10 keyword per topic  see plot generate reprex   idea  try use searchk   function stm determine optimal number topic  get error message decipher  happen least another user  see cause  subscript bound  error stm topic modeling miss datum  give reproducible example  help 1  2  appreciate  library  dplyr      attach package   dplyr    follow object mask  package  stat       filter  lag   follow object mask  package  base       intersect  setdiff  setequal  union library  quanteda    package version  331   unicode version  150   icu version  721   parallel computing  4 4 thread use    see   quantedaio tutorial example  library  stm    stm v1361 successfully load  see  stm help    papers  resource  material structuraltopicmodelcom library  rcurl  library  readtext      attach package   readtext    follow object mask  package  quanteda       text library  tidytext  library  ggplot2    download dfm matrix     epcloudlink  publink  show  code  xzmhfdzeobpintsgwfzublnvw2ryzatt1x7 dfmmat   readrds    dfmmat  rds     see   rstudio  pub  statics3amazonawscom4067929287b832dd9e413f97243628cb2f7ddbhtml   convert dfm format suitable stm  dfm2stm   convert  dfmmat     stm   modelstm   stm  dfm2stm  document  dfm2stm  vocab  k  9  datum  dfm2stm  meta  inittype    spectral     begin spectral initialization   calculate gram matrix    find anchor word       recover initialization       initialization complete       complete e  step  0 second     completed m  step    complete iteration 1  approx  per word bind  6780       complete e  step  0 second     completed m  step    complete iteration 2  approx  per word bind  6762  relative change  2715e03       complete e  step  0 second     completed m  step    complete iteration 3  approx  per word bind  6761  relative change  4260e05       complete e  step  0 second     completed m  step    complete iteration 4  approx  per word bind  6761  relative change  1602e05       complete e  step  0 second     completed m  step    complete iteration 5  approx  per word bind  6761  relative change  1024e05    topic 1  europe   european  new  need   topic 2  union  need  europe  today  us   topic 3  europe  union  work  european  need   topic 4  union  need  europe  today  us   topic 5  europe   european  new  need   topic 6  europe  union  work  european  need   topic 7  union  need  europe  today  us   topic 8  europe   european  new  need   topic 9  accelerate  union  need  europe  us      complete e  step  0 second     completed m  step    model converged   make model tidy    see   juliasilgecom  blog  sherlock  holme  stm stmtidy   tidy  modelstm  gpl   stmtidy   groupby  topic    topn  10  beta    ungroup     mutate  topic  paste0    topic    topic   term  reorderwithin  term  beta  topic     ggplot  aes  term  beta  fill  asfactor  topic     geomcol  alpha  08  showlegend  false   facetwrap   topic  scale    freey    coordflip    scalexreordere    lab  x  null   expression  beta   title    high word probability topic   subtitle    different word associate different topic   gpl   fit model stm choose number topic datum    try determine optimal number topic use searchk function    see   stackoverflowcom  questions64989642  use  dfm  in  searchk  calcuation setseed  02138  k   515 modelsearch   searchk  dfm2stm  document  dfm2stm  vocab  k  datum  dfm2stm  meta    begin spectral initialization   calculate gram matrix    find anchor word       recover initialization       initialization complete       complete e  step  0 second     completed m  step    complete iteration 1  approx  per word bind  6781       complete e  step  0 second     completed m  step    complete iteration 2  approx  per word bind  6761  relative change  2956e03       complete e  step  0 second     completed m  step    complete iteration 3  approx  per word bind  6761  relative change  2235e05       complete e  step  0 second     completed m  step    model converged   error miss  doc      subscript bound   fail understand  sessioninfo     r version 432  2023  10  31    platform  x8664  pc  linux  gnu  64  bit    run  debian gnu  linux 12  bookworm      matrix product  default   blas  usr  lib  x8664  linux  gnu  blas  libblasso3110   lapack  usr  lib  x8664  linux  gnu  lapack  liblapackso3110     locale     1  lcctype  engbutf8 lcnumeric  c    3  lctime  engbutf8 lccollate  engbutf8    5  lcmonetary  engbutf8 lcmessage  engbutf8    7  lcpaper  engbutf8 lcname  c    9  lcaddress  c lctelephone  c    11  lcmeasurement  engbutf8 lcidentification  c     time zone  europe  brussels   tzcode source  system  glibc      attach base package     1  stat graphic grdevice util dataset method base     attach package     1  ggplot2344 tidytext041 readtext090 rcurl198  113    5  stm1361 quanteda331 dplyr113     load via namespace  attach      1  janeaustenr100 utf8124 generics013 slam01  50    5  bitops10  7 stringi1712 lattice022  5 digest0633    9  magrittr203 evaluate023 grid432 fastmap111    13  plyr189 matrix16  2 httr147 stopwords23    17  fansi105 scales121 cli361 rlang112    21  tokenizers030 munsell050 reprex202 withr252    25  yaml237 tools432 reshape2144 colorspace21  0    29  fastmatch11  4 vctrs064 r6251 lifecycle104    33  stringr150 fs163 pkgconfig203 rcppparallel517    37  pillar190 gtable034 datatable1148 glue162    41  rcpp1011 xfun041 tibble321 tidyselect120    45  knitr145 farver211 htmltools057 snowballc071    49  rmarkdown225 labeling043 compiler432 create 2023  11  14 reprex v202 think happen  three document dfmmat  searchk   try default drop half use hold  out set  cause many feature zero  mean drop vocab default estimate topic model use searchk    stm   need non  zero feature  searchk   consider vocab set fix  s break code inside function   check code however    sum  colsum  dfmsample  dfmmat  size  2     0   1  603  sum  colsum  dfmsample  dfmmat  size  2     0   1  583  sum  colsum  dfmsample  dfmmat  size  2     0   1  582 three sample option drop 1 3 document  050 rounded   would need contact stm package maintainer potential bug report   problem  use document trim low frequency ,Library/Tool-Based Queries
NLP sentence summarization techniques with python,"I am trying to make a python script that takes a sentence and summarize it in 5-7 words with keywords and details. So far, I have used the nltk library to first remove any symbols and numbers, then remove all word types except nouns and verbs. I also included a function to remove all stopwords(words without value like, 'the', 'it). The code I have is extremely basic and the output isn't grammatically correct or understandable. My main objective is to take a sentence like: ""Drug stability refers to the ability of a pharmaceutical product to retain its quality, safety, and efficacy over time"" ...and turn it into: ""Drug stability is ability to retain quality, safety, and efficacy"" But when i run the code I get ""Drug stability refers ability pharmaceutical product retain quality, safety, efficacy time"" which isn't bad but I want to make the system able to produce more grammatically correct while still retaining major keywords. I am aware of libraries like gensin or nltk summarize but these libraries only take the important sentences of a paragraph through word frequency but this doesn't simplify single sentences. are there any other methods for sentence summarization? Here is the code I have so far: def shortenSentence(sentence): #sentence = ""%^Regulatory scientists must take measures to guarantee that the drug remains consistent and safe from the moment of production through packaging, storage, and shipping.907"" clean_sentence = re.sub(r'[^a-zA-Z\s]', '', sentence) # Added 0-9 and period (.) #print(clean_sentence) def remove_adj_adv(sentence): words = word_tokenize(sentence) pos_tags = pos_tag(words) shortened = [word for word, tag in pos_tags if tag in ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']] return ' '.join(shortened) shortened = remove_adj_adv(clean_sentence) #print(shortened) words = word_tokenize(shortened) # Get the list of English stopwords stop_words = set(stopwords.words('english')) # Remove stopwords from the list of words filtered_words = [word for word in words if word.lower() not in stop_words] # Join the filtered words back into a sentence filtered_sentence = ' '.join(filtered_words) #print(filtered_sentence) return(filtered_sentence)","['python', 'algorithm', 'nlp']",1,"What if you had a larger sentence? While your approach works summarizing longer sentences may pose additional issues. I would recommend adding two extra functions to your algorithm. First, to identify important words using a representation schema such as TFIDF. Second, set a threshold of min 10 to max 20 words to be included in the summary. Since you are pre-processing the input text to remove unwanted words (stopwords, adjectives, etc.), reconstructing the sentence to be syntactically or grammatically correct, is a different problem, which may require using generative LLMs. Thus, you should shift your focus from summarization to text generation based on keywords.",2023-11-14 02:29:18,2023-11-14 08:20:07,448,https://stackoverflow.com/questions/77477871/nlp-sentence-summarization-techniques-with-python,"NLP sentence summarization techniques with python I am trying to make a python script that takes a sentence and summarize it in 5-7 words with keywords and details. So far, I have used the nltk library to first remove any symbols and numbers, then remove all word types except nouns and verbs. I also included a function to remove all stopwords(words without value like, 'the', 'it). The code I have is extremely basic and the output isn't grammatically correct or understandable. My main objective is to take a sentence like: ""Drug stability refers to the ability of a pharmaceutical product to retain its quality, safety, and efficacy over time"" ...and turn it into: ""Drug stability is ability to retain quality, safety, and efficacy"" But when i run the code I get ""Drug stability refers ability pharmaceutical product retain quality, safety, efficacy time"" which isn't bad but I want to make the system able to produce more grammatically correct while still retaining major keywords. I am aware of libraries like gensin or nltk summarize but these libraries only take the important sentences of a paragraph through word frequency but this doesn't simplify single sentences. are there any other methods for sentence summarization? Here is the code I have so far: def shortenSentence(sentence): #sentence = ""%^Regulatory scientists must take measures to guarantee that the drug remains consistent and safe from the moment of production through packaging, storage, and shipping.907"" clean_sentence = re.sub(r'[^a-zA-Z\s]', '', sentence) # Added 0-9 and period (.) #print(clean_sentence) def remove_adj_adv(sentence): words = word_tokenize(sentence) pos_tags = pos_tag(words) shortened = [word for word, tag in pos_tags if tag in ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']] return ' '.join(shortened) shortened = remove_adj_adv(clean_sentence) #print(shortened) words = word_tokenize(shortened) # Get the list of English stopwords stop_words = set(stopwords.words('english')) # Remove stopwords from the list of words filtered_words = [word for word in words if word.lower() not in stop_words] # Join the filtered words back into a sentence filtered_sentence = ' '.join(filtered_words) #print(filtered_sentence) return(filtered_sentence)",nlp sentence summarization technique python try make python script take sentence summarize 5  7 word keyword detail  far  use nltk library first remove symbol number  remove word type except noun verb  also include function remove stopword  word without value like   the    it   code extremely basic output not grammatically correct understandable  main objective take sentence like    drug stability refer ability pharmaceutical product retain quality  safety  efficacy time   turn    drug stability ability retain quality  safety  efficacy  run code get   drug stability refer ability pharmaceutical product retain quality  safety  efficacy time  not bad want make system able produce grammatically correct still retain major keyword  aware library like gensin nltk summarize library take important sentence paragraph word frequency not simplify single sentence  method sentence summarization  code far  def shortensentence  sentence    sentence     regulatory scientist must take measure guarantee drug remain consistent safe moment production packaging  storage  shipping907  cleansentence  resub  r   a  za  zs       sentence   add 0  9 period     print  cleansentence  def removeadjadv  sentence   word  wordtokenize  sentence  postag  postag  word  shorten   word word  tag postags tag   nn    nns    vb    vbd    vbg    vbn    vbp    vbz    return   join  shorten  shorten  removeadjadv  cleansentence   print  shorten  word  wordtokenize  shorten   get list english stopword stopwords  set  stopwordsword   english     remove stopwords list word filteredwords   word word word wordlower   stopword   join filter word back sentence filteredsentence    join  filteredwords   print  filteredsentence  return  filteredsentence ,large sentence  approach work summarize long sentence may pose additional issue  would recommend add two extra function algorithm  first  identify important word use representation schema tfidf  second  set threshold min 10 max 20 word include summary  since pre  processing input text remove unwanted word  stopword  adjective  etc    reconstruct sentence syntactically grammatically correct  different problem  may require use generative llm  thus  shift focus summarization text generation base keyword ,nlp sentence summarization technique python try make python script take sentence summarize 5  7 word keyword detail  far  use nltk library first remove symbol number  remove word type except noun verb  also include function remove stopword  word without value like   the    it   code extremely basic output not grammatically correct understandable  main objective take sentence like    drug stability refer ability pharmaceutical product retain quality  safety  efficacy time   turn    drug stability ability retain quality  safety  efficacy  run code get   drug stability refer ability pharmaceutical product retain quality  safety  efficacy time  not bad want make system able produce grammatically correct still retain major keyword  aware library like gensin nltk summarize library take important sentence paragraph word frequency not simplify single sentence  method sentence summarization  code far  def shortensentence  sentence    sentence     regulatory scientist must take measure guarantee drug remain consistent safe moment production packaging  storage  shipping907  cleansentence  resub  r   a  za  zs       sentence   add 0  9 period     print  cleansentence  def removeadjadv  sentence   word  wordtokenize  sentence  postag  postag  word  shorten   word word  tag postags tag   nn    nns    vb    vbd    vbg    vbn    vbp    vbz    return   join  shorten  shorten  removeadjadv  cleansentence   print  shorten  word  wordtokenize  shorten   get list english stopword stopwords  set  stopwordsword   english     remove stopwords list word filteredwords   word word word wordlower   stopword   join filter word back sentence filteredsentence    join  filteredwords   print  filteredsentence  return  filteredsentence  large sentence  approach work summarize long sentence may pose additional issue  would recommend add two extra function algorithm  first  identify important word use representation schema tfidf  second  set threshold min 10 max 20 word include summary  since pre  processing input text remove unwanted word  stopword  adjective  etc    reconstruct sentence syntactically grammatically correct  different problem  may require use generative llm  thus  shift focus summarization text generation base keyword ,Library/Tool-Based Queries
Entity Extraction in Rasa,"I want to create a chatbot that extract words from the user message as entity and send it to dictionary and in return get the meaning of that word. But the problem is entity values are not getting extracted, and I am getting empty brackets [ ]. I am trying to solve this issue for weeks now. Now, I am exhausted and desperate. Please help me to figure this out. Here is all the files: https://github.com/Attiqakaleem0/Rasa-word-meaning-bot Installation versions on my system are: Rasa Version : 3.6.13 Minimum Compatible Version: 3.5.0 Rasa SDK Version : 3.6.2 Python Version : 3.10.0 Operating System : Windows-10-10.0.19045-SP0","['machine-learning', 'nlp', 'entity', 'rasa-nlu', 'rasa-sdk']",1,You need to add the RegexEntityExtractor to the pipeline in your config.yml. config.yml: pipeline: - name: RegexEntityExtractor case_sensitive: False use_lookup_tables: True use_regexes: True use_word_boundaries: True Additionally you will need to modify your NLU training data to match the correct format for extracting entities. Before: Clarify the term [sympathy] for me. (term) After: Clarify the term [sympathy](term) for me. The Rasa documentation goes into more detail about how to format NLU training data.,2023-11-10 16:35:32,2023-11-10 23:01:28,422,https://stackoverflow.com/questions/77461542/entity-extraction-in-rasa,"Entity Extraction in Rasa I want to create a chatbot that extract words from the user message as entity and send it to dictionary and in return get the meaning of that word. But the problem is entity values are not getting extracted, and I am getting empty brackets [ ]. I am trying to solve this issue for weeks now. Now, I am exhausted and desperate. Please help me to figure this out. Here is all the files: https://github.com/Attiqakaleem0/Rasa-word-meaning-bot Installation versions on my system are: Rasa Version : 3.6.13 Minimum Compatible Version: 3.5.0 Rasa SDK Version : 3.6.2 Python Version : 3.10.0 Operating System : Windows-10-10.0.19045-SP0",entity extraction rasa want create chatbot extract word user message entity send dictionary return get mean word  problem entity value getting extract  get empty bracket    try solve issue week   exhausted desperate  please help figure  file    githubcom  attiqakaleem0  rasa  word  mean  bot installation version system  rasa version  3613 minimum compatible version  350 rasa sdk version  362 python version  3100 operate system  windows10  10019045  sp0,need add regexentityextractor pipeline configyml  configyml  pipeline   name  regexentityextractor casesensitive  false uselookuptables  true useregexes  true usewordboundarie  true additionally need modify nlu training datum match correct format extract entity   clarify term  sympathy    term   clarify term  sympathy   term   rasa documentation go detail format nlu training datum ,entity extraction rasa want create chatbot extract word user message entity send dictionary return get mean word  problem entity value getting extract  get empty bracket    try solve issue week   exhausted desperate  please help figure  file    githubcom  attiqakaleem0  rasa  word  mean  bot installation version system  rasa version  3613 minimum compatible version  350 rasa sdk version  362 python version  3100 operate system  windows10  10019045  sp0 need add regexentityextractor pipeline configyml  configyml  pipeline   name  regexentityextractor casesensitive  false uselookuptables  true useregexes  true usewordboundarie  true additionally need modify nlu training datum match correct format extract entity   clarify term  sympathy    term   clarify term  sympathy   term   rasa documentation go detail format nlu training datum ,Task-Specific Queries
Finding the nouns in a sentence given the context in Python,"How to find the nouns in a sentence regarding the context? I am using the nltk library as follows: text = 'I bought a vintage car.' text = nltk.word_tokenize(text) result = nltk.pos_tag(text) result = [i for i in result if i[1] == 'NN'] #result = [('vintage', 'NN'), ('car', 'NN')] The problem with this script is that it considers vintage as a noun, which can be true, but given the context, it is an adjective. How can we achieve this task? Appendix: Using textblob , we get ""vintage car"" as the noun: !python -m textblob.download_corpora from textblob import TextBlob txt = ""I bought a vintage car."" blob = TextBlob(txt) print(blob.noun_phrases) #['vintage car']","['python', 'nlp', 'nltk', 'textblob']",2,"Using spacy might solve your task. Try this: import spacy nlp = spacy.load(""en_core_web_lg"") def analyze(text): doc = nlp(text) for token in doc: print(token.text, token.pos_) analyze(""I bought a vintage car."") print() analyze(""This old wine is a vintage."") Output I PRON bought VERB a DET vintage ADJ <- correctly identified as adjective car NOUN . PUNCT This DET old ADJ wine NOUN is AUX a DET vintage NOUN <- correctly identified as noun . PUNCT",2023-11-09 18:59:33,2023-11-09 19:38:54,180,https://stackoverflow.com/questions/77455738/finding-the-nouns-in-a-sentence-given-the-context-in-python,"Finding the nouns in a sentence given the context in Python How to find the nouns in a sentence regarding the context? I am using the nltk library as follows: text = 'I bought a vintage car.' text = nltk.word_tokenize(text) result = nltk.pos_tag(text) result = [i for i in result if i[1] == 'NN'] #result = [('vintage', 'NN'), ('car', 'NN')] The problem with this script is that it considers vintage as a noun, which can be true, but given the context, it is an adjective. How can we achieve this task? Appendix: Using textblob , we get ""vintage car"" as the noun: !python -m textblob.download_corpora from textblob import TextBlob txt = ""I bought a vintage car."" blob = TextBlob(txt) print(blob.noun_phrases) #['vintage car']",find noun sentence give context python find noun sentence regard context  use nltk library follow  text   buy vintage car   text  nltkwordtokenize  text  result  nltkpostag  text  result   result  1     nn    result     vintage    nn      car    nn    problem script consider vintage noun  true  give context  adjective  achieve task  appendix  use textblob  get   vintage car  noun   python m textblobdownloadcorpora textblob import textblob txt    buy vintage car   blob  textblob  txt  print  blobnounphrase     vintage car  ,use spacy might solve task  try  import spacy nlp  spacyload    encoreweblg   def analyze  text   doc  nlp  text  token doc  print  tokentext  tokenpos   analyze    buy vintage car    print   analyze    old wine vintage    output pron buy verb det vintage adj   correctly identify adjective car noun  punct det old adj wine noun aux det vintage noun   correctly identify noun  punct,find noun sentence give context python find noun sentence regard context  use nltk library follow  text   buy vintage car   text  nltkwordtokenize  text  result  nltkpostag  text  result   result  1     nn    result     vintage    nn      car    nn    problem script consider vintage noun  true  give context  adjective  achieve task  appendix  use textblob  get   vintage car  noun   python m textblobdownloadcorpora textblob import textblob txt    buy vintage car   blob  textblob  txt  print  blobnounphrase     vintage car   use spacy might solve task  try  import spacy nlp  spacyload    encoreweblg   def analyze  text   doc  nlp  text  token doc  print  tokentext  tokenpos   analyze    buy vintage car    print   analyze    old wine vintage    output pron buy verb det vintage adj   correctly identify adjective car noun  punct det old adj wine noun aux det vintage noun   correctly identify noun  punct,Library/Tool-Based Queries
Any alternative of Python Transfomer Package in java?,"I am loading a Python model from Hugging Face. A pretrained model used to generate answers of questions from a given context. Model Description: model link page Model loading sample: from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline model_name = ""deepset/roberta-base-squad2"" # a) Get predictions nlp = pipeline('question-answering, model=model_name, tokenizer=model_name) QA_input = { 'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers' } res = nlp(QA_input) It is using a Python transformer package to load it. Now I can create its API in Python easily, but I want to create an API of this model in Java, where its parameters ('Question' & 'Context') will be given by the user itself. Now I want to know, how will I load this model in Java? Is there any transformer dependency in Java?","['python', 'java', 'rest', 'machine-learning', 'nlp']",2,"you can convert model to ONNX runtime format and then run it through Apache OpenNLP, you can read this tutorial and this documentation the class SentenceFeatureGeneratorFactory might be helpful",2023-11-07 19:24:39,2023-11-07 22:04:18,559,https://stackoverflow.com/questions/77440929/any-alternative-of-python-transfomer-package-in-java,"Any alternative of Python Transfomer Package in java? I am loading a Python model from Hugging Face. A pretrained model used to generate answers of questions from a given context. Model Description: model link page Model loading sample: from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline model_name = ""deepset/roberta-base-squad2"" # a) Get predictions nlp = pipeline('question-answering, model=model_name, tokenizer=model_name) QA_input = { 'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers' } res = nlp(QA_input) It is using a Python transformer package to load it. Now I can create its API in Python easily, but I want to create an API of this model in Java, where its parameters ('Question' & 'Context') will be given by the user itself. Now I want to know, how will I load this model in Java? Is there any transformer dependency in Java?",alternative python transfomer package java  load python model hugging face  pretraine model use generate answer question give context  model description  model link page model loading sample  transformer import automodelforquestionanswering  autotokenizer  pipeline modelname    deepset  roberta  base  squad2    get prediction nlp  pipeline   question  answer  model  modelname  tokenizer  modelname  qainput    question    why model conversion important     context    the option convert model farm transformer   re  nlp  qainput  use python transformer package load  create api python easily  want create api model java  parameter   question    context   give user  want know  load model java  transformer dependency java ,convert model onnx runtime format run apache opennlp  read tutorial documentation class sentencefeaturegeneratorfactory might helpful,alternative python transfomer package java  load python model hugging face  pretraine model use generate answer question give context  model description  model link page model loading sample  transformer import automodelforquestionanswering  autotokenizer  pipeline modelname    deepset  roberta  base  squad2    get prediction nlp  pipeline   question  answer  model  modelname  tokenizer  modelname  qainput    question    why model conversion important     context    the option convert model farm transformer   re  nlp  qainput  use python transformer package load  create api python easily  want create api model java  parameter   question    context   give user  want know  load model java  transformer dependency java  convert model onnx runtime format run apache opennlp  read tutorial documentation class sentencefeaturegeneratorfactory might helpful,Library/Tool-Based Queries
How to get perplexity per token rather than average perplexity?,"I can get the perplexity of a whole sentence from here : device = ""cuda"" from transformers import GPT2LMHeadModel, GPT2TokenizerFast device = ""cuda"" model_id = ""gpt2"" model = GPT2LMHeadModel.from_pretrained(model_id).to(device) tokenizer = GPT2TokenizerFast.from_pretrained(model_id) sent = 'Happy Birthday!' input_ids = tokenizer(sent, return_tensors='pt')['input_ids'] target_ids = input_ids.clone() outputs = model(input_ids.to(device), labels=target_ids) ppl = torch.exp(outputs.loss) print(ppl) >>>tensor(1499.6934, device='cuda:0', grad_fn=<ExpBackward0>) But how can I get the perplexity value for each token, instead of of the average perplexity of the entire sequence of tokens? The input sentence in this example, 'Happy Birthday!' is composed of 3 tokens. Based on the formula for perplexity: This should result in 3 values: log probability of the first token, log probability of the second token given the first, and the log probability of the third token given the first 2. Each should be exponentiated to get the perplexity value of each token. I currently have the following: import torch from transformers import GPT2LMHeadModel, GPT2TokenizerFast device = ""cuda"" model_id = ""gpt2"" model = GPT2LMHeadModel.from_pretrained(model_id).to(device) tokenizer = GPT2TokenizerFast.from_pretrained(model_id) sent = 'Happy Birthday!' input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device) target_ids = input_ids.clone() # Initialize an empty list to store perplexities for each token perplexities = [] # Calculate perplexity for each token for i in range(input_ids.shape[1]): output = model(input_ids[:, :i+1], labels=target_ids[:, :i+1]) log_prob = output.loss.item() perplexity = torch.exp(torch.tensor(log_prob)) perplexities.append(perplexity.item()) # Perplexities is now a list containing the perplexity values for each token for i, token in enumerate([tokenizer.decode(i) for i in input_ids[0]]): print(f""Token: {token}, Perplexity: {perplexities[i]}"") >>> Token: Happy, Perplexity: nan Token: Birthday, Perplexity: 54192.46484375 Token: !, Perplexity: 1499.693359375 But I'm not sure what I'm doing wrong, as the last token seem to have the same perplexity as the entire sentence.","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'perplexity']",2,"this is happening because in the second code snippet, you loop over the input sequence by adding a new token at each iteration: i=0: input_ids[:, :i+1] := tensor([[25082]], device='cuda:0') i=1: input_ids[:, :i+1] := tensor([[25082, 33511]], device='cuda:0') i=2: input_ids[:, :i+1] := tensor([[25082, 33511, 0]], device='cuda:0') Then, the computation of the perplexity in the last iteration of the loop is essentially identical to doing this: outputs = model(input_ids.to(device), labels=target_ids) ppl = torch.exp(outputs.loss) Here's how you can compute the perplexity and per-token perplexity (see https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/models/gpt2/modeling_gpt2.py#L1103 ): import torch.nn.functional as F [...] sent = 'Happy Birthday!' input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device) labels = input_ids.clone() output = model(input_ids, labels=labels) logits = output.logits shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction='none') per_token_perplexity = torch.exp(loss) average_perplexity = torch.exp(torch.mean(loss)) print(per_token_perplexity) print(average_perplexity) The output: tensor([5.4192e+04, 4.1502e+01], device='cuda:0', grad_fn=<ExpBackward0>) tensor(1499.6934, device='cuda:0', grad_fn=<ExpBackward0>)",2023-11-06 17:30:20,2023-11-06 19:59:29,1742,https://stackoverflow.com/questions/77433100/how-to-get-perplexity-per-token-rather-than-average-perplexity,"How to get perplexity per token rather than average perplexity? I can get the perplexity of a whole sentence from here : device = ""cuda"" from transformers import GPT2LMHeadModel, GPT2TokenizerFast device = ""cuda"" model_id = ""gpt2"" model = GPT2LMHeadModel.from_pretrained(model_id).to(device) tokenizer = GPT2TokenizerFast.from_pretrained(model_id) sent = 'Happy Birthday!' input_ids = tokenizer(sent, return_tensors='pt')['input_ids'] target_ids = input_ids.clone() outputs = model(input_ids.to(device), labels=target_ids) ppl = torch.exp(outputs.loss) print(ppl) >>>tensor(1499.6934, device='cuda:0', grad_fn=<ExpBackward0>) But how can I get the perplexity value for each token, instead of of the average perplexity of the entire sequence of tokens? The input sentence in this example, 'Happy Birthday!' is composed of 3 tokens. Based on the formula for perplexity: This should result in 3 values: log probability of the first token, log probability of the second token given the first, and the log probability of the third token given the first 2. Each should be exponentiated to get the perplexity value of each token. I currently have the following: import torch from transformers import GPT2LMHeadModel, GPT2TokenizerFast device = ""cuda"" model_id = ""gpt2"" model = GPT2LMHeadModel.from_pretrained(model_id).to(device) tokenizer = GPT2TokenizerFast.from_pretrained(model_id) sent = 'Happy Birthday!' input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device) target_ids = input_ids.clone() # Initialize an empty list to store perplexities for each token perplexities = [] # Calculate perplexity for each token for i in range(input_ids.shape[1]): output = model(input_ids[:, :i+1], labels=target_ids[:, :i+1]) log_prob = output.loss.item() perplexity = torch.exp(torch.tensor(log_prob)) perplexities.append(perplexity.item()) # Perplexities is now a list containing the perplexity values for each token for i, token in enumerate([tokenizer.decode(i) for i in input_ids[0]]): print(f""Token: {token}, Perplexity: {perplexities[i]}"") >>> Token: Happy, Perplexity: nan Token: Birthday, Perplexity: 54192.46484375 Token: !, Perplexity: 1499.693359375 But I'm not sure what I'm doing wrong, as the last token seem to have the same perplexity as the entire sentence.",get perplexity per token rather average perplexity  get perplexity whole sentence  device    cuda  transformer import gpt2lmheadmodel  gpt2tokenizerfast device    cuda  modelid    gpt2  model  gpt2lmheadmodelfrompretrained  modelid  to  device  tokenizer  gpt2tokenizerfastfrompretrained  modelid  send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   targetid  inputidsclone   output  model  inputidsto  device   label  targetid  ppl  torchexp  outputsloss  print  ppl     tensor  14996934  devicecuda0   gradfn  expbackward0   get perplexity value token  instead average perplexity entire sequence token  input sentence example   happy birthday   compose 3 token  base formula perplexity  result 3 value  log probability first token  log probability second token give first  log probability third token give first 2  exponentiate get perplexity value token  currently follow  import torch transformer import gpt2lmheadmodel  gpt2tokenizerfast device    cuda  modelid    gpt2  model  gpt2lmheadmodelfrompretrained  modelid  to  device  tokenizer  gpt2tokenizerfastfrompretrained  modelid  send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   to  device  targetid  inputidsclone    initialize empty list store perplexity token perplexity     calculate perplexity token range  inputidsshape  1    output  model  inputids     i1   label  targetid     i1   logprob  outputlossitem   perplexity  torchexp  torchtensor  logprob   perplexitiesappend  perplexityitem     perplexity list contain perplexity value token  token enumerate   tokenizerdecode   inputid  0     print  f  token   token   perplexity   perplexity         token  happy  perplexity  nan token  birthday  perplexity  5419246484375 token    perplexity  1499693359375  m sure  m wrong  last token seem perplexity entire sentence ,happen second code snippet  loop input sequence add new token iteration  i0  inputids     i1    tensor    25082    devicecuda0   i1  inputids     i1    tensor    25082  33511    devicecuda0   i2  inputids     i1    tensor    25082  33511  0    devicecuda0    computation perplexity last iteration loop essentially identical  output  model  inputidsto  device   label  targetid  ppl  torchexp  outputsloss  s compute perplexity per  token perplexity  see   githubcom  huggingface  transformer  blob  v4350  src  transformer  model  gpt2  modelinggpt2py  l1103   import torchnnfunctional f    send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   to  device  label  inputidsclone   output  model  inputids  label  label  logit  outputlogit shiftlogit  logit     1    contiguous   shiftlabel  label    1   contiguous   loss  fcrossentropy  shiftlogitsview  1  shiftlogitssize  1    shiftlabelsview  1   reductionnone   pertokenperplexity  torchexp  loss  averageperplexity  torchexp  torchmean  loss   print  pertokenperplexity  print  averageperplexity  output  tensor   54192e04  41502e01   devicecuda0   gradfn  expbackward0   tensor  14996934  devicecuda0   gradfn  expbackward0  ,get perplexity per token rather average perplexity  get perplexity whole sentence  device    cuda  transformer import gpt2lmheadmodel  gpt2tokenizerfast device    cuda  modelid    gpt2  model  gpt2lmheadmodelfrompretrained  modelid  to  device  tokenizer  gpt2tokenizerfastfrompretrained  modelid  send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   targetid  inputidsclone   output  model  inputidsto  device   label  targetid  ppl  torchexp  outputsloss  print  ppl     tensor  14996934  devicecuda0   gradfn  expbackward0   get perplexity value token  instead average perplexity entire sequence token  input sentence example   happy birthday   compose 3 token  base formula perplexity  result 3 value  log probability first token  log probability second token give first  log probability third token give first 2  exponentiate get perplexity value token  currently follow  import torch transformer import gpt2lmheadmodel  gpt2tokenizerfast device    cuda  modelid    gpt2  model  gpt2lmheadmodelfrompretrained  modelid  to  device  tokenizer  gpt2tokenizerfastfrompretrained  modelid  send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   to  device  targetid  inputidsclone    initialize empty list store perplexity token perplexity     calculate perplexity token range  inputidsshape  1    output  model  inputids     i1   label  targetid     i1   logprob  outputlossitem   perplexity  torchexp  torchtensor  logprob   perplexitiesappend  perplexityitem     perplexity list contain perplexity value token  token enumerate   tokenizerdecode   inputid  0     print  f  token   token   perplexity   perplexity         token  happy  perplexity  nan token  birthday  perplexity  5419246484375 token    perplexity  1499693359375  m sure  m wrong  last token seem perplexity entire sentence  happen second code snippet  loop input sequence add new token iteration  i0  inputids     i1    tensor    25082    devicecuda0   i1  inputids     i1    tensor    25082  33511    devicecuda0   i2  inputids     i1    tensor    25082  33511  0    devicecuda0    computation perplexity last iteration loop essentially identical  output  model  inputidsto  device   label  targetid  ppl  torchexp  outputsloss  s compute perplexity per  token perplexity  see   githubcom  huggingface  transformer  blob  v4350  src  transformer  model  gpt2  modelinggpt2py  l1103   import torchnnfunctional f    send   happy birthday   inputids  tokenizer  send  returntensorspt     inputids   to  device  label  inputidsclone   output  model  inputids  label  label  logit  outputlogit shiftlogit  logit     1    contiguous   shiftlabel  label    1   contiguous   loss  fcrossentropy  shiftlogitsview  1  shiftlogitssize  1    shiftlabelsview  1   reductionnone   pertokenperplexity  torchexp  loss  averageperplexity  torchexp  torchmean  loss   print  pertokenperplexity  print  averageperplexity  output  tensor   54192e04  41502e01   devicecuda0   gradfn  expbackward0   tensor  14996934  devicecuda0   gradfn  expbackward0  ,Implementation Issues
Do I need to retrain an NLP model everytime because of incompatible shape after transforming?,"I'm trying to build an NLP model that uses XGBoost. In my following code. loaded_model = joblib.load('fraud.sav') def clean_data(user_input): ''' Cleaning data, removing digits, punctuation etc. ''' return data data_processed = clean_data(data_raw) input_cleaned = clean_data(user_data) total_data = pd.concat([data_processed,input_cleaned]) vectorizer=TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=15000, smooth_idf=True, sublinear_tf=True) vectorizer.fit(total_data['text']) X_training_vectorized = vectorizer.transform(total_data['text']) X_test = vectorizer.transform(input_cleaned['text']) pca = PCA(n_components=0.95) pca.fit(X_training_vectorized.toarray()) X_test_pca = pca.transform(X_test.toarray()) y_test = loaded_model.predict(X_test_pca) What I don't understand is, I previously trained my dataset with 10000+ data and got good results. I then decide to save the model so that I can make predictions using user data. My model detects whether a text document is fraudulent or real. I have a dataset thats labelled for fraudulent data. I understand that when transforming data, vectorizer and pca should both be fitted to our whole dataset so that it will result in the same shape. What I dont understand is, how do I make it such that, I can transform user input, and have it the same shape as my model that I pretrained? Whats the proper procedure for this? Would love answers that also consider performance/time needed to process the data.","['python', 'machine-learning', 'scikit-learn', 'nlp', 'xgboost']",1,"This is done automatically as part of the CountVectorizer - tokens which appear in a new dataset but not in the data it is fit upon are simply ignored and the shape of the output will remain the same. For example: from sklearn.feature_extraction.text import CountVectorizer import pandas as pd data = ['the cat in the hat', 'cats like milk', 'cats and rats'] cv = CountVectorizer() dtm = cv.fit_transform(data) pd.DataFrame(dtm.todense(), columns=cv.get_feature_names_out()) and cat cats hat in like milk rats the 0 1 0 1 1 0 0 0 2 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 Now if we pass new data to the fitted CountVectorizer that includes tokens it hasn't seen before (birds, dogs) they are ignored, and the dimensionality of the document-term matrix remains the same: data2 = ['dogs and cats', 'birds and cats', 'dogs and birds'] dtm = cv.transform(data2) pd.DataFrame(dtm.todense(), columns=cv.get_feature_names_out()) and cat cats hat in like milk rats the 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 Since unseen tokens are ignored, this stresses the importance of retraining and/or consistency in the distribution of tokens in your training data and any data the model is used upon. Additionally, I would also avoid using floating point value for n_components for PCA but instead pick a set number of components (pass an integer value as opposed to a float) so that the output dimensionality of preprocessing is consistent.",2023-11-06 15:57:57,2023-11-06 16:37:37,45,https://stackoverflow.com/questions/77432480/do-i-need-to-retrain-an-nlp-model-everytime-because-of-incompatible-shape-after,"Do I need to retrain an NLP model everytime because of incompatible shape after transforming? I'm trying to build an NLP model that uses XGBoost. In my following code. loaded_model = joblib.load('fraud.sav') def clean_data(user_input): ''' Cleaning data, removing digits, punctuation etc. ''' return data data_processed = clean_data(data_raw) input_cleaned = clean_data(user_data) total_data = pd.concat([data_processed,input_cleaned]) vectorizer=TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=15000, smooth_idf=True, sublinear_tf=True) vectorizer.fit(total_data['text']) X_training_vectorized = vectorizer.transform(total_data['text']) X_test = vectorizer.transform(input_cleaned['text']) pca = PCA(n_components=0.95) pca.fit(X_training_vectorized.toarray()) X_test_pca = pca.transform(X_test.toarray()) y_test = loaded_model.predict(X_test_pca) What I don't understand is, I previously trained my dataset with 10000+ data and got good results. I then decide to save the model so that I can make predictions using user data. My model detects whether a text document is fraudulent or real. I have a dataset thats labelled for fraudulent data. I understand that when transforming data, vectorizer and pca should both be fitted to our whole dataset so that it will result in the same shape. What I dont understand is, how do I make it such that, I can transform user input, and have it the same shape as my model that I pretrained? Whats the proper procedure for this? Would love answers that also consider performance/time needed to process the data.",need retrain nlp model everytime incompatible shape transforming   m try build nlp model use xgboost  follow code  loadedmodel  joblibload   fraudsav   def cleandata  userinput      clean datum  remove digit  punctuation etc     return datum dataprocesse  cleandata  dataraw  inputcleane  cleandata  userdata  totaldata  pdconcat   dataprocesse  inputcleaned   vectorizer  tfidfvectorizer  stripaccentsunicode   analyzerword   ngramrange  1  2   maxfeatures15000  smoothidf  true  sublineartf  true  vectorizerfit  totaldata   text    xtrainingvectorize  vectorizertransform  totaldata   text    xt  vectorizertransform  inputcleane   text    pca  pca  ncomponents095  pcafit  xtrainingvectorizedtoarray    xtestpca  pcatransform  xtesttoarray    yt  loadedmodelpredict  xtestpca  not understand  previously train dataset 10000  datum get good result  decide save model make prediction use user datum  model detect whether text document fraudulent real  dataset that s label fraudulent datum  understand transform datum  vectorizer pca fit whole dataset result shape  do not understand  make  transform user input  shape model pretraine  what s proper procedure  would love answer also consider performance  time need process datum ,do automatically part countvectorizer  tokens appear new dataset datum fit upon simply ignore shape output remain  example  sklearnfeatureextractiontext import countvectorizer import panda pd datum    the cat hat    cat like milk    cat rat   cv  countvectorizer   dtm  cvfittransform  datum  pd  dataframe  dtmtodense    column  cvgetfeaturenamesout    cat cat hat like milk rat 0 1 0 1 1 0 0 0 2 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 pass new datum fit countvectorizer include token not see  bird  dog  ignore  dimensionality document  term matrix remain  data2    dog cat    bird cat    dog bird   dtm  cvtransform  data2  pd  dataframe  dtmtodense    column  cvgetfeaturenamesout    cat cat hat like milk rat 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 since unseen token ignore  stress importance retrain andor consistency distribution token training datum datum model use upon  additionally  would also avoid use float point value ncomponent pca instead pick set number component  pass integer value oppose float  output dimensionality preprocesse consistent ,need retrain nlp model everytime incompatible shape transforming   m try build nlp model use xgboost  follow code  loadedmodel  joblibload   fraudsav   def cleandata  userinput      clean datum  remove digit  punctuation etc     return datum dataprocesse  cleandata  dataraw  inputcleane  cleandata  userdata  totaldata  pdconcat   dataprocesse  inputcleaned   vectorizer  tfidfvectorizer  stripaccentsunicode   analyzerword   ngramrange  1  2   maxfeatures15000  smoothidf  true  sublineartf  true  vectorizerfit  totaldata   text    xtrainingvectorize  vectorizertransform  totaldata   text    xt  vectorizertransform  inputcleane   text    pca  pca  ncomponents095  pcafit  xtrainingvectorizedtoarray    xtestpca  pcatransform  xtesttoarray    yt  loadedmodelpredict  xtestpca  not understand  previously train dataset 10000  datum get good result  decide save model make prediction use user datum  model detect whether text document fraudulent real  dataset that s label fraudulent datum  understand transform datum  vectorizer pca fit whole dataset result shape  do not understand  make  transform user input  shape model pretraine  what s proper procedure  would love answer also consider performance  time need process datum  do automatically part countvectorizer  tokens appear new dataset datum fit upon simply ignore shape output remain  example  sklearnfeatureextractiontext import countvectorizer import panda pd datum    the cat hat    cat like milk    cat rat   cv  countvectorizer   dtm  cvfittransform  datum  pd  dataframe  dtmtodense    column  cvgetfeaturenamesout    cat cat hat like milk rat 0 1 0 1 1 0 0 0 2 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 pass new datum fit countvectorizer include token not see  bird  dog  ignore  dimensionality document  term matrix remain  data2    dog cat    bird cat    dog bird   dtm  cvtransform  data2  pd  dataframe  dtmtodense    column  cvgetfeaturenamesout    cat cat hat like milk rat 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 since unseen token ignore  stress importance retrain andor consistency distribution token training datum datum model use upon  additionally  would also avoid use float point value ncomponent pca instead pick set number component  pass integer value oppose float  output dimensionality preprocesse consistent ,Implementation Issues
Custom spaCy tagger to tag all words that are in a dictionary,"I'm trying spaCy to extract specific information from a text. So I need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in JSON format. The tokenizer worked on several attempts, but the labeler has been having problems when processing simple text. I hope that the label I will add to the words is a custom POS-Tag ""UNM"" and that I can attribute it to token.pos_ like all other labels ""NOUN"", ""VERB"", etc. import requests #keywords dictionary dictionary = requests.get( ""https://github.com/dglopes/NBR15575/raw/main/unidades_medidas.json"").json() #Creating the Custom Tagger Doc.set_extension('pos_tag', default=None, force=True) @Language.factory(""keyword_pos_tagger"") class KeywordPosTagger: def __init__(self, name, nlp, keywords, pos_tag): self.keywords = keywords self.pos_tag = pos_tag #Doc.set_extension('pos_tag', default=None, force=True) def __call__(self, doc): for token in doc: if token.text in self.keywords: token._.pos_tag = self.pos_tag return doc nlp = spacy.load('pt_core_news_md') keywords = ('m²', 'm2', '(W/K)', 'ºC') pos_tag = 'UNM' # substitua por seu rótulo POS keyword_pos_tagger = KeywordPosTagger(nlp, 'keyword_pos_tagger', keywords, pos_tag) config = {""nlp"": nlp, ""keywords"": keywords, ""pos_tag"": pos_tag} nlp.add_pipe('keyword_pos_tagger', config = config) < main .KeywordPosTagger at 0x78d568e4cee0> And when I use the custom tagger: doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)') for token in doc: print(token.text, token._.pos_tag) it returns this error --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-5-3c241e1c89fd> in <cell line: 1>() ----> 1 doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)') 2 for token in doc: 3 print(token.text, token._.pos_tag) 4 frames /usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value) 74 def __setattr__(self, name: str, value: Any): 75 if name not in self._extensions: ---> 76 raise AttributeError(Errors.E047.format(name=name)) 77 default, method, getter, setter = self._extensions[name] 78 if setter is not None: AttributeError: [E047] Can't assign a value to unregistered extension attribute 'pos_tag'. Did you forget to call the `set_extension` method?","['python', 'nlp', 'spacy', 'pos-tagger']",1,"You need to provide the config settings in the add_pipe method through a config dict. In your code, the keyword_pos_tagger variable is a stranded component that's not actually added to the nlp pipeline. It shares the same vocab and you could use it for unit testing, but otherwise you can't add it to a pipeline when it's created like this. nlp.add_pipe(""keyword_pos_tagger"", config={""keywords"": keywords, ""pos_tag"": pos_tag}) Edited to expand answer: # tested with spacy==3.7.2 import spacy from spacy.language import Language from spacy.tokens import Token # Creating the Custom Tagger Token.set_extension(""pos_tag"", default=None, force=True) @Language.factory(""keyword_pos_tagger"") class KeywordPosTagger: def __init__(self, name, nlp, keywords, pos_tag): self.keywords = keywords self.pos_tag = pos_tag def __call__(self, doc): for token in doc: if token.text in self.keywords: token._.pos_tag = self.pos_tag return doc nlp = spacy.load(""pt_core_news_md"") keywords = (""m²"", ""m2"", ""(W/K)"", ""ºC"") pos_tag = ""UNM"" # substitua por seu rótulo POS config = {""keywords"": keywords, ""pos_tag"": pos_tag} nlp.add_pipe(""keyword_pos_tagger"", config=config) doc = nlp( ""A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)"" ) assert doc[16]._.pos_tag == ""UNM""",2023-11-05 23:07:34,2023-11-06 08:28:09,184,https://stackoverflow.com/questions/77427999/custom-spacy-tagger-to-tag-all-words-that-are-in-a-dictionary,"Custom spaCy tagger to tag all words that are in a dictionary I'm trying spaCy to extract specific information from a text. So I need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in JSON format. The tokenizer worked on several attempts, but the labeler has been having problems when processing simple text. I hope that the label I will add to the words is a custom POS-Tag ""UNM"" and that I can attribute it to token.pos_ like all other labels ""NOUN"", ""VERB"", etc. import requests #keywords dictionary dictionary = requests.get( ""https://github.com/dglopes/NBR15575/raw/main/unidades_medidas.json"").json() #Creating the Custom Tagger Doc.set_extension('pos_tag', default=None, force=True) @Language.factory(""keyword_pos_tagger"") class KeywordPosTagger: def __init__(self, name, nlp, keywords, pos_tag): self.keywords = keywords self.pos_tag = pos_tag #Doc.set_extension('pos_tag', default=None, force=True) def __call__(self, doc): for token in doc: if token.text in self.keywords: token._.pos_tag = self.pos_tag return doc nlp = spacy.load('pt_core_news_md') keywords = ('m²', 'm2', '(W/K)', 'ºC') pos_tag = 'UNM' # substitua por seu rótulo POS keyword_pos_tagger = KeywordPosTagger(nlp, 'keyword_pos_tagger', keywords, pos_tag) config = {""nlp"": nlp, ""keywords"": keywords, ""pos_tag"": pos_tag} nlp.add_pipe('keyword_pos_tagger', config = config) < main .KeywordPosTagger at 0x78d568e4cee0> And when I use the custom tagger: doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)') for token in doc: print(token.text, token._.pos_tag) it returns this error --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-5-3c241e1c89fd> in <cell line: 1>() ----> 1 doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)') 2 for token in doc: 3 print(token.text, token._.pos_tag) 4 frames /usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value) 74 def __setattr__(self, name: str, value: Any): 75 if name not in self._extensions: ---> 76 raise AttributeError(Errors.E047.format(name=name)) 77 default, method, getter, setter = self._extensions[name] 78 if setter is not None: AttributeError: [E047] Can't assign a value to unregistered extension attribute 'pos_tag'. Did you forget to call the `set_extension` method?",custom spacy tagger tag word dictionary  m try spacy extract specific information text  need configure custom tokenizer identify custom tagger label word external dictionary json format  tokenizer work several attempt  labeler problem process simple text  hope label add word custom pos  tag   unm  attribute tokenpos  like label   noun     verb   etc  import request  keyword dictionary dictionary  requestsget      githubcom  dglopes  nbr15575  raw  main  unidadesmedidasjson   json    create custom tagger docsetextension   postag   default  none  force  true   languagefactory    keywordpostagger   class keywordpostagger  def   init    self  name  nlp  keyword  postag   selfkeyword  keyword selfpostag  postag  docsetextension   postag   default  none  force  true  def   call    self  doc   token doc  tokentext selfkeyword  tokenpostag  selfpostag return doc nlp  spacyload   ptcorenewsmd   keyword    m    m2     w  k     c   postag   unm   substitua por seu rtulo pos keywordpostagger  keywordpostagger  nlp   keywordpostagger   keyword  postag  config     nlp   nlp    keyword   keyword    postag   postag  nlpaddpipe   keywordpostagger   config  config   main keywordpostagger 0x78d568e4cee0  use custom tagger  doc  nlp   temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    token doc  print  tokentext  tokenpostag  return error                                       attributeerror traceback  recent call last   ipython  input5  3c241e1c89fd   cell line  1       1 doc  nlp   temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    2 token doc  3 print  tokentext  tokenpostag  4 frame usr  local  lib  python310  dist  package  spacy  token  underscorepy   setattr    self  name  value  74 def   setattr    self  name  str  value    75 name selfextension     76 raise attributeerror  errorse047format  name  name   77 default  method  getter  setter  selfextensions  name  78 setter none  attributeerror   e047  can not assign value unregistered extension attribute  postag   forget call  setextension  method ,need provide config setting addpipe method config dict  code  keywordpostagger variable strand component s actually add nlp pipeline  share vocab could use unit testing  otherwise can not add pipeline be create like  nlpaddpipe    keywordpostagger   config    keyword   keyword    postag   postag   edit expand answer   test spacy372 import spacy spacylanguage import language spacytoken import token  create custom tagger tokensetextension    postag   default  none  force  true   languagefactory    keywordpostagger   class keywordpostagger  def   init    self  name  nlp  keyword  postag   selfkeyword  keyword selfpostag  postag def   call    self  doc   token doc  tokentext selfkeyword  tokenpostag  selfpostag return doc nlp  spacyload    ptcorenewsmd   keyword     m     m2      w  k      c   postag    unm   substitua por seu rtulo pos config     keyword   keyword    postag   postag  nlpaddpipe    keywordpostagger   config  config  doc  nlp    temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    assert doc  16  postag     unm ,custom spacy tagger tag word dictionary  m try spacy extract specific information text  need configure custom tokenizer identify custom tagger label word external dictionary json format  tokenizer work several attempt  labeler problem process simple text  hope label add word custom pos  tag   unm  attribute tokenpos  like label   noun     verb   etc  import request  keyword dictionary dictionary  requestsget      githubcom  dglopes  nbr15575  raw  main  unidadesmedidasjson   json    create custom tagger docsetextension   postag   default  none  force  true   languagefactory    keywordpostagger   class keywordpostagger  def   init    self  name  nlp  keyword  postag   selfkeyword  keyword selfpostag  postag  docsetextension   postag   default  none  force  true  def   call    self  doc   token doc  tokentext selfkeyword  tokenpostag  selfpostag return doc nlp  spacyload   ptcorenewsmd   keyword    m    m2     w  k     c   postag   unm   substitua por seu rtulo pos keywordpostagger  keywordpostagger  nlp   keywordpostagger   keyword  postag  config     nlp   nlp    keyword   keyword    postag   postag  nlpaddpipe   keywordpostagger   config  config   main keywordpostagger 0x78d568e4cee0  use custom tagger  doc  nlp   temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    token doc  print  tokentext  tokenpostag  return error                                       attributeerror traceback  recent call last   ipython  input5  3c241e1c89fd   cell line  1       1 doc  nlp   temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    2 token doc  3 print  tokentext  tokenpostag  4 frame usr  local  lib  python310  dist  package  spacy  token  underscorepy   setattr    self  name  value  74 def   setattr    self  name  str  value    75 name selfextension     76 raise attributeerror  errorse047format  name  name   77 default  method  getter  setter  selfextensions  name  78 setter none  attributeerror   e047  can not assign value unregistered extension attribute  postag   forget call  setextension  method  need provide config setting addpipe method config dict  code  keywordpostagger variable strand component s actually add nlp pipeline  share vocab could use unit testing  otherwise can not add pipeline be create like  nlpaddpipe    keywordpostagger   config    keyword   keyword    postag   postag   edit expand answer   test spacy372 import spacy spacylanguage import language spacytoken import token  create custom tagger tokensetextension    postag   default  none  force  true   languagefactory    keywordpostagger   class keywordpostagger  def   init    self  name  nlp  keyword  postag   selfkeyword  keyword selfpostag  postag def   call    self  doc   token doc  tokentext selfkeyword  tokenpostag  selfpostag return doc nlp  spacyload    ptcorenewsmd   keyword     m     m2      w  k      c   postag    unm   substitua por seu rtulo pos config     keyword   keyword    postag   postag  nlpaddpipe    keywordpostagger   config  config  doc  nlp    temperatura tem 159c ou 20 c  tambm precisa ter 20 m de largura e 14 m de rea  caso contrrio ter 1 kelvin  w  k    assert doc  16  postag     unm ,Implementation Issues
How to make named entity recognition provide better categorization of data,Following is a default categorization of data from a news article. Christiane Amanpour 268 287 PERSON Hamas 155 160 ORG Rania 6 11 PERSON Warner 0 6 ORG But I would like to change the behavior as follows I would want to categorize `Christiane Amanpour` as a journalist I would want to categorize `Rania` as a queen I would want to categorize `Warner` as a cricket player How exactly I train the data to do this,"['nlp', 'spacy', 'large-language-model']",1,"Hey @user352290 👋 You're using a Named Entity Recognition (NER) model, which has provided the labels for PERSON and ORG in the example specified (I think you're using the default spaCy model?) Based on your question, it seems you want a more specific categorisation (e.g. Christiane Amanpour -> journalist). Broadly speaking, NER models fall into one of two categories: 4-class models, as the name would suggest, recognise 4 classes of entities. Typically, people, places, organisations, and time/dates. Most 4-class models are trained on the CoNLL 2003 corpus. 18-class models can recognise 18 entity types, however, they tend to have a lower F1 / accuracy. Most 18-class models are trained on the OntoNotes 5.0 dataset. Even an 18-class NER model will fail to categorise entities as you'd like. For example, there's no category for journalist in the OntoNotes corpus. As @petezurich suggests, you can achieve this task via Entity Linking , which recognises entities in a text, and maps them to an external source of knowledge (e.g. Wikipedia, DBpedia). In order to properly implement this, you will have to decide on a ""data point"" you'd like to locate for each entity. Take a look at the DBpedia entry for Christiane Amanpour . The property dbo:occupation seems to achieve what you're looking for. This gist has some code I've previously written to perform NER then Entity Linking. You'll need to make some tweaks, but it's a decent place to start. Change line 190 to modify the SPARQL query to fetch dbo:occupation , or whichever property you'd like to find. Good luck!",2023-10-30 04:28:52,2023-11-02 10:00:05,359,https://stackoverflow.com/questions/77386212/how-to-make-named-entity-recognition-provide-better-categorization-of-data,How to make named entity recognition provide better categorization of data Following is a default categorization of data from a news article. Christiane Amanpour 268 287 PERSON Hamas 155 160 ORG Rania 6 11 PERSON Warner 0 6 ORG But I would like to change the behavior as follows I would want to categorize `Christiane Amanpour` as a journalist I would want to categorize `Rania` as a queen I would want to categorize `Warner` as a cricket player How exactly I train the data to do this,make name entity recognition provide well categorization datum follow default categorization datum news article  christiane amanpour 268 287 person hamas 155 160 org rania 6 11 person warner 0 6 org would like change behavior follow would want categorize  christiane amanpour  journalist would want categorize  rania  queen would want categorize  warner  cricket player exactly train datum,hey  user352290  be use name entity recognition  ner  model  provide label person org example specify  think be use default spacy model   base question  seem want specific categorisation  eg  christiane amanpour   journalist   broadly speak  ner model fall one two category  4  class model  name would suggest  recognise 4 class entity  typically  people  place  organisation  time  date  4  class model train conll 2003 corpus  18  class model recognise 18 entity type  however  tend low f1  accuracy  18  class model train ontonotes 50 dataset  even 18  class ner model fail categorise entity would like  example  s category journalist ontonotes corpus   petezurich suggest  achieve task via entity linking  recognise entity text  map external source knowledge  eg  wikipedia  dbpedia   order properly implement  decide   datum point  would like locate entity  take look dbpedia entry christiane amanpour  property dbo  occupation seem achieve be look  gist code  ve previously write perform ner entity linking  will need make tweak  s decent place start  change line 190 modify sparql query fetch dbo  occupation  whichever property would like find  good luck ,make name entity recognition provide well categorization datum follow default categorization datum news article  christiane amanpour 268 287 person hamas 155 160 org rania 6 11 person warner 0 6 org would like change behavior follow would want categorize  christiane amanpour  journalist would want categorize  rania  queen would want categorize  warner  cricket player exactly train datum hey  user352290  be use name entity recognition  ner  model  provide label person org example specify  think be use default spacy model   base question  seem want specific categorisation  eg  christiane amanpour   journalist   broadly speak  ner model fall one two category  4  class model  name would suggest  recognise 4 class entity  typically  people  place  organisation  time  date  4  class model train conll 2003 corpus  18  class model recognise 18 entity type  however  tend low f1  accuracy  18  class model train ontonotes 50 dataset  even 18  class ner model fail categorise entity would like  example  s category journalist ontonotes corpus   petezurich suggest  achieve task via entity linking  recognise entity text  map external source knowledge  eg  wikipedia  dbpedia   order properly implement  decide   datum point  would like locate entity  take look dbpedia entry christiane amanpour  property dbo  occupation seem achieve be look  gist code  ve previously write perform ner entity linking  will need make tweak  s decent place start  change line 190 modify sparql query fetch dbo  occupation  whichever property would like find  good luck ,Task-Specific Queries
"Laptop stopped when training a pytorch lstm model, while tensorflow counterpart works","I have the following NN module in PyTorch class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.emb = nn.Embedding(num_embeddings=10000, embedding_dim=512) self.drop1 = nn.Dropout(p=0.25) self.lstm = nn.LSTM(input_size=512, hidden_size=32, num_layers=1) self.drop2 = nn.Dropout(p=0.25) self.dense = nn.Linear(32, 1) self.activ = nn.Sigmoid() def forward(self, x): t1 = self.emb(x) t2 = self.drop1(t1) outputs, (hidden, cell) = self.lstm(t2) t4 = self.drop2(outputs[:,-1,:]) t5 = self.dense(t4) return self.activ(t5) The training code is the following: model = Model() criterion = nn.BCELoss() optimizer = torch.optim.Adam(model.parameters()) for epoch in range(3): outputs = model(torch.from_numpy(x_train)) loss = criterion(torch.flatten(outputs).to(torch.float32), torch.flatten(torch.from_numpy(y_train)).to(torch.float32)) optimizer.zero_grad() loss.backward() optimizer.step() The code works fine when I drastically lower the dimensions in the different network layer (2 or 4 instead of 512 and 32 and so on). I did that to debug my implementation and make sure it works. However with the given parameters in the code I provided, my laptop stops (the mouse doesn't move anymore, nothing works, I had to unplug the laptop and restart it). Same thing when executing on Google Colab, there is an error and the session resets. I added prints everywhere, the code seemingly stops at outputs = model(torch.from_numpy(x_train)). I didn't check which step of the forward pass though. What is surprising is that the exact same module coded using Tensorflow.Keras works fine both on my laptop and on Google Colab. What am I missing here? Thanks a lot! I expect the training to work correctly. Data download and processing import tensorflow as tf import numpy as np data = tf.keras.datasets.imdb.load_data(num_words=10000) train, test = data[0], data[1] x_train, y_train = train[0], train[1] x_test, y_test = test[0], test[1] review_length = 500 from tensorflow.keras.preprocessing import sequence x_train = sequence.pad_sequences(x_train, maxlen = review_length) x_test = sequence.pad_sequences(x_test, maxlen = review_length) The same model in Tensorflow Keras that works fine from tensorflow.keras.models import Sequential model = Sequential() model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=512, input_length=500)) model.add(tf.keras.layers.Dropout(rate=0.25)) model.add(tf.keras.layers.LSTM(units=32)) model.add(tf.keras.layers.Dropout(rate=0.25)) model.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) model.compile(optimizer=tf.keras.optimizers.Adam(), loss=""binary_crossentropy"", metrics=[""accuracy""]) model.fit(np.asarray(x_train), y_train, epochs=3, batch_size=256, validation_split=0.2)","['python', 'deep-learning', 'pytorch', 'nlp']",1,"The problem is passing the whole training set to the model in a single call, the simple solution to this is to use batching, and it is why the Keras model works. as model.fit applies batching automatically, while in PyTorch batching has to be manually implemented.",2023-10-29 10:41:41,2023-10-29 18:29:19,75,https://stackoverflow.com/questions/77382923/laptop-stopped-when-training-a-pytorch-lstm-model-while-tensorflow-counterpart,"Laptop stopped when training a pytorch lstm model, while tensorflow counterpart works I have the following NN module in PyTorch class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.emb = nn.Embedding(num_embeddings=10000, embedding_dim=512) self.drop1 = nn.Dropout(p=0.25) self.lstm = nn.LSTM(input_size=512, hidden_size=32, num_layers=1) self.drop2 = nn.Dropout(p=0.25) self.dense = nn.Linear(32, 1) self.activ = nn.Sigmoid() def forward(self, x): t1 = self.emb(x) t2 = self.drop1(t1) outputs, (hidden, cell) = self.lstm(t2) t4 = self.drop2(outputs[:,-1,:]) t5 = self.dense(t4) return self.activ(t5) The training code is the following: model = Model() criterion = nn.BCELoss() optimizer = torch.optim.Adam(model.parameters()) for epoch in range(3): outputs = model(torch.from_numpy(x_train)) loss = criterion(torch.flatten(outputs).to(torch.float32), torch.flatten(torch.from_numpy(y_train)).to(torch.float32)) optimizer.zero_grad() loss.backward() optimizer.step() The code works fine when I drastically lower the dimensions in the different network layer (2 or 4 instead of 512 and 32 and so on). I did that to debug my implementation and make sure it works. However with the given parameters in the code I provided, my laptop stops (the mouse doesn't move anymore, nothing works, I had to unplug the laptop and restart it). Same thing when executing on Google Colab, there is an error and the session resets. I added prints everywhere, the code seemingly stops at outputs = model(torch.from_numpy(x_train)). I didn't check which step of the forward pass though. What is surprising is that the exact same module coded using Tensorflow.Keras works fine both on my laptop and on Google Colab. What am I missing here? Thanks a lot! I expect the training to work correctly. Data download and processing import tensorflow as tf import numpy as np data = tf.keras.datasets.imdb.load_data(num_words=10000) train, test = data[0], data[1] x_train, y_train = train[0], train[1] x_test, y_test = test[0], test[1] review_length = 500 from tensorflow.keras.preprocessing import sequence x_train = sequence.pad_sequences(x_train, maxlen = review_length) x_test = sequence.pad_sequences(x_test, maxlen = review_length) The same model in Tensorflow Keras that works fine from tensorflow.keras.models import Sequential model = Sequential() model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=512, input_length=500)) model.add(tf.keras.layers.Dropout(rate=0.25)) model.add(tf.keras.layers.LSTM(units=32)) model.add(tf.keras.layers.Dropout(rate=0.25)) model.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) model.compile(optimizer=tf.keras.optimizers.Adam(), loss=""binary_crossentropy"", metrics=[""accuracy""]) model.fit(np.asarray(x_train), y_train, epochs=3, batch_size=256, validation_split=0.2)",laptop stop train pytorch lstm model  tensorflow counterpart work follow nn module pytorch class model  nn  module   def   init    self   super  model  self  init     selfemb  nn  embed  numembeddings10000  embeddingdim512  selfdrop1  nn  dropout  p025  selflstm  nn  lstm  inputsize512  hiddensize32  numlayers1  selfdrop2  nn  dropout  p025  selfdense  nn  linear  32  1  selfactiv  nn  sigmoid   def forward  self  x   t1  selfemb  x  t2  selfdrop1  t1  output   hide  cell   selflstm  t2  t4  selfdrop2  output    1     t5  selfdense  t4  return selfactiv  t5  training code following  model  model   criterion  nn  bceloss   optimizer  torchoptim  adam  modelparameter    epoch range  3   output  model  torchfromnumpy  xtrain   loss  criterion  torchflatten  output  to  torchfloat32   torchflatten  torchfromnumpy  ytrain   to  torchfloat32   optimizerzerograd   lossbackward   optimizerstep   code work fine drastically low dimension different network layer  2 4 instead 512 32   debug implementation make sure work  however give parameter code provide  laptop stop  mouse not move anymore  nothing work  unplug laptop restart   thing execute google colab  error session reset  add print everywhere  code seemingly stop output  model  torchfromnumpy  xtrain    not check step forward pass though  surprising exact module code use tensorflow  keras work fine laptop google colab  miss  thank lot  expect training work correctly  datum download process import tensorflow tf import numpy np datum  tfkerasdatasetsimdbloaddata  numwords10000  train  test  datum  0   datum  1  xtrain  ytrain  train  0   train  1  xtest  ytest  test  0   test  1  reviewlength  500 tensorflowkeraspreprocesse import sequence xtrain  sequencepadsequence  xtrain  maxlen  reviewlength  xtest  sequencepadsequence  xtest  maxlen  reviewlength  model tensorflow keras work fine tensorflowkerasmodel import sequential model  sequential   modeladd  tfkeraslayer  embed  inputdim10000  outputdim512  inputlength500   modeladd  tfkeraslayer  dropout  rate025   modeladd  tfkeraslayer  lstm  units32   modeladd  tfkeraslayer  dropout  rate025   modeladd  tfkeraslayer  dense  units1  activationsigmoid    modelcompile  optimizer  tfkerasoptimizer  adam    loss  binarycrossentropy   metrics    accuracy    modelfit  npasarray  xtrain   ytrain  epochs3  batchsize256  validationsplit02 ,problem pass whole training set model single call  simple solution use batching  keras model work  modelfit apply batch automatically  pytorch batch manually implement ,laptop stop train pytorch lstm model  tensorflow counterpart work follow nn module pytorch class model  nn  module   def   init    self   super  model  self  init     selfemb  nn  embed  numembeddings10000  embeddingdim512  selfdrop1  nn  dropout  p025  selflstm  nn  lstm  inputsize512  hiddensize32  numlayers1  selfdrop2  nn  dropout  p025  selfdense  nn  linear  32  1  selfactiv  nn  sigmoid   def forward  self  x   t1  selfemb  x  t2  selfdrop1  t1  output   hide  cell   selflstm  t2  t4  selfdrop2  output    1     t5  selfdense  t4  return selfactiv  t5  training code following  model  model   criterion  nn  bceloss   optimizer  torchoptim  adam  modelparameter    epoch range  3   output  model  torchfromnumpy  xtrain   loss  criterion  torchflatten  output  to  torchfloat32   torchflatten  torchfromnumpy  ytrain   to  torchfloat32   optimizerzerograd   lossbackward   optimizerstep   code work fine drastically low dimension different network layer  2 4 instead 512 32   debug implementation make sure work  however give parameter code provide  laptop stop  mouse not move anymore  nothing work  unplug laptop restart   thing execute google colab  error session reset  add print everywhere  code seemingly stop output  model  torchfromnumpy  xtrain    not check step forward pass though  surprising exact module code use tensorflow  keras work fine laptop google colab  miss  thank lot  expect training work correctly  datum download process import tensorflow tf import numpy np datum  tfkerasdatasetsimdbloaddata  numwords10000  train  test  datum  0   datum  1  xtrain  ytrain  train  0   train  1  xtest  ytest  test  0   test  1  reviewlength  500 tensorflowkeraspreprocesse import sequence xtrain  sequencepadsequence  xtrain  maxlen  reviewlength  xtest  sequencepadsequence  xtest  maxlen  reviewlength  model tensorflow keras work fine tensorflowkerasmodel import sequential model  sequential   modeladd  tfkeraslayer  embed  inputdim10000  outputdim512  inputlength500   modeladd  tfkeraslayer  dropout  rate025   modeladd  tfkeraslayer  lstm  units32   modeladd  tfkeraslayer  dropout  rate025   modeladd  tfkeraslayer  dense  units1  activationsigmoid    modelcompile  optimizer  tfkerasoptimizer  adam    loss  binarycrossentropy   metrics    accuracy    modelfit  npasarray  xtrain   ytrain  epochs3  batchsize256  validationsplit02  problem pass whole training set model single call  simple solution use batching  keras model work  modelfit apply batch automatically  pytorch batch manually implement ,Implementation Issues
How does an instance of pytorch&#39;s `nn.Linear()` process a tuple of tensors?,"In the annotated transformer's implementation of multi-head attention , three tensors (query, key, value) are all passed to a nn.Linear(d_model, d_model) : # some class definition ... self.linears = clones(nn.Linear(d_model, d_model), 4) # deep-copied list of nn.Linear-modules concatenated via nn.ModuleList # more code ... query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ] My question: what happens at lin(x) , when an instance of nn.Linear() is called on the tuple (query, key, value) ? Is the tuple somehow concatenated to a tensor? If so, how - on which dimension are the tensors concatenated?","['python', 'machine-learning', 'pytorch', 'nlp', 'transformer-model']",1,"self.linears = clones(nn.Linear(d_model, d_model), 4) # deep-copied list of nn.Linear-modules concatenated via nn.ModuleList # more code ... query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ] Actually, the nn.Linear does not process input as a tuple of a Q,K,V . In your code, the result similar like this out_Q = self.linears[0](Q) out_K = self.linears[1](K) out_V = self.linears[2](V) When you use zip(iterable A, iterable B) So you will get the pairs (A[0], B[0]) (A[1], B[1]) ,... independently Or more specific query = self.linears[0](query) key = self.linears[1](key) value = self.linears[2](value)",2023-10-27 10:43:27,2023-10-27 11:31:18,177,https://stackoverflow.com/questions/77373522/how-does-an-instance-of-pytorchs-nn-linear-process-a-tuple-of-tensors,"How does an instance of pytorch&#39;s `nn.Linear()` process a tuple of tensors? In the annotated transformer's implementation of multi-head attention , three tensors (query, key, value) are all passed to a nn.Linear(d_model, d_model) : # some class definition ... self.linears = clones(nn.Linear(d_model, d_model), 4) # deep-copied list of nn.Linear-modules concatenated via nn.ModuleList # more code ... query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ] My question: what happens at lin(x) , when an instance of nn.Linear() is called on the tuple (query, key, value) ? Is the tuple somehow concatenated to a tensor? If so, how - on which dimension are the tensors concatenated?",instance pytorch   39   nn  linear    process tuple tensor  annotate transformer s implementation multi  head attention  three tensor  query  key  value  passed nn  linear  dmodel  dmodel    class definition  selflinear  clone  nn  linear  dmodel  dmodel   4   deep  copy list nn  linear  module concatenate via nn  modulelist  code  query  key  value   lin  x  view  nbatche  1  selfh  selfdk  transpose  1  2  lin  x zip  selflinear   query  key  value    question  happen lin  x   instance nn  linear   call tuple  query  key  value   tuple somehow concatenate tensor    dimension tensor concatenate ,selflinear  clone  nn  linear  dmodel  dmodel   4   deep  copy list nn  linear  module concatenate via nn  modulelist  code  query  key  value   lin  x  view  nbatche  1  selfh  selfdk  transpose  1  2  lin  x zip  selflinear   query  key  value    actually  nn  linear process input tuple q  k  v  code  result similar like outq  selflinear  0   q  outk  selflinear  1   k  outv  selflinear  2   v  use zip  iterable  iterable b  get pair   0   b  0     1   b  1     independently specific query  selflinear  0   query  key  selflinear  1   key  value  selflinear  2   value ,instance pytorch   39   nn  linear    process tuple tensor  annotate transformer s implementation multi  head attention  three tensor  query  key  value  passed nn  linear  dmodel  dmodel    class definition  selflinear  clone  nn  linear  dmodel  dmodel   4   deep  copy list nn  linear  module concatenate via nn  modulelist  code  query  key  value   lin  x  view  nbatche  1  selfh  selfdk  transpose  1  2  lin  x zip  selflinear   query  key  value    question  happen lin  x   instance nn  linear   call tuple  query  key  value   tuple somehow concatenate tensor    dimension tensor concatenate  selflinear  clone  nn  linear  dmodel  dmodel   4   deep  copy list nn  linear  module concatenate via nn  modulelist  code  query  key  value   lin  x  view  nbatche  1  selfh  selfdk  transpose  1  2  lin  x zip  selflinear   query  key  value    actually  nn  linear process input tuple q  k  v  code  result similar like outq  selflinear  0   q  outk  selflinear  1   k  outv  selflinear  2   v  use zip  iterable  iterable b  get pair   0   b  0     1   b  1     independently specific query  selflinear  0   query  key  selflinear  1   key  value  selflinear  2   value ,Implementation Issues
What components of spaCy Pipeline can be disabled so that the sentence tokenization can still work and the pipeline be faster?,"I want to use the spaCy pipeline only for sentence tokenization as it's the best for my language but I want it to be as minimal as possible. So far I figured I could get rid of tagger and ner components: nlp = spacy.load(""pl_core_news_sm"", disable=['tagger', 'ner']) I noticed that without tok2vec it doesn't work (which seems very odd). I don't want to try all combinations because I'd surely miss something. So does anyone know what components can be disabled so that the tokenization can still work and the pipeline be faster?","['python', 'nlp', 'spacy']",1,"For the exact same sentence segmentation from pl_core_news_sm with the fewest components, enable only tok2vec + parser . For faster sentence segmentation, disable everything that's enabled by default and then enable senter . For sentences with sentence-final punctuation, the performance is probably similar to the parser. If you don't have sentence-final punctuation, then the parser may perform better, but evaluate it for your task. See: https://spacy.io/models/#design-modify",2023-10-23 17:40:34,2023-10-24 06:51:21,491,https://stackoverflow.com/questions/77347287/what-components-of-spacy-pipeline-can-be-disabled-so-that-the-sentence-tokenizat,"What components of spaCy Pipeline can be disabled so that the sentence tokenization can still work and the pipeline be faster? I want to use the spaCy pipeline only for sentence tokenization as it's the best for my language but I want it to be as minimal as possible. So far I figured I could get rid of tagger and ner components: nlp = spacy.load(""pl_core_news_sm"", disable=['tagger', 'ner']) I noticed that without tok2vec it doesn't work (which seems very odd). I don't want to try all combinations because I'd surely miss something. So does anyone know what components can be disabled so that the tokenization can still work and the pipeline be faster?",component spacy pipeline disable sentence tokenization still work pipeline fast  want use spacy pipeline sentence tokenization s good language want minimal possible  far figure could get rid tagger ner component  nlp  spacyload    plcorenewssm   disable   tagger    ner    notice without tok2vec not work  seem odd   not want try combination would surely miss something  anyone know component disabled tokenization still work pipeline fast ,exact sentence segmentation plcorenewssm few component  enable tok2vec  parser  fast sentence segmentation  disable everything be enable default enable senter  sentence sentence  final punctuation  performance probably similar parser  not sentence  final punctuation  parser may perform well  evaluate task  see    spacyio  models  design  modify,component spacy pipeline disable sentence tokenization still work pipeline fast  want use spacy pipeline sentence tokenization s good language want minimal possible  far figure could get rid tagger ner component  nlp  spacyload    plcorenewssm   disable   tagger    ner    notice without tok2vec not work  seem odd   not want try combination would surely miss something  anyone know component disabled tokenization still work pipeline fast  exact sentence segmentation plcorenewssm few component  enable tok2vec  parser  fast sentence segmentation  disable everything be enable default enable senter  sentence sentence  final punctuation  performance probably similar parser  not sentence  final punctuation  parser may perform well  evaluate task  see    spacyio  models  design  modify,Implementation Issues
Why does my transformer model have more parameters than the Huggingface implementation?,"I'm loading a GPT model from huggingface as follows: from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig config = AutoConfig.from_pretrained( ""gpt2"", vocab_size=len(tokenizer), n_ctx=context_length, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, ) standard_gpt2 = GPT2LMHeadModel(config).to(device) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters"") # >>> GPT-2 size: 124.4M parameters If I print the model architecture I get: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) Focusing on the last layer -- lm_head , it has in_features=768, out_features=50257 So why when I replace just that one layer with the exact same number of parameters I get different results? standard_gpt2.lm_head = nn.Sequential( nn.Linear(in_features = 768, out_features = 50257, bias=False) ) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters"") # >>> GPT-2 size: 163.0M parameters","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface']",1,"That is because the linear layer of lm_head doesn't have separate weights. It shares its weight tensor with the token embedding layer. You can confirm this with data-ptr , which returns the address of the first element of the tensor: from torch import nn from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig model_id = ""gpt2"" tokenizer = AutoTokenizer.from_pretrained(model_id) standard_gpt2 = GPT2LMHeadModel.from_pretrained(model_id) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size: {standard_gpt2_model_size} parameters"") print(f""Token embedding layer address {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"") print(f""LM_head address {standard_gpt2.lm_head.weight.untyped_storage().data_ptr()}"") # Replacing the default head standard_gpt2.lm_head = nn.Linear(in_features = 768, out_features = 50257, bias=False) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size after replacing lm_head: {standard_gpt2_model_size} parameters"") print(f""Token embedding layer address after replacing lm_head {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"") print(f""LM_head address after replacing lm_head {standard_gpt2.lm_head.weight.untyped_storage().data_ptr()}"") Output: GPT-2 size: 124439808 parameters Token embedding layer address: 96251233152832 LM_head address: 96251233152832 GPT-2 size after replacing lm_head: 163037184 parameters Token embedding layer address after replacing lm_head: 96251233152832 LM_head address after replacing lm_head: 134800505946176 I assume you want to keep sharing the weights, in this case, you should call something like this after assigning your new head: standard_gpt2.lm_head = nn.Sequential( nn.Linear(in_features = 768, out_features = 50257, bias=False) ) standard_gpt2.lm_head[0].weight = standard_gpt2.transformer.wte.weight standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size with tied weights+custom head: {standard_gpt2_model_size} parameters"") print(f""Token embedding layer address with tied weights+custom head: {standard_gpt2.transformer.wte.weight.untyped_storage().data_ptr()}"") print(f""LM_head address with tied weights+custom head: {standard_gpt2.lm_head[0].weight.untyped_storage().data_ptr()}"") Output: GPT-2 size: 124439808 parameters Token embedding layer address 134800505946176 LM_head address 134800505946176 GPT-2 size with tied weights+custom head: 124439808 parameters Token embedding layer address with tied weights+custom head: 134800505946176 LM_head address with tied weights+custom head: 134800505946176",2023-10-22 19:43:18,2023-10-22 21:28:16,753,https://stackoverflow.com/questions/77341456/why-does-my-transformer-model-have-more-parameters-than-the-huggingface-implemen,"Why does my transformer model have more parameters than the Huggingface implementation? I'm loading a GPT model from huggingface as follows: from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig config = AutoConfig.from_pretrained( ""gpt2"", vocab_size=len(tokenizer), n_ctx=context_length, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, ) standard_gpt2 = GPT2LMHeadModel(config).to(device) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters"") # >>> GPT-2 size: 124.4M parameters If I print the model architecture I get: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) Focusing on the last layer -- lm_head , it has in_features=768, out_features=50257 So why when I replace just that one layer with the exact same number of parameters I get different results? standard_gpt2.lm_head = nn.Sequential( nn.Linear(in_features = 768, out_features = 50257, bias=False) ) standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters()) print(f""GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters"") # >>> GPT-2 size: 163.0M parameters",transformer model parameter huggingface implementation   m load gpt model huggingface follow  transformer import autotokenizer  gpt2lmheadmodel  autoconfig config  autoconfigfrompretrained    gpt2   vocabsize  len  tokenizer   nctx  contextlength  bostokenid  tokenizerbostokenid  eostokenid  tokenizereostokenid   standardgpt2  gpt2lmheadmodel  config  to  device  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize1000   2  1f  parameter       gpt2 size  1244 m parameter print model architecture get  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   focus last layer  lmhead  infeatures768  outfeatures50257 replace one layer exact number parameter get different result  standardgpt2lmhead  nn  sequential  nn  linear  infeature  768  outfeature  50257  bia  false   standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize1000   2  1f  parameter       gpt2 size  1630 m parameter,linear layer lmhead not separate weight  share weight tensor token embed layer  confirm data  ptr  return address first element tensor  torch import nn transformer import autotokenizer  gpt2lmheadmodel  autoconfig modelid    gpt2  tokenizer  autotokenizerfrompretraine  modelid  standardgpt2  gpt2lmheadmodelfrompretrained  modelid  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize  parameter   print  f  token embed layer address  standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address  standardgpt2lmheadweightuntypedstorage   dataptr       replace default head standardgpt2lmhead  nn  linear  infeature  768  outfeature  50257  bia  false  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size replace lmhead   standardgpt2modelsize  parameter   print  f  token embed layer address replace lmhead  standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address replace lmhead  standardgpt2lmheadweightuntypedstorage   dataptr      output  gpt2 size  124439808 parameter token embed layer address  96251233152832 lmhead address  96251233152832 gpt2 size replace lmhead  163037184 parameter token embed layer address replace lmhead  96251233152832 lmhead address replace lmhead  134800505946176 assume want keep share weight  case  call something like assign new head  standardgpt2lmhead  nn  sequential  nn  linear  infeature  768  outfeature  50257  bia  false   standardgpt2lmhead  0  weight  standardgpt2transformerwteweight standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size tie weightscustom head   standardgpt2modelsize  parameter   print  f  token embed layer address tie weightscustom head   standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address tie weightscustom head   standardgpt2lmhead  0  weightuntypedstorage   dataptr      output  gpt2 size  124439808 parameter token embed layer address 134800505946176 lmhead address 134800505946176 gpt2 size tie weightscustom head  124439808 parameter token embed layer address tie weightscustom head  134800505946176 lmhead address tie weightscustom head  134800505946176,transformer model parameter huggingface implementation   m load gpt model huggingface follow  transformer import autotokenizer  gpt2lmheadmodel  autoconfig config  autoconfigfrompretrained    gpt2   vocabsize  len  tokenizer   nctx  contextlength  bostokenid  tokenizerbostokenid  eostokenid  tokenizereostokenid   standardgpt2  gpt2lmheadmodel  config  to  device  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize1000   2  1f  parameter       gpt2 size  1244 m parameter print model architecture get  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   focus last layer  lmhead  infeatures768  outfeatures50257 replace one layer exact number parameter get different result  standardgpt2lmhead  nn  sequential  nn  linear  infeature  768  outfeature  50257  bia  false   standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize1000   2  1f  parameter       gpt2 size  1630 m parameter linear layer lmhead not separate weight  share weight tensor token embed layer  confirm data  ptr  return address first element tensor  torch import nn transformer import autotokenizer  gpt2lmheadmodel  autoconfig modelid    gpt2  tokenizer  autotokenizerfrompretraine  modelid  standardgpt2  gpt2lmheadmodelfrompretrained  modelid  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size   standardgpt2modelsize  parameter   print  f  token embed layer address  standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address  standardgpt2lmheadweightuntypedstorage   dataptr       replace default head standardgpt2lmhead  nn  linear  infeature  768  outfeature  50257  bia  false  standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size replace lmhead   standardgpt2modelsize  parameter   print  f  token embed layer address replace lmhead  standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address replace lmhead  standardgpt2lmheadweightuntypedstorage   dataptr      output  gpt2 size  124439808 parameter token embed layer address  96251233152832 lmhead address  96251233152832 gpt2 size replace lmhead  163037184 parameter token embed layer address replace lmhead  96251233152832 lmhead address replace lmhead  134800505946176 assume want keep share weight  case  call something like assign new head  standardgpt2lmhead  nn  sequential  nn  linear  infeature  768  outfeature  50257  bia  false   standardgpt2lmhead  0  weight  standardgpt2transformerwteweight standardgpt2modelsize  sum  tnumel   standardgpt2parameter    print  f  gpt2 size tie weightscustom head   standardgpt2modelsize  parameter   print  f  token embed layer address tie weightscustom head   standardgpt2transformerwteweightuntypedstorage   dataptr      print  f  lmhead address tie weightscustom head   standardgpt2lmhead  0  weightuntypedstorage   dataptr      output  gpt2 size  124439808 parameter token embed layer address 134800505946176 lmhead address 134800505946176 gpt2 size tie weightscustom head  124439808 parameter token embed layer address tie weightscustom head  134800505946176 lmhead address tie weightscustom head  134800505946176,Basic Understanding
How to change the fully connected network in a GPT model on Huggingface?,"I'm following this tutorial on training a causal language model from scratch . In the tutorial they load the standard GPT2 as follows: from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig config = AutoConfig.from_pretrained( ""gpt2"", vocab_size=len(tokenizer), n_ctx=context_length, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, ) model = GPT2LMHeadModel(config) How can I load the same model, but use my custom fully connected network instead of the standard one? Mainly want to experiment with variations such as more/less layers, different activation functions, etc. I found the source code here , but it's very convoluted and I can't figure out how to replace the fully connected parts with a custom ones or what structure the custom one should have in the first place (e.g., input/output size). Update For example, using a FC network as such: class FC_model(nn.Module): def __init__(self): super(FC_model, self).__init__() self.fc1 = nn.Linear(768,256) self.fc2 = nn.Linear(256,256) self.fc3 = nn.Linear(256,50000) def forward(self, x): x = torch.sin(self.fc1(x)) + torch.rand(1) x = torch.sin(self.fc2(x)) x = self.fc3(x) return x","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'gpt-2']",2,"I'm assuming by the fully connected network you're referring to the Fully Connected (FC) / Linear layer. from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, GPT2Config configuration = GPT2Config() model = GPT2LMHeadModel(configuration) print(model) The above would show you the modules inside the model: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) You can now access and update the FC layer by: model.lm_head = nn.Sequential( nn.Linear(in_features = 768, out_features = 256), nn.ReLU(inplace = True), nn.Dropout1d(0.25), nn.Linear(in_features = 256, out_features = 128) ) The above is just a sample, you can experiment with different combinations.",2023-10-21 20:40:53,2023-10-22 10:06:05,364,https://stackoverflow.com/questions/77337720/how-to-change-the-fully-connected-network-in-a-gpt-model-on-huggingface,"How to change the fully connected network in a GPT model on Huggingface? I'm following this tutorial on training a causal language model from scratch . In the tutorial they load the standard GPT2 as follows: from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig config = AutoConfig.from_pretrained( ""gpt2"", vocab_size=len(tokenizer), n_ctx=context_length, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, ) model = GPT2LMHeadModel(config) How can I load the same model, but use my custom fully connected network instead of the standard one? Mainly want to experiment with variations such as more/less layers, different activation functions, etc. I found the source code here , but it's very convoluted and I can't figure out how to replace the fully connected parts with a custom ones or what structure the custom one should have in the first place (e.g., input/output size). Update For example, using a FC network as such: class FC_model(nn.Module): def __init__(self): super(FC_model, self).__init__() self.fc1 = nn.Linear(768,256) self.fc2 = nn.Linear(256,256) self.fc3 = nn.Linear(256,50000) def forward(self, x): x = torch.sin(self.fc1(x)) + torch.rand(1) x = torch.sin(self.fc2(x)) x = self.fc3(x) return x",change fully connect network gpt model huggingface   m follow tutorial training causal language model scratch  tutorial load standard gpt2 follow  transformer import autotokenizer  gpt2lmheadmodel  autoconfig config  autoconfigfrompretrained    gpt2   vocabsize  len  tokenizer   nctx  contextlength  bostokenid  tokenizerbostokenid  eostokenid  tokenizereostokenid   model  gpt2lmheadmodel  config  load model  use custom fully connect network instead standard one  mainly want experiment variation more  less layer  different activation function  etc  find source code  s convoluted can not figure replace fully connect part custom one structure custom one first place  eg  input  output size   update example  use fc network  class fcmodel  nn  module   def   init    self   super  fcmodel  self  init     selffc1  nn  linear  768256  selffc2  nn  linear  256256  selffc3  nn  linear  25650000  def forward  self  x   x  torchsin  selffc1  x    torchrand  1  x  torchsin  selffc2  x   x  selffc3  x  return x, m assume fully connect network be refer fully connected  fc   linear layer  transformer import autotokenizer  gpt2lmheadmodel  autoconfig  gpt2config configuration  gpt2config   model  gpt2lmheadmodel  configuration  print  model  would show module inside model  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   access update fc layer  modellmhead  nn  sequential  nn  linear  infeature  768  outfeature  256   nn  relu  inplace  true   nn  dropout1d  025   nn  linear  infeature  256  outfeature  128   sample  experiment different combination ,change fully connect network gpt model huggingface   m follow tutorial training causal language model scratch  tutorial load standard gpt2 follow  transformer import autotokenizer  gpt2lmheadmodel  autoconfig config  autoconfigfrompretrained    gpt2   vocabsize  len  tokenizer   nctx  contextlength  bostokenid  tokenizerbostokenid  eostokenid  tokenizereostokenid   model  gpt2lmheadmodel  config  load model  use custom fully connect network instead standard one  mainly want experiment variation more  less layer  different activation function  etc  find source code  s convoluted can not figure replace fully connect part custom one structure custom one first place  eg  input  output size   update example  use fc network  class fcmodel  nn  module   def   init    self   super  fcmodel  self  init     selffc1  nn  linear  768256  selffc2  nn  linear  256256  selffc3  nn  linear  25650000  def forward  self  x   x  torchsin  selffc1  x    torchrand  1  x  torchsin  selffc2  x   x  selffc3  x  return x  m assume fully connect network be refer fully connected  fc   linear layer  transformer import autotokenizer  gpt2lmheadmodel  autoconfig  gpt2config configuration  gpt2config   model  gpt2lmheadmodel  configuration  print  model  would show module inside model  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   access update fc layer  modellmhead  nn  sequential  nn  linear  infeature  768  outfeature  256   nn  relu  inplace  true   nn  dropout1d  025   nn  linear  infeature  256  outfeature  128   sample  experiment different combination ,Implementation Issues
Reverse Index in SQL,"In this assignment, you will create a table of documents and then produce a reverse index for those documents that identifies each document which contains a particular word using SQL. FYI: In contrast with the provided sample SQL, you will map all the words in the reverse index to lower case (i.e. Python, PYTHON, and python should all end up as ""python"" in the inverted index). CREATE TABLE docs01 (id SERIAL, doc TEXT, PRIMARY KEY(id)); CREATE TABLE invert01 ( keyword TEXT, doc_id INTEGER REFERENCES docs01(id) ON DELETE CASCADE ); Here are the one-line documents that you are to insert into docs01: INSERT INTO docs01 (doc) VALUES ('The building blocks of programs'), ('In the next few chapters we will learn more about the vocabulary'), ('sentence structure paragraph structure and story structure of Python'), ('We will learn about the powerful capabilities of Python and how to'), ('compose those capabilities together to create useful programs'), ('There are some lowlevel conceptual patterns that we use to construct'), ('programs These constructs are not just for Python programs they are'), ('part of every programming language from machine language up to the'), ('file or even some kind of sensor like a microphone or GPS In our'), ('initial programs our input will come from the user typing data on'); Here is a sample for the first few expected rows of your reverse index: SELECT keyword, doc_id FROM invert01 ORDER BY keyword, doc_id LIMIT 10; keyword doc_id a 9 about 2 about 4 and 3 and 4 are 6 are 7 blocks 1 building 1 capabilities 4 INSERT INTO invert01 (keyword, doc_id) SELECT lower(keyword) AS keyword, doc_id FROM ( SELECT id AS doc_id, unnest(string_to_array(doc, ' ')) AS keyword FROM docs01 ) AS words; I tried this but error is showing that ""Keyword 'are' should be in 2 documents and was only in 3 documents"". Please Help me to find the error.","['sql', 'json', 'database', 'postgresql', 'nlp']",1,"As already pointed out , each doc with multiple instances of a keyword, like number 7: ('programs These constructs are not just for Python programs they are '), causes your query to produce a (keyword,doc_id) pairing multiple times: keyword doc_id are 6 are 7 are 7 because a SELECT is by default a SELECT ALL that accepts duplicates. You can prevent that by switching to a SELECT DISTINCT that discards duplicates. You also don't need to use a subquery to apply lower() and you can use string_to_table() to split documents directly into rows of keywords, instead of splitting into an array first and then calling unnest() . Demo : INSERT INTO invert01 (keyword, doc_id) SELECT DISTINCT lower(string_to_table(doc, ' ')) AS keyword, id AS doc_id FROM docs01; Result SELECT keyword, doc_id FROM invert01 WHERE keyword='are' ORDER BY keyword, doc_id LIMIT 10; keyword doc_id are 6 are 7",2023-10-20 18:52:20,2023-10-22 11:45:14,773,https://stackoverflow.com/questions/77333202/reverse-index-in-sql,"Reverse Index in SQL In this assignment, you will create a table of documents and then produce a reverse index for those documents that identifies each document which contains a particular word using SQL. FYI: In contrast with the provided sample SQL, you will map all the words in the reverse index to lower case (i.e. Python, PYTHON, and python should all end up as ""python"" in the inverted index). CREATE TABLE docs01 (id SERIAL, doc TEXT, PRIMARY KEY(id)); CREATE TABLE invert01 ( keyword TEXT, doc_id INTEGER REFERENCES docs01(id) ON DELETE CASCADE ); Here are the one-line documents that you are to insert into docs01: INSERT INTO docs01 (doc) VALUES ('The building blocks of programs'), ('In the next few chapters we will learn more about the vocabulary'), ('sentence structure paragraph structure and story structure of Python'), ('We will learn about the powerful capabilities of Python and how to'), ('compose those capabilities together to create useful programs'), ('There are some lowlevel conceptual patterns that we use to construct'), ('programs These constructs are not just for Python programs they are'), ('part of every programming language from machine language up to the'), ('file or even some kind of sensor like a microphone or GPS In our'), ('initial programs our input will come from the user typing data on'); Here is a sample for the first few expected rows of your reverse index: SELECT keyword, doc_id FROM invert01 ORDER BY keyword, doc_id LIMIT 10; keyword doc_id a 9 about 2 about 4 and 3 and 4 are 6 are 7 blocks 1 building 1 capabilities 4 INSERT INTO invert01 (keyword, doc_id) SELECT lower(keyword) AS keyword, doc_id FROM ( SELECT id AS doc_id, unnest(string_to_array(doc, ' ')) AS keyword FROM docs01 ) AS words; I tried this but error is showing that ""Keyword 'are' should be in 2 documents and was only in 3 documents"". Please Help me to find the error.",reverse index sql assignment  create table document produce reverse index document identify document contain particular word use sql  fyi  contrast provide sample sql  map word reverse index low case  ie  python  python  python end   python  invert index   create table docs01  i d serial  doc text  primary key  i d    create table invert01  keyword text  docid integer references docs01  i d  delete cascade   one  line document insert docs01  insert docs01  doc  value   the building block program      in next chapter learn vocabulary      sentence structure paragraph structure story structure python      we learn powerful capability python      compose capability together create useful program      there lowlevel conceptual pattern use construct      program construct python program      part every programming language machine language      file even kind sensor like microphone gps      initial programs input come user type datum    sample first expect row reverse index  select keyword  docid invert01 order keyword  docid limit 10  keyword docid 9 2 4 3 4 6 7 block 1 build 1 capability 4 insert invert01  keyword  docid  select low  keyword  keyword  docid  select i d docid  unnest  stringtoarray  doc      keyword docs01  word  try error show   keyword  be  2 document 3 document   please help find error ,already point  doc multiple instance keyword  like number 7    program construct python program    cause query produce  keyword  docid  pair multiple time  keyword docid 6 7 7 select default select accept duplicate  prevent switch select distinct discards duplicate  also not need use subquery apply lower   use stringtotable   split document directly row keyword  instead split array first call unnest    demo  insert invert01  keyword  docid  select distinct lower  stringtotable  doc      keyword  i d docid docs01  result select keyword  docid invert01 keywordare  order keyword  docid limit 10  keyword docid 6 7,reverse index sql assignment  create table document produce reverse index document identify document contain particular word use sql  fyi  contrast provide sample sql  map word reverse index low case  ie  python  python  python end   python  invert index   create table docs01  i d serial  doc text  primary key  i d    create table invert01  keyword text  docid integer references docs01  i d  delete cascade   one  line document insert docs01  insert docs01  doc  value   the building block program      in next chapter learn vocabulary      sentence structure paragraph structure story structure python      we learn powerful capability python      compose capability together create useful program      there lowlevel conceptual pattern use construct      program construct python program      part every programming language machine language      file even kind sensor like microphone gps      initial programs input come user type datum    sample first expect row reverse index  select keyword  docid invert01 order keyword  docid limit 10  keyword docid 9 2 4 3 4 6 7 block 1 build 1 capability 4 insert invert01  keyword  docid  select low  keyword  keyword  docid  select i d docid  unnest  stringtoarray  doc      keyword docs01  word  try error show   keyword  be  2 document 3 document   please help find error  already point  doc multiple instance keyword  like number 7    program construct python program    cause query produce  keyword  docid  pair multiple time  keyword docid 6 7 7 select default select accept duplicate  prevent switch select distinct discards duplicate  also not need use subquery apply lower   use stringtotable   split document directly row keyword  instead split array first call unnest    demo  insert invert01  keyword  docid  select distinct lower  stringtotable  doc      keyword  i d docid docs01  result select keyword  docid invert01 keywordare  order keyword  docid limit 10  keyword docid 6 7,Library/Tool-Based Queries
How to get most similar words to a tagged document in gensim doc2vec,"I have trained a doc2vec model. doc2vec = Doc2Vec(vector_size= 300, window=10, min_count=100, dm=1, epochs=40) doc2vec.build_vocab(corpus_file=train_data, progress_per=1000) doc2vec.train(....) The documents are tagged with incremental integer 0, 1, ...1000. To get the top-n similar words to a document with tag=0, I used: doc_vector = doc2vec.dv[tag] sims = doc2vec.wv.similar_by_vector(doc_vector, top_n=20) The similarity makes sense, however, the similarity score are really looked ""weird"", all of them are almost 1.0 . I checked top_n=3000 and it is still around 1.0. Does it make sense to get all words with high similarity score.","['nlp', 'gensim', 'cosine-similarity', 'doc2vec']",1,"In traditional uses of this algorithm with varies natural-language texts, no, it's not typical to have the most-similar 'nearest neighbors' of texts all have near- 1.0 similarities. That suggests something may amiss with your setup – unless of course your data does include lots of 'texts' that are nearly-identical. Are you perhaps using some atypical corpus, maybe not natural language, where so many super-close similarity scores are still accurate & useful? That's the ultimate test. That is: if for a bunch of doc probes, the ""similar words"" vary, and are individually sensible/useful with regard to the origin documents, I'd not worry too much about the absolute magnitudes of the similarity scores. Such scores have more meaning in comparison to each other than on any absolute scale. A similarity of 0.9 is not meaningfully interpretable as ""X% similar"" or even ""among top X% most-similar candidates. It only means, ""more similar than items with 0.8 similarity, and less similar than items with 0.95 similarity"". Some things to look at, if wanting to get better sense if things may be going wrong: Are there at least some words that have far-lower similarity-scores, and do those make sense? Do doc-to-doc comparisons seem roughly sensible? What is is the rough character & size of your data, and how many docs & unique-words are in your corpus? How many surviving-words (after applying the min_count=100 cutoff) remain in the trained model? If you run with logging enabled, do the various step & progress reports suggest that a model of the expected size, & training effort, is being created?",2023-10-19 21:08:16,2023-10-19 21:37:32,53,https://stackoverflow.com/questions/77327117/how-to-get-most-similar-words-to-a-tagged-document-in-gensim-doc2vec,"How to get most similar words to a tagged document in gensim doc2vec I have trained a doc2vec model. doc2vec = Doc2Vec(vector_size= 300, window=10, min_count=100, dm=1, epochs=40) doc2vec.build_vocab(corpus_file=train_data, progress_per=1000) doc2vec.train(....) The documents are tagged with incremental integer 0, 1, ...1000. To get the top-n similar words to a document with tag=0, I used: doc_vector = doc2vec.dv[tag] sims = doc2vec.wv.similar_by_vector(doc_vector, top_n=20) The similarity makes sense, however, the similarity score are really looked ""weird"", all of them are almost 1.0 . I checked top_n=3000 and it is still around 1.0. Does it make sense to get all words with high similarity score.",get similar word tag document gensim doc2vec train doc2vec model  doc2vec  doc2vec  vectorsize 300  window10  mincount100  dm1  epochs40  doc2vecbuildvocab  corpusfile  traindata  progressper1000  doc2vectrain    document tag incremental integer 0  1   1000  get top  n similar word document tag0  use  docvector  doc2vecdv  tag  sim  doc2vecwvsimilarbyvector  docvector  topn20  similarity make sense  however  similarity score really look   weird   almost 10  check topn3000 still around 10  make sense get word high similarity score ,traditional use algorithm vary natural  language text   be typical most  similar  near neighbor  text near 10 similarity  suggest something may amiss setup  unless course datum include lot  text  nearly  identical  perhaps use atypical corpus  maybe natural language  many super  close similarity score still accurate  useful  s ultimate test   bunch doc probe    similar word  vary  individually sensible  useful regard origin document  would worry much absolute magnitude similarity score  score mean comparison absolute scale  similarity 09 meaningfully interpretable   x  similar  even   among top x  most  similar candidate  mean    similar item 08 similarity  less similar item 095 similarity   thing look  want get well sense thing may go wrong  least word far  low similarity  score  make sense  doc  to  doc comparison seem roughly sensible  rough character  size datum  many doc  unique  word corpus  many surviving  word  apply mincount100 cutoff  remain train model  run logging enable  various step  progress report suggest model expect size   training effort  create ,get similar word tag document gensim doc2vec train doc2vec model  doc2vec  doc2vec  vectorsize 300  window10  mincount100  dm1  epochs40  doc2vecbuildvocab  corpusfile  traindata  progressper1000  doc2vectrain    document tag incremental integer 0  1   1000  get top  n similar word document tag0  use  docvector  doc2vecdv  tag  sim  doc2vecwvsimilarbyvector  docvector  topn20  similarity make sense  however  similarity score really look   weird   almost 10  check topn3000 still around 10  make sense get word high similarity score  traditional use algorithm vary natural  language text   be typical most  similar  near neighbor  text near 10 similarity  suggest something may amiss setup  unless course datum include lot  text  nearly  identical  perhaps use atypical corpus  maybe natural language  many super  close similarity score still accurate  useful  s ultimate test   bunch doc probe    similar word  vary  individually sensible  useful regard origin document  would worry much absolute magnitude similarity score  score mean comparison absolute scale  similarity 09 meaningfully interpretable   x  similar  even   among top x  most  similar candidate  mean    similar item 08 similarity  less similar item 095 similarity   thing look  want get well sense thing may go wrong  least word far  low similarity  score  make sense  doc  to  doc comparison seem roughly sensible  rough character  size datum  many doc  unique  word corpus  many surviving  word  apply mincount100 cutoff  remain train model  run logging enable  various step  progress report suggest model expect size   training effort  create ,Implementation Issues
AttributeError when Initializing a Custom Decoder Class in TensorFlow with Non-Default Tokenizer,"I'm encountering an AttributeError while trying to create an instance of a Decoder class in Python with TensorFlow. The error message I'm getting is as follows: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-76-debfaa5162f4> in <cell line: 2>() 1 # That will be sufficient for training. Create an instance of the decoder to test out: ----> 2 decoder = Decoder(tokenizer, UNITS) 3 logits = decoder(ex_context, tf.expand_dims(ex_tar_in, axis=0)) 4 print(f'encoder output shape: (batch, s, units) {ex_context.shape}') 5 print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}') 3 frames /usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocabulary, idf_weights) 517 idf_weights = np.array(idf_weights) 518 --> 519 if vocabulary.size == 0: 520 raise ValueError( 521 f""Cannot set an empty vocabulary, you passed {vocabulary}."" AttributeError: 'dict' object has no attribute 'size' I suspect that the error is related to the usage of the tf.keras.layers.StringLookup() layer, which expects a 'size' attribute in the vocabulary. However, I'm not using the default TensorFlow tokenizer; I'm using the RobertaTokenizer from the Transformers library. I tried manually adding variable to tokenizer but it doesnot work. This code is adapted from the NNCourse , and I've made some modifications to it. Here is my decoder code: # The decoder """""" The decoder's job is to generate predictions for the next token at each location in the target sequence. The Decoder Structure: 1. It looks up embeddings for each token in the target sequence. 2. It uses an RNN to process the target sequence, and keep track of what it has generated so far. 3. It uses RNN output as the ""query"" to the attention layer, when attending to the encoder's output. 4. At each location in the output it predicts the next token. Note: When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence. > When running inference with this model it produces one word at a time, and those are fed back into the model. """""" class Decoder(tf.keras.layers.Layer): @classmethod def add_method(cls, fun): setattr(cls, fun.__name__, fun) return fun def __init__(self, text_processor, units): super(Decoder, self).__init__() self.text_processor = text_processor # Tokenizer self.vocab_size = text_processor.size # Vocab size # https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup self.word_to_id = tf.keras.layers.StringLookup( vocabulary=text_processor.vocab, mask_token=text_processor.mask_token, oov_token=text_processor.unk_token) # https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup self.id_to_word = tf.keras.layers.StringLookup( vocabulary=text_processor.vocab, mask_token=text_processor.mask_token, oov_token=text_processor.unk_token, invert=True) self.start_token = self.word_to_id(START_TOKEN) self.end_token = self.word_to_id(END_TOKEN) self.units = units # 1. The embedding layer converts token IDs to vectors self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True) # 2. The RNN keeps track of what's been generated so far. self.rnn = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform') # 3. The RNN output will be the query for the attention layer. self.attention = CrossAttention(units) # 4. This fully connected layer produces the logits for each # output token. self.output_layer = tf.keras.layers.Dense(self.vocab_size) Specs: Python = 3.8 tensorflow = 2.14.0 transformers = 4.34.0","['python', 'python-3.x', 'tensorflow', 'nlp', 'huggingface-transformers']",1,Solution: It was solved by separating the tokenizer from model and removing the tf.keras.layers.StringLookup layer as it only works on tensorflow vectorizer/tokenizer.,2023-10-17 20:13:08,2023-12-03 10:48:14,171,https://stackoverflow.com/questions/77311999/attributeerror-when-initializing-a-custom-decoder-class-in-tensorflow-with-non-d,"AttributeError when Initializing a Custom Decoder Class in TensorFlow with Non-Default Tokenizer I'm encountering an AttributeError while trying to create an instance of a Decoder class in Python with TensorFlow. The error message I'm getting is as follows: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-76-debfaa5162f4> in <cell line: 2>() 1 # That will be sufficient for training. Create an instance of the decoder to test out: ----> 2 decoder = Decoder(tokenizer, UNITS) 3 logits = decoder(ex_context, tf.expand_dims(ex_tar_in, axis=0)) 4 print(f'encoder output shape: (batch, s, units) {ex_context.shape}') 5 print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}') 3 frames /usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocabulary, idf_weights) 517 idf_weights = np.array(idf_weights) 518 --> 519 if vocabulary.size == 0: 520 raise ValueError( 521 f""Cannot set an empty vocabulary, you passed {vocabulary}."" AttributeError: 'dict' object has no attribute 'size' I suspect that the error is related to the usage of the tf.keras.layers.StringLookup() layer, which expects a 'size' attribute in the vocabulary. However, I'm not using the default TensorFlow tokenizer; I'm using the RobertaTokenizer from the Transformers library. I tried manually adding variable to tokenizer but it doesnot work. This code is adapted from the NNCourse , and I've made some modifications to it. Here is my decoder code: # The decoder """""" The decoder's job is to generate predictions for the next token at each location in the target sequence. The Decoder Structure: 1. It looks up embeddings for each token in the target sequence. 2. It uses an RNN to process the target sequence, and keep track of what it has generated so far. 3. It uses RNN output as the ""query"" to the attention layer, when attending to the encoder's output. 4. At each location in the output it predicts the next token. Note: When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence. > When running inference with this model it produces one word at a time, and those are fed back into the model. """""" class Decoder(tf.keras.layers.Layer): @classmethod def add_method(cls, fun): setattr(cls, fun.__name__, fun) return fun def __init__(self, text_processor, units): super(Decoder, self).__init__() self.text_processor = text_processor # Tokenizer self.vocab_size = text_processor.size # Vocab size # https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup self.word_to_id = tf.keras.layers.StringLookup( vocabulary=text_processor.vocab, mask_token=text_processor.mask_token, oov_token=text_processor.unk_token) # https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup self.id_to_word = tf.keras.layers.StringLookup( vocabulary=text_processor.vocab, mask_token=text_processor.mask_token, oov_token=text_processor.unk_token, invert=True) self.start_token = self.word_to_id(START_TOKEN) self.end_token = self.word_to_id(END_TOKEN) self.units = units # 1. The embedding layer converts token IDs to vectors self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True) # 2. The RNN keeps track of what's been generated so far. self.rnn = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform') # 3. The RNN output will be the query for the attention layer. self.attention = CrossAttention(units) # 4. This fully connected layer produces the logits for each # output token. self.output_layer = tf.keras.layers.Dense(self.vocab_size) Specs: Python = 3.8 tensorflow = 2.14.0 transformers = 4.34.0",attributeerror initializing custom decoder class tensorflow non  default tokenizer  m encounter attributeerror try create instance decoder class python tensorflow  error message  m getting follow                                        attributeerror traceback  recent call last   ipython  input76  debfaa5162f4   cell line  2    1  sufficient training  create instance decoder test     2 decoder  decoder  tokenizer  units  3 logit  decoder  excontext  tfexpanddim  extarin  axis0   4 print  fencoder output shape   batch   unit   excontextshape    5 print  finput target token shape   batch    extarinshape    3 frame usr  local  lib  python310  dist  package  keras  src  layer  preprocessing  indexlookuppy setvocabulary  self  vocabulary  idfweight  517 idfweight  nparray  idfweight  518   519 vocabularysize   0  520 raise valueerror  521 f  set empty vocabulary  pass  vocabulary    attributeerror   dict  object attribute  size  suspect error relate usage tfkeraslayer  stringlookup   layer  expect  size  attribute vocabulary  however   m use default tensorflow tokenizer   m use robertatokenizer transformers library  try manually add variable tokenizer doesnot work  code adapt nncourse   ve make modification  decoder code   decoder     decoder s job generate prediction next token location target sequence  decoder structure  1  look embedding token target sequence  2  use rnn process target sequence  keep track generate far  3  use rnn output   query  attention layer  attend encoder s output  4  location output predict next token  note  training  model predict next word location  s important information flow one direction model  decoder use unidirectional  bidirectional  rnn process target sequence   run inference model produce one word time  feed back model      class decoder  tfkeraslayers  layer    classmethod def addmethod  cls  fun   setattr  cls  funname    fun  return fun def   init    self  textprocessor  unit   super  decoder  self  init     selftextprocessor  textprocessor  tokenizer selfvocabsize  textprocessorsize  vocab size    wwwtensorfloworg  apidocs  python  tf  keras  layer  stringlookup selfwordtoid  tfkeraslayer  stringlookup  vocabulary  textprocessorvocab  masktoken  textprocessormasktoken  oovtoken  textprocessorunktoken     wwwtensorfloworg  apidocs  python  tf  keras  layer  stringlookup selfidtoword  tfkeraslayer  stringlookup  vocabulary  textprocessorvocab  masktoken  textprocessormasktoken  oovtoken  textprocessorunktoken  invert  true  selfstarttoken  selfwordtoid  starttoken  selfendtoken  selfwordtoid  endtoken  selfunit  unit  1  embed layer convert token id vector selfembedde  tfkeraslayer  embed  selfvocabsize  unit  maskzero  true   2  rnn keep track s generate far  selfrnn  tfkeraslayer  gru  unit  returnsequence  true  returnstate  true  recurrentinitializerglorotuniform    3  rnn output query attention layer  selfattention  crossattention  unit   4  fully connect layer produce logit  output token  selfoutputlayer  tfkeraslayer  dense  selfvocabsize  spec  python  38 tensorflow  2140 transformer  4340,solution  solve separate tokenizer model remove tfkeraslayer  stringlookup layer work tensorflow vectorizer  tokenizer ,attributeerror initializing custom decoder class tensorflow non  default tokenizer  m encounter attributeerror try create instance decoder class python tensorflow  error message  m getting follow                                        attributeerror traceback  recent call last   ipython  input76  debfaa5162f4   cell line  2    1  sufficient training  create instance decoder test     2 decoder  decoder  tokenizer  units  3 logit  decoder  excontext  tfexpanddim  extarin  axis0   4 print  fencoder output shape   batch   unit   excontextshape    5 print  finput target token shape   batch    extarinshape    3 frame usr  local  lib  python310  dist  package  keras  src  layer  preprocessing  indexlookuppy setvocabulary  self  vocabulary  idfweight  517 idfweight  nparray  idfweight  518   519 vocabularysize   0  520 raise valueerror  521 f  set empty vocabulary  pass  vocabulary    attributeerror   dict  object attribute  size  suspect error relate usage tfkeraslayer  stringlookup   layer  expect  size  attribute vocabulary  however   m use default tensorflow tokenizer   m use robertatokenizer transformers library  try manually add variable tokenizer doesnot work  code adapt nncourse   ve make modification  decoder code   decoder     decoder s job generate prediction next token location target sequence  decoder structure  1  look embedding token target sequence  2  use rnn process target sequence  keep track generate far  3  use rnn output   query  attention layer  attend encoder s output  4  location output predict next token  note  training  model predict next word location  s important information flow one direction model  decoder use unidirectional  bidirectional  rnn process target sequence   run inference model produce one word time  feed back model      class decoder  tfkeraslayers  layer    classmethod def addmethod  cls  fun   setattr  cls  funname    fun  return fun def   init    self  textprocessor  unit   super  decoder  self  init     selftextprocessor  textprocessor  tokenizer selfvocabsize  textprocessorsize  vocab size    wwwtensorfloworg  apidocs  python  tf  keras  layer  stringlookup selfwordtoid  tfkeraslayer  stringlookup  vocabulary  textprocessorvocab  masktoken  textprocessormasktoken  oovtoken  textprocessorunktoken     wwwtensorfloworg  apidocs  python  tf  keras  layer  stringlookup selfidtoword  tfkeraslayer  stringlookup  vocabulary  textprocessorvocab  masktoken  textprocessormasktoken  oovtoken  textprocessorunktoken  invert  true  selfstarttoken  selfwordtoid  starttoken  selfendtoken  selfwordtoid  endtoken  selfunit  unit  1  embed layer convert token id vector selfembedde  tfkeraslayer  embed  selfvocabsize  unit  maskzero  true   2  rnn keep track s generate far  selfrnn  tfkeraslayer  gru  unit  returnsequence  true  returnstate  true  recurrentinitializerglorotuniform    3  rnn output query attention layer  selfattention  crossattention  unit   4  fully connect layer produce logit  output token  selfoutputlayer  tfkeraslayer  dense  selfvocabsize  spec  python  38 tensorflow  2140 transformer  4340 solution  solve separate tokenizer model remove tfkeraslayer  stringlookup layer work tensorflow vectorizer  tokenizer ,Task-Specific Queries
Feeding my classifier one document at a time,"I want my ModelBuilder class to feed a self._classifier = MultinomialNB() with the content of some webpages I scraped. The documents are many and pretty big, so I can't load the whole set to memory. I'm reading them file by file. Here's the relevant portion of code: X = [] y = [] # loop over all files in my docs folder and for each file: X.append(self._vectorize_text(file.read())) y.append(category['label']) # end of the loop X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) self._classifier.fit(X_train, y_train) # ... Text pre-processing and vectorization functions: def _vectorize_text(self, text): preprocessed_content = preprocess_text(text) tfidf_vector = self._vectorizer.fit_transform([preprocessed_content]) return tfidf_vector.toarray() def preprocess_text(text): words = word_tokenize(text) words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')] cleaned_text = ' '.join(words) return cleaned_text I get an error at classifier.fit when I start training my model: ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (121, 1) + inhomogeneous part. How can I resolve this?","['python', 'machine-learning', 'scikit-learn', 'nlp', 'tfidfvectorizer']",1,"You're going to have two problems doing this: Vocabulary . Suppose that in document #2, you have the word ""eggs."" In document #1, the word ""eggs"" does not appear. In order for the vector for #1 and the vector for #2 to have the same meaning, the vectorizer for step #1 needs to know to leave an empty column for the word ""eggs."" The immediate symptom you see is that the vectors end up having different lengths, and NumPy cannot represent jagged arrays . If you tried to solve this by padding all arrays to the same length, you would have the problem that counts representing the same word are assigned to different columns. One approach to solving this is to run a CountVectorizer over your dataset, keeping the vocabulary from each document, but throwing away the vector. Then, you use TF-IDF with a fixed vocabulary representing all words that appear in your dataset. This answer describes how to do this. Inverse Document Frequency . TF-IDF is term frequency, within one document, multiplied by inverse document frequency for that term, within all documents. If you fit this one document at a time, you're essentially setting the IDF term to 1. In order to compute IDF, it must have either all documents, or at least a count of how many documents have each term in the vocabulary. This answer has a library which can deal with this problem; I haven't personally tried it.",2023-10-14 11:59:02,2023-10-14 18:53:40,59,https://stackoverflow.com/questions/77292726/feeding-my-classifier-one-document-at-a-time,"Feeding my classifier one document at a time I want my ModelBuilder class to feed a self._classifier = MultinomialNB() with the content of some webpages I scraped. The documents are many and pretty big, so I can't load the whole set to memory. I'm reading them file by file. Here's the relevant portion of code: X = [] y = [] # loop over all files in my docs folder and for each file: X.append(self._vectorize_text(file.read())) y.append(category['label']) # end of the loop X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) self._classifier.fit(X_train, y_train) # ... Text pre-processing and vectorization functions: def _vectorize_text(self, text): preprocessed_content = preprocess_text(text) tfidf_vector = self._vectorizer.fit_transform([preprocessed_content]) return tfidf_vector.toarray() def preprocess_text(text): words = word_tokenize(text) words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')] cleaned_text = ' '.join(words) return cleaned_text I get an error at classifier.fit when I start training my model: ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (121, 1) + inhomogeneous part. How can I resolve this?",feed classifier one document time want modelbuilder class feed selfclassifi  multinomialnb   content webpage scrape  document many pretty big  can not load whole set memory   m read file file  s relevant portion code  x        loop file docs folder file  xappend  selfvectorizetext  fileread     yappend  category   label     end loop xtrain  xt  ytrain  yt  traintestsplit  x   testsize03  randomstate42  selfclassifierfit  xtrain  ytrain    text pre  processing vectorization function  def  vectorizetext  self  text   preprocessedcontent  preprocesstext  text  tfidfvector  selfvectorizerfittransform   preprocessedcontent   return tfidfvectortoarray   def preprocesstext  text   word  wordtokenize  text  word   wordlower   word word wordisalnum   wordlower   stopwordsword   english    cleanedtext    join  word  return cleanedtext get error classifierfit start training model  valueerror  set array element sequence  request array inhomogeneous shape 2 dimension  detect shape  121  1   inhomogeneous part  resolve ,be go two problem  vocabulary  suppose document  2  word   egg   document  1  word   egg  appear  order vector  1 vector  2 meaning  vectorizer step  1 need know leave empty column word   egg   immediate symptom see vector end different length  numpy represent jagged array  try solve padding array length  would problem count represent word assign different column  one approach solve run countvectorizer dataset  keep vocabulary document  throw away vector   use tf  idf fix vocabulary represent word appear dataset  answer describe  inverse document frequency  tf  idf term frequency  within one document  multiply inverse document frequency term  within document  fit one document time  be essentially set idf term 1  order compute idf  must either document  least count many document term vocabulary  answer library deal problem  not personally try ,feed classifier one document time want modelbuilder class feed selfclassifi  multinomialnb   content webpage scrape  document many pretty big  can not load whole set memory   m read file file  s relevant portion code  x        loop file docs folder file  xappend  selfvectorizetext  fileread     yappend  category   label     end loop xtrain  xt  ytrain  yt  traintestsplit  x   testsize03  randomstate42  selfclassifierfit  xtrain  ytrain    text pre  processing vectorization function  def  vectorizetext  self  text   preprocessedcontent  preprocesstext  text  tfidfvector  selfvectorizerfittransform   preprocessedcontent   return tfidfvectortoarray   def preprocesstext  text   word  wordtokenize  text  word   wordlower   word word wordisalnum   wordlower   stopwordsword   english    cleanedtext    join  word  return cleanedtext get error classifierfit start training model  valueerror  set array element sequence  request array inhomogeneous shape 2 dimension  detect shape  121  1   inhomogeneous part  resolve  be go two problem  vocabulary  suppose document  2  word   egg   document  1  word   egg  appear  order vector  1 vector  2 meaning  vectorizer step  1 need know leave empty column word   egg   immediate symptom see vector end different length  numpy represent jagged array  try solve padding array length  would problem count represent word assign different column  one approach solve run countvectorizer dataset  keep vocabulary document  throw away vector   use tf  idf fix vocabulary represent word appear dataset  answer describe  inverse document frequency  tf  idf term frequency  within one document  multiply inverse document frequency term  within document  fit one document time  be essentially set idf term 1  order compute idf  must either document  least count many document term vocabulary  answer library deal problem  not personally try ,Library/Tool-Based Queries
Get page number of certain string using pdfminer,"I would like to find the page number of certain string in a pdf document using pdfminer.six . Here you can find some reproducible pdf document. We can use the extract_pages function to find the number of pages and extract_text to extract the text. But I'm not sure how to find the page of certain string. Imagine we want to find the page number of string ""File 2"", which is on page 2. According to his answer , we could use the page_numbers argument from extract_pages . Here is some code I tried: from pdfminer.high_level import extract_pages, extract_text file = 'sample.pdf' for i in range(len(list(extract_pages(file)))): extract_pages(file, page_numbers=i, maxpages=len(list(extract_pages(file)))) But now I don't understand how to get the page number of certain string, so I was wondering if anyone could explain how to get the page number of certain string in a pdf document?","['python', 'pdf', 'nlp', 'pdfminer']",1,"To find the page number of a certain string, you can search for the desired string in the extracted text using ""extract_pages"" function. When the string is found, you can record the page number. Here's an example: from pdfminer.high_level import extract_pages, extract_text file = 'sample.pdf' search_string = ""abc"" for page_number, page in enumerate(extract_pages(file)): for element in page: page_text = element.get_text() if search_string in page_text: print(f""Found '{search_string}' on page {page_number + 1}"") This code will iterate through each page, extract the text, and search for the ""abc"" string. When the string is found, it will print the page number where the string is located.",2023-10-12 08:17:45,2023-10-12 08:25:21,531,https://stackoverflow.com/questions/77278868/get-page-number-of-certain-string-using-pdfminer,"Get page number of certain string using pdfminer I would like to find the page number of certain string in a pdf document using pdfminer.six . Here you can find some reproducible pdf document. We can use the extract_pages function to find the number of pages and extract_text to extract the text. But I'm not sure how to find the page of certain string. Imagine we want to find the page number of string ""File 2"", which is on page 2. According to his answer , we could use the page_numbers argument from extract_pages . Here is some code I tried: from pdfminer.high_level import extract_pages, extract_text file = 'sample.pdf' for i in range(len(list(extract_pages(file)))): extract_pages(file, page_numbers=i, maxpages=len(list(extract_pages(file)))) But now I don't understand how to get the page number of certain string, so I was wondering if anyone could explain how to get the page number of certain string in a pdf document?",get page number certain string use pdfminer would like find page number certain string pdf document use pdfminersix  find reproducible pdf document  use extractpages function find number page extracttext extract text   m sure find page certain string  imagine want find page number string   file 2   page 2  accord answer  could use pagenumbers argument extractpages  code try  pdfminerhighlevel import extractpage  extracttext file   samplepdf  range  len  list  extractpage  file      extractpage  file  pagenumber  i  maxpage  len  list  extractpage  file     not understand get page number certain string  wonder anyone could explain get page number certain string pdf document ,find page number certain string  search desire string extract text use   extractpages  function  string find  record page number  s example  pdfminerhighlevel import extractpage  extracttext file   samplepdf  searchstre    abc  pagenumber  page enumerate  extractpage  file    element page  pagetext  elementgettext   searchstre pagetext  print  f  find   searchstre   page  pagenumber  1    code iterate page  extract text  search   abc  string  string find  print page number string locate ,get page number certain string use pdfminer would like find page number certain string pdf document use pdfminersix  find reproducible pdf document  use extractpages function find number page extracttext extract text   m sure find page certain string  imagine want find page number string   file 2   page 2  accord answer  could use pagenumbers argument extractpages  code try  pdfminerhighlevel import extractpage  extracttext file   samplepdf  range  len  list  extractpage  file      extractpage  file  pagenumber  i  maxpage  len  list  extractpage  file     not understand get page number certain string  wonder anyone could explain get page number certain string pdf document  find page number certain string  search desire string extract text use   extractpages  function  string find  record page number  s example  pdfminerhighlevel import extractpage  extracttext file   samplepdf  searchstre    abc  pagenumber  page enumerate  extractpage  file    element page  pagetext  elementgettext   searchstre pagetext  print  f  find   searchstre   page  pagenumber  1    code iterate page  extract text  search   abc  string  string find  print page number string locate ,Library/Tool-Based Queries
How does SQLDatabase Chain work internally? (Langchain),"Langchain Doc I want to understand underlying implementation. I know it uses NLP. But how it is determining whether requested thing is table or column. Maybe they are using spacy but customised a bit to understand database terms. What does it store in memory? Obviously they are not storing whole database. From this answer,i got to know they are storing DDL of Database. But huge database will probably have large ddl. Won't that create issue?","['nlp', 'langchain']",2,"This is the implementation for SQLDatabaseChain https://github.com/langchain-ai/langchain/blob/master/libs/experimental/langchain_experimental/sql/base.py Regarding your queries What does it store in memory? Obviously they are not storing whole database. Answer : Yes SQLDatabaseChain does not store entire database, it works based on metadata From this answer,i got to know they are storing DDL of Database. But huge database will mostly have large ddl. Won't that create issue? Answer : Metadata mostly includes table names, column names, primary and foreign keys, all these information together sums up to very small compared to DDL.",2023-10-10 11:31:31,2023-10-10 13:47:40,1122,https://stackoverflow.com/questions/77265396/how-does-sqldatabase-chain-work-internally-langchain,"How does SQLDatabase Chain work internally? (Langchain) Langchain Doc I want to understand underlying implementation. I know it uses NLP. But how it is determining whether requested thing is table or column. Maybe they are using spacy but customised a bit to understand database terms. What does it store in memory? Obviously they are not storing whole database. From this answer,i got to know they are storing DDL of Database. But huge database will probably have large ddl. Won't that create issue?",sqldatabase chain work internally   langchain  langchain doc want understand underlying implementation  know use nlp  determine whether request thing table column  maybe use spacy customise bit understand database term  store memory  obviously store whole database  answer  get know store ddl database  huge database probably large ddl  will not create issue ,implementation sqldatabasechain   githubcom  langchain  ai  langchain  blob  master  libs  experimental  langchainexperimental  sql  basepy regard query store memory  obviously store whole database  answer  yes sqldatabasechain store entire database  work base metadata answer  get know store ddl database  huge database mostly large ddl  will not create issue  answer  metadata mostly include table name  column name  primary foreign key  information together sum small compare ddl ,sqldatabase chain work internally   langchain  langchain doc want understand underlying implementation  know use nlp  determine whether request thing table column  maybe use spacy customise bit understand database term  store memory  obviously store whole database  answer  get know store ddl database  huge database probably large ddl  will not create issue  implementation sqldatabasechain   githubcom  langchain  ai  langchain  blob  master  libs  experimental  langchainexperimental  sql  basepy regard query store memory  obviously store whole database  answer  yes sqldatabasechain store entire database  work base metadata answer  get know store ddl database  huge database mostly large ddl  will not create issue  answer  metadata mostly include table name  column name  primary foreign key  information together sum small compare ddl ,Library/Tool-Based Queries
CompressFastText pqkmeans does not install,"I would like to use compressfasttext library: https://github.com/avidale/compress-fasttext I use this command to install: pip install compress-fasttext[full] I get this pqkmeans error: running install_egg_info Copying lshash3.egg-info to build/bdist.linux-x86_64/wheel/lshash3-0.0.8-py3.8.egg-info running install_scripts error: invalid command 'bdist_wininst' [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for lshash3 I tried python2.7, python3.7, python3.8 with Ubunto 18.0. For python 2.7, I can pip install the pqkmeans without this error. But compressFast Text library does not work. For python3.7, 3.8, I get the same error message. Any clue about this please? UPDATE: Just want to add that I made a related issue here in case anybody wants to check: https://github.com/avidale/compress-fasttext/issues/19","['python', 'machine-learning', 'pip', 'nlp', 'fasttext']",1,"As per my comment, from the error message, it looks like the real problem is in the install of lshash3 (not updated since 2017), which is only brought in via what appears to be an undocumented dataset package texmex-python used only for optional eval/demo purposes in pqkmeans . So, if you can make pip simply skip that dependency, you may get over this error with no other ill effects. I haven't tested this, but you may want to try, in a fresh Python 3.X virtual environment: manually installing all the requirements of pqkmeans except texmex-python - see that project's requirements.txt for the list then, install pqkmeans with the pip -no-deps option: pip install --no-deps pqkmeans if that succeeds, perhaps pip install compress-fasttext[full] will then consider pqkmeans already present, and not try the problem texmex-python & lshash3 installs, and you're in business but if it does still try, you could try manually installing compress-fastttext[full] 's unique non- pqkmeans requirements – really just gensim – then pip install --no-deps compress-fasttext[full] The potential end-result would be you'd have everything but the problem packages texmex-python and lshash3 , which you probably don't really need for any of the code you'll be calling. Good luck, & let me know if any form of this works!",2023-10-09 11:06:05,2023-10-11 03:08:07,85,https://stackoverflow.com/questions/77258294/compressfasttext-pqkmeans-does-not-install,"CompressFastText pqkmeans does not install I would like to use compressfasttext library: https://github.com/avidale/compress-fasttext I use this command to install: pip install compress-fasttext[full] I get this pqkmeans error: running install_egg_info Copying lshash3.egg-info to build/bdist.linux-x86_64/wheel/lshash3-0.0.8-py3.8.egg-info running install_scripts error: invalid command 'bdist_wininst' [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for lshash3 I tried python2.7, python3.7, python3.8 with Ubunto 18.0. For python 2.7, I can pip install the pqkmeans without this error. But compressFast Text library does not work. For python3.7, 3.8, I get the same error message. Any clue about this please? UPDATE: Just want to add that I made a related issue here in case anybody wants to check: https://github.com/avidale/compress-fasttext/issues/19",compressfasttext pqkmean install would like use compressfasttext library    githubcom  avidale  compress  fasttext use command install  pip install compress  fasttext  full  get pqkmean error  run installegginfo copy lshash3egg  info build  bdistlinux  x8664  wheel  lshash3  008  py38egg  info run installscript error  invalid command  bdistwininst   end output  note  error originate subprocess  likely problem pip  error  fail building wheel lshash3 try python27  python37  python38 ubunto 180  python 27  pip install pqkmean without error  compressfast text library work  python37  38  get error message  clue please  update  want add make related issue case anybody want check    githubcom  avidale  compress  fasttext  issues19,per comment  error message  look like real problem install lshash3  update since 2017   bring via appear undocumented dataset package texmex  python use optional eval  demo purpose pqkmean   make pip simply skip dependency  may get error ill effect  not test  may want try  fresh python 3x virtual environment  manually instal requirement pqkmean except texmex  python  see project s requirementstxt list  install pqkmean pip no  deps option  pip install  no  dep pqkmean succeed  perhaps pip install compress  fasttext  full  consider pqkmean already present  try problem texmex  python  lshash3 install  be business still try  could try manually instal compress  fastttext  full  s unique non pqkmean requirement  really gensim  pip install  no  deps compress  fasttext  full  potential end  result would d everything problem package texmex  python lshash3  probably not really need code will call  good luck   let know form work ,compressfasttext pqkmean install would like use compressfasttext library    githubcom  avidale  compress  fasttext use command install  pip install compress  fasttext  full  get pqkmean error  run installegginfo copy lshash3egg  info build  bdistlinux  x8664  wheel  lshash3  008  py38egg  info run installscript error  invalid command  bdistwininst   end output  note  error originate subprocess  likely problem pip  error  fail building wheel lshash3 try python27  python37  python38 ubunto 180  python 27  pip install pqkmean without error  compressfast text library work  python37  38  get error message  clue please  update  want add make related issue case anybody want check    githubcom  avidale  compress  fasttext  issues19 per comment  error message  look like real problem install lshash3  update since 2017   bring via appear undocumented dataset package texmex  python use optional eval  demo purpose pqkmean   make pip simply skip dependency  may get error ill effect  not test  may want try  fresh python 3x virtual environment  manually instal requirement pqkmean except texmex  python  see project s requirementstxt list  install pqkmean pip no  deps option  pip install  no  dep pqkmean succeed  perhaps pip install compress  fasttext  full  consider pqkmean already present  try problem texmex  python  lshash3 install  be business still try  could try manually instal compress  fastttext  full  s unique non pqkmean requirement  really gensim  pip install  no  deps compress  fasttext  full  potential end  result would d everything problem package texmex  python lshash3  probably not really need code will call  good luck   let know form work ,Task-Specific Queries
How to convert Doccano exported JSONL format to spaCy format？,"I want to use my own data set to train a named entity recognition model. The data set is exported by the annotation tool Doccano. The format is JSONL (not JSON), but spaCy does not support such a data format input model. How do I convert it? ? Here's what my dataset looks like: {""id"":17,""text"":""In this work, the effect of CFs with zeolite on the mechanical, tribological prop-erties, and structure of PTFE was investigated. \nThe developed materials with a CF content of 1–5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nThe compressive stress of PCM increased by 7–53%, and the yield point by 30% relative to the initial polymer. \nIt was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled PTFE. \nCombining fillers (CF\/Zt) into PTFE reduced the wear rate by 810 times relative to the initial polymer. Tribochemical reactions were shown by IR spectroscopy.\nSEM established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with CFs, protect the surface layer of the material from destruction during friction. \nThe wear resistance of the composite material PTFE\/CF\/Zt was effectively improved, and the coefficient of friction was low compared to PTFE\/CF\/Kl and PTFE\/CF\/Vl."",""entities"":[{""id"":298,""label"":""composite"",""start_offset"":1049,""end_offset"":1059},{""id"":545,""label"":""composite"",""start_offset"":960,""end_offset"":971},{""id"":299,""label"":""composite"",""start_offset"":1064,""end_offset"":1074},{""id"":607,""label"":""value"",""start_offset"":176,""end_offset"":184}],""relations"":[],""Comments"":[]} I have also tried many online methods, but none of them seem to work.","['nlp', 'spacy', 'named-entity-recognition', 'doccano']",2,"You can modify the script given at explosion/projects/pipelines/ner_demo/scripts /convert.py : import json import warnings import spacy from spacy.tokens import DocBin def read_jsonl(fpath): with open(fpath, ""r"") as f: for line in f: yield json.loads(line) nlp = spacy.blank(""en"") doc_bin = DocBin() docs = [] for data in read_jsonl(""data.jsonl""): doc = nlp.make_doc(data[""text""]) ents = [] for entity in data[""entities""]: start = entity[""start_offset""] end = entity[""end_offset""] label = entity[""label""] span = doc.char_span( start_idx=start, end_idx=end, label=label, alignment_mode=""strict"", ) if span is None: msg = ( f""Skipping entity [{start}, {end}, {label}] in the "" ""following text because the character span "" ""'{doc.text[start:end]}' does not align with token "" ""boundaries:\n\n{repr(text)}\n"" ) warnings.warn(msg) else: ents.append(span) doc.set_ents(entities=ents) doc_bin.add(doc) doc_bin.to_disk(""train.spacy"") The data format you have given looks a bit different from the Doccano format that I'm used to, but the above should work.",2023-10-07 02:55:05,2023-10-11 18:56:46,840,https://stackoverflow.com/questions/77248199/how-to-convert-doccano-exported-jsonl-format-to-spacy-format,"How to convert Doccano exported JSONL format to spaCy format？ I want to use my own data set to train a named entity recognition model. The data set is exported by the annotation tool Doccano. The format is JSONL (not JSON), but spaCy does not support such a data format input model. How do I convert it? ? Here's what my dataset looks like: {""id"":17,""text"":""In this work, the effect of CFs with zeolite on the mechanical, tribological prop-erties, and structure of PTFE was investigated. \nThe developed materials with a CF content of 1–5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nThe compressive stress of PCM increased by 7–53%, and the yield point by 30% relative to the initial polymer. \nIt was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled PTFE. \nCombining fillers (CF\/Zt) into PTFE reduced the wear rate by 810 times relative to the initial polymer. Tribochemical reactions were shown by IR spectroscopy.\nSEM established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with CFs, protect the surface layer of the material from destruction during friction. \nThe wear resistance of the composite material PTFE\/CF\/Zt was effectively improved, and the coefficient of friction was low compared to PTFE\/CF\/Kl and PTFE\/CF\/Vl."",""entities"":[{""id"":298,""label"":""composite"",""start_offset"":1049,""end_offset"":1059},{""id"":545,""label"":""composite"",""start_offset"":960,""end_offset"":971},{""id"":299,""label"":""composite"",""start_offset"":1064,""end_offset"":1074},{""id"":607,""label"":""value"",""start_offset"":176,""end_offset"":184}],""relations"":[],""Comments"":[]} I have also tried many online methods, but none of them seem to work.",convert doccano export jsonl format spacy format  want use datum set train name entity recognition model  datum set export annotation tool doccano  format jsonl  json   spacy support datum format input model  convert   s dataset look like     i d   17   text    work  effect cf zeolite mechanical  tribological prop  ertie  structure ptfe investigate  nthe develop material cf content 15 wt   retain deformation strength property level initial polymer  nthe compressive stress pcm increase 753   yield point 30  relative initial polymer  nit find increase content filler  degree crystallinity increase  density decrease comparison unfilled ptfe  ncombine filler  cfzt  ptfe reduce wear rate 810 time relative initial polymer  tribochemical reaction show ir spectroscopynsem establish formation secondary structure form tribofilm friction surface   together cf  protect surface layer material destruction friction  nthe wear resistance composite material ptfecfzt effectively improve  coefficient friction low compare ptfecfkl ptfecfvl      entity       i d   298   label    composite    startoffset   1049   endoffset   1059      i d   545   label    composite    startoffset   960   endoffset   971      i d   299   label    composite    startoffset   1064   endoffset   1074      i d   607   label    value    startoffset   176   endoffset   184     relation       comment      also try many online method  none seem work ,modify script give explosion  project  pipeline  nerdemo  script convertpy  import json import warning import spacy spacytoken import docbin def readjsonl  fpath   open  fpath    r   f  line f  yield jsonloads  line  nlp  spacyblank    en   docbin  docbin   doc    datum readjsonl    datajsonl    doc  nlpmakedoc  datum    text    ent    entity datum    entity    start  entity    startoffset   end  entity    endoffset   label  entity    label   span  doccharspan  startidx  start  endidx  end  label  label  alignmentmode  strict    span none  msg   f  skip entity   start    end    label       follow text character span       doctext  start  end    align token     boundary  nn  repr  text   n   warningswarn  msg  else  entsappend  span  docsetents  entity  ent  docbinadd  doc  docbintodisk    trainspacy   datum format give look bit different doccano format  m use  work ,convert doccano export jsonl format spacy format  want use datum set train name entity recognition model  datum set export annotation tool doccano  format jsonl  json   spacy support datum format input model  convert   s dataset look like     i d   17   text    work  effect cf zeolite mechanical  tribological prop  ertie  structure ptfe investigate  nthe develop material cf content 15 wt   retain deformation strength property level initial polymer  nthe compressive stress pcm increase 753   yield point 30  relative initial polymer  nit find increase content filler  degree crystallinity increase  density decrease comparison unfilled ptfe  ncombine filler  cfzt  ptfe reduce wear rate 810 time relative initial polymer  tribochemical reaction show ir spectroscopynsem establish formation secondary structure form tribofilm friction surface   together cf  protect surface layer material destruction friction  nthe wear resistance composite material ptfecfzt effectively improve  coefficient friction low compare ptfecfkl ptfecfvl      entity       i d   298   label    composite    startoffset   1049   endoffset   1059      i d   545   label    composite    startoffset   960   endoffset   971      i d   299   label    composite    startoffset   1064   endoffset   1074      i d   607   label    value    startoffset   176   endoffset   184     relation       comment      also try many online method  none seem work  modify script give explosion  project  pipeline  nerdemo  script convertpy  import json import warning import spacy spacytoken import docbin def readjsonl  fpath   open  fpath    r   f  line f  yield jsonloads  line  nlp  spacyblank    en   docbin  docbin   doc    datum readjsonl    datajsonl    doc  nlpmakedoc  datum    text    ent    entity datum    entity    start  entity    startoffset   end  entity    endoffset   label  entity    label   span  doccharspan  startidx  start  endidx  end  label  label  alignmentmode  strict    span none  msg   f  skip entity   start    end    label       follow text character span       doctext  start  end    align token     boundary  nn  repr  text   n   warningswarn  msg  else  entsappend  span  docsetents  entity  ent  docbinadd  doc  docbintodisk    trainspacy   datum format give look bit different doccano format  m use  work ,Implementation Issues
What does the vocabulary of a pre-trained / fine-tuned T5 model look like?,"My question is regarding the pre-trained T5 models found on Huggingface. In either case of taking the fully-trained model, or after fine-tuning it, is there an API function for directly downloading the vocabulary? More specifically, the default vocab_size for T5 is 32128 ( from the documentation ). Does that mean that after the model is trained, its decoder can generate up to 32128 unique words? As an aside, I have noticed that capitalization does sometimes appear in my fine-tuned T5, does that mean the 32128 vocabulary could also be comprised of capitalized variants of words, e.g., is there one vocab index for ""hello"" and another index for ""Hello""?","['python', 'pytorch', 'nlp', 'huggingface-transformers']",1,"The T5 default vocabulary consists of 32,128 subword tokens (utilizing the SentencePiece tokenizer), not word tokens. Thus, it can generate a larger vocabulary than the specified 32,128. ""hello"" and ""Hello"" are treated as different tokens because T5's tokenizer is case-sensitive .",2023-10-07 02:35:03,2023-10-07 03:17:00,649,https://stackoverflow.com/questions/77248165/what-does-the-vocabulary-of-a-pre-trained-fine-tuned-t5-model-look-like,"What does the vocabulary of a pre-trained / fine-tuned T5 model look like? My question is regarding the pre-trained T5 models found on Huggingface. In either case of taking the fully-trained model, or after fine-tuning it, is there an API function for directly downloading the vocabulary? More specifically, the default vocab_size for T5 is 32128 ( from the documentation ). Does that mean that after the model is trained, its decoder can generate up to 32128 unique words? As an aside, I have noticed that capitalization does sometimes appear in my fine-tuned T5, does that mean the 32128 vocabulary could also be comprised of capitalized variants of words, e.g., is there one vocab index for ""hello"" and another index for ""Hello""?",vocabulary pre  train  fine  tune t5 model look like  question regard pre  trained t5 model find huggingface  either case take fully  train model  fine  tuning  api function directly download vocabulary  specifically  default vocabsize t5 32128  documentation   mean model train  decoder generate 32128 unique word  aside  noticed capitalization sometimes appear fine  tuned t5  mean 32128 vocabulary could also comprise capitalize variant word  eg  one vocab index   hello  another index   hello  ,t5 default vocabulary consist 32128 subword token  utilize sentencepiece tokenizer   word token  thus  generate large vocabulary specify 32128    hello    hello  treat different token t5 s tokenizer case  sensitive ,vocabulary pre  train  fine  tune t5 model look like  question regard pre  trained t5 model find huggingface  either case take fully  train model  fine  tuning  api function directly download vocabulary  specifically  default vocabsize t5 32128  documentation   mean model train  decoder generate 32128 unique word  aside  noticed capitalization sometimes appear fine  tuned t5  mean 32128 vocabulary could also comprise capitalize variant word  eg  one vocab index   hello  another index   hello   t5 default vocabulary consist 32128 subword token  utilize sentencepiece tokenizer   word token  thus  generate large vocabulary specify 32128    hello    hello  treat different token t5 s tokenizer case  sensitive ,Library/Tool-Based Queries
Summarizing n-grams efficiently in Python on big data,"I have a very large dataset of roughly 6 million records, it does look like this snippet: data = pd.DataFrame({ 'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], 'TEXT': [ ""Mouthwatering BBQ ribs cheese, and coleslaw."", ""Delicious pizza with pepperoni and extra cheese."", ""Spicy Thai curry with cheese and jasmine rice."", ""Tiramisu dessert topped with cocoa powder."", ""Sushi rolls with fresh fish and soy sauce."", ""Freshly baked chocolate chip cookies."", ""Homemade lasagna with layers of cheese and pasta."", ""Gourmet burgers with all the toppings and extra cheese."", ""Crispy fried chicken with mashed potatoes and extra cheese."", ""Creamy tomato soup with a grilled cheese sandwich."" ], 'DATE': [ '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02' ] }) I want to generate bigrams and trigrams from the column 'TEXT.' I'm interested in two types of ngrams for both trigrams and bigrams: those that start with 'extra' and those that don't start with 'extra.' Once we have those, I want to summarize (count the unique ID frequency) of those ngrams by unique 'DATE.' This means that if an ngram appears in an ID more than once, I will count it only once because I want to know in how many different 'IDs' it ultimately appeared. I'm very new to Python. I come from the R world, in which there is a library called quanteda that uses C programming and parallel computing. Searching for those ngrams looks something like this: corpus_food %>% tokens(remove_punct = TRUE) %>% tokens_ngrams(n = 2) %>% tokens_select(pattern = ""^extra"", valuetype = ""regex"") %>% dfm() %>% dfm_group(groups = lubridate::date(DATE)) %>% textstat_frequency() yielding my desired results: feature frequency rank docfreq group 1 extra_cheese 3 1 2 all My desired result would look like this: ngram nunique group cheese and 3 1/02/2023 and extra 2 1/02/2023 extra cheese 2 1/02/2023 and extra cheese 2 1/02/2023 mouthwatering bbq 1 1/02/2023 bbq ribs 1 1/02/2023 ribs cheese 1 1/02/2023 and coleslaw 1 1/02/2023 mouthwatering bbq ribs 1 1/02/2023 bbq ribs cheese 1 1/02/2023 ribs cheese and 1 1/02/2023 cheese and coleslaw 1 1/02/2023 delicious pizza 1 1/02/2023 pizza with 1 1/02/2023 with pepperoni 1 1/02/2023 pepperoni and 1 1/02/2023 delicious pizza with 1 1/02/2023 pizza with pepperoni 1 1/02/2023 with pepperoni and 1 1/02/2023 pepperoni and extra 1 1/02/2023 spicy thai 1 1/02/2023 thai curry 1 1/02/2023 I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. I am open to hearing of a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python. So far I have found a way to create the bigrams and trigrams but I have no idea as to how perform the selection of those that start with ""extra"" and those who don't and this very process of creating the ngrams is taking over an hour so I will take all advice on how to reduce the time. Work around: import nltk from nltk import bigrams from nltk.util import trigrams from nltk.tokenize import word_tokenize data['bigrams'] = data['TEXT'].apply(lambda x: list(bigrams(word_tokenize(x)))) data['trigrams'] = data['TEXT'].apply(lambda x: list(trigrams(word_tokenize(x)))) Reading through some posts, some people suggest on using the gensim lib. Would that be a good direction?","['python', 'pandas', 'dataframe', 'nlp', 'n-gram']",1,"It is easy to find ngrams using sklearn's CountVectorizer using the ngram_range argument. You can create a document-term matrix with ngrams of size 2 and 3 only, then append to your original dataset and doing pivoting and aggregation with pandas to find what you need. First we'll get the document-term matrix and append to our original data: # Perform the count vectorization, keeping bigrams and trigrams only from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer(ngram_range=(2,3)) X = cv.fit_transform(data['TEXT']) # Create dataframe of document-term matrix cv_df = pd.DataFrame(X.todense(), columns=cv.get_feature_names_out()) # Append to original data df = pd.concat([data, cv_df], axis=1) Then we group by ID and date, and filter where the count is greater than 0, to find the ID-date combinations where each 2 or 3-gram appears, then count the unique IDs for each: # Group and pivot pivoted_df = df.groupby(['ID','DATE']).sum().stack().reset_index() pivoted_df.columns = ['ID', 'DATE', 'ngram', 'count'] # Find n-grams which appear for each ID-DATE combo and count unique ids pivoted_df = pivoted_df[pivoted_df['count']>0] pivoted_df.groupby(['DATE','ngram']).nunique('ID').reset_index() Finally, we can create additional columns for the ngram size, and also whether or not the ngram starts with extra , and use for filtering: # Add additional columns for ngram size pivoted_df['ngram_size'] = pivoted_df['ngram'].str.split().str.len() # Add additional column for starting with extra pivoted_df['extra'] = pivoted_df['ngram'].str.startswith('extra') # Find all the 2-grams that start with ""extra"" pivoted_df[(pivoted_df['extra']) & (pivoted_df['ngram_size']==2)] That being said, if you have 6M records, you have a large dataset, and with this approach you will definitely run into memory issues. You will probably want to filter your data down to what you are most interested in to start, and also make sure you use the min_df parameter in the CountVectorizer in order to keep your data tractable.",2023-10-07 00:44:48,2023-10-10 13:59:56,283,https://stackoverflow.com/questions/77247941/summarizing-n-grams-efficiently-in-python-on-big-data,"Summarizing n-grams efficiently in Python on big data I have a very large dataset of roughly 6 million records, it does look like this snippet: data = pd.DataFrame({ 'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], 'TEXT': [ ""Mouthwatering BBQ ribs cheese, and coleslaw."", ""Delicious pizza with pepperoni and extra cheese."", ""Spicy Thai curry with cheese and jasmine rice."", ""Tiramisu dessert topped with cocoa powder."", ""Sushi rolls with fresh fish and soy sauce."", ""Freshly baked chocolate chip cookies."", ""Homemade lasagna with layers of cheese and pasta."", ""Gourmet burgers with all the toppings and extra cheese."", ""Crispy fried chicken with mashed potatoes and extra cheese."", ""Creamy tomato soup with a grilled cheese sandwich."" ], 'DATE': [ '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02' ] }) I want to generate bigrams and trigrams from the column 'TEXT.' I'm interested in two types of ngrams for both trigrams and bigrams: those that start with 'extra' and those that don't start with 'extra.' Once we have those, I want to summarize (count the unique ID frequency) of those ngrams by unique 'DATE.' This means that if an ngram appears in an ID more than once, I will count it only once because I want to know in how many different 'IDs' it ultimately appeared. I'm very new to Python. I come from the R world, in which there is a library called quanteda that uses C programming and parallel computing. Searching for those ngrams looks something like this: corpus_food %>% tokens(remove_punct = TRUE) %>% tokens_ngrams(n = 2) %>% tokens_select(pattern = ""^extra"", valuetype = ""regex"") %>% dfm() %>% dfm_group(groups = lubridate::date(DATE)) %>% textstat_frequency() yielding my desired results: feature frequency rank docfreq group 1 extra_cheese 3 1 2 all My desired result would look like this: ngram nunique group cheese and 3 1/02/2023 and extra 2 1/02/2023 extra cheese 2 1/02/2023 and extra cheese 2 1/02/2023 mouthwatering bbq 1 1/02/2023 bbq ribs 1 1/02/2023 ribs cheese 1 1/02/2023 and coleslaw 1 1/02/2023 mouthwatering bbq ribs 1 1/02/2023 bbq ribs cheese 1 1/02/2023 ribs cheese and 1 1/02/2023 cheese and coleslaw 1 1/02/2023 delicious pizza 1 1/02/2023 pizza with 1 1/02/2023 with pepperoni 1 1/02/2023 pepperoni and 1 1/02/2023 delicious pizza with 1 1/02/2023 pizza with pepperoni 1 1/02/2023 with pepperoni and 1 1/02/2023 pepperoni and extra 1 1/02/2023 spicy thai 1 1/02/2023 thai curry 1 1/02/2023 I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. I am open to hearing of a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python. So far I have found a way to create the bigrams and trigrams but I have no idea as to how perform the selection of those that start with ""extra"" and those who don't and this very process of creating the ngrams is taking over an hour so I will take all advice on how to reduce the time. Work around: import nltk from nltk import bigrams from nltk.util import trigrams from nltk.tokenize import word_tokenize data['bigrams'] = data['TEXT'].apply(lambda x: list(bigrams(word_tokenize(x)))) data['trigrams'] = data['TEXT'].apply(lambda x: list(trigrams(word_tokenize(x)))) Reading through some posts, some people suggest on using the gensim lib. Would that be a good direction?",summarizing n  gram efficiently python big datum large dataset roughly 6 million record  look like snippet  datum  pd  dataframe    id        b    c    d    e    f    g    h       j     text      mouthwatering bbq rib cheese  coleslaw       delicious pizza pepperoni extra cheese       spicy thai curry cheese jasmine rice       tiramisu dessert top cocoa powder       sushi roll fresh fish soy sauce       freshly bake chocolate chip cookie       homemade lasagna layers cheese pasta       gourmet burger topping extra cheese       crispy fry chicken mash potato extra cheese       creamy tomato soup grill cheese sandwich      date     2023  02  01    2023  02  01    2023  02  01    2023  02  01    2023  02  02    2023  02  02    2023  02  01    2023  02  01    2023  02  02    2023  02  02     want generate bigrams trigram column  text    m interested two type ngram trigram bigram  start  extra  not start  extra    want summarize  count unique id frequency  ngram unique  date   mean ngram appear id  count want know many different  id  ultimately appear   m new python  come r world  library call quanteda use c programming parallel computing  search ngram look something like  corpusfood    token  removepunct  true     tokensngrams  n  2     tokensselect  pattern    extra   valuetype    regex      dfm      dfmgroup  group  lubridate   date  date      textstatfrequency   yield desire result  feature frequency rank docfreq group 1 extracheese 3 1 2 desire result would look like  ngram nunique group cheese 3 1022023 extra 2 1022023 extra cheese 2 1022023 extra cheese 2 1022023 mouthwatering bbq 1 1022023 bbq rib 1 1022023 rib cheese 1 1022023 coleslaw 1 1022023 mouthwatering bbq rib 1 1022023 bbq rib cheese 1 1022023 rib cheese 1 1022023 cheese coleslaw 1 1022023 delicious pizza 1 1022023 pizza 1 1022023 pepperoni 1 1022023 pepperoni 1 1022023 delicious pizza 1 1022023 pizza pepperoni 1 1022023 pepperoni 1 1022023 pepperoni extra 1 1022023 spicy thai 1 1022023 thai curry 1 1022023 way compare two language  python r amazing  moment   m interested straightforward fast method achieve result python  open hearing way achieve  m look fast efficient way python   m new python  far find way create bigram trigram idea perform selection start   extra  not process create ngram take hour take advice reduce time  work around  import nltk nltk import bigrams nltkutil import trigram nltktokenize import wordtokenize datum   bigram    datum   text   apply  lambda x  list  bigram  wordtokenize  x     datum   trigram    datum   text   apply  lambda x  list  trigram  wordtokenize  x     read post  people suggest use gensim lib  would good direction ,easy find ngram use sklearn s countvectorizer use ngramrange argument  create document  term matrix ngram size 2 3  append original dataset pivot aggregation panda find need  first will get document  term matrix append original datum   perform count vectorization  keep bigrams trigram sklearnfeatureextractiontext import countvectorizer cv  countvectorizer  ngramrange  23   x  cvfittransform  datum   text     create dataframe document  term matrix cvdf  pd  dataframe  xtodense    column  cvgetfeaturenamesout     append original datum df  pdconcat   datum  cvdf   axis1  group id date  filter count great 0  find id  date combination 2 3  gram appear  count unique id   group pivot pivoteddf  dfgroupby    id    date    sum   stack   resetindex   pivoteddfcolumn    id    date    ngram    count    find n  gram appear id  date combo count unique ids pivoteddf  pivoteddf  pivoteddf   count    0  pivoteddfgroupby    date    ngram    nunique   id   resetindex   finally  create additional column ngram size  also whether ngram start extra  use filtering   add additional column ngram size pivoteddf   ngramsize    pivoteddf   ngram   strsplit   strlen    add additional column start extra pivoteddf   extra    pivoteddf   ngram   strstartswith   extra    find 2  gram start   extra  pivoteddf   pivoteddf   extra      pivoteddf   ngramsize     2   say  6 m record  large dataset  approach definitely run memory issue  probably want filter datum interested start  also make sure use mindf parameter countvectorizer order keep datum tractable ,summarizing n  gram efficiently python big datum large dataset roughly 6 million record  look like snippet  datum  pd  dataframe    id        b    c    d    e    f    g    h       j     text      mouthwatering bbq rib cheese  coleslaw       delicious pizza pepperoni extra cheese       spicy thai curry cheese jasmine rice       tiramisu dessert top cocoa powder       sushi roll fresh fish soy sauce       freshly bake chocolate chip cookie       homemade lasagna layers cheese pasta       gourmet burger topping extra cheese       crispy fry chicken mash potato extra cheese       creamy tomato soup grill cheese sandwich      date     2023  02  01    2023  02  01    2023  02  01    2023  02  01    2023  02  02    2023  02  02    2023  02  01    2023  02  01    2023  02  02    2023  02  02     want generate bigrams trigram column  text    m interested two type ngram trigram bigram  start  extra  not start  extra    want summarize  count unique id frequency  ngram unique  date   mean ngram appear id  count want know many different  id  ultimately appear   m new python  come r world  library call quanteda use c programming parallel computing  search ngram look something like  corpusfood    token  removepunct  true     tokensngrams  n  2     tokensselect  pattern    extra   valuetype    regex      dfm      dfmgroup  group  lubridate   date  date      textstatfrequency   yield desire result  feature frequency rank docfreq group 1 extracheese 3 1 2 desire result would look like  ngram nunique group cheese 3 1022023 extra 2 1022023 extra cheese 2 1022023 extra cheese 2 1022023 mouthwatering bbq 1 1022023 bbq rib 1 1022023 rib cheese 1 1022023 coleslaw 1 1022023 mouthwatering bbq rib 1 1022023 bbq rib cheese 1 1022023 rib cheese 1 1022023 cheese coleslaw 1 1022023 delicious pizza 1 1022023 pizza 1 1022023 pepperoni 1 1022023 pepperoni 1 1022023 delicious pizza 1 1022023 pizza pepperoni 1 1022023 pepperoni 1 1022023 pepperoni extra 1 1022023 spicy thai 1 1022023 thai curry 1 1022023 way compare two language  python r amazing  moment   m interested straightforward fast method achieve result python  open hearing way achieve  m look fast efficient way python   m new python  far find way create bigram trigram idea perform selection start   extra  not process create ngram take hour take advice reduce time  work around  import nltk nltk import bigrams nltkutil import trigram nltktokenize import wordtokenize datum   bigram    datum   text   apply  lambda x  list  bigram  wordtokenize  x     datum   trigram    datum   text   apply  lambda x  list  trigram  wordtokenize  x     read post  people suggest use gensim lib  would good direction  easy find ngram use sklearn s countvectorizer use ngramrange argument  create document  term matrix ngram size 2 3  append original dataset pivot aggregation panda find need  first will get document  term matrix append original datum   perform count vectorization  keep bigrams trigram sklearnfeatureextractiontext import countvectorizer cv  countvectorizer  ngramrange  23   x  cvfittransform  datum   text     create dataframe document  term matrix cvdf  pd  dataframe  xtodense    column  cvgetfeaturenamesout     append original datum df  pdconcat   datum  cvdf   axis1  group id date  filter count great 0  find id  date combination 2 3  gram appear  count unique id   group pivot pivoteddf  dfgroupby    id    date    sum   stack   resetindex   pivoteddfcolumn    id    date    ngram    count    find n  gram appear id  date combo count unique ids pivoteddf  pivoteddf  pivoteddf   count    0  pivoteddfgroupby    date    ngram    nunique   id   resetindex   finally  create additional column ngram size  also whether ngram start extra  use filtering   add additional column ngram size pivoteddf   ngramsize    pivoteddf   ngram   strsplit   strlen    add additional column start extra pivoteddf   extra    pivoteddf   ngram   strstartswith   extra    find 2  gram start   extra  pivoteddf   pivoteddf   extra      pivoteddf   ngramsize     2   say  6 m record  large dataset  approach definitely run memory issue  probably want filter datum interested start  also make sure use mindf parameter countvectorizer order keep datum tractable ,Library/Tool-Based Queries
Tokenizing and summarizing Textual data by group efficiently in Python,"I have a dataset in Python that look like this one: data = pd.DataFrame({ 'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], 'TEXT': [ ""Mouthwatering BBQ ribs cheese, and coleslaw."", ""Delicious pizza with pepperoni and extra cheese."", ""Spicy Thai curry with cheese and jasmine rice."", ""Tiramisu dessert topped with cocoa powder."", ""Sushi rolls with fresh fish and soy sauce."", ""Freshly baked chocolate chip cookies."", ""Homemade lasagna with layers of cheese and pasta."", ""Gourmet burgers with all the toppings and extra cheese."", ""Crispy fried chicken with mashed potatoes and extra cheese."", ""Creamy tomato soup with a grilled cheese sandwich."" ], 'DATE': [ '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02' ] }) What I'd like to do is group by DATE and get the frequency of each token after removing punctuation. I'm very new to the Python environment; I come from R, and I have been looking into the gensim library for further reference. It looks quite complicated to me. My desired output would look like this: for each group (DATE), we'll have the frequency of each unique token. TOKEN SUBTOTAL DATE cheese 5 1/02/2023 and 5 1/02/2023 with 5 1/02/2023 extra 2 1/02/2023 mouthwatering 1 1/02/2023 bbq 1 1/02/2023 ribs 1 1/02/2023 coleslaw 1 1/02/2023 delicious 1 1/02/2023 pizza 1 1/02/2023 pepperoni 1 1/02/2023 In R this can be done very easy with quanteda like this: corpus_food<-corpus(data, docid_field = ""ID"", text_field = ""TEXT"") corpus_food %>% tokens(remove_punct = TRUE) %>% dfm() %>% textstat_frequency(groups = lubridate::date(DATE)) Which only creates a corpus and then tokenizes to remove punctuation. Later, it creates a document-term matrix and finally summarizes the tokens and their frequencies by group. I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. If perhaps you don't use the gensim library, I'd still be interested in a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python.","['python', 'pandas', 'dataframe', 'nlp', 'gensim']",1,"I would simply extractall the words then value_counts : out = ( data[[""DATE""]].join( data[""TEXT""].str.extractall(""(\w+)"")[0] .droplevel(1).rename(""TOKEN"").str.lower() ).groupby([""DATE"", ""TOKEN""]).value_counts().reset_index(name=""SUBTOTAL"") .sort_values([""DATE"", ""SUBTOTAL""], ascending=[True, False]) ) Output : print(out) DATE TOKEN SUBTOTAL 1 2023-02-01 and 5 4 2023-02-01 cheese 5 30 2023-02-01 with 5 10 2023-02-01 extra 2 0 2023-02-01 all 1 .. ... ... ... 51 2023-02-02 sauce 1 52 2023-02-02 soup 1 53 2023-02-02 soy 1 54 2023-02-02 sushi 1 55 2023-02-02 tomato 1 [57 rows x 3 columns]",2023-10-05 22:16:39,2023-10-06 07:51:03,61,https://stackoverflow.com/questions/77240878/tokenizing-and-summarizing-textual-data-by-group-efficiently-in-python,"Tokenizing and summarizing Textual data by group efficiently in Python I have a dataset in Python that look like this one: data = pd.DataFrame({ 'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], 'TEXT': [ ""Mouthwatering BBQ ribs cheese, and coleslaw."", ""Delicious pizza with pepperoni and extra cheese."", ""Spicy Thai curry with cheese and jasmine rice."", ""Tiramisu dessert topped with cocoa powder."", ""Sushi rolls with fresh fish and soy sauce."", ""Freshly baked chocolate chip cookies."", ""Homemade lasagna with layers of cheese and pasta."", ""Gourmet burgers with all the toppings and extra cheese."", ""Crispy fried chicken with mashed potatoes and extra cheese."", ""Creamy tomato soup with a grilled cheese sandwich."" ], 'DATE': [ '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02' ] }) What I'd like to do is group by DATE and get the frequency of each token after removing punctuation. I'm very new to the Python environment; I come from R, and I have been looking into the gensim library for further reference. It looks quite complicated to me. My desired output would look like this: for each group (DATE), we'll have the frequency of each unique token. TOKEN SUBTOTAL DATE cheese 5 1/02/2023 and 5 1/02/2023 with 5 1/02/2023 extra 2 1/02/2023 mouthwatering 1 1/02/2023 bbq 1 1/02/2023 ribs 1 1/02/2023 coleslaw 1 1/02/2023 delicious 1 1/02/2023 pizza 1 1/02/2023 pepperoni 1 1/02/2023 In R this can be done very easy with quanteda like this: corpus_food<-corpus(data, docid_field = ""ID"", text_field = ""TEXT"") corpus_food %>% tokens(remove_punct = TRUE) %>% dfm() %>% textstat_frequency(groups = lubridate::date(DATE)) Which only creates a corpus and then tokenizes to remove punctuation. Later, it creates a document-term matrix and finally summarizes the tokens and their frequencies by group. I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. If perhaps you don't use the gensim library, I'd still be interested in a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python.",tokenize summarize textual datum group efficiently python dataset python look like one  datum  pd  dataframe    id        b    c    d    e    f    g    h       j     text      mouthwatering bbq rib cheese  coleslaw       delicious pizza pepperoni extra cheese       spicy thai curry cheese jasmine rice       tiramisu dessert top cocoa powder       sushi roll fresh fish soy sauce       freshly bake chocolate chip cookie       homemade lasagna layers cheese pasta       gourmet burger topping extra cheese       crispy fry chicken mash potato extra cheese       creamy tomato soup grill cheese sandwich      date     2023  02  01    2023  02  01    2023  02  01    2023  02  01    2023  02  02    2023  02  02    2023  02  01    2023  02  01    2023  02  02    2023  02  02     would like group date get frequency token remove punctuation   m new python environment  come r  look gensim library reference  look quite complicated  desire output would look like  group  date   will frequency unique token  token subtotal date cheese 5 1022023 5 1022023 5 1022023 extra 2 1022023 mouthwatering 1 1022023 bbq 1 1022023 rib 1 1022023 coleslaw 1 1022023 delicious 1 1022023 pizza 1 1022023 pepperoni 1 1022023 r do easy quanteda like  corpusfood  corpus  datum  docidfield    id   textfield    text   corpusfood    token  removepunct  true     dfm      textstatfrequency  group  lubridate   date  date   create corpus tokenizes remove punctuation  later  create document  term matrix finally summarize tokens frequency group  way compare two language  python r amazing  moment   m interested straightforward fast method achieve result python  perhaps not use gensim library  have still interested way achieve  m look fast efficient way python   m new python ,would simply extractall word valuecount    datum     date    join  datum    text   strextractall     w     0  droplevel  1  rename    token   strlower    groupby     date     token    valuecounts   resetindex  name  subtotal   sortvalues     date     subtotal    ascending  true  false    output  print   date token subtotal 1 2023  02  01 5 4 2023  02  01 cheese 5 30 2023  02  01 5 10 2023  02  01 extra 2 0 2023  02  01 1     51 2023  02  02 sauce 1 52 2023  02  02 soup 1 53 2023  02  02 soy 1 54 2023  02  02 sushi 1 55 2023  02  02 tomato 1  57 row x 3 column ,tokenize summarize textual datum group efficiently python dataset python look like one  datum  pd  dataframe    id        b    c    d    e    f    g    h       j     text      mouthwatering bbq rib cheese  coleslaw       delicious pizza pepperoni extra cheese       spicy thai curry cheese jasmine rice       tiramisu dessert top cocoa powder       sushi roll fresh fish soy sauce       freshly bake chocolate chip cookie       homemade lasagna layers cheese pasta       gourmet burger topping extra cheese       crispy fry chicken mash potato extra cheese       creamy tomato soup grill cheese sandwich      date     2023  02  01    2023  02  01    2023  02  01    2023  02  01    2023  02  02    2023  02  02    2023  02  01    2023  02  01    2023  02  02    2023  02  02     would like group date get frequency token remove punctuation   m new python environment  come r  look gensim library reference  look quite complicated  desire output would look like  group  date   will frequency unique token  token subtotal date cheese 5 1022023 5 1022023 5 1022023 extra 2 1022023 mouthwatering 1 1022023 bbq 1 1022023 rib 1 1022023 coleslaw 1 1022023 delicious 1 1022023 pizza 1 1022023 pepperoni 1 1022023 r do easy quanteda like  corpusfood  corpus  datum  docidfield    id   textfield    text   corpusfood    token  removepunct  true     dfm      textstatfrequency  group  lubridate   date  date   create corpus tokenizes remove punctuation  later  create document  term matrix finally summarize tokens frequency group  way compare two language  python r amazing  moment   m interested straightforward fast method achieve result python  perhaps not use gensim library  have still interested way achieve  m look fast efficient way python   m new python  would simply extractall word valuecount    datum     date    join  datum    text   strextractall     w     0  droplevel  1  rename    token   strlower    groupby     date     token    valuecounts   resetindex  name  subtotal   sortvalues     date     subtotal    ascending  true  false    output  print   date token subtotal 1 2023  02  01 5 4 2023  02  01 cheese 5 30 2023  02  01 5 10 2023  02  01 extra 2 0 2023  02  01 1     51 2023  02  02 sauce 1 52 2023  02  02 soup 1 53 2023  02  02 soy 1 54 2023  02  02 sushi 1 55 2023  02  02 tomato 1  57 row x 3 column ,Task-Specific Queries
How to load a huggingface pretrained transformer model directly to GPU?,"I want to load a huggingface pretrained transformer model directly to GPU (not enough CPU space) e.g. loading BERT from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""bert-base-uncased"") would be loaded to CPU until executing model.to('cuda') now the model is loaded into GPU I want to load the model directly into GPU when executing from_pretrained . Is this possible?","['python', 'nlp', 'huggingface-transformers']",1,"I'm answering my own question. Hugging Face accelerate (add via pip install accelerate ) could be helpful in moving the model to GPU before it's fully loaded in CPU. It's useful when: GPU memory > model size > CPU memory Also specify device_map=""cuda"" : from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""bert-base-uncased"", device_map=""cuda"")",2023-10-05 13:57:40,2023-10-05 14:23:04,58290,https://stackoverflow.com/questions/77237818/how-to-load-a-huggingface-pretrained-transformer-model-directly-to-gpu,"How to load a huggingface pretrained transformer model directly to GPU? I want to load a huggingface pretrained transformer model directly to GPU (not enough CPU space) e.g. loading BERT from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""bert-base-uncased"") would be loaded to CPU until executing model.to('cuda') now the model is loaded into GPU I want to load the model directly into GPU when executing from_pretrained . Is this possible?",load huggingface pretraine transformer model directly gpu  want load huggingface pretraine transformer model directly gpu  enough cpu space  eg  load bert transformer import automodelforcausallm model  automodelforcausallmfrompretraine    bert  base  uncased   would load cpu execute modelto   cuda   model load gpu want load model directly gpu execute frompretrained  possible , m answer question  hug face accelerate  add via pip install accelerate  could helpful moving model gpu s fully loaded cpu  s useful  gpu memory  model size  cpu memory also specify devicemap  cuda   transformer import automodelforcausallm model  automodelforcausallmfrompretraine    bert  base  uncased   devicemap  cuda  ,load huggingface pretraine transformer model directly gpu  want load huggingface pretraine transformer model directly gpu  enough cpu space  eg  load bert transformer import automodelforcausallm model  automodelforcausallmfrompretraine    bert  base  uncased   would load cpu execute modelto   cuda   model load gpu want load model directly gpu execute frompretrained  possible   m answer question  hug face accelerate  add via pip install accelerate  could helpful moving model gpu s fully loaded cpu  s useful  gpu memory  model size  cpu memory also specify devicemap  cuda   transformer import automodelforcausallm model  automodelforcausallmfrompretraine    bert  base  uncased   devicemap  cuda  ,Basic Understanding
"In GCP&#39;s DocumentAI, when importing documents via API, is it possible to add a Document Type label?","I am creating a Custom Document Classification Processor in GCP's DocumentAI platform, and am trying to understand whether it is possible to assign a Document Type label to documents when importing them to train the Processor. This StackOverflow answer notes that GCP's DocumentAI platform does expose an API to create a Dataset and upload documents to it. With that in mind, I know that it is possible to use the DocumentAI API to create a dataset, and then (as in the code below) to update that Dataset's schema with document types: from google.cloud import documentai_v1beta3 as documentai document_processor_service_client = documentai.DocumentProcessorServiceClient() processor_name = 'projects/123456789/locations/us/processors/example123' processor = document_processor_service_client.get_processor(documentai.GetProcessorRequest(name=processor_name)) dataset_schema = document_service_client.get_dataset_schema(documentai.GetDatasetSchemaRequest(name=f'{processor.name}/dataset/datasetSchema')) dataset_schema dataset_schema.document_schema.entity_types = [ { ""name"": ""test1"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test1"" }, { ""name"": ""test2"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test2 }, { ""name"": ""test4"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test4"" } ] update_schema_request = document_service_client.update_dataset_schema(documentai.UpdateDatasetSchemaRequest(dataset_schema=dataset_schema)) I know that the API also allows importing one or more documents, as in this code: import_documents_request = document_service_client.import_documents( documentai.ImportDocumentsRequest( dataset=f""{processor.name}/dataset"", batch_documents_import_configs=[ documentai.ImportDocumentsRequest.BatchDocumentsImportConfig( auto_split_config=documentai.ImportDocumentsRequest.BatchDocumentsImportConfig.AutoSplitConfig( training_split_ratio=0.7 ), batch_input_config=documentai.BatchDocumentsInputConfig( gcs_documents=documentai.GcsDocuments( documents=[ documentai.GcsDocument( gcs_uri=""gs://path/to/document.pdf"", mime_type=""application/pdf"", ) ] ) ), ) ], ), ) When manually uploading documents in Cloud Console, there is an option for applying a Document Type label to all imported documents: I can't tell from the DocumentAI documentation: Is it possible to similarly assign a Document Type label to one or more Documents via the API? Whether during upload, or after? I have a lot of documents ready to use in a training set, and just need to give each an overall Document Type label (vs. annotating specific fields in each document), so I am looking for a way to do so programmatically, rather than manually.","['machine-learning', 'google-cloud-platform', 'nlp', 'cloud-document-ai']",3,The Document AI API does not currently support applying a label on import when using the importDocuments() method. You need to use the Cloud Console to do bulk labeling. I would recommend adding more details to the public issue tracker nestor-ceniza-jr@ created so that this can be prioritized by the product development team. https://issuetracker.google.com/303285767,2023-10-02 17:06:48,2023-10-16 16:55:01,614,https://stackoverflow.com/questions/77217287/in-gcps-documentai-when-importing-documents-via-api-is-it-possible-to-add-a-d,"In GCP&#39;s DocumentAI, when importing documents via API, is it possible to add a Document Type label? I am creating a Custom Document Classification Processor in GCP's DocumentAI platform, and am trying to understand whether it is possible to assign a Document Type label to documents when importing them to train the Processor. This StackOverflow answer notes that GCP's DocumentAI platform does expose an API to create a Dataset and upload documents to it. With that in mind, I know that it is possible to use the DocumentAI API to create a dataset, and then (as in the code below) to update that Dataset's schema with document types: from google.cloud import documentai_v1beta3 as documentai document_processor_service_client = documentai.DocumentProcessorServiceClient() processor_name = 'projects/123456789/locations/us/processors/example123' processor = document_processor_service_client.get_processor(documentai.GetProcessorRequest(name=processor_name)) dataset_schema = document_service_client.get_dataset_schema(documentai.GetDatasetSchemaRequest(name=f'{processor.name}/dataset/datasetSchema')) dataset_schema dataset_schema.document_schema.entity_types = [ { ""name"": ""test1"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test1"" }, { ""name"": ""test2"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test2 }, { ""name"": ""test4"", ""base_types"": [""document""], ""entity_type_metadata"": { }, ""display_name"": ""test4"" } ] update_schema_request = document_service_client.update_dataset_schema(documentai.UpdateDatasetSchemaRequest(dataset_schema=dataset_schema)) I know that the API also allows importing one or more documents, as in this code: import_documents_request = document_service_client.import_documents( documentai.ImportDocumentsRequest( dataset=f""{processor.name}/dataset"", batch_documents_import_configs=[ documentai.ImportDocumentsRequest.BatchDocumentsImportConfig( auto_split_config=documentai.ImportDocumentsRequest.BatchDocumentsImportConfig.AutoSplitConfig( training_split_ratio=0.7 ), batch_input_config=documentai.BatchDocumentsInputConfig( gcs_documents=documentai.GcsDocuments( documents=[ documentai.GcsDocument( gcs_uri=""gs://path/to/document.pdf"", mime_type=""application/pdf"", ) ] ) ), ) ], ), ) When manually uploading documents in Cloud Console, there is an option for applying a Document Type label to all imported documents: I can't tell from the DocumentAI documentation: Is it possible to similarly assign a Document Type label to one or more Documents via the API? Whether during upload, or after? I have a lot of documents ready to use in a training set, and just need to give each an overall Document Type label (vs. annotating specific fields in each document), so I am looking for a way to do so programmatically, rather than manually.",gcp   39  documentai  import document via api  possible add document type label  create custom document classification processor gcp s documentai platform  try understand whether possible assign document type label document import train processor  stackoverflow answer note gcp s documentai platform expose api create dataset upload document  mind  know possible use documentai api create dataset   code  update dataset s schema document type  googlecloud import documentaiv1beta3 documentai documentprocessorserviceclient  documentai  documentprocessorserviceclient   processorname   projects123456789  location  us  processor  example123  processor  documentprocessorserviceclientgetprocessor  documentai  getprocessorrequest  name  processorname   datasetschema  documentserviceclientgetdatasetschema  documentai  getdatasetschemarequest  name  f   processorname  dataset  datasetschema    datasetschema datasetschemadocumentschemaentitytypes      name     test1     basetypes      document      entitytypemetadata        displayname     test1       name     test2     basetypes      document      entitytypemetadata        displayname     test2      name     test4     basetypes      document      entitytypemetadata        displayname     test4    updateschemarequest  documentserviceclientupdatedatasetschema  documentai  updatedatasetschemarequest  datasetschema  datasetschema   know api also allow import one document  code  importdocumentsrequest  documentserviceclientimportdocument  documentai  importdocumentsrequ  dataset  f   processorname  dataset   batchdocumentsimportconfigs  documentai  importdocumentsrequ  batchdocumentsimportconfig  autosplitconfig  documentai  importdocumentsrequ  batchdocumentsimportconfig  autosplitconfig  trainingsplitratio07   batchinputconfig  documentai  batchdocumentsinputconfig  gcsdocument  documentai  gcsdocument  documents  documentai  gcsdocument  gcsuri  gs  path  to  documentpdf   mimetype  application  pdf              manually upload document cloud console  option apply document type label import document  can not tell documentai documentation  possible similarly assign document type label one document via api  whether upload   lot document ready use training set  need give overall document type label  vs annotate specific field document   look way programmatically  rather manually ,document ai api currently support apply label import use importdocument   method  need use cloud console bulk labeling  would recommend add detail public issue tracker nestor  ceniza  jr  create prioritize product development team    issuetrackergooglecom303285767,gcp   39  documentai  import document via api  possible add document type label  create custom document classification processor gcp s documentai platform  try understand whether possible assign document type label document import train processor  stackoverflow answer note gcp s documentai platform expose api create dataset upload document  mind  know possible use documentai api create dataset   code  update dataset s schema document type  googlecloud import documentaiv1beta3 documentai documentprocessorserviceclient  documentai  documentprocessorserviceclient   processorname   projects123456789  location  us  processor  example123  processor  documentprocessorserviceclientgetprocessor  documentai  getprocessorrequest  name  processorname   datasetschema  documentserviceclientgetdatasetschema  documentai  getdatasetschemarequest  name  f   processorname  dataset  datasetschema    datasetschema datasetschemadocumentschemaentitytypes      name     test1     basetypes      document      entitytypemetadata        displayname     test1       name     test2     basetypes      document      entitytypemetadata        displayname     test2      name     test4     basetypes      document      entitytypemetadata        displayname     test4    updateschemarequest  documentserviceclientupdatedatasetschema  documentai  updatedatasetschemarequest  datasetschema  datasetschema   know api also allow import one document  code  importdocumentsrequest  documentserviceclientimportdocument  documentai  importdocumentsrequ  dataset  f   processorname  dataset   batchdocumentsimportconfigs  documentai  importdocumentsrequ  batchdocumentsimportconfig  autosplitconfig  documentai  importdocumentsrequ  batchdocumentsimportconfig  autosplitconfig  trainingsplitratio07   batchinputconfig  documentai  batchdocumentsinputconfig  gcsdocument  documentai  gcsdocument  documents  documentai  gcsdocument  gcsuri  gs  path  to  documentpdf   mimetype  application  pdf              manually upload document cloud console  option apply document type label import document  can not tell documentai documentation  possible similarly assign document type label one document via api  whether upload   lot document ready use training set  need give overall document type label  vs annotate specific field document   look way programmatically  rather manually  document ai api currently support apply label import use importdocument   method  need use cloud console bulk labeling  would recommend add detail public issue tracker nestor  ceniza  jr  create prioritize product development team    issuetrackergooglecom303285767,Library/Tool-Based Queries
Why am I getting Hidden size error with PyTorch RNN,"I am trying to build a RNN for next word prediction, following a next character prediction example ( tutorial , github , colab (runtime ~1min)). In the example, the input shape is (3,14,17) for batch_size, sequence_length and nb_features. Then the hidden size is defined as (1,3,12) for n_layers, batch_size and hidden_dim. I followed this example except for my batch_size is 1. Also, my input sequences are not padded since I'm using batch_size 1. So I run my train() method and I get an error on my first training data example : RuntimeError: Expected hidden size (1, 25, 12), got [1, 1, 12] (25 being the sequence length). So it seems pytorch ask me to give sequence length as dimension for my hidden layer but in the example code I followed it isn't the case and the code works fine. What am I doing wrong? Additionally, here is the colab I am using (runtime ~1min).","['deep-learning', 'pytorch', 'nlp', 'recurrent-neural-network', 'one-hot-encoding']",1,"There were 2 main differences between the example code I use and my code which couldn't compute : 1- batch_first=True passed to the RNN when initiating the model 2- the target preprocessing had to differ from the input preprocessing : I am using sparse one-hot vectors to encode words and while sparse vectors work in input, the target had to be encoded with only the index of the word in the one-hot instead of the whole one-hot vector Thanks @erip for help debugging this!",2023-10-01 10:34:40,2023-10-01 11:35:36,140,https://stackoverflow.com/questions/77210103/why-am-i-getting-hidden-size-error-with-pytorch-rnn,"Why am I getting Hidden size error with PyTorch RNN I am trying to build a RNN for next word prediction, following a next character prediction example ( tutorial , github , colab (runtime ~1min)). In the example, the input shape is (3,14,17) for batch_size, sequence_length and nb_features. Then the hidden size is defined as (1,3,12) for n_layers, batch_size and hidden_dim. I followed this example except for my batch_size is 1. Also, my input sequences are not padded since I'm using batch_size 1. So I run my train() method and I get an error on my first training data example : RuntimeError: Expected hidden size (1, 25, 12), got [1, 1, 12] (25 being the sequence length). So it seems pytorch ask me to give sequence length as dimension for my hidden layer but in the example code I followed it isn't the case and the code works fine. What am I doing wrong? Additionally, here is the colab I am using (runtime ~1min).",get hidden size error pytorch rnn try build rnn next word prediction  follow next character prediction example  tutorial  github  colab  runtime 1min    example  input shape  31417  batchsize  sequencelength nbfeature  hide size define  1312  nlayer  batchsize hiddendim  follow example except batchsize 1  also  input sequence pad since  m use batchsize 1  run train   method get error first training datum example  runtimeerror  expect hide size  1  25  12   get  1  1  12   25 sequence length   seem pytorch ask give sequence length dimension hide layer example code follow not case code work fine  wrong  additionally  colab use  runtime 1min  ,2 main difference example code use code could not compute  1 batchfirst  true pass rnn initiate model 2 target preprocessing differ input preprocessing  use sparse one  hot vector encode word sparse vector work input  target encode index word one  hot instead whole one  hot vector thank  erip help debug ,get hidden size error pytorch rnn try build rnn next word prediction  follow next character prediction example  tutorial  github  colab  runtime 1min    example  input shape  31417  batchsize  sequencelength nbfeature  hide size define  1312  nlayer  batchsize hiddendim  follow example except batchsize 1  also  input sequence pad since  m use batchsize 1  run train   method get error first training datum example  runtimeerror  expect hide size  1  25  12   get  1  1  12   25 sequence length   seem pytorch ask give sequence length dimension hide layer example code follow not case code work fine  wrong  additionally  colab use  runtime 1min   2 main difference example code use code could not compute  1 batchfirst  true pass rnn initiate model 2 target preprocessing differ input preprocessing  use sparse one  hot vector encode word sparse vector work input  target encode index word one  hot instead whole one  hot vector thank  erip help debug ,Implementation Issues
troubleshooting PyTorch and Hugging Face&#39;s Pre-trained deBerta Model on Windows 11 with an RTX 3070 GPU,"I'm running Windows 11 on my desktop, which has an NVIDIA RTX 3070 GPU. I'm working on an NLP task using Hugging Face's AutoModelForSequenceClassification and I want to utilize my GPU for training. I've successfully installed PyTorch 1.9.0 with CUDA 11.1 and confirmed that CUDA is available on my system. However, when I try to run my script, I encounter an ImportError suggesting that I need to install the accelerate library. When I attempt to do so, it not only fails but also replaces my existing PyTorch 1.9.0 installation with version 2.1.0. I've tried various commands like pip install transformers[torch] and pip install accelerate -U , but they all result in the same issue. The error message also indicates that accelerate requires at least PyTorch 1.10, but I can't find a compatible CUDA version for my RTX 3070. Does anyone have a solution for running a proper installation of transformers + torch + accelerate?","['pytorch', 'nlp', 'gpu', 'huggingface-transformers', 'huggingface-tokenizers']",1,I have installed PyTorch on multiple combinations (OS+Hardware). I have installed PyTorch successfully using those commands (in a virtual environment): %pip install --upgrade transformers %pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 %pip install accelerate (will take latest at the time of writing @0.23.0) %pip install evaluate datasets These helped me kick-start any of the projects which require HuggingFace. I hope it helps you.,2023-10-01 10:15:10,2023-10-03 12:01:28,520,https://stackoverflow.com/questions/77210041/troubleshooting-pytorch-and-hugging-faces-pre-trained-deberta-model-on-windows,"troubleshooting PyTorch and Hugging Face&#39;s Pre-trained deBerta Model on Windows 11 with an RTX 3070 GPU I'm running Windows 11 on my desktop, which has an NVIDIA RTX 3070 GPU. I'm working on an NLP task using Hugging Face's AutoModelForSequenceClassification and I want to utilize my GPU for training. I've successfully installed PyTorch 1.9.0 with CUDA 11.1 and confirmed that CUDA is available on my system. However, when I try to run my script, I encounter an ImportError suggesting that I need to install the accelerate library. When I attempt to do so, it not only fails but also replaces my existing PyTorch 1.9.0 installation with version 2.1.0. I've tried various commands like pip install transformers[torch] and pip install accelerate -U , but they all result in the same issue. The error message also indicates that accelerate requires at least PyTorch 1.10, but I can't find a compatible CUDA version for my RTX 3070. Does anyone have a solution for running a proper installation of transformers + torch + accelerate?",troubleshoot pytorch hugging face   39  pre  train deberta model windows 11 rtx 3070 gpu  m run windows 11 desktop  nvidia rtx 3070 gpu   m work nlp task use hugging face s automodelforsequenceclassification want utilize gpu training   ve successfully instal pytorch 190 cuda 111 confirm cuda available system  however  try run script  encounter importerror suggest need install accelerate library  attempt  fails also replace exist pytorch 190 installation version 210   ve try various command like pip install transformer  torch  pip install accelerate u  result issue  error message also indicate accelerate require least pytorch 110  can not find compatible cuda version rtx 3070  anyone solution run proper installation transformer  torch  accelerate ,instal pytorch multiple combination  oshardware   instal pytorch successfully use command  virtual environment    pip install  upgrade transformer  pip install  upgrade torch torchvision torchaudio  index  url   downloadpytorchorg  whl  cu117  pip install accelerate  take late time write  0230   pip install evaluate dataset help kick  start project require huggingface  hope help ,troubleshoot pytorch hugging face   39  pre  train deberta model windows 11 rtx 3070 gpu  m run windows 11 desktop  nvidia rtx 3070 gpu   m work nlp task use hugging face s automodelforsequenceclassification want utilize gpu training   ve successfully instal pytorch 190 cuda 111 confirm cuda available system  however  try run script  encounter importerror suggest need install accelerate library  attempt  fails also replace exist pytorch 190 installation version 210   ve try various command like pip install transformer  torch  pip install accelerate u  result issue  error message also indicate accelerate require least pytorch 110  can not find compatible cuda version rtx 3070  anyone solution run proper installation transformer  torch  accelerate  instal pytorch multiple combination  oshardware   instal pytorch successfully use command  virtual environment    pip install  upgrade transformer  pip install  upgrade torch torchvision torchaudio  index  url   downloadpytorchorg  whl  cu117  pip install accelerate  take late time write  0230   pip install evaluate dataset help kick  start project require huggingface  hope help ,Implementation Issues
Is there a way to measure the margins of a pdf file using image processing in python?,"Up to now, I have used different types of packages in Python to extract information from PDF files, but I couldn't find a way to measure the margins of the PDF document. As an example, I wanted to get the size of the margins on the four sides of the pdf page. I have used pdfcrop package to remove the white spaces(margins) but couldn't measure the margin size.","['python', 'pdf', 'image-processing', 'nlp', 'html-content-extraction']",1,"Example using PyMuPDF import fitz # package PyMuPDF doc = fitz.open(""input.pdf"") for page in doc: rect = fitz.EMPTY_RECT() # prepare an empty rectangle blocks = page.get_text(""blocks"") # extract text and image blocks for b in blocks: rect |= b[:4] # join rect with the block bbox print(f""Page {page.number} margins:"") print(f"" Top {rect.y0}"") print(f"" Left {rect.x0}"") print(f"" Right {page.rect.width - rect.x1}"") print(f"" Bottom {page.rect.height - rect.y1}"") If someone is interested to extend the above to all possible page content - nd not just text or image, use the even faster method page.get_bboxlog() . This won't read or extract anything, instead only returns covered rectangles. The above loop would then be: ... for page in doc: rect = fitz.EMPTY_RECT() # prepare an empty rectangle for item in page.get_bboxlog(): b = item[1] # the bbox of the covered area rect |= b[:4] # join rect with the block bbox print(f""Page {page.number} margins:"") print(f"" Top {rect.y0}"") print(f"" Left {rect.x0}"") print(f"" Right {page.rect.width - rect.x1}"") print(f"" Bottom {page.rect.height - rect.y1}"")",2023-09-26 18:57:48,2023-09-26 19:53:34,677,https://stackoverflow.com/questions/77182740/is-there-a-way-to-measure-the-margins-of-a-pdf-file-using-image-processing-in-py,"Is there a way to measure the margins of a pdf file using image processing in python? Up to now, I have used different types of packages in Python to extract information from PDF files, but I couldn't find a way to measure the margins of the PDF document. As an example, I wanted to get the size of the margins on the four sides of the pdf page. I have used pdfcrop package to remove the white spaces(margins) but couldn't measure the margin size.",way measure margin pdf file use image processing python   use different type package python extract information pdf file  could not find way measure margin pdf document  example  want get size margin four side pdf page  use pdfcrop package remove white space  margin  could not measure margin size ,example use pymupdf import fitz  package pymupdf doc  fitzopen    inputpdf   page doc  rect  fitz  emptyrect    prepare empty rectangle block  pagegettext    block    extract text image block b block  rect  b   4   join rect block bbox print  f  page  pagenumber  margin    print  f  top  recty0    print  f  left  rectx0    print  f  right  pagerectwidth  rectx1    print  f  bottom  pagerectheight  recty1    someone interested extend possible page content  nd text image  use even fast method pagegetbboxlog    will not read extract anything  instead return cover rectangle  loop would   page doc  rect  fitz  emptyrect    prepare empty rectangle item pagegetbboxlog    b  item  1   bbox cover area rect  b   4   join rect block bbox print  f  page  pagenumber  margin    print  f  top  recty0    print  f  left  rectx0    print  f  right  pagerectwidth  rectx1    print  f  bottom  pagerectheight  recty1   ,way measure margin pdf file use image processing python   use different type package python extract information pdf file  could not find way measure margin pdf document  example  want get size margin four side pdf page  use pdfcrop package remove white space  margin  could not measure margin size  example use pymupdf import fitz  package pymupdf doc  fitzopen    inputpdf   page doc  rect  fitz  emptyrect    prepare empty rectangle block  pagegettext    block    extract text image block b block  rect  b   4   join rect block bbox print  f  page  pagenumber  margin    print  f  top  recty0    print  f  left  rectx0    print  f  right  pagerectwidth  rectx1    print  f  bottom  pagerectheight  recty1    someone interested extend possible page content  nd text image  use even fast method pagegetbboxlog    will not read extract anything  instead return cover rectangle  loop would   page doc  rect  fitz  emptyrect    prepare empty rectangle item pagegetbboxlog    b  item  1   bbox cover area rect  b   4   join rect block bbox print  f  page  pagenumber  margin    print  f  top  recty0    print  f  left  rectx0    print  f  right  pagerectwidth  rectx1    print  f  bottom  pagerectheight  recty1   ,Task-Specific Queries
Question about data_collator throwing a key error in Hugging face,"I am trying to use data_collator function in hugging face using this code: datasets = dataset.train_test_split(test_size=0.1) train_dataset = datasets[""train""] val_dataset = datasets[""test""] print(type(train_dataset)) def data_collator(data): # Initialize lists to store pixel values and input ids pixel_values_list = [] input_ids_list = [] # Iterate over each sample in the data for item in data: pixel_values_list.append(torch.tensor(item[""pixel_values""])) input_ids_list.append(torch.tensor(item[""input_ids""])) return { ""pixel_values"": torch.stack(pixel_values_list), ""labels"": torch.stack(input_ids_list) } the train_data has 5 keys including input_ids . However, when I print(data[0]) inside the data_collator function, I only see 1 key, which is giving an error when running the trainer: Traceback (most recent call last): File ""caption-code.py"", line 134, in <module> trainer.train() File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\transformers\trainer.py"", line 1321, in train ignore_keys_for_eval=ignore_keys_for_eval, File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\transformers\trainer.py"", line 1528, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\dataloader.py"", line 521, in __next__ data = self._next_data() File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\dataloader.py"", line 561, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\_utils\fetch.py"", line 52, in fetch return self.collate_fn(data) File ""caption-code.py"", line 102, in data_collator input_ids_list.append(item[""input_ids""]) KeyError: 'input_ids' I am using the trainer function as follows: training_args = Seq2SeqTrainingArguments( predict_with_generate=True, evaluation_strategy=""epoch"", per_device_train_batch_size=4, per_device_eval_batch_size=4, output_dir=""C:/Users/moham/Desktop/Euler/output"", logging_dir=""./logs"", logging_steps=10, save_steps=10, eval_steps=10, warmup_steps=10, max_steps=100, # adjust as needed overwrite_output_dir=True, save_total_limit=3, ) trainer = Seq2SeqTrainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_exact_match ) trainer.train()","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'huggingface-tokenizers']",1,"The actual issue is in your Seq2SeqTrainingArguments which is leading the error in your data_collator() . Reason: The .trainer() is by default removing any unknown columns (not present in the model's forward method) from your data when you are providing a custom data_collator() . As a result even though each sample in your train_dataset has all the keys, when you send that to data_collator() , the .trainer() automatically removes the unknown columns. Solution: You need to include an argument in your training arguments like the following: training_args = Seq2SeqTrainingArguments( predict_with_generate=True, remove_unused_columns=False, ...) The remove_unused_columns=False, would prevent the default behaviour and you'd get the entire data in data_collator() . This issue would be useful for further reference.",2023-09-26 17:42:46,2023-09-26 18:58:37,1010,https://stackoverflow.com/questions/77182311/question-about-data-collator-throwing-a-key-error-in-hugging-face,"Question about data_collator throwing a key error in Hugging face I am trying to use data_collator function in hugging face using this code: datasets = dataset.train_test_split(test_size=0.1) train_dataset = datasets[""train""] val_dataset = datasets[""test""] print(type(train_dataset)) def data_collator(data): # Initialize lists to store pixel values and input ids pixel_values_list = [] input_ids_list = [] # Iterate over each sample in the data for item in data: pixel_values_list.append(torch.tensor(item[""pixel_values""])) input_ids_list.append(torch.tensor(item[""input_ids""])) return { ""pixel_values"": torch.stack(pixel_values_list), ""labels"": torch.stack(input_ids_list) } the train_data has 5 keys including input_ids . However, when I print(data[0]) inside the data_collator function, I only see 1 key, which is giving an error when running the trainer: Traceback (most recent call last): File ""caption-code.py"", line 134, in <module> trainer.train() File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\transformers\trainer.py"", line 1321, in train ignore_keys_for_eval=ignore_keys_for_eval, File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\transformers\trainer.py"", line 1528, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\dataloader.py"", line 521, in __next__ data = self._next_data() File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\dataloader.py"", line 561, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- packages\torch\utils\data\_utils\fetch.py"", line 52, in fetch return self.collate_fn(data) File ""caption-code.py"", line 102, in data_collator input_ids_list.append(item[""input_ids""]) KeyError: 'input_ids' I am using the trainer function as follows: training_args = Seq2SeqTrainingArguments( predict_with_generate=True, evaluation_strategy=""epoch"", per_device_train_batch_size=4, per_device_eval_batch_size=4, output_dir=""C:/Users/moham/Desktop/Euler/output"", logging_dir=""./logs"", logging_steps=10, save_steps=10, eval_steps=10, warmup_steps=10, max_steps=100, # adjust as needed overwrite_output_dir=True, save_total_limit=3, ) trainer = Seq2SeqTrainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_exact_match ) trainer.train()",question datacollator throw key error hugging face try use datacollator function hug face use code  dataset  datasettraintestsplit  testsize01  traindataset  dataset    train   valdataset  dataset    test   print  type  traindataset   def datacollator  datum    initialize list store pixel value input id pixelvalueslist    inputidslist     iterate sample datum item datum  pixelvalueslistappend  torchtensor  item    pixelvalue     inputidslistappend  torchtensor  item    inputids     return    pixelvalue   torchstack  pixelvalueslist     label   torchstack  inputidslist   traindata 5 key include inputids  however  print  datum  0   inside datacollator function  see 1 key  give error run trainer  traceback  recent call last   file   captioncodepy   line 134   module  trainertrain   file   c  usersmohamanaconda3envstransformerlibsite packagestransformerstrainerpy   line 1321  train ignorekeysforeval  ignorekeysforeval  file   c  usersmohamanaconda3envstransformerlibsite packagestransformerstrainerpy   line 1528   innertrainingloop step  input enumerate  epochiterator   file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatadataloaderpy   line 521    next   datum  selfnextdata   file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatadataloaderpy   line 561   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatautilsfetchpy   line 52  fetch return selfcollatefn  datum  file   captioncodepy   line 102  datacollator inputidslistappend  item    inputids    keyerror   inputids  use trainer function follow  trainingargs  seq2seqtrainingargument  predictwithgenerate  true  evaluationstrategy  epoch   perdevicetrainbatchsize4  perdeviceevalbatchsize4  outputdir  c  users  moham  desktop  euler  output   loggingdir  logs   loggingsteps10  savesteps10  evalsteps10  warmupsteps10  maxsteps100   adjust need overwriteoutputdir  true  savetotallimit3   trainer  seq2seqtrainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  tokenizer  tokenizer  computemetric  computeexactmatch  trainertrain  ,actual issue seq2seqtrainingargument lead error datacollator    reason  trainer   default remove unknown column  present model s forward method  datum provide custom datacollator    result even though sample traindataset key  send datacollator    trainer   automatically remove unknown column  solution  need include argument training argument like follow  trainingargs  seq2seqtrainingargument  predictwithgenerate  true  removeunusedcolumns  false    removeunusedcolumns  false  would prevent default behaviour would get entire datum datacollator    issue would useful reference ,question datacollator throw key error hugging face try use datacollator function hug face use code  dataset  datasettraintestsplit  testsize01  traindataset  dataset    train   valdataset  dataset    test   print  type  traindataset   def datacollator  datum    initialize list store pixel value input id pixelvalueslist    inputidslist     iterate sample datum item datum  pixelvalueslistappend  torchtensor  item    pixelvalue     inputidslistappend  torchtensor  item    inputids     return    pixelvalue   torchstack  pixelvalueslist     label   torchstack  inputidslist   traindata 5 key include inputids  however  print  datum  0   inside datacollator function  see 1 key  give error run trainer  traceback  recent call last   file   captioncodepy   line 134   module  trainertrain   file   c  usersmohamanaconda3envstransformerlibsite packagestransformerstrainerpy   line 1321  train ignorekeysforeval  ignorekeysforeval  file   c  usersmohamanaconda3envstransformerlibsite packagestransformerstrainerpy   line 1528   innertrainingloop step  input enumerate  epochiterator   file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatadataloaderpy   line 521    next   datum  selfnextdata   file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatadataloaderpy   line 561   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   c  usersmohamanaconda3envstransformerlibsite packagestorchutilsdatautilsfetchpy   line 52  fetch return selfcollatefn  datum  file   captioncodepy   line 102  datacollator inputidslistappend  item    inputids    keyerror   inputids  use trainer function follow  trainingargs  seq2seqtrainingargument  predictwithgenerate  true  evaluationstrategy  epoch   perdevicetrainbatchsize4  perdeviceevalbatchsize4  outputdir  c  users  moham  desktop  euler  output   loggingdir  logs   loggingsteps10  savesteps10  evalsteps10  warmupsteps10  maxsteps100   adjust need overwriteoutputdir  true  savetotallimit3   trainer  seq2seqtrainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  tokenizer  tokenizer  computemetric  computeexactmatch  trainertrain   actual issue seq2seqtrainingargument lead error datacollator    reason  trainer   default remove unknown column  present model s forward method  datum provide custom datacollator    result even though sample traindataset key  send datacollator    trainer   automatically remove unknown column  solution  need include argument training argument like follow  trainingargs  seq2seqtrainingargument  predictwithgenerate  true  removeunusedcolumns  false    removeunusedcolumns  false  would prevent default behaviour would get entire datum datacollator    issue would useful reference ,Implementation Issues
How to Set Safety Parameters for Text Generation Model in Google Cloud Vertex AI?,"I am working on a research project where I need to summarize news articles using the Google Palm2 Text Generation Model. I have encountered an issue with certain news articles in my dataset where I'm getting empty responses along with safety attributes that block the output. Here is the code I'm using: from vertexai.language_models import TextGenerationModel parameters = { # default values 'max_output_tokens': 256, 'temperature': 0.0, 'top_p': 1.0, 'top_k': 40, } prompt = ""..."" model = TextGenerationModel.from_pretrained('text-bison@001') response = model.predict( prompt, **parameters, ) The following is an example prediction: Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None) The issue seems to be related to safety parameters preventing the model from generating a summary for certain news articles. I've been trying to find documentation on how to configure these safety parameters using the Python API, but I could not locate the relevant information. Could someone please provide guidance on how to set the safety parameters for the TextGenerationModel? Any help or pointers to documentation would be greatly appreciated. Thank you!","['python', 'machine-learning', 'nlp', 'google-cloud-vertex-ai', 'large-language-model']",1,"I'm not sure about Vertex AI but you can set the safety_settings of the PaLM model (from google generative AI) by the following: import google.generativeai as palm completion = palm.generate_text( model=model, prompt=prompt, safety_settings=[ { ""category"": safety_types.HarmCategory.HARM_CATEGORY_DEROGATORY, ""threshold"": safety_types.HarmBlockThreshold.BLOCK_NONE, }, { ""category"": safety_types.HarmCategory.HARM_CATEGORY_VIOLENCE, ""threshold"": safety_types.HarmBlockThreshold.BLOCK_NONE, }, ] ) You should checkout this guide to get complete details of the safety catalogue and how to set threshold for each category as there are multiple categories and different threshold levels. NOTE: To use the PaLM API from generative AI, you'd need to install it first via: pip install -q google-generativeai and then set an API key which you'll get from here : import google.generativeai as palm palm.configure(api_key='YOUR_API_KEY') and then to access the same text-bison-001 model: models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods] model = models[0].name # use this model on the first code snippet print(model) # prints 'models/text-bison-001'",2023-09-26 07:58:38,2023-09-26 15:31:06,2558,https://stackoverflow.com/questions/77178058/how-to-set-safety-parameters-for-text-generation-model-in-google-cloud-vertex-ai,"How to Set Safety Parameters for Text Generation Model in Google Cloud Vertex AI? I am working on a research project where I need to summarize news articles using the Google Palm2 Text Generation Model. I have encountered an issue with certain news articles in my dataset where I'm getting empty responses along with safety attributes that block the output. Here is the code I'm using: from vertexai.language_models import TextGenerationModel parameters = { # default values 'max_output_tokens': 256, 'temperature': 0.0, 'top_p': 1.0, 'top_k': 40, } prompt = ""..."" model = TextGenerationModel.from_pretrained('text-bison@001') response = model.predict( prompt, **parameters, ) The following is an example prediction: Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None) The issue seems to be related to safety parameters preventing the model from generating a summary for certain news articles. I've been trying to find documentation on how to configure these safety parameters using the Python API, but I could not locate the relevant information. Could someone please provide guidance on how to set the safety parameters for the TextGenerationModel? Any help or pointers to documentation would be greatly appreciated. Thank you!",set safety parameters text generation model google cloud vertex ai  work research project need summarize news article use google palm2 text generation model  encounter issue certain news article dataset  m get empty response along safety attribute block output  code  m use  vertexailanguagemodel import textgenerationmodel parameter    default value  maxoutputtoken   256   temperature   00   topp   10   topk   40   prompt      model  textgenerationmodelfrompretraine   text  bison  001   response  modelpredict  prompt    parameter   follow example prediction  prediction  predictions    content       citationmetadata   none   safetyattribute     block   true   error    2530      deployedmodelid   modelversionid   modelresourcename   explanation  none  issue seem related safety parameter prevent model generating summary certain news article   ve try find documentation configure safety parameter use python api  could locate relevant information  could someone please provide guidance set safety parameter textgenerationmodel  help pointer documentation would greatly appreciate  thank , m sure vertex ai set safetysettings palm model  google generative ai  follow  import googlegenerativeai palm completion  palmgeneratetext  model  model  prompt  prompt  safetysettings     category   safetytype  harmcategory  harmcategoryderogatory    threshold   safetytypes  harmblockthreshold  blocknone       category   safetytype  harmcategory  harmcategoryviolence    threshold   safetytypes  harmblockthreshold  blocknone      checkout guide get complete detail safety catalogue set threshold category multiple category different threshold level  note  use palm api generative ai  would need install first via  pip install q google  generativeai set api key will get  import googlegenerativeai palm palmconfigure  apikeyyourapikey   access text  bison001 model  model   palmlistmodel    generatetext  msupportedgenerationmethods  model  model  0  name  use model first code snippet print  model   print  model  text  bison001 ,set safety parameters text generation model google cloud vertex ai  work research project need summarize news article use google palm2 text generation model  encounter issue certain news article dataset  m get empty response along safety attribute block output  code  m use  vertexailanguagemodel import textgenerationmodel parameter    default value  maxoutputtoken   256   temperature   00   topp   10   topk   40   prompt      model  textgenerationmodelfrompretraine   text  bison  001   response  modelpredict  prompt    parameter   follow example prediction  prediction  predictions    content       citationmetadata   none   safetyattribute     block   true   error    2530      deployedmodelid   modelversionid   modelresourcename   explanation  none  issue seem related safety parameter prevent model generating summary certain news article   ve try find documentation configure safety parameter use python api  could locate relevant information  could someone please provide guidance set safety parameter textgenerationmodel  help pointer documentation would greatly appreciate  thank   m sure vertex ai set safetysettings palm model  google generative ai  follow  import googlegenerativeai palm completion  palmgeneratetext  model  model  prompt  prompt  safetysettings     category   safetytype  harmcategory  harmcategoryderogatory    threshold   safetytypes  harmblockthreshold  blocknone       category   safetytype  harmcategory  harmcategoryviolence    threshold   safetytypes  harmblockthreshold  blocknone      checkout guide get complete detail safety catalogue set threshold category multiple category different threshold level  note  use palm api generative ai  would need install first via  pip install q google  generativeai set api key will get  import googlegenerativeai palm palmconfigure  apikeyyourapikey   access text  bison001 model  model   palmlistmodel    generatetext  msupportedgenerationmethods  model  model  0  name  use model first code snippet print  model   print  model  text  bison001 ,Library/Tool-Based Queries
Getting `TypeError: issubclass() arg 1 must be a class` when trying to load `nlp = spacy.load(&quot;en_ner_bc5cdr_md&quot;)`,"I'm using spaCy to analyze a large set of medical text for commentary about diagnoses, which was working fine when I left it last week. Now when I try to load the scispaCy library en_ner_bc5cdr_md I am getting the type error TypeError: issubclass() arg 1 must be a class I used this exact library to analyze the same text last week and the only thing that has changed is that I've restarted my computer. I'm running an anaconda distribution with python 3.8.8 on OSX Version 13.5.2 (22G91) Any ideas what's going on? This is the code, it never gets past importing the model. import spacy import scispacy import pandas as pd text = """"""The patient was diagnosed with pneumonia last year. He has a history of asthma and hypertension. His COPD symptoms have worsened over the last 2 months. The patient also suffers from migraines."""""" nlp = spacy.load(""en_ner_bc5cdr_md"") doc = nlp(text) labels = [] counts = [] for ent in doc.ents: if ent.label_ == 'DISEASE': if ent.text not in labels: labels.append(ent.text) counts.append(1) else: idx = labels.index(ent.text) counts[idx] += 1 df = pd.DataFrame({'Diagnosis': labels, 'Count': counts}) print(df) If you are able to run on you machine please let me know the parameters. You will also need to run - !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz","['python', 'tensorflow', 'machine-learning', 'nlp', 'spacy']",1,"I was able to load en_ner_bc5cdr_md with spacy == 3.0.9 and python 3.10.12 . You can try the following steps: !pip install spacy==3.0.9 ('!' is for notebook cell) !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz And then try: import spacy nlp = spacy.load(""en_ner_bc5cdr_md"") Sorry, currently I don't have any local machine but I tested it on google colab and it's working fine on my end.",2023-09-25 15:37:10,2023-09-25 18:15:32,863,https://stackoverflow.com/questions/77174018/getting-typeerror-issubclass-arg-1-must-be-a-class-when-trying-to-load-nlp,"Getting `TypeError: issubclass() arg 1 must be a class` when trying to load `nlp = spacy.load(&quot;en_ner_bc5cdr_md&quot;)` I'm using spaCy to analyze a large set of medical text for commentary about diagnoses, which was working fine when I left it last week. Now when I try to load the scispaCy library en_ner_bc5cdr_md I am getting the type error TypeError: issubclass() arg 1 must be a class I used this exact library to analyze the same text last week and the only thing that has changed is that I've restarted my computer. I'm running an anaconda distribution with python 3.8.8 on OSX Version 13.5.2 (22G91) Any ideas what's going on? This is the code, it never gets past importing the model. import spacy import scispacy import pandas as pd text = """"""The patient was diagnosed with pneumonia last year. He has a history of asthma and hypertension. His COPD symptoms have worsened over the last 2 months. The patient also suffers from migraines."""""" nlp = spacy.load(""en_ner_bc5cdr_md"") doc = nlp(text) labels = [] counts = [] for ent in doc.ents: if ent.label_ == 'DISEASE': if ent.text not in labels: labels.append(ent.text) counts.append(1) else: idx = labels.index(ent.text) counts[idx] += 1 df = pd.DataFrame({'Diagnosis': labels, 'Count': counts}) print(df) If you are able to run on you machine please let me know the parameters. You will also need to run - !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz",get  typeerror  issubclass   arg 1 must class  try load  nlp  spacyload   quot  ennerbc5cdrmd  quot     m use spacy analyze large set medical text commentary diagnose  work fine leave last week  try load scispacy library ennerbc5cdrmd get type error typeerror  issubclass   arg 1 must class use exact library analyze text last week thing change  ve restart computer   m run anaconda distribution python 388 osx version 1352  22g91  idea be go  code  never get past import model  import spacy import scispacy import panda pd text      patient diagnose pneumonia last year  history asthma hypertension  copd symptom worsen last 2 month  patient also suffer migraine     nlp  spacyload    ennerbc5cdrmd   doc  nlp  text  label    count    ent docent  entlabel     disease   enttext label  labelsappend  enttext  countsappend  1  else  idx  labelsindex  enttext  count  idx    1 df  pd  dataframe    diagnosis   label   count   count   print  df  able run machine please let know parameter  also need run   pip install   s3  us  west2amazonawscom  ai2  s2  scispacy  release  v040  ennerbc5cdrmd040targz,able load ennerbc5cdrmd spacy   309 python 31012  try follow step   pip install spacy309     notebook cell   pip install   s3  us  west2amazonawscom  ai2  s2  scispacy  release  v040  ennerbc5cdrmd040targz try  import spacy nlp  spacyload    ennerbc5cdrmd   sorry  currently not local machine test google colab s work fine end ,get  typeerror  issubclass   arg 1 must class  try load  nlp  spacyload   quot  ennerbc5cdrmd  quot     m use spacy analyze large set medical text commentary diagnose  work fine leave last week  try load scispacy library ennerbc5cdrmd get type error typeerror  issubclass   arg 1 must class use exact library analyze text last week thing change  ve restart computer   m run anaconda distribution python 388 osx version 1352  22g91  idea be go  code  never get past import model  import spacy import scispacy import panda pd text      patient diagnose pneumonia last year  history asthma hypertension  copd symptom worsen last 2 month  patient also suffer migraine     nlp  spacyload    ennerbc5cdrmd   doc  nlp  text  label    count    ent docent  entlabel     disease   enttext label  labelsappend  enttext  countsappend  1  else  idx  labelsindex  enttext  count  idx    1 df  pd  dataframe    diagnosis   label   count   count   print  df  able run machine please let know parameter  also need run   pip install   s3  us  west2amazonawscom  ai2  s2  scispacy  release  v040  ennerbc5cdrmd040targz able load ennerbc5cdrmd spacy   309 python 31012  try follow step   pip install spacy309     notebook cell   pip install   s3  us  west2amazonawscom  ai2  s2  scispacy  release  v040  ennerbc5cdrmd040targz try  import spacy nlp  spacyload    ennerbc5cdrmd   sorry  currently not local machine test google colab s work fine end ,Task-Specific Queries
HuggingFacePipeline and Langchain,"this is my current code: from langchain.llms import HuggingFacePipeline from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig from langchain import PromptTemplate, LLMChain import torch model_id = ""../models/openbuddy-llama2-34b-v11.1-bf16"" tokenizer = AutoTokenizer.from_pretrained(model_id) nf4_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=False, max_memory=24000 ) model = AutoModelForCausalLM.from_pretrained( model_id, quantization_config=nf4_config, ) pipe = pipeline( ""text-generation"", model=model, tokenizer=tokenizer, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, ) hf = HuggingFacePipeline(pipeline=pipe) template = """"""SYSTEM: You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User. Always answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. You like to use emojis. You can speak fluently in many languages, for example: English, Chinese. You cannot access the internet, but you have vast knowledge, cutoff: 2021-09. You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI. USER: {question} ASSISTANT: """""" prompt = PromptTemplate(template=template, input_variables=[""question""]) llm_chain = LLMChain(prompt=prompt, llm=hf) print(llm_chain.run(""Who is the Pope ?"")) This is not putting out something. If i change the last row into: print(hf(""Who is the Pope ?"")) Everything is working fine, but i need to use a chain. Im running on windows wsl ubuntu. enter code here","['nlp', 'langchain', 'huggingface']",3,I think the model was broke. I have changed the model and everything works fine. Vg,2023-09-21 18:46:02,2023-09-23 11:25:26,3019,https://stackoverflow.com/questions/77152888/huggingfacepipeline-and-langchain,"HuggingFacePipeline and Langchain this is my current code: from langchain.llms import HuggingFacePipeline from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig from langchain import PromptTemplate, LLMChain import torch model_id = ""../models/openbuddy-llama2-34b-v11.1-bf16"" tokenizer = AutoTokenizer.from_pretrained(model_id) nf4_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=False, max_memory=24000 ) model = AutoModelForCausalLM.from_pretrained( model_id, quantization_config=nf4_config, ) pipe = pipeline( ""text-generation"", model=model, tokenizer=tokenizer, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, ) hf = HuggingFacePipeline(pipeline=pipe) template = """"""SYSTEM: You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User. Always answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. You like to use emojis. You can speak fluently in many languages, for example: English, Chinese. You cannot access the internet, but you have vast knowledge, cutoff: 2021-09. You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI. USER: {question} ASSISTANT: """""" prompt = PromptTemplate(template=template, input_variables=[""question""]) llm_chain = LLMChain(prompt=prompt, llm=hf) print(llm_chain.run(""Who is the Pope ?"")) This is not putting out something. If i change the last row into: print(hf(""Who is the Pope ?"")) Everything is working fine, but i need to use a chain. Im running on windows wsl ubuntu. enter code here",huggingfacepipeline langchain current code  langchainllm import huggingfacepipeline transformer import automodelforcausallm  autotokenizer  pipeline  bitsandbytesconfig langchain import prompttemplate  llmchain import torch modelid     models  openbuddy  llama2  34b  v111  bf16  tokenizer  autotokenizerfrompretraine  modelid  nf4config  bitsandbytesconfig  loadin4bit  true  bnb4bitcomputedtype  torchbfloat16  bnb4bitquanttypenf4   bnb4bitusedoublequant  false  maxmemory24000  model  automodelforcausallmfrompretraine  modelid  quantizationconfig  nf4config   pipe  pipeline    text  generation   model  model  tokenizer  tokenizer  maxnewtokens100  eostokenid  tokenizereostokenid  padtokenid  tokenizereostokenid   hf  huggingfacepipeline  pipeline  pipe  template      system  helpful  respectful honest intp  t ai assistant name buddy  talk human user  always answer helpfully logically possible  safe  answer include harmful  political  religious  unethical  racist  sexist  toxic  dangerous  illegal content  please ensure response socially unbiased positive nature  question make sense  factually coherent  explain instead answer something correct  not know answer question  please not share false information  like use emoji  speak fluently many language  example  english  chinese  access internet  vast knowledge  cutoff  2021  09  train openbuddy team     openbuddyai    githubcom  openbuddy  openbuddy   base llama falcon transformer model  relate gpt openai  user   question  assistant      prompt  prompttemplate  template  template  inputvariables    question    llmchain  llmchain  prompt  prompt  llm  hf  print  llmchainrun    pope     put something  change last row  print  hf    pope     everything work fine  need use chain  i m run window wsl ubuntu  enter code,think model break  change model everything work fine  vg,huggingfacepipeline langchain current code  langchainllm import huggingfacepipeline transformer import automodelforcausallm  autotokenizer  pipeline  bitsandbytesconfig langchain import prompttemplate  llmchain import torch modelid     models  openbuddy  llama2  34b  v111  bf16  tokenizer  autotokenizerfrompretraine  modelid  nf4config  bitsandbytesconfig  loadin4bit  true  bnb4bitcomputedtype  torchbfloat16  bnb4bitquanttypenf4   bnb4bitusedoublequant  false  maxmemory24000  model  automodelforcausallmfrompretraine  modelid  quantizationconfig  nf4config   pipe  pipeline    text  generation   model  model  tokenizer  tokenizer  maxnewtokens100  eostokenid  tokenizereostokenid  padtokenid  tokenizereostokenid   hf  huggingfacepipeline  pipeline  pipe  template      system  helpful  respectful honest intp  t ai assistant name buddy  talk human user  always answer helpfully logically possible  safe  answer include harmful  political  religious  unethical  racist  sexist  toxic  dangerous  illegal content  please ensure response socially unbiased positive nature  question make sense  factually coherent  explain instead answer something correct  not know answer question  please not share false information  like use emoji  speak fluently many language  example  english  chinese  access internet  vast knowledge  cutoff  2021  09  train openbuddy team     openbuddyai    githubcom  openbuddy  openbuddy   base llama falcon transformer model  relate gpt openai  user   question  assistant      prompt  prompttemplate  template  template  inputvariables    question    llmchain  llmchain  prompt  prompt  llm  hf  print  llmchainrun    pope     put something  change last row  print  hf    pope     everything work fine  need use chain  i m run window wsl ubuntu  enter code think model break  change model everything work fine  vg,Library/Tool-Based Queries
"Python API usage for coreference, semantic graph and NERC","Intro Hi, I have been using freeling for a few months now to extract triplets. So far I have succeded in doing so by using the dependency tree and the full parse tree, but I am trying to add NERC. My work so far I checked the tutorial for python, but I couldn't find anything beyond depdency parsing. So I went through the class list (since the same classes should be available for python and c++) but it is not very clear how to retrieve the named entities and after checking the output of the analyzer sampler I have a few questions about the performance of the NER module. Problems So what I'm asking if anyone can help me with is the following: Doubt about entities: Using the example ""Sobre la mesa María ve y coge una manzana, un sombrero, una llave y dos paraguas rojo."" I realized that working with capitalized words and lowercase produce different results, but by making it all lowercase the entity recognition stops recognizing ""maría"" as a person. Is there are workaround for this or am I going in the wrong direction? The main problem is that ""maría"" not recognized as a named entity (which i need it to be by the way) results in ""maría"" not being the subject of the sentence anymore. Im using: neclass = pyfreeling.ner(lpath + ""/nerc/ner/ner-ab-rich.dat"") How to retrieve named entities: Kind of a follow up of the previous question, how do I get the named entities? I couldn't find any code related to this and the semantic graph i obtain holds 0 entities. Any comments and suggestions are welcomed, thanks in advance.","['python', 'nlp', 'named-entity-recognition', 'freeling']",1,"Well aparently there are 3 NERC modules, one rule-based and two ML-based. All of them use capitalization as a feature, and since both models are trained on standard text, all NEs seen in training are capitalized. Therefore lowercase named entities are not likely to be recognized. About the retrieval it seems that the get_label() from the nodes can provide this info if a word (or multiword) has a pos-tag starting with ""NP"", then it means it was recognized by the NERC module. This is based on freelings authors own explanation which you can find here",2023-09-21 08:42:53,2023-09-22 08:20:48,50,https://stackoverflow.com/questions/77148560/python-api-usage-for-coreference-semantic-graph-and-nerc,"Python API usage for coreference, semantic graph and NERC Intro Hi, I have been using freeling for a few months now to extract triplets. So far I have succeded in doing so by using the dependency tree and the full parse tree, but I am trying to add NERC. My work so far I checked the tutorial for python, but I couldn't find anything beyond depdency parsing. So I went through the class list (since the same classes should be available for python and c++) but it is not very clear how to retrieve the named entities and after checking the output of the analyzer sampler I have a few questions about the performance of the NER module. Problems So what I'm asking if anyone can help me with is the following: Doubt about entities: Using the example ""Sobre la mesa María ve y coge una manzana, un sombrero, una llave y dos paraguas rojo."" I realized that working with capitalized words and lowercase produce different results, but by making it all lowercase the entity recognition stops recognizing ""maría"" as a person. Is there are workaround for this or am I going in the wrong direction? The main problem is that ""maría"" not recognized as a named entity (which i need it to be by the way) results in ""maría"" not being the subject of the sentence anymore. Im using: neclass = pyfreeling.ner(lpath + ""/nerc/ner/ner-ab-rich.dat"") How to retrieve named entities: Kind of a follow up of the previous question, how do I get the named entities? I couldn't find any code related to this and the semantic graph i obtain holds 0 entities. Any comments and suggestions are welcomed, thanks in advance.",python api usage coreference  semantic graph nerc intro hi  use freele month extract triplet  far succede use dependency tree full parse tree  try add nerc  work far check tutorial python  could not find anything beyond depdency parsing  go class list  since class available python c  clear retrieve name entity check output analyzer sampler question performance ner module  problem  m ask anyone help follow  doubt entity  use example   sobre la mesa mara coge una manzana  un sombrero  una llave do paraguas rojo   realize working capitalize word lowercase produce different result  make lowercase entity recognition stop recognize   mara  person  workaround go wrong direction  main problem   mara  recognize name entity  need way  result   mara  subject sentence anymore  i m use  neclass  pyfreelingner  lpath    nerc  ner  ner  ab  richdat   retrieve name entity  kind follow previous question  get name entity  could not find code relate semantic graph obtain hold 0 entity  comment suggestion welcome  thank advance ,well aparently 3 nerc module  one rule  base two ml  base  use capitalization feature  since model train standard text  nes see training capitalize  therefore lowercase name entity likely recognize  retrieval seem getlabel   node provide info word  multiword  pos  tag start   np   mean recognize nerc module  base freeling author explanation find,python api usage coreference  semantic graph nerc intro hi  use freele month extract triplet  far succede use dependency tree full parse tree  try add nerc  work far check tutorial python  could not find anything beyond depdency parsing  go class list  since class available python c  clear retrieve name entity check output analyzer sampler question performance ner module  problem  m ask anyone help follow  doubt entity  use example   sobre la mesa mara coge una manzana  un sombrero  una llave do paraguas rojo   realize working capitalize word lowercase produce different result  make lowercase entity recognition stop recognize   mara  person  workaround go wrong direction  main problem   mara  recognize name entity  need way  result   mara  subject sentence anymore  i m use  neclass  pyfreelingner  lpath    nerc  ner  ner  ab  richdat   retrieve name entity  kind follow previous question  get name entity  could not find code relate semantic graph obtain hold 0 entity  comment suggestion welcome  thank advance  well aparently 3 nerc module  one rule  base two ml  base  use capitalization feature  since model train standard text  nes see training capitalize  therefore lowercase name entity likely recognize  retrieval seem getlabel   node provide info word  multiword  pos  tag start   np   mean recognize nerc module  base freeling author explanation find,Library/Tool-Based Queries
looking for an efficient way to split columns in a text in pandas,"I have pandas dataframe and want to split the text column in such a way that each row has just two words. when splitting, I need to maintain the order so that I can combine them together based on line . Is there efficient way to do this. I can do list comprehension but was looking at more efficient way. Thanks df = pd.DataFrame({'col1':[22,23,44], 'col2': ['rr','gg','xx'], 'text': ['this is a sample text', 'this is another one','third example is a longer text']})","['python', 'pandas', 'nlp']",3,"Using str.findall , explode , and groupby.cumcount : out = (df.assign(text=df['text'].str.findall(r'(\S+(?:\s+\S+)?)')) .explode('text') .assign(line=lambda d: d.groupby(level=0).cumcount()) ) Regexes variant to handle any number of words: N=2 out = (df.assign(text=df['text'].str.findall(fr'((?:\S+\s+){{,{N-1}}}(?:\S+))\s*')) .explode('text') .assign(line=lambda d: d.groupby(level=0).cumcount()) ) Alternative with itertools ' batched recipe: from itertools import islice def batched(iterable, n): ""Batch data into tuples of length n. The last batch may be shorter."" # batched('ABCDEFG', 3) --> ABC DEF G if n < 1: raise ValueError('n must be at least one') it = iter(iterable) while batch := tuple(islice(it, n)): yield batch N = 2 out = (df.assign(text=df['text'].map(lambda x: list(map(' '.join, batched(x.split(), N))))) .explode('text') .assign(line=lambda d: d.groupby(level=0).cumcount()) ) Output: col1 col2 text line 0 22 rr this is 0 0 22 rr a sample 1 0 22 rr text 2 1 23 gg this is 0 1 23 gg another one 1 2 44 xx third example 0 2 44 xx is a 1 2 44 xx longer text 2 Example output with N=4 : col1 col2 text line 0 22 rr this is a sample 0 0 22 rr text 1 1 23 gg this is another one 0 2 44 xx third example is a 0 2 44 xx longer text 1",2023-09-20 19:14:38,2023-09-20 19:30:54,64,https://stackoverflow.com/questions/77145360/looking-for-an-efficient-way-to-split-columns-in-a-text-in-pandas,"looking for an efficient way to split columns in a text in pandas I have pandas dataframe and want to split the text column in such a way that each row has just two words. when splitting, I need to maintain the order so that I can combine them together based on line . Is there efficient way to do this. I can do list comprehension but was looking at more efficient way. Thanks df = pd.DataFrame({'col1':[22,23,44], 'col2': ['rr','gg','xx'], 'text': ['this is a sample text', 'this is another one','third example is a longer text']})",look efficient way split column text panda panda dataframe want split text column way row two word  splitting  need maintain order combine together base line  efficient way  list comprehension look efficient way  thank df  pd  dataframe    col1    222344    col2     rr    gg    xx     text     this sample text    this another one    third example long text    ,use strfindall  explode  groupbycumcount    dfassign  text  df   text   strfindall  r   s    ss       explode   text   assign  line  lambda  dgroupby  level0  cumcount     regexe variant handle number word  n2   dfassign  text  df   text   strfindall  fr      ss      n1       s   s     explode   text   assign  line  lambda  dgroupby  level0  cumcount     alternative itertool  batch recipe  itertool import islice def batch  iterable  n     batch data tuple length n last batch may shorter    batch   abcdefg   3    abc def g n  1  raise valueerror   n must least one    iter  iterable  batch   tuple  islice   n    yield batch n  2   dfassign  text  df   text   map  lambda x  list  map    join  batch  xsplit    n      explode   text   assign  line  lambda  dgroupby  level0  cumcount     output  col1 col2 text line 0 22 rr 0 0 22 rr sample 1 0 22 rr text 2 1 23 gg 0 1 23 gg another one 1 2 44 xx third example 0 2 44 xx 1 2 44 xx long text 2 example output n4  col1 col2 text line 0 22 rr sample 0 0 22 rr text 1 1 23 gg another one 0 2 44 xx third example 0 2 44 xx long text 1,look efficient way split column text panda panda dataframe want split text column way row two word  splitting  need maintain order combine together base line  efficient way  list comprehension look efficient way  thank df  pd  dataframe    col1    222344    col2     rr    gg    xx     text     this sample text    this another one    third example long text     use strfindall  explode  groupbycumcount    dfassign  text  df   text   strfindall  r   s    ss       explode   text   assign  line  lambda  dgroupby  level0  cumcount     regexe variant handle number word  n2   dfassign  text  df   text   strfindall  fr      ss      n1       s   s     explode   text   assign  line  lambda  dgroupby  level0  cumcount     alternative itertool  batch recipe  itertool import islice def batch  iterable  n     batch data tuple length n last batch may shorter    batch   abcdefg   3    abc def g n  1  raise valueerror   n must least one    iter  iterable  batch   tuple  islice   n    yield batch n  2   dfassign  text  df   text   map  lambda x  list  map    join  batch  xsplit    n      explode   text   assign  line  lambda  dgroupby  level0  cumcount     output  col1 col2 text line 0 22 rr 0 0 22 rr sample 1 0 22 rr text 2 1 23 gg 0 1 23 gg another one 1 2 44 xx third example 0 2 44 xx 1 2 44 xx long text 2 example output n4  col1 col2 text line 0 22 rr sample 0 0 22 rr text 1 1 23 gg another one 0 2 44 xx third example 0 2 44 xx long text 1,Library/Tool-Based Queries
How to download the Spacy en_core_web_sm model?,"I ran the command , pip install spacy and after that worked fine i ran another command python -m spacy download en_core_web_sm but I am receiving Timeout errors in my terminal saying sock.connect(sa) TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 714, in urlopen httplib_response = self._make_request( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 403, in _make_request self._validate_conn(conn) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 1053, in _validate_conn conn.connect() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connection.py"", line 363, in connect self.sock = conn = self._new_conn() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connection.py"", line 179, in _new_conn raise ConnectTimeoutError( urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)') During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\adapters.py"", line 486, in send resp = conn.urlopen( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 798, in urlopen retries = retries.increment( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\util\retry.py"", line 592, in increment raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)')) During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main return _run_code(code, main_globals, None, File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in run_code exec(code, run_globals) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy_ main .py"", line 4, in setup_cli() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli_util.py"", line 92, in setup_cli command(prog_name=COMMAND) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1157, in call return self.main(*args, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\core.py"", line 778, in main return _main( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\core.py"", line 216, in _main rv = self.invoke(ctx) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1434, in invoke return ctx.invoke(self.callback, **ctx.params) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 783, in invoke return __callback(*args, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\main.py"", line 683, in wrapper return callback(**use_params) # type: ignore File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 36, in download_cli download(model, direct, sdist, *ctx.args) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 70, in download compatibility = get_compatibility() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 94, in get_compatibility r = requests.get(about. compatibility ) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\api.py"", line 73, in get return request(""get"", url, params=params, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\api.py"", line 59, in request return session.request(method=method, url=url, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\sessions.py"", line 589, in request resp = self.send(prep, **send_kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\sessions.py"", line 703, in send r = adapter.send(request, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\adapters.py"", line 507, in send raise ConnectTimeout(e, request=request) requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)')) please help me resolve this issue i was expeciting the command to execute properly so that i can proceed with my NLP tasks","['machine-learning', 'debugging', 'nlp', 'spacy']",1,"The error is clear it states: Connection to raw.githubusercontent.com timed out. Probably your side or the server isn't available for some reason. You can download using pip, or download locally and installing. For manual installation: wget https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl pip install en_core_web_sm-3.6.0-py3-none-any.whl Or you can use the requirements.txt syntax spacy==3.6.0 en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl Then pip install -r requirements.txt",2023-09-17 18:39:45,2023-09-17 20:25:12,350,https://stackoverflow.com/questions/77123153/how-to-download-the-spacy-en-core-web-sm-model,"How to download the Spacy en_core_web_sm model? I ran the command , pip install spacy and after that worked fine i ran another command python -m spacy download en_core_web_sm but I am receiving Timeout errors in my terminal saying sock.connect(sa) TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 714, in urlopen httplib_response = self._make_request( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 403, in _make_request self._validate_conn(conn) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 1053, in _validate_conn conn.connect() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connection.py"", line 363, in connect self.sock = conn = self._new_conn() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connection.py"", line 179, in _new_conn raise ConnectTimeoutError( urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)') During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\adapters.py"", line 486, in send resp = conn.urlopen( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\connectionpool.py"", line 798, in urlopen retries = retries.increment( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\urllib3\util\retry.py"", line 592, in increment raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)')) During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main return _run_code(code, main_globals, None, File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in run_code exec(code, run_globals) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy_ main .py"", line 4, in setup_cli() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli_util.py"", line 92, in setup_cli command(prog_name=COMMAND) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1157, in call return self.main(*args, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\core.py"", line 778, in main return _main( File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\core.py"", line 216, in _main rv = self.invoke(ctx) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 1434, in invoke return ctx.invoke(self.callback, **ctx.params) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\click\core.py"", line 783, in invoke return __callback(*args, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\typer\main.py"", line 683, in wrapper return callback(**use_params) # type: ignore File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 36, in download_cli download(model, direct, sdist, *ctx.args) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 70, in download compatibility = get_compatibility() File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\cli\download.py"", line 94, in get_compatibility r = requests.get(about. compatibility ) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\api.py"", line 73, in get return request(""get"", url, params=params, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\api.py"", line 59, in request return session.request(method=method, url=url, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\sessions.py"", line 589, in request resp = self.send(prep, **send_kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\sessions.py"", line 703, in send r = adapter.send(request, **kwargs) File ""C:\Users\adars\AppData\Local\Programs\Python\Python310\lib\site-packages\requests\adapters.py"", line 507, in send raise ConnectTimeout(e, request=request) requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D0099E3610>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)')) please help me resolve this issue i was expeciting the command to execute properly so that i can proceed with my NLP tasks",download spacy encorewebsm model  ran command  pip install spacy work fine run another command python m spacy download encorewebsm receive timeout error terminal say sockconnect  sa  timeouterror   winerror 10060  connection attempt fail connect party properly respond period time  establish connection fail connect host fail respond handling exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 714  urlopen   selfmakerequest  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 403   makerequest selfvalidateconn  conn  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 1053   validateconn connconnect   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpy   line 363  connect selfsock  conn  selfnewconn   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpy   line 179   newconn raise connecttimeouterror  urllib3exception  connecttimeouterror    urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none    handle exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsadapterspy   line 486  send resp  connurlopen  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 798  urlopen retrie  retriesincrement  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3utilretrypy   line 592  increment raise maxretryerror   pool  url  error responseerror  cause   urllib3exception  maxretryerror    hostrawgithubusercontentcom   port443   max retrie exceed url  explosion  spacy  model  master  compatibilityjson  caused connecttimeouterror   urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none     handle exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310librunpypy   line 196   runmoduleasmain return  runcode  code  mainglobal  none  file   c  usersadarsappdatalocalprogramspythonpython310librunpypy   line 86  runcode exec  code  runglobal  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacy  main py   line 4  setupcli   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacycliutilpy   line 92  setupcli command  progname  command  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1157  call return selfmain   args    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypercorepy   line 778  main return  main  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypercorepy   line 216   main rv  selfinvoke  ctx  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1688  invoke return  processresult  subctxcommandinvoke  subctx   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1434  invoke return ctxinvoke  selfcallback    ctxparam  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 783  invoke return   callback   args    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypermainpy   line 683  wrapper return callback    useparams   type  ignore file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 36  downloadcli download  model  direct  sdist   ctxargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 70  download compatibility  getcompatibility   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 94  getcompatibility r  requestsget   compatibility  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsapipy   line 73  get return request    get   url  param  param    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsapipy   line 59  request return sessionrequest  method  method  url  url    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestssessionspy   line 589  request resp  selfsend  prep    sendkwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestssessionspy   line 703  send r  adaptersend  request    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsadapterspy   line 507  send raise connecttimeout  e  request  request  requestsexception  connecttimeout    hostrawgithubusercontentcom   port443   max retrie exceed url  explosion  spacy  model  master  compatibilityjson  caused connecttimeouterror   urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none     please help resolve issue expecite command execute properly proceed nlp task,error clear state  connection rawgithubusercontentcom time  probably side server not available reason  download use pip  download locally instal  manual installation  wget   githubcom  explosion  spacy  model  release  download  encorewebsm360  encorewebsm360  py3  none  anywhl pip install encorewebsm360py3noneanywhl use requirementstxt syntax spacy360 encorewebsm    githubcom  explosion  spacy  model  release  download  encorewebsm360  encorewebsm360  py3  none  anywhl pip install r requirementstxt,download spacy encorewebsm model  ran command  pip install spacy work fine run another command python m spacy download encorewebsm receive timeout error terminal say sockconnect  sa  timeouterror   winerror 10060  connection attempt fail connect party properly respond period time  establish connection fail connect host fail respond handling exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 714  urlopen   selfmakerequest  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 403   makerequest selfvalidateconn  conn  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 1053   validateconn connconnect   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpy   line 363  connect selfsock  conn  selfnewconn   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpy   line 179   newconn raise connecttimeouterror  urllib3exception  connecttimeouterror    urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none    handle exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsadapterspy   line 486  send resp  connurlopen  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3connectionpoolpy   line 798  urlopen retrie  retriesincrement  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesurllib3utilretrypy   line 592  increment raise maxretryerror   pool  url  error responseerror  cause   urllib3exception  maxretryerror    hostrawgithubusercontentcom   port443   max retrie exceed url  explosion  spacy  model  master  compatibilityjson  caused connecttimeouterror   urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none     handle exception  another exception occur  traceback  recent call last   file   c  usersadarsappdatalocalprogramspythonpython310librunpypy   line 196   runmoduleasmain return  runcode  code  mainglobal  none  file   c  usersadarsappdatalocalprogramspythonpython310librunpypy   line 86  runcode exec  code  runglobal  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacy  main py   line 4  setupcli   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacycliutilpy   line 92  setupcli command  progname  command  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1157  call return selfmain   args    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypercorepy   line 778  main return  main  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypercorepy   line 216   main rv  selfinvoke  ctx  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1688  invoke return  processresult  subctxcommandinvoke  subctx   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 1434  invoke return ctxinvoke  selfcallback    ctxparam  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesclickcorepy   line 783  invoke return   callback   args    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagestypermainpy   line 683  wrapper return callback    useparams   type  ignore file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 36  downloadcli download  model  direct  sdist   ctxargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 70  download compatibility  getcompatibility   file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesspacyclidownloadpy   line 94  getcompatibility r  requestsget   compatibility  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsapipy   line 73  get return request    get   url  param  param    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsapipy   line 59  request return sessionrequest  method  method  url  url    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestssessionspy   line 589  request resp  selfsend  prep    sendkwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestssessionspy   line 703  send r  adaptersend  request    kwargs  file   c  usersadarsappdatalocalprogramspythonpython310libsite  packagesrequestsadapterspy   line 507  send raise connecttimeout  e  request  request  requestsexception  connecttimeout    hostrawgithubusercontentcom   port443   max retrie exceed url  explosion  spacy  model  master  compatibilityjson  caused connecttimeouterror   urllib3connection   object 0x000001d0099e3610    connection rawgithubusercontentcom time   connect timeout  none     please help resolve issue expecite command execute properly proceed nlp task error clear state  connection rawgithubusercontentcom time  probably side server not available reason  download use pip  download locally instal  manual installation  wget   githubcom  explosion  spacy  model  release  download  encorewebsm360  encorewebsm360  py3  none  anywhl pip install encorewebsm360py3noneanywhl use requirementstxt syntax spacy360 encorewebsm    githubcom  explosion  spacy  model  release  download  encorewebsm360  encorewebsm360  py3  none  anywhl pip install r requirementstxt,Implementation Issues
What is the correct approach to evaluate Huggingface models on the masked language modeling task?,"I'm trying to test how well different models are doing on the masked language modeling task . Given a prompt prompt = ""The Milky Way is a [MASK] galaxy"" I'm trying to get an output for the masked token from different models. The issue is that when I load a model for the masked language modeling task: from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained('bert-base-cased') model.eval() tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', truncation=True) I get the warning: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight'] From the huggingface forum they had a similar question, but only referred to parts of these weights: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] . The answer there was: >>> It tells you that by loading the bert-base-uncased checkpoint in the BertForMaskedLM architecture, you're dropping two weights: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']. These are the weights used for next-sentence prediction, which aren't necessary for Masked Language Modeling. If you're only interested in doing masked language modeling, then you can safely disregard this warning. But it seems like when I'm loading it I'm also removing ['bert.pooler.dense.bias', 'bert.pooler.dense.weight'] , which I could not find out if dropping these is going to result in different performance for the masked language modeling without fine-tuning the model. If I just load the model with model = AutoModel.from_pretrained('bert-base-cased') I get no error, but then I cannot use it to predict the mask (as far as I know). So is the correct approach to load the model with AutoModelForMaskedLM as I've done and just ignore the warning (assuming that it makes no difference because the dropped weights are useless for the masked task), or is there a different approach?","['machine-learning', 'nlp', 'huggingface-transformers', 'huggingface']",1,"As pointed out in the linked post , this is a warning to indicate that those weights are not used. This is raised since you're loading a model that has its pooler weights initialised ( bert-base-cased ), but aren't used by a *MaskedLM model. The bert.pooler.* weights are typically used for classification tasks (such as BertForSequenceClassification ). For bert-base-cased , the model was also trained on the Next Sentence Prediction (NSP) task, so the pooler weights are also trained. As pointed out in this GitHub comment : After passing a sentence through the model, the representation corresponding to the first token in the output is used for fine-tuning on tasks like SQuAD and GLUE. So the pooler layer does precisely that, applies a linear transformation over the representation of the first token. The linear transformation is trained while using the Next Sentence Prediction (NSP) strategy. In fact, whenever initialising the model with the Next Sentence Prediction task, the warning isn't raised: from transformers import AutoModelForMaskedLM, AutoModelForNextSentencePrediction, AutoModelForPreTraining # raises a warning AutoModelForMaskedLM.from_pretrained(""bert-base-cased"") # doesn't raise a warning AutoModelForNextSentencePrediction.from_pretrained(""bert-base-cased"") # doesn't raise a warning; initialised with both MLM & NSP AutoModelForPreTraining.from_pretrained(""bert-base-cased"") Since you're interested in Masked Language Modelling (MLM), you can disregard the warning since this isn't used for this task. For the masked language modelling task you should initialise using AutoModelForMaskedLM , since this includes the appropriate head to predict the masked token. This forum post has further details about the differences in initialisations.",2023-09-16 03:08:05,2023-09-16 14:11:03,419,https://stackoverflow.com/questions/77116207/what-is-the-correct-approach-to-evaluate-huggingface-models-on-the-masked-langua,"What is the correct approach to evaluate Huggingface models on the masked language modeling task? I'm trying to test how well different models are doing on the masked language modeling task . Given a prompt prompt = ""The Milky Way is a [MASK] galaxy"" I'm trying to get an output for the masked token from different models. The issue is that when I load a model for the masked language modeling task: from transformers import AutoModelForMaskedLM, AutoTokenizer model = AutoModelForMaskedLM.from_pretrained('bert-base-cased') model.eval() tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', truncation=True) I get the warning: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight'] From the huggingface forum they had a similar question, but only referred to parts of these weights: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] . The answer there was: >>> It tells you that by loading the bert-base-uncased checkpoint in the BertForMaskedLM architecture, you're dropping two weights: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']. These are the weights used for next-sentence prediction, which aren't necessary for Masked Language Modeling. If you're only interested in doing masked language modeling, then you can safely disregard this warning. But it seems like when I'm loading it I'm also removing ['bert.pooler.dense.bias', 'bert.pooler.dense.weight'] , which I could not find out if dropping these is going to result in different performance for the masked language modeling without fine-tuning the model. If I just load the model with model = AutoModel.from_pretrained('bert-base-cased') I get no error, but then I cannot use it to predict the mask (as far as I know). So is the correct approach to load the model with AutoModelForMaskedLM as I've done and just ignore the warning (assuming that it makes no difference because the dropped weights are useless for the masked task), or is there a different approach?",correct approach evaluate huggingface model mask language modeling task   m try test well different model mask language modeling task  give prompt prompt    milky way  mask  galaxy   m try get output mask token different model  issue load model mask language modeling task  transformer import automodelformaskedlm  autotokenizer model  automodelformaskedlmfrompretraine   bert  base  case   modeleval   tokenizer  autotokenizerfrompretraine   bert  base  case   truncation  true  get warn  weights model checkpoint bert  base  uncased use initialize bertformaskedlm    clsseqrelationshipbias    bertpoolerdensebia    clsseqrelationshipweight    bertpoolerdenseweight   huggingface forum similar question  refer part weight    clsseqrelationshipweight    clsseqrelationshipbias    answer     tell load bert  base  uncase checkpoint bertformaskedlm architecture  be drop two weight    clsseqrelationshipweight    clsseqrelationshipbias    weight use next  sentence prediction  not necessary masked language modeling  be interested mask language modeling  safely disregard warning  seem like  m load  m also remove   bertpoolerdensebia    bertpoolerdenseweight    could find dropping go result different performance mask language modeling without fine  tuning model  load model model  automodelfrompretraine   bert  base  case   get error  use predict mask  far know   correct approach load model automodelformaskedlm  ve do ignore warn  assume make difference drop weight useless mask task   different approach ,point link post  warning indicate weight use  raise since be load model pooler weight initialise  bert  base  cased   not use  maskedlm model  bertpooler   weight typically use classification task  bertforsequenceclassification   bert  base  case  model also train next sentence prediction  nsp  task  pooler weight also train  point github comment  pass sentence model  representation correspond first token output use fine  tuning task like squad glue  pooler layer precisely  apply linear transformation representation first token  linear transformation train use next sentence prediction  nsp  strategy  fact  whenever initialise model next sentence prediction task  warning not raise  transformer import automodelformaskedlm  automodelfornextsentenceprediction  automodelforpretraine  raise warning automodelformaskedlmfrompretraine    bert  base  case    not raise warning automodelfornextsentencepredictionfrompretraine    bert  base  case    not raise warning  initialise mlm  nsp automodelforpretrainingfrompretrained    bert  base  case   since be interested masked language modelling  mlm   disregard warning since not use task  mask language modelling task initialise use automodelformaskedlm  since include appropriate head predict mask token  forum post detail difference initialisation ,correct approach evaluate huggingface model mask language modeling task   m try test well different model mask language modeling task  give prompt prompt    milky way  mask  galaxy   m try get output mask token different model  issue load model mask language modeling task  transformer import automodelformaskedlm  autotokenizer model  automodelformaskedlmfrompretraine   bert  base  case   modeleval   tokenizer  autotokenizerfrompretraine   bert  base  case   truncation  true  get warn  weights model checkpoint bert  base  uncased use initialize bertformaskedlm    clsseqrelationshipbias    bertpoolerdensebia    clsseqrelationshipweight    bertpoolerdenseweight   huggingface forum similar question  refer part weight    clsseqrelationshipweight    clsseqrelationshipbias    answer     tell load bert  base  uncase checkpoint bertformaskedlm architecture  be drop two weight    clsseqrelationshipweight    clsseqrelationshipbias    weight use next  sentence prediction  not necessary masked language modeling  be interested mask language modeling  safely disregard warning  seem like  m load  m also remove   bertpoolerdensebia    bertpoolerdenseweight    could find dropping go result different performance mask language modeling without fine  tuning model  load model model  automodelfrompretraine   bert  base  case   get error  use predict mask  far know   correct approach load model automodelformaskedlm  ve do ignore warn  assume make difference drop weight useless mask task   different approach  point link post  warning indicate weight use  raise since be load model pooler weight initialise  bert  base  cased   not use  maskedlm model  bertpooler   weight typically use classification task  bertforsequenceclassification   bert  base  case  model also train next sentence prediction  nsp  task  pooler weight also train  point github comment  pass sentence model  representation correspond first token output use fine  tuning task like squad glue  pooler layer precisely  apply linear transformation representation first token  linear transformation train use next sentence prediction  nsp  strategy  fact  whenever initialise model next sentence prediction task  warning not raise  transformer import automodelformaskedlm  automodelfornextsentenceprediction  automodelforpretraine  raise warning automodelformaskedlmfrompretraine    bert  base  case    not raise warning automodelfornextsentencepredictionfrompretraine    bert  base  case    not raise warning  initialise mlm  nsp automodelforpretrainingfrompretrained    bert  base  case   since be interested masked language modelling  mlm   disregard warning since not use task  mask language modelling task initialise use automodelformaskedlm  since include appropriate head predict mask token  forum post detail difference initialisation ,Implementation Issues
AIML chatbot not learning from aiml file,"I am making a voice-assistant for food ordering in fast-food restaurants. For dialog management tasks I am making aiml chatbot. This is my std-startup.xml code ''' <!-- Category is an atomic AIML unit --> <category> <!-- Pattern to match in user input --> <!-- If user enters ""LOAD AIML B"" --> <pattern>LOAD AIML B</pattern> <!-- Template is the response to the pattern --> <!-- This learn an aiml file --> <template> <learn>output.aiml</learn> <!-- You can add more aiml files here --> <!--<learn>more_aiml.aiml</learn>--> </template> </category> BELOW IS MAIN.PY CODE import aiml import os kernel = aiml.Kernel() if os.path.isfile(""bot_brain.brn""): kernel.bootstrap(brainFile = ""bot_brain.brn"") else: kernel.bootstrap(learnFiles = ""std-startup.xml"", commands = ""load aiml b"") kernel.saveBrain(""bot_brain.brn"") kernel now ready for use while True: print (kernel.respond(input(""Enter your message >> ""))) ''' After entering a msg which is from output.aiml itself the bot says the msg does not exists PLease can someone tell how to fix it. The bot should generate a response as given in output.aiml file Here is another example with category and here is bot's repsonse","['python', 'nlp', 'chatbot', 'aiml']",1,Some interpreters need the patterns to be in upper case. Try this instead: <category> <pattern>THANK YOU</pattern> <template> Bye. </template> </category>,2023-09-15 15:07:40,2023-09-18 12:05:22,74,https://stackoverflow.com/questions/77113484/aiml-chatbot-not-learning-from-aiml-file,"AIML chatbot not learning from aiml file I am making a voice-assistant for food ordering in fast-food restaurants. For dialog management tasks I am making aiml chatbot. This is my std-startup.xml code ''' <!-- Category is an atomic AIML unit --> <category> <!-- Pattern to match in user input --> <!-- If user enters ""LOAD AIML B"" --> <pattern>LOAD AIML B</pattern> <!-- Template is the response to the pattern --> <!-- This learn an aiml file --> <template> <learn>output.aiml</learn> <!-- You can add more aiml files here --> <!--<learn>more_aiml.aiml</learn>--> </template> </category> BELOW IS MAIN.PY CODE import aiml import os kernel = aiml.Kernel() if os.path.isfile(""bot_brain.brn""): kernel.bootstrap(brainFile = ""bot_brain.brn"") else: kernel.bootstrap(learnFiles = ""std-startup.xml"", commands = ""load aiml b"") kernel.saveBrain(""bot_brain.brn"") kernel now ready for use while True: print (kernel.respond(input(""Enter your message >> ""))) ''' After entering a msg which is from output.aiml itself the bot says the msg does not exists PLease can someone tell how to fix it. The bot should generate a response as given in output.aiml file Here is another example with category and here is bot's repsonse",aiml chatbot learn aiml file make voice  assistant food order fast  food restaurant  dialog management task make aiml chatbot  stdstartupxml code       category atomic aiml unit    category     pattern match user input      user enter   load aiml b     pattern  load aiml b  pattern     template response pattern      learn aiml file    template   learn  outputaiml  learn     add aiml file       learn  moreaimlaiml  learn     template   category  mainpy code import aiml import os kernel  aiml  kernel   ospathisfile    botbrainbrn    kernelbootstrap  brainfile    botbrainbrn   else  kernelbootstrap  learnfile    stdstartupxml   command    load aiml b   kernelsavebrain    botbrainbrn   kernel ready use true  print  kernelrespond  input    enter message           enter msg outputaiml bot say msg exist please someone tell fix  bot generate response give outputaiml file another example category bot s repsonse,interpreter need pattern upper case  try instead   category   pattern  thank  pattern   template  bye   template   category ,aiml chatbot learn aiml file make voice  assistant food order fast  food restaurant  dialog management task make aiml chatbot  stdstartupxml code       category atomic aiml unit    category     pattern match user input      user enter   load aiml b     pattern  load aiml b  pattern     template response pattern      learn aiml file    template   learn  outputaiml  learn     add aiml file       learn  moreaimlaiml  learn     template   category  mainpy code import aiml import os kernel  aiml  kernel   ospathisfile    botbrainbrn    kernelbootstrap  brainfile    botbrainbrn   else  kernelbootstrap  learnfile    stdstartupxml   command    load aiml b   kernelsavebrain    botbrainbrn   kernel ready use true  print  kernelrespond  input    enter message           enter msg outputaiml bot say msg exist please someone tell fix  bot generate response give outputaiml file another example category bot s repsonse interpreter need pattern upper case  try instead   category   pattern  thank  pattern   template  bye   template   category ,Task-Specific Queries
Use NLP (NLTK) to identify groups of phrases in a python dataframe,"I have a table containing diagnosis information for a large group of patients. I would like to determine what the most common groupings of those diagnoses are, for example is it ""Bloaty Head Syndrome"" and ""Slack Tongue"", or ""Broken Wind"", ""Chronic Nosehair"" and ""Corrugated Ankles""... or some other combination. Data is structured like so: import pandas as pd import numpy as np # List of ids ids = ['id1', 'id2', 'id3','id4','id5'] # List of sample sentences diagnosis = [""Broken Wind"",""Chronic Nosehair"",""Corrugated Ankles"",""Discrete Itching""] # Create dataframe df = pd.DataFrame({'id': ids}) # Generate list of sentences for each id df['diagnosis'] = df['id'].apply(lambda x: np.random.choice(diagnosis, 5).tolist()) # Explode into separate rows df = df.explode('diagnosis') print(df) For example if both id2 and id5 contain ""Broken Wind"" and Chronic Nosehair"" that would be 2 of that combination. If id1, id3 and id4 contain ""Chronic Nosehair"",""Corrugated Ankles"", and ""Discrete Itching"" that would be 3 of that combination. With the goal of determining which combination is most common. I'm wondering is there an nlp library such as NLTK , or a method, that can be used to process data stored like this in a pandas dataframe? Most of what I have been able to find so far is geared toward sentiment analysis or analyzing single words as opposed to phrases...","['python', 'pandas', 'machine-learning', 'nlp', 'nltk']",1,"I would offer that what you are trying to do here is not necessarily an NLP problem, but a much more general frequent pattern mining problem which is typically seen in recommendation. You can find the most frequent diagnosis combinations of any size by using the fpgrowth algorithm in the mlxtend library and looking at the support for each symptom or combinations thereof: import pandas as pd from mlxtend.preprocessing import TransactionEncoder from mlxtend.frequent_patterns import fpgrowth # Create list of diagnoses for each patient x = df.groupby('id').apply(lambda x:list(x['diagnosis'])) # Encode to wide dataframe with column for each symptom te = TransactionEncoder() te_ary = te.fit(x).transform(x) te_df = pd.DataFrame(te_ary, columns=te.columns_) # Calculate most frequent diagnosis co-occurrences fp_df = fpgrowth(te_df, min_support=0.01, use_colnames=True) # Sort and show fp_df.sort_values(by='support', ascending=False) The resultant table is a list of tuples with the support , the percentage of ""transactions"" (here, patients) for which the combinations occur: | support | itemsets | | ------- | -------------------------------------------------------- | | 0.8 | {'Broken Wind'} | | 0.6 | {'Corrugated Ankles'} | | 0.6 | {'Chronic Nosehair'} | | 0.6 | {'Discrete Itching'} | | 0.6 | {'Corrugated Ankles', 'Broken Wind'} | | 0.4 | {'Chronic Nosehair', 'Broken Wind'} | | 0.4 | {'Discrete Itching', 'Chronic Nosehair'} | | 0.4 | {'Discrete Itching', 'Broken Wind'} | | 0.2 | {'Corrugated Ankles', 'Discrete Itching'} | | 0.2 | {'Discrete Itching', 'Corrugated Ankles', 'Broken Wind'} | | 0.2 | {'Corrugated Ankles', 'Chronic Nosehair'} | | 0.2 | {'Chronic Nosehair', 'Discrete Itching', 'Broken Wind'} | | 0.2 | {'Chronic Nosehair', 'Corrugated Ankles', 'Broken Wind'} |",2023-09-14 19:04:49,2023-09-15 13:47:19,333,https://stackoverflow.com/questions/77107744/use-nlp-nltk-to-identify-groups-of-phrases-in-a-python-dataframe,"Use NLP (NLTK) to identify groups of phrases in a python dataframe I have a table containing diagnosis information for a large group of patients. I would like to determine what the most common groupings of those diagnoses are, for example is it ""Bloaty Head Syndrome"" and ""Slack Tongue"", or ""Broken Wind"", ""Chronic Nosehair"" and ""Corrugated Ankles""... or some other combination. Data is structured like so: import pandas as pd import numpy as np # List of ids ids = ['id1', 'id2', 'id3','id4','id5'] # List of sample sentences diagnosis = [""Broken Wind"",""Chronic Nosehair"",""Corrugated Ankles"",""Discrete Itching""] # Create dataframe df = pd.DataFrame({'id': ids}) # Generate list of sentences for each id df['diagnosis'] = df['id'].apply(lambda x: np.random.choice(diagnosis, 5).tolist()) # Explode into separate rows df = df.explode('diagnosis') print(df) For example if both id2 and id5 contain ""Broken Wind"" and Chronic Nosehair"" that would be 2 of that combination. If id1, id3 and id4 contain ""Chronic Nosehair"",""Corrugated Ankles"", and ""Discrete Itching"" that would be 3 of that combination. With the goal of determining which combination is most common. I'm wondering is there an nlp library such as NLTK , or a method, that can be used to process data stored like this in a pandas dataframe? Most of what I have been able to find so far is geared toward sentiment analysis or analyzing single words as opposed to phrases...",use nlp  nltk  identify group phrase python dataframe table contain diagnosis information large group patient  would like determine common grouping diagnosis  example   bloaty head syndrome    slack tongue     broken wind     chronic nosehair    corrugated ankles   combination  datum structure like  import panda pd import numpy np  list id id    id1    id2    id3    id4    id5    list sample sentence diagnosis     broken wind    chronic nosehair    corrugated ankles    discrete itching    create dataframe df  pd  dataframe    i d   id    generate list sentence i d df   diagnosis    df   i d   apply  lambda x  nprandomchoice  diagnosis  5  tolist     explode separate row df  dfexplode   diagnosis   print  df  example id2 id5 contain   broken wind  chronic nosehair  would 2 combination  id1  id3 id4 contain   chronic nosehair    corrugated ankles     discrete itching  would 3 combination  goal determine combination common   m wonder nlp library nltk  method  used process datum store like panda dataframe  able find far gear toward sentiment analysis analyze single word oppose phrase ,would offer try necessarily nlp problem  much general frequent pattern mining problem typically see recommendation  find frequent diagnosis combination size use fpgrowth algorithm mlxtend library look support symptom combination thereof  import panda pd mlxtendpreprocessing import transactionencoder mlxtendfrequentpattern import fpgrowth  create list diagnose patient x  dfgroupby   i d   apply  lambda x  list  x   diagnosis      encode wide dataframe column symptom te  transactionencoder   teary  tefit  x  transform  x  tedf  pd  dataframe  teary  column  tecolumns    calculate frequent diagnosis co  occurrences fpdf  fpgrowth  tedf  minsupport001  usecolname  true   sort show fpdfsortvalue  bysupport   ascend  false  resultant table list tuple support  percentage   transaction    patient  combination occur   support  itemset                                      08    broken wind     06    corrugated ankles     06    chronic nosehair     06    discrete itching     06    corrugated ankles    broken wind     04    chronic nosehair    broken wind     04    discrete itching    chronic nosehair     04    discrete itching    broken wind     02    corrugated ankles    discrete itching     02    discrete itching    corrugated ankles    broken wind     02    corrugated ankles    chronic nosehair     02    chronic nosehair    discrete itching    broken wind     02    chronic nosehair    corrugated ankles    broken wind   ,use nlp  nltk  identify group phrase python dataframe table contain diagnosis information large group patient  would like determine common grouping diagnosis  example   bloaty head syndrome    slack tongue     broken wind     chronic nosehair    corrugated ankles   combination  datum structure like  import panda pd import numpy np  list id id    id1    id2    id3    id4    id5    list sample sentence diagnosis     broken wind    chronic nosehair    corrugated ankles    discrete itching    create dataframe df  pd  dataframe    i d   id    generate list sentence i d df   diagnosis    df   i d   apply  lambda x  nprandomchoice  diagnosis  5  tolist     explode separate row df  dfexplode   diagnosis   print  df  example id2 id5 contain   broken wind  chronic nosehair  would 2 combination  id1  id3 id4 contain   chronic nosehair    corrugated ankles     discrete itching  would 3 combination  goal determine combination common   m wonder nlp library nltk  method  used process datum store like panda dataframe  able find far gear toward sentiment analysis analyze single word oppose phrase  would offer try necessarily nlp problem  much general frequent pattern mining problem typically see recommendation  find frequent diagnosis combination size use fpgrowth algorithm mlxtend library look support symptom combination thereof  import panda pd mlxtendpreprocessing import transactionencoder mlxtendfrequentpattern import fpgrowth  create list diagnose patient x  dfgroupby   i d   apply  lambda x  list  x   diagnosis      encode wide dataframe column symptom te  transactionencoder   teary  tefit  x  transform  x  tedf  pd  dataframe  teary  column  tecolumns    calculate frequent diagnosis co  occurrences fpdf  fpgrowth  tedf  minsupport001  usecolname  true   sort show fpdfsortvalue  bysupport   ascend  false  resultant table list tuple support  percentage   transaction    patient  combination occur   support  itemset                                      08    broken wind     06    corrugated ankles     06    chronic nosehair     06    discrete itching     06    corrugated ankles    broken wind     04    chronic nosehair    broken wind     04    discrete itching    chronic nosehair     04    discrete itching    broken wind     02    corrugated ankles    discrete itching     02    discrete itching    corrugated ankles    broken wind     02    corrugated ankles    chronic nosehair     02    chronic nosehair    discrete itching    broken wind     02    chronic nosehair    corrugated ankles    broken wind   ,Library/Tool-Based Queries
How to use both gpus in kaggle for training in pytorch?,I was training a model in kaggle gpu. But as I can see only one GPU is working. I use the ordinary method for training like device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') model = model.to(device) How can I use both the gpus?,"['machine-learning', 'deep-learning', 'nlp', 'gpu', 'kaggle']",2,"Using multiple GPUs is specific to machine learning libraries. I stumbled upon the same problem while doing image segmentation in Pytorch. The solution is to use the module torch.nn.DataParallel() with the model. The given code can be changed as follows: device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device) here, the device_ids is the index of GPUs. Suppose if you have 4 GPUs then it would be device_ids = [0,1,2,3] or whatever the index it maybe. And the result of using both GPUs is here!. PS: This is my first post in the prestigious stack overflow, please do share your comments and views.",2023-09-13 04:56:24,2023-12-28 18:19:04,4054,https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch,How to use both gpus in kaggle for training in pytorch? I was training a model in kaggle gpu. But as I can see only one GPU is working. I use the ordinary method for training like device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') model = model.to(device) How can I use both the gpus?,use gpus kaggle training pytorch  training model kaggle gpu  see one gpu work  use ordinary method training like device  torchdevice   cuda   torchcudaisavailable   else torchdevice   cpu   model  modelto  device  use gpu ,use multiple gpu specific machine learn library  stumble upon problem image segmentation pytorch  solution use module torchnn  dataparallel   model  give code change follow  device  torchdevice   cuda   torchcudaisavailable   else torchdevice   cpu   model  torchnn  dataparallel  model  deviceid   01   to  device   deviceid index gpu  suppose 4 gpu would deviceid   0123  whatever index maybe  result use gpu   ps  first post prestigious stack overflow  please share comment view ,use gpus kaggle training pytorch  training model kaggle gpu  see one gpu work  use ordinary method training like device  torchdevice   cuda   torchcudaisavailable   else torchdevice   cpu   model  modelto  device  use gpu  use multiple gpu specific machine learn library  stumble upon problem image segmentation pytorch  solution use module torchnn  dataparallel   model  give code change follow  device  torchdevice   cuda   torchcudaisavailable   else torchdevice   cpu   model  torchnn  dataparallel  model  deviceid   01   to  device   deviceid index gpu  suppose 4 gpu would deviceid   0123  whatever index maybe  result use gpu   ps  first post prestigious stack overflow  please share comment view ,Library/Tool-Based Queries
How to get Enhanced++ dependency labels with a java command line in the terminal?,"I don't really know java, but I was just trying to use the documentation of the Stanford NLP parser to get the Enhanced++ dependency labels. This is the line I ran: java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,depparse"" -file input.txt And indeed I get an output. But I don't get the labels that I expect. For example, in the input.txt file there is a sentence ""The older couple is picnicking with wine"", and the dependency between picnicking and wine should be nmod, but instead it is obl:with. Another sentence is ""What do you call it?"", where I expect a dobj relationship between ""call"" and ""it"", but instead I get ""obj"". What should I fix to get the labels of the enhanced universal dependencies? (Also, do I really need to specify the options ""tokenize,ssplit,pos,lemma"" if I am interested only in ""depparse""?) Thank you.","['java', 'nlp', 'stanford-nlp']",1,"You actually are getting enhanced++ dependency labels. However, it looks like you are looking for something else or an older version. UD was somewhat revised between UDv1 and UDv2. One of the changes was to make oblique modifiers (PPs in English) of predicates into the relation obl rather than nmod , restricting nmod to modifiers of nominals. Hence, obl not nmod . And then part of being enhanced dependencies rather than basic dependencies is getting incorporation of the case or preposition in the label, so you get obl:with . Similarly, in UDv2, the label dobj was changed to simply obj . (And, yes, you do need to use all the annotators ""tokenize,ssplit,pos,lemma"", because they are needed preprocessing steps before dependency parsing.)",2023-09-13 00:12:52,2023-10-01 22:54:10,84,https://stackoverflow.com/questions/77093345/how-to-get-enhanced-dependency-labels-with-a-java-command-line-in-the-terminal,"How to get Enhanced++ dependency labels with a java command line in the terminal? I don't really know java, but I was just trying to use the documentation of the Stanford NLP parser to get the Enhanced++ dependency labels. This is the line I ran: java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,depparse"" -file input.txt And indeed I get an output. But I don't get the labels that I expect. For example, in the input.txt file there is a sentence ""The older couple is picnicking with wine"", and the dependency between picnicking and wine should be nmod, but instead it is obl:with. Another sentence is ""What do you call it?"", where I expect a dobj relationship between ""call"" and ""it"", but instead I get ""obj"". What should I fix to get the labels of the enhanced universal dependencies? (Also, do I really need to specify the options ""tokenize,ssplit,pos,lemma"" if I am interested only in ""depparse""?) Thank you.",get enhanced dependency label java command line terminal  not really know java  try use documentation stanford nlp parser get enhanced dependency label  line run  java cp     xmx2 g edustanfordnlppipeline  stanfordcorenlp annotators   tokenize  ssplit  pos  lemma  depparse  file inputtxt indeed get output  not get label expect  example  inputtxt file sentence   old couple picnic wine   dependency picnic wine nmod  instead obl   another sentence   call     expect dobj relationship   call      instead get   obj   fix get label enhance universal dependency   also  really need specify option   tokenize  ssplit  pos  lemma  interested   depparse    thank ,actually get enhanced dependency label  however  look like look something else old version  ud somewhat revise udv1 udv2  one change make oblique modifier  pps english  predicate relation obl rather nmod  restrict nmod modifier nominal  hence  obl nmod  part enhance dependency rather basic dependency get incorporation case preposition label  get obl   similarly  udv2  label dobj change simply obj    yes  need use annotator   tokenize  ssplit  pos  lemma   need preprocesse step dependency parsing  ,get enhanced dependency label java command line terminal  not really know java  try use documentation stanford nlp parser get enhanced dependency label  line run  java cp     xmx2 g edustanfordnlppipeline  stanfordcorenlp annotators   tokenize  ssplit  pos  lemma  depparse  file inputtxt indeed get output  not get label expect  example  inputtxt file sentence   old couple picnic wine   dependency picnic wine nmod  instead obl   another sentence   call     expect dobj relationship   call      instead get   obj   fix get label enhance universal dependency   also  really need specify option   tokenize  ssplit  pos  lemma  interested   depparse    thank  actually get enhanced dependency label  however  look like look something else old version  ud somewhat revise udv1 udv2  one change make oblique modifier  pps english  predicate relation obl rather nmod  restrict nmod modifier nominal  hence  obl nmod  part enhance dependency rather basic dependency get incorporation case preposition label  get obl   similarly  udv2  label dobj change simply obj    yes  need use annotator   tokenize  ssplit  pos  lemma   need preprocesse step dependency parsing  ,Basic Understanding
PyTorch SimCSE loss implementation,"I’m looking to implement the supervised Simple Contrastive Learning of Sentence Embeddings (SimCSE) loss using positive and negative pairs with PyTorch. Is there a way to vectorize the naïve implementation below with broadcasting operations and/or matrix multiplication? import torch import torch.nn.functional as F batch_size = 4 feature_dim = 1024 h = -2*torch.randn(batch_size, 3, feature_dim)+1 # (batch dim, contrastive triplet, features dim) temp = 10 num = torch.exp(F.cosine_similarity(h[:, 0, :], h[:, 1, :], dim=1) / temp) denom = torch.empty_like(num) for j in range(batch_size): denomjj = 0 for jj in range(batch_size): denomjj += torch.exp(F.cosine_similarity(h[j, 0, :], h[jj, 1, :], dim=0) / temp) denomjj += torch.exp(F.cosine_similarity(h[j, 0, :], h[jj, 2, :], dim=0) / temp) denom[j] = denomjj loss = -torch.log(num / denom)","['python', 'machine-learning', 'pytorch', 'nlp']",1,"Sure. Your numerator seems fine so I'll vectorize the denominator. I'll try to stick to your notation as closely as I can: norm_hi = torch.sqrt(torch.sum(torch.square(h[:, 0, :]), dim=1)) norm_hj_plus = torch.sqrt(torch.sum(torch.square(h[:, 1, :]), dim=1)) norm_hj_minus = torch.sqrt(torch.sum(torch.square(h[:, 2, :]), dim=1)) sim_denom1 = torch.outer(norm_hi, norm_hj_plus) * temp sim_denom2 = torch.outer(norm_hi, norm_hj_minus) * temp v1 = h[:, 0, :] @ h[:, 1, :].t() / sim_denom1 v2 = h[:, 0, :] @ h[:, 2, :].t() / sim_denom2 vec_denom = torch.sum(torch.exp(v1) + torch.exp(v2), dim=1) You can verify that this computes your denominator like this: print(torch.allclose(loss, -torch.log(num / vec_denom)))",2023-09-12 16:52:24,2023-09-13 13:38:00,596,https://stackoverflow.com/questions/77091208/pytorch-simcse-loss-implementation,"PyTorch SimCSE loss implementation I’m looking to implement the supervised Simple Contrastive Learning of Sentence Embeddings (SimCSE) loss using positive and negative pairs with PyTorch. Is there a way to vectorize the naïve implementation below with broadcasting operations and/or matrix multiplication? import torch import torch.nn.functional as F batch_size = 4 feature_dim = 1024 h = -2*torch.randn(batch_size, 3, feature_dim)+1 # (batch dim, contrastive triplet, features dim) temp = 10 num = torch.exp(F.cosine_similarity(h[:, 0, :], h[:, 1, :], dim=1) / temp) denom = torch.empty_like(num) for j in range(batch_size): denomjj = 0 for jj in range(batch_size): denomjj += torch.exp(F.cosine_similarity(h[j, 0, :], h[jj, 1, :], dim=0) / temp) denomjj += torch.exp(F.cosine_similarity(h[j, 0, :], h[jj, 2, :], dim=0) / temp) denom[j] = denomjj loss = -torch.log(num / denom)",pytorch simcse loss implementation  look implement supervised simple contrastive learning sentence embeddings  simcse  loss use positive negative pair pytorch  way vectorize nave implementation broadcasting operation andor matrix multiplication  import torch import torchnnfunctional f batchsize  4 featuredim  1024 h  2  torchrandn  batchsize  3  featuredim  1   batch dim  contrastive triplet  feature dim  temp  10 num  torchexp  fcosinesimilarity  h    0     h    1     dim1   temp  denom  torchemptylike  num  j range  batchsize   denomjj  0 jj range  batchsize   denomjj   torchexp  fcosinesimilarity  h  j  0     h  jj  1     dim0   temp  denomjj   torchexp  fcosinesimilarity  h  j  0     h  jj  2     dim0   temp  denom  j   denomjj loss  torchlog  num  denom ,sure  numerator seem fine will vectorize denominator  will try stick notation closely  normhi  torchsqrt  torchsum  torchsquare  h    0      dim1   normhjplus  torchsqrt  torchsum  torchsquare  h    1      dim1   normhjminus  torchsqrt  torchsum  torchsquare  h    2      dim1   simdenom1  torchouter  normhi  normhjplus   temp simdenom2  torchouter  normhi  normhjminus   temp v1  h    0     h    1    t    simdenom1 v2  h    0     h    2    t    simdenom2 vecdenom  torchsum  torchexp  v1   torchexp  v2   dim1  verify compute denominator like  print  torchallclose  loss  torchlog  num  vecdenom   ,pytorch simcse loss implementation  look implement supervised simple contrastive learning sentence embeddings  simcse  loss use positive negative pair pytorch  way vectorize nave implementation broadcasting operation andor matrix multiplication  import torch import torchnnfunctional f batchsize  4 featuredim  1024 h  2  torchrandn  batchsize  3  featuredim  1   batch dim  contrastive triplet  feature dim  temp  10 num  torchexp  fcosinesimilarity  h    0     h    1     dim1   temp  denom  torchemptylike  num  j range  batchsize   denomjj  0 jj range  batchsize   denomjj   torchexp  fcosinesimilarity  h  j  0     h  jj  1     dim0   temp  denomjj   torchexp  fcosinesimilarity  h  j  0     h  jj  2     dim0   temp  denom  j   denomjj loss  torchlog  num  denom  sure  numerator seem fine will vectorize denominator  will try stick notation closely  normhi  torchsqrt  torchsum  torchsquare  h    0      dim1   normhjplus  torchsqrt  torchsum  torchsquare  h    1      dim1   normhjminus  torchsqrt  torchsum  torchsquare  h    2      dim1   simdenom1  torchouter  normhi  normhjplus   temp simdenom2  torchouter  normhi  normhjminus   temp v1  h    0     h    1    t    simdenom1 v2  h    0     h    2    t    simdenom2 vecdenom  torchsum  torchexp  v1   torchexp  v2   dim1  verify compute denominator like  print  torchallclose  loss  torchlog  num  vecdenom   ,Implementation Issues
