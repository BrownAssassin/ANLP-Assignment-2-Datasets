title,description,tags,answer_count,accepted_answers,creation_date,accepted_answer_creation_date,view_count,link,combined_text_desc,fully_processed_combined_desc,fully_processed_answers,combine_answer_with_desc_and_title,category
NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models,"I am trying to set up hugging face locally and im running into this issue. NameError: name 'init_empty_weights' is not defined Here is the code I have tested my installation with from transformers import pipeline classifier = pipeline(""sentiment-analysis"") text = ""I love using Hugging Face Transformers!"" result = classifier(text) print(result) transformers: 4.51.0 tokenizers: 0.21.1 accelerate: 1.6.0 sentence-transformers: 4.0.2 huggingface_hub: 0.30.1 I am currently using pytorch-metal mac M3 pro. What causes this, and how can I fix it?","['nlp', 'huggingface-transformers', 'huggingface']",2,"Try using this version, it should resolve the issue. transformers==4.50.3",2025-04-07 11:02:41,2025-04-16 10:57:35,638,https://stackoverflow.com/questions/79559702/nameerror-name-init-empty-weights-is-not-defined-while-using-hugging-face-mod,"NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models I am trying to set up hugging face locally and im running into this issue. NameError: name 'init_empty_weights' is not defined Here is the code I have tested my installation with from transformers import pipeline classifier = pipeline(""sentiment-analysis"") text = ""I love using Hugging Face Transformers!"" result = classifier(text) print(result) transformers: 4.51.0 tokenizers: 0.21.1 accelerate: 1.6.0 sentence-transformers: 4.0.2 huggingface_hub: 0.30.1 I am currently using pytorch-metal mac M3 pro. What causes this, and how can I fix it?",nameerror  name   39  initemptyweight   39  define use hug face model try set hug face locally i m run issue  nameerror  name  initemptyweight  define code test installation transformer import pipeline classifier  pipeline    sentiment  analysis   text    love use hugging face transformers   result  classifier  text  print  result  transformer  4510 tokenizer  0211 accelerate  160 sentence  transformer  402 huggingfacehub  0301 currently use pytorch  metal mac m3 pro  cause  fix ,try use version  resolve issue  transformers4503,nameerror  name   39  initemptyweight   39  define use hug face model try set hug face locally i m run issue  nameerror  name  initemptyweight  define code test installation transformer import pipeline classifier  pipeline    sentiment  analysis   text    love use hugging face transformers   result  classifier  text  print  result  transformer  4510 tokenizer  0211 accelerate  160 sentence  transformer  402 huggingfacehub  0301 currently use pytorch  metal mac m3 pro  cause  fix  try use version  resolve issue  transformers4503,Library/Tool-Based Queries
Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?,"I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER): import spacy nlp = spacy.load(""pl_core_news_lg"") text = ""Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google."" doc = nlp(text) entities = [(ent.text, ent.label_) for ent in doc.ents] print(entities) Output: [('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')] However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities. from presidio_analyzer import AnalyzerEngine, RecognizerRegistry from presidio_analyzer.nlp_engine import NlpEngineProvider provider = NlpEngineProvider(conf_file=""path_to_my_file/nlp_config.yaml"") nlp_engine = provider.create_engine() print(f""Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}"") supported_languages = list(nlp_engine.get_supported_languages()) registry = RecognizerRegistry(supported_languages=[""pl""]) registry.load_predefined_recognizers([""pl""]) print(f""Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}"") analyzer = AnalyzerEngine( registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine ) results = analyzer.analyze(text, ""pl"") for entity in results: print(f""Found entity: {entity.entity_type} with score {entity.score}"") Output: Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION'] Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS'] Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text. My config file: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?","['python', 'nlp', 'spacy', 'presidio']",1,"The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine : labels_to_ignore: - O On your configuration it would look like this: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: labels_to_ignore: - O model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Edit: This was fixed to be the default if 'labels_to_ignore' is not specified Will be part of version 2.2.359 release",2025-04-02 05:56:11,2025-04-03 07:02:54,97,https://stackoverflow.com/questions/79549787/why-does-presidio-with-spacy-nlp-engine-not-recognize-organizations-and-pesel-wh,"Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does? I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER): import spacy nlp = spacy.load(""pl_core_news_lg"") text = ""Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google."" doc = nlp(text) entities = [(ent.text, ent.label_) for ent in doc.ents] print(entities) Output: [('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')] However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities. from presidio_analyzer import AnalyzerEngine, RecognizerRegistry from presidio_analyzer.nlp_engine import NlpEngineProvider provider = NlpEngineProvider(conf_file=""path_to_my_file/nlp_config.yaml"") nlp_engine = provider.create_engine() print(f""Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}"") supported_languages = list(nlp_engine.get_supported_languages()) registry = RecognizerRegistry(supported_languages=[""pl""]) registry.load_predefined_recognizers([""pl""]) print(f""Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}"") analyzer = AnalyzerEngine( registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine ) results = analyzer.analyze(text, ""pl"") for entity in results: print(f""Found entity: {entity.entity_type} with score {entity.score}"") Output: Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION'] Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS'] Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text. My config file: nlp_engine_name: spacy models: - lang_code: pl model_name: pl_core_news_lg ner_model_configuration: model_to_presidio_entity_mapping: persName: PERSON orgName: ORGANIZATION # orgName: ORG placeName: LOCATION geogName: LOCATION LOC: LOCATION GPE: LOCATION FAC: LOCATION DATE: DATE_TIME TIME: DATE_TIME NORP: NRP ID: ID Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?",presidio spacy nlp engine recognize organization pesel spacy   m use spacy plcorenewslg model extract name entity polish text  correctly detect organization  org  people s name  per   import spacy nlp  spacyload    plcorenewslg   text    jan kowalski pracuje w ibm wsppracuje z microsoft oraz google   doc  nlp  text  entity    enttext  entlabel   ent docent  print  entity  output     jan kowalski    persname      ibm    orgname      microsoft    orgname      google    orgname    however  use presidio plcorenewslg model configuration file  recognizer correctly detect organization  org  pesel number  even though appear list support entity  presidioanalyzer import analyzerengine  recognizerregistry presidioanalyzernlpengine import nlpengineprovider provider  nlpengineprovider  conffile  pathtomyfile  nlpconfigyaml   nlpengine  providercreateengine   print  f  supported recognizer  nlp engine    nlpenginegetsupportedentitie      supportedlanguage  list  nlpenginegetsupportedlanguage    registry  recognizerregistry  supportedlanguages    pl    registryloadpredefinedrecognizer     pl    print  f  supported recognizer  registry    registrygetsupportedentitie    pl       analyzer  analyzerengine  registry  registry  supportedlanguage  supportedlanguage  nlpengine  nlpengine  result  analyzeranalyze  text    pl   entity result  print  f  find entity   entityentitytype  score  entityscore    output  support recognizer  nlp engine     id    nrp    datetime    person    location   support recognizer  registry     invoter    url    ibancode    creditcard    datetime    nrp    phonenumber    medicallicense    person    ipaddress    organization    crypto    location    plpesel    emailaddress   even though  organization   plpesel  list  org list nlp engine  support recognizer  presidio detect correctly text  config file  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id presidio fail detect organization  org  pesel number  plpesel   spacy correctly detect ,configuration file miss  labelstoignore  field  state entity ignore nlp engine  labelstoignore   configuration would look like  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  labelstoignore   modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id edit  fix default  labelstoignore  specify part version 22359 release,presidio spacy nlp engine recognize organization pesel spacy   m use spacy plcorenewslg model extract name entity polish text  correctly detect organization  org  people s name  per   import spacy nlp  spacyload    plcorenewslg   text    jan kowalski pracuje w ibm wsppracuje z microsoft oraz google   doc  nlp  text  entity    enttext  entlabel   ent docent  print  entity  output     jan kowalski    persname      ibm    orgname      microsoft    orgname      google    orgname    however  use presidio plcorenewslg model configuration file  recognizer correctly detect organization  org  pesel number  even though appear list support entity  presidioanalyzer import analyzerengine  recognizerregistry presidioanalyzernlpengine import nlpengineprovider provider  nlpengineprovider  conffile  pathtomyfile  nlpconfigyaml   nlpengine  providercreateengine   print  f  supported recognizer  nlp engine    nlpenginegetsupportedentitie      supportedlanguage  list  nlpenginegetsupportedlanguage    registry  recognizerregistry  supportedlanguages    pl    registryloadpredefinedrecognizer     pl    print  f  supported recognizer  registry    registrygetsupportedentitie    pl       analyzer  analyzerengine  registry  registry  supportedlanguage  supportedlanguage  nlpengine  nlpengine  result  analyzeranalyze  text    pl   entity result  print  f  find entity   entityentitytype  score  entityscore    output  support recognizer  nlp engine     id    nrp    datetime    person    location   support recognizer  registry     invoter    url    ibancode    creditcard    datetime    nrp    phonenumber    medicallicense    person    ipaddress    organization    crypto    location    plpesel    emailaddress   even though  organization   plpesel  list  org list nlp engine  support recognizer  presidio detect correctly text  config file  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id presidio fail detect organization  org  pesel number  plpesel   spacy correctly detect  configuration file miss  labelstoignore  field  state entity ignore nlp engine  labelstoignore   configuration would look like  nlpenginename  spacy model   langcode  pl modelname  plcorenewslg nermodelconfiguration  labelstoignore   modeltopresidioentitymapping  persname  person orgname  organization  orgname  org placename  location geogname  location loc  location gpe  location fac  location date  datetime time  datetime norp  nrp id  id edit  fix default  labelstoignore  specify part version 22359 release,Implementation Issues
"GPT-2 and other models from huggingface -100 label index for training, instead of pad token","I understand the -100 label id is used so that the predictions for these are not included when calculating the loss. However on huggingface , they state ""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument ""ignore_index"". Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same? The way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.","['nlp', 'huggingface-transformers', 'pre-trained-model']",1,The author of the tutorial you mentioned sets it to -100 and uses ignore_index to save a few lines of code. You don't see the line where the author pass something to ignore_index because it has a default value. The default value of ignore_index for nn.CrossEntropyLoss is -100 . Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.,2025-04-01 09:21:17,2025-04-02 17:11:43,54,https://stackoverflow.com/questions/79548202/gpt-2-and-other-models-from-huggingface-100-label-index-for-training-instead-o,"GPT-2 and other models from huggingface -100 label index for training, instead of pad token I understand the -100 label id is used so that the predictions for these are not included when calculating the loss. However on huggingface , they state ""complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not"", when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument ""ignore_index"". Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same? The way it is written makes me think there is some benefit, but the description of ""ignore_index"" appears to achieve what is wanted.",gpt2 model huggingface 100 label index training  instead pad token understand 100 label i d use prediction include calculate loss  however huggingface  state   complicated list comprehension padtokenid alone good enough know whether label exclude   replace pad token  implementation  use nn  crossentropyloss    argument   ignoreindex   benefit change i d 100 oppose add argument ignoreindex loss set pad token i d  result  way write make think benefit  description   ignoreindex  appear achieve want ,author tutorial mention set 100 use ignoreindex save line code  not see line author pass something ignoreindex default value  default value ignoreindex nn  crossentropyloss 100  use value instead respective pad token i d allow write model indepent training code not pass pad token i d tokenizer loss function ,gpt2 model huggingface 100 label index training  instead pad token understand 100 label i d use prediction include calculate loss  however huggingface  state   complicated list comprehension padtokenid alone good enough know whether label exclude   replace pad token  implementation  use nn  crossentropyloss    argument   ignoreindex   benefit change i d 100 oppose add argument ignoreindex loss set pad token i d  result  way write make think benefit  description   ignoreindex  appear achieve want  author tutorial mention set 100 use ignoreindex save line code  not see line author pass something ignoreindex default value  default value ignoreindex nn  crossentropyloss 100  use value instead respective pad token i d allow write model indepent training code not pass pad token i d tokenizer loss function ,Implementation Issues
Trouble getting importing gensim to work in colab,"I am trying to import gensim into colab. !pip install gensim I get the following error: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr) 365 raise AssertionError() 366 except AssertionError: --> 367 msg = (""The current Numpy installation ({!r}) fails to "" 368 ""pass simple sanity checks. This can be caused for example "" 369 ""by incorrect BLAS library being linked in, or by mixing "" ModuleNotFoundError: No module named 'numpy.char' my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks","['numpy', 'nlp', 'dependencies', 'google-colaboratory', 'gensim']",1,"You have to restart the session for the underlying runtime to notice the package changes. See: https://stackoverflow.com/a/79518359/130288 I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded numpy /etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade. But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two. Explicitly restarting after the Gensim-install & numpy / scipy downgrades resolves the errors.",2025-03-20 14:36:02,2025-03-20 18:07:50,213,https://stackoverflow.com/questions/79523269/trouble-getting-importing-gensim-to-work-in-colab,"Trouble getting importing gensim to work in colab I am trying to import gensim into colab. !pip install gensim I get the following error: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr) 365 raise AssertionError() 366 except AssertionError: --> 367 msg = (""The current Numpy installation ({!r}) fails to "" 368 ""pass simple sanity checks. This can be caused for example "" 369 ""by incorrect BLAS library being linked in, or by mixing "" ModuleNotFoundError: No module named 'numpy.char' my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks",trouble get import gensim work colab try import gensim colab   pip install gensim get follow error  usr  local  lib  python311  dist  package  numpyinitpy   getattr    attr  365 raise assertionerror   366 except assertionerror    367 msg     current numpy installation    r   fail   368   pass simple sanity check  cause example   369   incorrect blas library link  mix   modulenotfounderror  module name  numpychar  numpy version 202  downgrade numpy another version like say 1264 get different error always numpy string relate issue  thank,restart session underlie runtime notice package change  see    stackoverflowcom  a79518359130288 recall past colab offering warning  possibly also  past  colab not yet load numpy etc fresh environment  ok downgrade behind scene without problem  1st import downgrade  something change colab recently  maybe fast  start optimization   bunch report problem like last day two  explicitly restart gensim  install  numpy  scipy downgrade resolve error ,trouble get import gensim work colab try import gensim colab   pip install gensim get follow error  usr  local  lib  python311  dist  package  numpyinitpy   getattr    attr  365 raise assertionerror   366 except assertionerror    367 msg     current numpy installation    r   fail   368   pass simple sanity check  cause example   369   incorrect blas library link  mix   modulenotfounderror  module name  numpychar  numpy version 202  downgrade numpy another version like say 1264 get different error always numpy string relate issue  thank restart session underlie runtime notice package change  see    stackoverflowcom  a79518359130288 recall past colab offering warning  possibly also  past  colab not yet load numpy etc fresh environment  ok downgrade behind scene without problem  1st import downgrade  something change colab recently  maybe fast  start optimization   bunch report problem like last day two  explicitly restart gensim  install  numpy  scipy downgrade resolve error ,Library/Tool-Based Queries
Store images instead of showing in a server,"I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my server via an SSH connection. The code is for instance this one: skip_tokens = [1] # skip the special token for the start of the text <s> inp = TextTokenInput( eval_prompt, tokenizer, skip_tokens=skip_tokens, ) target = ""playing guitar, hiking, and spending time with his family."" attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens) attr_res.plot_token_attr(show=True) How to store the files locally instead of showing them?","['python', 'nlp', 'large-language-model']",1,"I can't test it but ... I checked source code and it uses matplotlib for this. If you remove show=True then it shouldn't show it but it should only get fig, ax . I think you could use matplotlib.pyplot.savefig(filename) to save it in file. import matplotlib.pyplot as plt # ... code ... attr_res.plot_token_attr() # without `show=True plt.savefig(""output.png"") #plt.show() # eventually show it after saving Probably you can also use fig for this fig, ax = attr_res.plot_token_attr() # without `show=True fig.savefig(""output.png"")",2025-03-11 14:50:31,2025-03-11 15:44:10,39,https://stackoverflow.com/questions/79501178/store-images-instead-of-showing-in-a-server,"Store images instead of showing in a server I am running the code found on this site in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my server via an SSH connection. The code is for instance this one: skip_tokens = [1] # skip the special token for the start of the text <s> inp = TextTokenInput( eval_prompt, tokenizer, skip_tokens=skip_tokens, ) target = ""playing guitar, hiking, and spending time with his family."" attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens) attr_res.plot_token_attr(show=True) How to store the files locally instead of showing them?",store image instead show server run code find site server would like store image instead show since connect remotely ssh connection server via ssh connection  code instance one  skiptokens   1   skip special token start text   inp  texttokeninput  evalprompt  tokenizer  skiptoken  skiptokens   target    play guitar  hike  spend time family   attrre  llmattrattribute  inp  target  target  skiptoken  skiptokens  attrresplottokenattr  show  true  store file locally instead show ,can not test  check source code use matplotlib  remove show  true not show get fig  ax  think could use matplotlibpyplotsavefig  filename  save file  import matplotlibpyplot plt   code  attrresplottokenattr    without  show  true pltsavefig    outputpng    pltshow    eventually show save probably also use fig fig  ax  attrresplottokenattr    without  show  true figsavefig    outputpng  ,store image instead show server run code find site server would like store image instead show since connect remotely ssh connection server via ssh connection  code instance one  skiptokens   1   skip special token start text   inp  texttokeninput  evalprompt  tokenizer  skiptoken  skiptokens   target    play guitar  hike  spend time family   attrre  llmattrattribute  inp  target  target  skiptoken  skiptokens  attrresplottokenattr  show  true  store file locally instead show  can not test  check source code use matplotlib  remove show  true not show get fig  ax  think could use matplotlibpyplotsavefig  filename  save file  import matplotlibpyplot plt   code  attrresplottokenattr    without  show  true pltsavefig    outputpng    pltshow    eventually show save probably also use fig fig  ax  attrresplottokenattr    without  show  true figsavefig    outputpng  ,Implementation Issues
Presidio with Langchain Experimental does not detect Polish names,"I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., ""Jan Kowalski""). Here is my code: from presidio_anonymizer import PresidioAnonymizer from presidio_reversible_anonymizer import PresidioReversibleAnonymizer config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": ""pl"", ""model_name"": ""pl_core_news_lg""}], } anonymizer = PresidioAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) text = ""Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com."" anonymized_result = anonymizer_tool.anonymize(text) anon_result = anonymizer.anonymize(text) deanonymized_result = anonymizer_tool.deanonymize(anonymized_result) print(""Anonymized text:"", anonymized_result) print(""Deanonymized text:"", deanonymized_result) print(""Map:"", anonymizer_tool.deanonymizer_mapping) print(""Anonymized text:"", anon_result) Output: Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {} Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. I expected the name ""Jan Kowalski"" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: python -m spacy download pl_core_news_lg Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only anonymizer_tool = PresidioReversibleAnonymizer() Then the output look like this: Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}} As mentioned below if I use only spaCy: nlp = spacy.load(""pl_core_news_lg"") doc = nlp(text) Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: Jan Kowalski persName Warszawie placeName So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.","['python', 'nlp', 'spacy', 'langchain', 'presidio']",2,"After some test I was able to find the solution: config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": 'pl', ""model_name"": ""pl_core_news_lg""}], } spacy_recognizer = SpacyRecognizer( supported_language=""pl"", supported_entities=[""persName""] ) anonymizer.add_recognizer(spacy_recognizer) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS"", ""CREDIT_CARD""], languages_config=config) The output look like this: Anonymized text: <persName> mieszka w Warszawie i ma e-mail glenn58@example.org. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'persName': {'<persName>': 'Jan Kowalski', '<persName_2>': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}} You need to directly add SpacyRecognizer with supported_entities formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.",2025-03-03 22:27:07,2025-03-09 13:15:41,241,https://stackoverflow.com/questions/79482283/presidio-with-langchain-experimental-does-not-detect-polish-names,"Presidio with Langchain Experimental does not detect Polish names I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., ""Jan Kowalski""). Here is my code: from presidio_anonymizer import PresidioAnonymizer from presidio_reversible_anonymizer import PresidioReversibleAnonymizer config = { ""nlp_engine_name"": ""spacy"", ""models"": [{""lang_code"": ""pl"", ""model_name"": ""pl_core_news_lg""}], } anonymizer = PresidioAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[""PERSON"", ""PHONE_NUMBER"", ""EMAIL_ADDRESS""], languages_config=config) text = ""Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com."" anonymized_result = anonymizer_tool.anonymize(text) anon_result = anonymizer.anonymize(text) deanonymized_result = anonymizer_tool.deanonymize(anonymized_result) print(""Anonymized text:"", anonymized_result) print(""Deanonymized text:"", deanonymized_result) print(""Map:"", anonymizer_tool.deanonymizer_mapping) print(""Anonymized text:"", anon_result) Output: Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {} Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. I expected the name ""Jan Kowalski"" and the email address to be anonymized, but the output remains unchanged. I have installed the pl_core_news_lg model using: python -m spacy download pl_core_news_lg Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly? Any suggestions on how to make it detect names in Polish? The interesting thing is that when I use only anonymizer_tool = PresidioReversibleAnonymizer() Then the output look like this: Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. Map: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}} As mentioned below if I use only spaCy: nlp = spacy.load(""pl_core_news_lg"") doc = nlp(text) Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy: Jan Kowalski persName Warszawie placeName So I would not like to create custom analyzer for that but use spaCy in Presidio as it works as expected.",presidio langchain experimental detect polish name use presidio  langchainexperimental anonymize text polish  detect name  eg    jan kowalski    code  presidioanonymizer import presidioanonymizer presidioreversibleanonymizer import presidioreversibleanonymizer config     nlpenginename     spacy     model       langcode     pl     modelname     plcorenewslg      anonymizer  presidioanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  text    jan kowalski mieszka w warszawie e  mail jankowalski  examplecom   anonymizedresult  anonymizertoolanonymize  text  anonresult  anonymizeranonymize  text  deanonymizedresult  anonymizertooldeanonymize  anonymizedresult  print    anonymized text    anonymizedresult  print    deanonymized text    deanonymizedresult  print    map    anonymizertooldeanonymizermappe  print    anonymized text    anonresult  output  anonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    anonymize text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  expect name   jan kowalski  email address anonymize  output remain unchanged  instal plcorenewslg model use  python m spacy download plcorenewslg miss something configuration  presidio support polish entity recognition properly  suggestion make detect name polish  interesting thing use anonymizertool  presidioreversibleanonymizer   output look like  anonymized text  elizabeth tate mieszka w warszawie e  mail christinemurray  examplenet  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    person     elizabeth tate    jan kowalski     emailaddress     christinemurray  examplenet    jankowalski  examplecom    mention use spacy  nlp  spacyload    plcorenewslg   doc  nlp  text  output correct guess s problem presidio  output spacy  jan kowalski persname warszawie placename would like create custom analyzer use spacy presidio work expect ,test able find solution  config     nlpenginename     spacy     model       langcode    pl     modelname     plcorenewslg      spacyrecognizer  spacyrecognizer  supportedlanguage  pl   supportedentities    persname    anonymizeraddrecognizer  spacyrecognizer  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress     creditcard    languagesconfig  config  output look like  anonymized text   persname  mieszka w warszawie e  mail glenn58  exampleorg  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    persname      persname     jan kowalski     persname2     jana kowalskiego     emailaddress     glenn58  exampleorg    jankowalski  examplecom    need directly add spacyrecognizer supportedentities format accord spacy s requirement  believe s something miss unclear documentation  cause misunderstanding ,presidio langchain experimental detect polish name use presidio  langchainexperimental anonymize text polish  detect name  eg    jan kowalski    code  presidioanonymizer import presidioanonymizer presidioreversibleanonymizer import presidioreversibleanonymizer config     nlpenginename     spacy     model       langcode     pl     modelname     plcorenewslg      anonymizer  presidioanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress    languagesconfig  config  text    jan kowalski mieszka w warszawie e  mail jankowalski  examplecom   anonymizedresult  anonymizertoolanonymize  text  anonresult  anonymizeranonymize  text  deanonymizedresult  anonymizertooldeanonymize  anonymizedresult  print    anonymized text    anonymizedresult  print    deanonymized text    deanonymizedresult  print    map    anonymizertooldeanonymizermappe  print    anonymized text    anonresult  output  anonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    anonymize text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  expect name   jan kowalski  email address anonymize  output remain unchanged  instal plcorenewslg model use  python m spacy download plcorenewslg miss something configuration  presidio support polish entity recognition properly  suggestion make detect name polish  interesting thing use anonymizertool  presidioreversibleanonymizer   output look like  anonymized text  elizabeth tate mieszka w warszawie e  mail christinemurray  examplenet  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    person     elizabeth tate    jan kowalski     emailaddress     christinemurray  examplenet    jankowalski  examplecom    mention use spacy  nlp  spacyload    plcorenewslg   doc  nlp  text  output correct guess s problem presidio  output spacy  jan kowalski persname warszawie placename would like create custom analyzer use spacy presidio work expect  test able find solution  config     nlpenginename     spacy     model       langcode    pl     modelname     plcorenewslg      spacyrecognizer  spacyrecognizer  supportedlanguage  pl   supportedentities    persname    anonymizeraddrecognizer  spacyrecognizer  anonymizertool  presidioreversibleanonymizer  analyzedfields    person     phonenumber     emailaddress     creditcard    languagesconfig  config  output look like  anonymized text   persname  mieszka w warszawie e  mail glenn58  exampleorg  deanonymized text  jan kowalski mieszka w warszawie e  mail jankowalski  examplecom  map    persname      persname     jan kowalski     persname2     jana kowalskiego     emailaddress     glenn58  exampleorg    jankowalski  examplecom    need directly add spacyrecognizer supportedentities format accord spacy s requirement  believe s something miss unclear documentation  cause misunderstanding ,Library/Tool-Based Queries
OpenNLP POSTaggerME and ChunkerME synergy,"I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from ""arvore deitada"" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.","['nlp', 'opennlp']",1,"Q1 Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner: Penn -> UD should work well. UD -> Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the ""classic' Penn tag set. Using a custom, language specific tag-set can work, but it is a matter of ""mapping"" from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy. Q2 No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang. Q3 This needs details/discussion on the Apache OpenNLP user and/or dev mailing lists . Alternatively, feel free to open a Jira issue if you can drill the topic down to a clear idea or proposed code addition.",2025-02-22 16:06:11,2025-02-28 11:55:50,42,https://stackoverflow.com/questions/79459888/opennlp-postaggerme-and-chunkerme-synergy,"OpenNLP POSTaggerME and ChunkerME synergy I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using TokenizerME , then I tagged it with POSTaggerME . For both I used the ready-made models provided by the project here . For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the UD POS Tags . As there is no ready-made model for ChunkerME in portuguese, I followed the instructions and did the training first using the ChunkerConverter tool (to convert from ""arvore deitada"" to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]). But, for more complex sentences, it hasn't produced such good results. I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus ( Bosque 8.0 ) seems to be using portuguese tags. For example, instead of PROPN , the corpus uses prop and instead of DET , it uses art . It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag... But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if this does indeed have an impact, there is a tool that already does this translation and there are any other suggestions for improving the chunker precision/recall.",opennlp postaggerme chunkerme synergy  m try use opennlp chunk api chunk portuguese sentence   first tokenized sentence use tokenizerme  tag postaggerme  use ready  make model provide project  sentence  ivo viu uva   postaggerme return tag  propn  verb  det  noun   model seem use ud pos tags  ready  make model chunkerme portuguese  follow instruction training first use chunkerconverter tool  convert   arvore deitada  conll2000  generating model chunkertrainerme tool  everything work well  sentence  chunker produce correct tag   b  np  b  vp  b  np  i  np     complex sentence  not produce good result  try identify could improve chunker training  one thing notice difference type tag  portuguese corpus  bosque 80  seem use portuguese tag  example  instead propn  corpus use prop instead det  use art  seem could lead problem  especially since one parameter chunker receive array ud tag  train another type tag  write code create routine convert portuguese notation ud  penn  want ask  indeed impact  tool already translation suggestion improve chunker precision  recall ,q1 yes  choose tag set  ud  penn  custom  impact  conversion possible bi  directional manner  penn   ud work well  ud   penn good idea lossy conversion  ud tag set less detailed compare   classic  penn tag set  use custom  language specific tag  set work  matter   mapping  from  to ud correctly  might work tag set language  other might complicated  lossy  q2  not  opennlp project take code donation upcoming release  want provide mapping  translation pt lang  q3 need detail  discussion apache opennlp user andor dev mailing list  alternatively  feel free open jira issue drill topic clear idea propose code addition ,opennlp postaggerme chunkerme synergy  m try use opennlp chunk api chunk portuguese sentence   first tokenized sentence use tokenizerme  tag postaggerme  use ready  make model provide project  sentence  ivo viu uva   postaggerme return tag  propn  verb  det  noun   model seem use ud pos tags  ready  make model chunkerme portuguese  follow instruction training first use chunkerconverter tool  convert   arvore deitada  conll2000  generating model chunkertrainerme tool  everything work well  sentence  chunker produce correct tag   b  np  b  vp  b  np  i  np     complex sentence  not produce good result  try identify could improve chunker training  one thing notice difference type tag  portuguese corpus  bosque 80  seem use portuguese tag  example  instead propn  corpus use prop instead det  use art  seem could lead problem  especially since one parameter chunker receive array ud tag  train another type tag  write code create routine convert portuguese notation ud  penn  want ask  indeed impact  tool already translation suggestion improve chunker precision  recall  q1 yes  choose tag set  ud  penn  custom  impact  conversion possible bi  directional manner  penn   ud work well  ud   penn good idea lossy conversion  ud tag set less detailed compare   classic  penn tag set  use custom  language specific tag  set work  matter   mapping  from  to ud correctly  might work tag set language  other might complicated  lossy  q2  not  opennlp project take code donation upcoming release  want provide mapping  translation pt lang  q3 need detail  discussion apache opennlp user andor dev mailing list  alternatively  feel free open jira issue drill topic clear idea propose code addition ,Basic Understanding
Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged,"I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: lstm_model = Sequential() lstm_model.add(Input(shape=(max_len,))) lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)) lstm_model.add(LSTM(256, return_sequences=True)) lstm_model.add(LSTM(128, return_sequences=True)) lstm_model.add(LSTM(64)) lstm_model.add(Dense(128, activation='relu')) lstm_model.add(Dense(units=3, activation='softmax')) lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy']) lstm_model.summary()","['keras', 'deep-learning', 'nlp', 'lstm', 'sentiment-analysis']",1,"Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort. I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then. Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So ""queen"" might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece. This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context. (It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)",2025-02-07 02:48:25,2025-02-09 16:00:51,50,https://stackoverflow.com/questions/79419884/underfitting-pre-trained-glove-lstm-model-accurcacy-unchanged,"Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes. However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue. This is several information about the dataset: Embedding size: (41151, 100) Maximum sequence length: 731 Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106} Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106} Total x training set (padded): (140997, 200) Total x validation set (padded): (17625, 200) Total x testing set (padded): (17625, 200) Total y training set (one hot): (140997, 3) Total y validation set (one hot): (17625, 3) Total y testing set (one hot): (17625, 2003 This is my full code: enter link description here This is my highlight code for this issue: lstm_model = Sequential() lstm_model.add(Input(shape=(max_len,))) lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)) lstm_model.add(LSTM(256, return_sequences=True)) lstm_model.add(LSTM(128, return_sequences=True)) lstm_model.add(LSTM(64)) lstm_model.add(Dense(128, activation='relu')) lstm_model.add(Dense(units=3, activation='softmax')) lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy']) lstm_model.summary()",underfitting pre  train glove  lstm model  accurcacy unchanged sentiment classification use pre  trained glove lstm model  use google play review scrap  result 50k text  implement random sampling minority class  however  train lstm model  training accuracy remain unchanged several epoch  need insight fix issue  several information dataset  embed size   41151  100  maximum sequence length  731 label distribution random sampling    positive   58749   negative   26643   neutral   9106  label distribution random sampling    positive   58749   negative   26643   neutral   9106  total x training set  pad    140997  200  total x validation set  pad    17625  200  total x testing set  pad    17625  200  total training set  one hot    140997  3  total validation set  one hot    17625  3  total testing set  one hot    17625  2003 full code  enter link description highlight code issue  lstmmodel  sequential   lstmmodeladd  input  shape  maxlen     lstmmodeladd  embed  inputdim  totalvocab  outputdim  embeddingdim  weights  embeddingmatrix   trainable  false   lstmmodeladd  lstm  256  returnsequence  true   lstmmodeladd  lstm  128  returnsequence  true   lstmmodeladd  lstm  64   lstmmodeladd  dense  128  activationrelu    lstmmodeladd  dense  units3  activationsoftmax    lstmmodelcompile  losscategoricalcrossentropy   optimizer  adam  learningrate0001   metrics   accuracy    lstmmodelsummary  ,base extra information comment   m go say reason lstm model hit wall  unspecified  low accuracy 85  try reach good type model problem  case tweak parameter likely waste effort   m fairly sure encoder transformer  eg  bert  surpass sentiment analysis benchmark number year back  sorry  quick search could not find killer reference insert   transformer got big well since  extra think  build top glove embedding present problem not handle multiple meaning word    queen  might female king  embed s party trick  king  male  female  queen  might pop group  might gay man  might chess piece  going put limit accuracy model build  whereas transformer not limitation look whole string see word context   possible argue  course  bring context lstm come  transformer still scale strongly 20  layer  whereas lstm tend choke two layer  ,underfitting pre  train glove  lstm model  accurcacy unchanged sentiment classification use pre  trained glove lstm model  use google play review scrap  result 50k text  implement random sampling minority class  however  train lstm model  training accuracy remain unchanged several epoch  need insight fix issue  several information dataset  embed size   41151  100  maximum sequence length  731 label distribution random sampling    positive   58749   negative   26643   neutral   9106  label distribution random sampling    positive   58749   negative   26643   neutral   9106  total x training set  pad    140997  200  total x validation set  pad    17625  200  total x testing set  pad    17625  200  total training set  one hot    140997  3  total validation set  one hot    17625  3  total testing set  one hot    17625  2003 full code  enter link description highlight code issue  lstmmodel  sequential   lstmmodeladd  input  shape  maxlen     lstmmodeladd  embed  inputdim  totalvocab  outputdim  embeddingdim  weights  embeddingmatrix   trainable  false   lstmmodeladd  lstm  256  returnsequence  true   lstmmodeladd  lstm  128  returnsequence  true   lstmmodeladd  lstm  64   lstmmodeladd  dense  128  activationrelu    lstmmodeladd  dense  units3  activationsoftmax    lstmmodelcompile  losscategoricalcrossentropy   optimizer  adam  learningrate0001   metrics   accuracy    lstmmodelsummary   base extra information comment   m go say reason lstm model hit wall  unspecified  low accuracy 85  try reach good type model problem  case tweak parameter likely waste effort   m fairly sure encoder transformer  eg  bert  surpass sentiment analysis benchmark number year back  sorry  quick search could not find killer reference insert   transformer got big well since  extra think  build top glove embedding present problem not handle multiple meaning word    queen  might female king  embed s party trick  king  male  female  queen  might pop group  might gay man  might chess piece  going put limit accuracy model build  whereas transformer not limitation look whole string see word context   possible argue  course  bring context lstm come  transformer still scale strongly 20  layer  whereas lstm tend choke two layer  ,Implementation Issues
Can&#39;t compile Marian NMT,"I'm using endeavouros. I'm trying to compile Marian with these instructions: https://marian-nmt.github.io/docs/#installation . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the CMakeLists.txt files of the repo, there is the line set (CMAKE_CXX_STANDARD 11) . These are the steps that I followed: git clone https://github.com/marian-nmt/marian mkdir marian/build cd marian/build cmake .. make -j4 This is the result I had: ➜ make -j4 [ 1%] Built target 3rd_party_installs [ 1%] Built target marian_version [ 6%] Built target sentencepiece_train-static [ 19%] Built target libyaml-cpp [ 25%] Built target SQLiteCpp [ 25%] Built target pathie-cpp [ 32%] Built target zlib [ 35%] Built target intgemm [ 35%] Built target faiss [ 53%] Built target sentencepiece-static [ 55%] Built target spm_decode [ 55%] Built target spm_normalize [ 55%] Built target spm_encode [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o [ 56%] Built target spm_train [ 57%] Built target spm_export_vocab [ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o [ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/fastopt.h:3, from /data/tools/marian/src/common/fastopt.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/utils.cpp:2: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/logging.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/cli_wrapper.h:6, from /data/tools/marian/src/common/config_parser.h:4, from /data/tools/marian/src/common/aliases.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Please help.","['gcc', 'cmake', 'nlp', 'g++']",1,"The diagnostic that your build is tripping, Wtemplate-id-cdtor , was introduced with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to errors, so it breaks your build. Although your build specifies -std=c++11 in src/3rd_party/spdlog/CMakeLists.txt , which generates the failure, g++-14 emits Wtemplate-id-cdtor to warn you that the code would be illegal under the more recent standard c++20 (and later). Then the warning is made an error. The warning is made an error by the compile option -Werror . This option is included in the list of compile options ALL_WARNINGS , which is created in the top-level marian/CMakeLists.txt at line 227 et seq : # These are used in src/CMakeLists.txt on a per-target basis list(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated; -Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function; -Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare; -Wno-missing-field-initializers;) and then applied as compile options for the marian library target in src/CMakeLists.txt at line 133: target_compile_options(marian PRIVATE ${ALL_WARNINGS}) whence the options are operative for the failing compilation of src/CMakeFiles/marian.dir/common/logging.cpp . This failure is a bug in the marian repo which you should report to the maintainers , as it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14. Pending a fix, you seem to have three interim options to get your build done. Either: Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ e.g. make it registry_t(const registry_t<Mutex>&) = delete; in this occurrence. Or: Locally disable -Wtemplate-id-cdtor at each occurrence, e.g: #pragma GCC diagnostic push #pragma GCC diagnostic ignored ""-Wtemplate-id-cdtor"" registry_t<Mutex>(const registry_t<Mutex>&) = delete; #pragma GCC diagnostic pop Or: Remove -Werror from the ALL_WARNINGS list in marian/CMakeLists.txt so that Wtemplate-id-cdtor remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status). I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.",2025-01-05 06:04:59,2025-01-06 10:09:03,76,https://stackoverflow.com/questions/79330283/cant-compile-marian-nmt,"Can&#39;t compile Marian NMT I'm using endeavouros. I'm trying to compile Marian with these instructions: https://marian-nmt.github.io/docs/#installation . But it fails. The error message seemingly indicates a conflict between the code and c++20. But in all the CMakeLists.txt files of the repo, there is the line set (CMAKE_CXX_STANDARD 11) . These are the steps that I followed: git clone https://github.com/marian-nmt/marian mkdir marian/build cd marian/build cmake .. make -j4 This is the result I had: ➜ make -j4 [ 1%] Built target 3rd_party_installs [ 1%] Built target marian_version [ 6%] Built target sentencepiece_train-static [ 19%] Built target libyaml-cpp [ 25%] Built target SQLiteCpp [ 25%] Built target pathie-cpp [ 32%] Built target zlib [ 35%] Built target intgemm [ 35%] Built target faiss [ 53%] Built target sentencepiece-static [ 55%] Built target spm_decode [ 55%] Built target spm_normalize [ 55%] Built target spm_encode [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o [ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o [ 56%] Built target spm_train [ 57%] Built target spm_export_vocab [ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o [ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/fastopt.h:3, from /data/tools/marian/src/common/fastopt.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/utils.cpp:2: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/logging.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12, from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139, from /data/tools/marian/src/common/logging.h:5, from /data/tools/marian/src/common/definitions.h:3, from /data/tools/marian/src/common/cli_wrapper.h:6, from /data/tools/marian/src/common/config_parser.h:4, from /data/tools/marian/src/common/aliases.cpp:1: /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 138 | registry_t<Mutex>() {} | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘< >’ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor] 139 | registry_t<Mutex>(const registry_t<Mutex>&) = delete; | ^ /data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘< >’ cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1 cc1plus: all warnings being treated as errors make[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Please help.",  39  compile marian nmt  m use endeavouro   m try compile marian instruction    marian  nmtgithubio  docs  installation  fail  error message seemingly indicate conflict code c20  cmakeliststxt file repo  line set  cmakecxxstandard 11   step follow  git clone   githubcom  marian  nmt  marian mkdir marian  build cd marian  build cmake  make j4 result   make j4  1   build target 3rdpartyinstall  1   build target marianversion  6   build target sentencepiecetrain  static  19   build target libyaml  cpp  25   build target sqlitecpp  25   build target pathie  cpp  32   build target zlib  35   build target intgemm  35   build target faiss  53   build target sentencepiece  static  55   build target spmdecode  55   build target spmnormalize  55   build target spmencode  55   building cxx object src  cmakefile  mariandir  common  aliasescppo  55   building cxx object src  cmakefile  mariandir  common  fastoptcppo  56   build target spmtrain  57   build target spmexportvocab  57   building cxx object src  cmakefile  mariandir  common  utilscppo  58   building cxx object src  cmakefile  mariandir  common  loggingcppo file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  fastopth3  data  tool  marian  src  common  fastoptcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  utilscpp2  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  loggingcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  cliwrapperh6  data  tool  marian  src  common  configparserh4  data  tool  marian  src  common  aliasescpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     cc1plu  warning treat error make  2       src  cmakefile  mariandir  buildmake93  src  cmakefile  mariandir  common  fastoptcppo  error 1 make  2      wait unfinished job  cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake121  src  cmakefile  mariandir  common  utilscppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake79  src  cmakefile  mariandir  common  aliasescppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake135  src  cmakefile  mariandir  common  loggingcppo  error 1 make  1       cmakefile  makefile2374  src  cmakefile  mariandir  all  error 2 make      makefile156   error 2 please help ,diagnostic build tripping  wtemplate  id  cdtor  introduce gcc 141  warning  error  build promote warning error  break build  although build specifie std  c11 src3rdparty  spdlog  cmakeliststxt  generate failure  g14 emit wtemplate  id  cdtor warn code would illegal recent standard c20  later   warn make error  warning make error compile option werror  option include list compile option allwarning  create top  level marian  cmakeliststxt line 227 et seq   use src  cmakeliststxt per  target basis list  append allwarning wall  werror  wextra  wno  unuse  result  wno  deprecate  wno  pragma  wno  unuse  parameter  wno  unuse  function  wno  unuse  value  wno  unknown  pragma  wno  sign  compare  wno  miss  field  initializer   apply compile option marian library target src  cmakeliststxt line 133  targetcompileoptions  marian private   allwarning   whence option operative failing compilation src  cmakefile  mariandir  common  loggingcpp  failure bug marian repo report maintainer  seem report already  head revision v1120 year old gcc 14  pende fix  seem three interim option get build do  either  make code legal c11 c20 diagnostic advice say occurrence  data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     eg  make registryt  const registryt  mutex     delete  occurrence   locally disable wtemplate  id  cdtor occurrence  eg   pragma gcc diagnostic push  pragma gcc diagnostic ignore   wtemplate  id  cdtor  registryt  mutex   const registryt  mutex     delete   pragma gcc diagnostic pop  remove werror allwarning list marian  cmakeliststxt wtemplate  id  cdtor remains warn  may result diagnostic demote error warning  default status   not test option would need go trouble instal cuda ,  39  compile marian nmt  m use endeavouro   m try compile marian instruction    marian  nmtgithubio  docs  installation  fail  error message seemingly indicate conflict code c20  cmakeliststxt file repo  line set  cmakecxxstandard 11   step follow  git clone   githubcom  marian  nmt  marian mkdir marian  build cd marian  build cmake  make j4 result   make j4  1   build target 3rdpartyinstall  1   build target marianversion  6   build target sentencepiecetrain  static  19   build target libyaml  cpp  25   build target sqlitecpp  25   build target pathie  cpp  32   build target zlib  35   build target intgemm  35   build target faiss  53   build target sentencepiece  static  55   build target spmdecode  55   build target spmnormalize  55   build target spmencode  55   building cxx object src  cmakefile  mariandir  common  aliasescppo  55   building cxx object src  cmakefile  mariandir  common  fastoptcppo  56   build target spmtrain  57   build target spmexportvocab  57   building cxx object src  cmakefile  mariandir  common  utilscppo  58   building cxx object src  cmakefile  mariandir  common  loggingcppo file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  fastopth3  data  tool  marian  src  common  fastoptcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  utilscpp2  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  loggingcpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     file include data  tool  marian  src3rdparty  spdlog  detail  spdlogimplh12  data  tool  marian  src3rdparty  spdlog  spdlogh139  data  tool  marian  src  common  loggingh5  data  tool  marian  src  common  definitionsh3  data  tool  marian  src  common  cliwrapperh6  data  tool  marian  src  common  configparserh4  data  tool  marian  src  common  aliasescpp1  data  tool  marian  src3rdparty  spdlog  detail  registryh13822  error  template  id allow constructor c20  werror  template  id  cdtor  138  registryt  mutex        data  tool  marian  src3rdparty  spdlog  detail  registryh13822  note  remove     data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     cc1plu  warning treat error make  2       src  cmakefile  mariandir  buildmake93  src  cmakefile  mariandir  common  fastoptcppo  error 1 make  2      wait unfinished job  cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake121  src  cmakefile  mariandir  common  utilscppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake79  src  cmakefile  mariandir  common  aliasescppo  error 1 cc1plus  warning treat error make  2       src  cmakefile  mariandir  buildmake135  src  cmakefile  mariandir  common  loggingcppo  error 1 make  1       cmakefile  makefile2374  src  cmakefile  mariandir  all  error 2 make      makefile156   error 2 please help  diagnostic build tripping  wtemplate  id  cdtor  introduce gcc 141  warning  error  build promote warning error  break build  although build specifie std  c11 src3rdparty  spdlog  cmakeliststxt  generate failure  g14 emit wtemplate  id  cdtor warn code would illegal recent standard c20  later   warn make error  warning make error compile option werror  option include list compile option allwarning  create top  level marian  cmakeliststxt line 227 et seq   use src  cmakeliststxt per  target basis list  append allwarning wall  werror  wextra  wno  unuse  result  wno  deprecate  wno  pragma  wno  unuse  parameter  wno  unuse  function  wno  unuse  value  wno  unknown  pragma  wno  sign  compare  wno  miss  field  initializer   apply compile option marian library target src  cmakeliststxt line 133  targetcompileoptions  marian private   allwarning   whence option operative failing compilation src  cmakefile  mariandir  common  loggingcpp  failure bug marian repo report maintainer  seem report already  head revision v1120 year old gcc 14  pende fix  seem three interim option get build do  either  make code legal c11 c20 diagnostic advice say occurrence  data  tool  marian  src3rdparty  spdlog  detail  registryh13922  error  template  id allow constructor c20  werror  template  id  cdtor  139  registryt  mutex   const registryt  mutex     delete    data  tool  marian  src3rdparty  spdlog  detail  registryh13922  note  remove     eg  make registryt  const registryt  mutex     delete  occurrence   locally disable wtemplate  id  cdtor occurrence  eg   pragma gcc diagnostic push  pragma gcc diagnostic ignore   wtemplate  id  cdtor  registryt  mutex   const registryt  mutex     delete   pragma gcc diagnostic pop  remove werror allwarning list marian  cmakeliststxt wtemplate  id  cdtor remains warn  may result diagnostic demote error warning  default status   not test option would need go trouble instal cuda ,Basic Understanding
how to get custom column in the model&#39;s forward() function when training with Huggingface Trainer?,"I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' input_ids ', ' labels ' and so on, and I additionally add 2 custom colunms ' interact_ids ' and ' candidate_ids '. But i can't get these custom fields in the forward() function of my Model ' class LLMWithCustomLayer(LlamaForCausalLM) '. def forward( self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, interact_ids = None, candidate_ids = None, ): print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none interact_embs = [] candidate_embs = [] for i in range(interact_ids.shape(0)): # O_i = F_i (e_i) interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids))) # O_i = F_i (e_i) candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids))) # replace [CandidateEmb] and [HistoryEmb] inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs) return super().forward( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels = labels ) I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.","['pytorch', 'nlp', 'large-language-model', 'huggingface-trainer']",1,"You need to modify the data collator to pass interact_ids and candidate_ids to your model, as Trainer ignores extra columns by default. To modify the data collator class CustomDataCollator(DataCollatorWithPadding): def __call__(self, features): batch = super().__call__(features) batch[""interact_ids""] = torch.tensor([f[""interact_ids""] for f in features]) batch[""candidate_ids""] = torch.tensor([f[""candidate_ids""] for f in features]) return batch then pass it to Trainer trainer = Trainer( model=LLMWithCustomLayer.from_pretrained(""your-llama-model""), args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer, data_collator=CustomDataCollator(tokenizer) ) Now, your forward() method will receive interact_ids and candidate_ids . Hope, it will work!",2025-01-04 08:57:44,2025-01-04 10:45:55,44,https://stackoverflow.com/questions/79328514/how-to-get-custom-column-in-the-models-forward-function-when-training-with-hu,"how to get custom column in the model&#39;s forward() function when training with Huggingface Trainer? I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields ' input_ids ', ' labels ' and so on, and I additionally add 2 custom colunms ' interact_ids ' and ' candidate_ids '. But i can't get these custom fields in the forward() function of my Model ' class LLMWithCustomLayer(LlamaForCausalLM) '. def forward( self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, interact_ids = None, candidate_ids = None, ): print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none interact_embs = [] candidate_embs = [] for i in range(interact_ids.shape(0)): # O_i = F_i (e_i) interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids))) # O_i = F_i (e_i) candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids))) # replace [CandidateEmb] and [HistoryEmb] inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs) return super().forward( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels = labels ) I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.",get custom column model   39  forward   function train huggingface trainer  use huggingface trainer train cumstom model subclasse llama llm  tokenized tokenizer  dataset field  inputids    label   additionally add 2 custom colunms  interactids   candidateids   can not get custom field forward   function model  class llmwithcustomlayer  llamaforcausallm    def forward  self  inputid  torch  longtensor  none  attentionmask  optional  torch  tensor   none  positionid  optional  torch  longtensor   none  pastkeyvalues  optional  list  torch  floattensor    none  inputsembed  optional  torch  floattensor   none  label  optional  torch  longtensor   none  usecache  optional  bool   none  outputattention  optional  bool   none  outputhiddenstates  optional  bool   none  returndict  optional  bool   none  interactid  none  candidateid  none    print   interactids  candidateids   interactids  candidateids   none interactemb    candidateembs    range  interactidsshape  0     oi  fi  ei  interactembsappend  selfitemembproj  selfgetitememb  interactids     oi  fi  ei  candidateembsappend  selfitemembproj  selfgetitememb  candidateids     replace  candidateemb   historyemb  inputsembed  selfreplacehistcanditoken  inputids  inputsembeds  interactembs  candidateembs  return super   forward  inputid  inputids  attentionmask  attentionmask  positionid  positionid  pastkeyvalue  pastkeyvalues  inputsembed  inputsembeds  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict  label  label  new llm fine tuning  anyone help  would grateful much ,need modify datum collator pass interactids candidateids model  trainer ignore extra column default  modify datum collator class customdatacollator  datacollatorwithpadding   def   call    self  feature   batch  super   call    feature  batch    interactids    torchtensor   f    interactids   f feature   batch    candidateids    torchtensor   f    candidateids   f feature   return batch pass trainer trainer  trainer  model  llmwithcustomlayerfrompretraine    your  llama  model    arg  trainingargs  traindataset  traindataset  evaldataset  evaldataset  tokenizer  tokenizer  datacollator  customdatacollator  tokenizer    forward   method receive interactids candidateids  hope  work ,get custom column model   39  forward   function train huggingface trainer  use huggingface trainer train cumstom model subclasse llama llm  tokenized tokenizer  dataset field  inputids    label   additionally add 2 custom colunms  interactids   candidateids   can not get custom field forward   function model  class llmwithcustomlayer  llamaforcausallm    def forward  self  inputid  torch  longtensor  none  attentionmask  optional  torch  tensor   none  positionid  optional  torch  longtensor   none  pastkeyvalues  optional  list  torch  floattensor    none  inputsembed  optional  torch  floattensor   none  label  optional  torch  longtensor   none  usecache  optional  bool   none  outputattention  optional  bool   none  outputhiddenstates  optional  bool   none  returndict  optional  bool   none  interactid  none  candidateid  none    print   interactids  candidateids   interactids  candidateids   none interactemb    candidateembs    range  interactidsshape  0     oi  fi  ei  interactembsappend  selfitemembproj  selfgetitememb  interactids     oi  fi  ei  candidateembsappend  selfitemembproj  selfgetitememb  candidateids     replace  candidateemb   historyemb  inputsembed  selfreplacehistcanditoken  inputids  inputsembeds  interactembs  candidateembs  return super   forward  inputid  inputids  attentionmask  attentionmask  positionid  positionid  pastkeyvalue  pastkeyvalues  inputsembed  inputsembeds  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict  label  label  new llm fine tuning  anyone help  would grateful much  need modify datum collator pass interactids candidateids model  trainer ignore extra column default  modify datum collator class customdatacollator  datacollatorwithpadding   def   call    self  feature   batch  super   call    feature  batch    interactids    torchtensor   f    interactids   f feature   batch    candidateids    torchtensor   f    candidateids   f feature   return batch pass trainer trainer  trainer  model  llmwithcustomlayerfrompretraine    your  llama  model    arg  trainingargs  traindataset  traindataset  evaldataset  evaldataset  tokenizer  tokenizer  datacollator  customdatacollator  tokenizer    forward   method receive interactids candidateids  hope  work ,Library/Tool-Based Queries
Getting all leaf words (reverse stemming) into one Python List,"On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package get_word_forms Imagine I have a shorter sample word list as follows: my_list = [' jail', ' belief',' board',' target', ' challenge', ' command'] If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words): get_word_forms(""command"") and get the following output: {'n': {'command', 'commandant', 'commandants', 'commander', 'commanders', 'commandership', 'commanderships', 'commandment', 'commandments', 'commands'}, 'a': set(), 'v': {'command', 'commanded', 'commanding', 'commands'}, 'r': set()} 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: [get_word_forms(word) for word in sample] I fail at getting any output: [{'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}] I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: ['command','commandant','commandants', 'commander', 'commanders', 'commandership', 'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.","['python', 'nlp', 'nltk']",1,"One solution using nested list comprehensions after stripping forgotten spaces: all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)] # Flatten the list of sets all_words = [word for setx in all_words for word in setx] # Remove the repetitions and sort the set all_words = sorted(set(all_words)) print(all_words) ['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']",2024-12-27 15:04:05,2024-12-27 23:39:51,52,https://stackoverflow.com/questions/79312133/getting-all-leaf-words-reverse-stemming-into-one-python-list,"Getting all leaf words (reverse stemming) into one Python List On the same lines as the solution provided in this link , I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package get_word_forms Imagine I have a shorter sample word list as follows: my_list = [' jail', ' belief',' board',' target', ' challenge', ' command'] If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words): get_word_forms(""command"") and get the following output: {'n': {'command', 'commandant', 'commandants', 'commander', 'commanders', 'commandership', 'commanderships', 'commandment', 'commandments', 'commands'}, 'a': set(), 'v': {'command', 'commanded', 'commanding', 'commands'}, 'r': set()} 'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb. If I try to reverse-stem the entire list in one go: [get_word_forms(word) for word in sample] I fail at getting any output: [{'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}, {'n': set(), 'a': set(), 'v': set(), 'r': set()}] I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb: something like: ['command','commandant','commandants', 'commander', 'commanders', 'commandership', 'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on.",get leaf word  reverse stem  one python list line solution provide link  try get leaf word one stem word  use community  contribute   divyanshu srivastava  package getwordform imagine short sample word list follow  mylist    jail    belief    board    target    challenge    command   work manually  follow  go word  by  word  time  consume list 200 word   getwordform    command   get follow output    n     command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command       set     v     command    command    command    command     r   set     n  noun    adjective   v  verb   r  adverb  try reverse  stem entire list one go   getwordform  word  word sample  fail get output     n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set     think fail save output dictionary  eventually  would like output list without break noun  adjective  adverb  verb  something like    command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command    command    command    command    jail    jailer    jailer    jailor    jailor    jail    jail    jail     ,one solution use nested list comprehension strip forget space  allword   setx word mylist setx getwordform  wordstrip    values   len  setx    flatten list set allword   word setx allword word setx   remove repetition sort set allwords  sort  set  allwords   print  allword    belief    belief    believability    believability    believable    believably    believe    believe    believer    believer    believe    believe    board    board    boarder    boarder    boarding    board    challenge    challengeable    challenge    challenger    challenger    challenge    challenge    command    commandant    commandant    command    commander    commander    commandership    commandership    command    commandment    commandment    command    jail    jail    jailer    jailer    jail    jailor    jailor    jail    target    target    target    target  ,get leaf word  reverse stem  one python list line solution provide link  try get leaf word one stem word  use community  contribute   divyanshu srivastava  package getwordform imagine short sample word list follow  mylist    jail    belief    board    target    challenge    command   work manually  follow  go word  by  word  time  consume list 200 word   getwordform    command   get follow output    n     command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command       set     v     command    command    command    command     r   set     n  noun    adjective   v  verb   r  adverb  try reverse  stem entire list one go   getwordform  word  word sample  fail get output     n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set       n   set       set     v   set     r   set     think fail save output dictionary  eventually  would like output list without break noun  adjective  adverb  verb  something like    command    commandant    commandant    commander    commander    commandership    commandership    commandment    commandment    command    command    command    command    jail    jailer    jailer    jailor    jailor    jail    jail    jail      one solution use nested list comprehension strip forget space  allword   setx word mylist setx getwordform  wordstrip    values   len  setx    flatten list set allword   word setx allword word setx   remove repetition sort set allwords  sort  set  allwords   print  allword    belief    belief    believability    believability    believable    believably    believe    believe    believer    believer    believe    believe    board    board    boarder    boarder    boarding    board    challenge    challengeable    challenge    challenger    challenger    challenge    challenge    command    commandant    commandant    command    commander    commander    commandership    commandership    command    commandment    commandment    command    jail    jail    jailer    jailer    jail    jailor    jailor    jail    target    target    target    target  ,Basic Understanding
Inspect all probabilities of BERTopic model,"Say I build a BERTopic model using from bertopic import BERTopic topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20) topics, probs = topic_model.fit_transform(docs) Inspecting probs gives me just a single value for each item in docs . probs array([0.51914467, 0. , 0. , ..., 1. , 1. , 1. ]) I would like the entire probability vector across all topics (so in this case, where nr_topics=20 , I want a vector of 20 probabilities for each item in docs ). In other words, if I have N items in docs and K topics, I would like an NxK output.","['python', 'nlp', 'topic-modeling']",1,"For individual topic probability across each document you need to add one more argument. topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True) Note: This calculate_probabilities = True will only work if you are using HDBSCAN clustering embedding model. And Bertopic by default uses all-MiniLM-L6-v2 . Official documentation: https://maartengr.github.io/BERTopic/api/bertopic.html They have mentioned the same in document as well.",2024-12-20 20:49:34,2024-12-21 16:03:22,58,https://stackoverflow.com/questions/79298368/inspect-all-probabilities-of-bertopic-model,"Inspect all probabilities of BERTopic model Say I build a BERTopic model using from bertopic import BERTopic topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20) topics, probs = topic_model.fit_transform(docs) Inspecting probs gives me just a single value for each item in docs . probs array([0.51914467, 0. , 0. , ..., 1. , 1. , 1. ]) I would like the entire probability vector across all topics (so in this case, where nr_topics=20 , I want a vector of 20 probabilities for each item in docs ). In other words, if I have N items in docs and K topics, I would like an NxK output.",inspect probability bertopic model say build bertopic model use bertopic import bertopic topicmodel  bertopic  ngramrange  1  1   nrtopics20  topic  prob  topicmodelfittransform  doc  inspect prob give single value item doc  prob array   051914467  0   0     1   1   1    would like entire probability vector across topic  case  nrtopics20  want vector 20 probability item doc   word  n item doc k topic  would like nxk output ,individual topic probability across document need add one argument  topicmodel  bertopic  ngramrange  1  1   nrtopics20  calculateprobabilitie  true  note  calculateprobabilitie  true work use hdbscan cluster embed model  bertopic default use all  minilm  l6  v2  official documentation    maartengrgithubio  bertopic  api  bertopichtml mention document well ,inspect probability bertopic model say build bertopic model use bertopic import bertopic topicmodel  bertopic  ngramrange  1  1   nrtopics20  topic  prob  topicmodelfittransform  doc  inspect prob give single value item doc  prob array   051914467  0   0     1   1   1    would like entire probability vector across topic  case  nrtopics20  want vector 20 probability item doc   word  n item doc k topic  would like nxk output  individual topic probability across document need add one argument  topicmodel  bertopic  ngramrange  1  1   nrtopics20  calculateprobabilitie  true  note  calculateprobabilitie  true work use hdbscan cluster embed model  bertopic default use all  minilm  l6  v2  official documentation    maartengrgithubio  bertopic  api  bertopichtml mention document well ,Basic Understanding
Determining most popular words in the English dictionary within a dictionary of words,"Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence ""I enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.","['python', 'nlp', 'nltk', 'detection']",2,"You need a external dataset for this task. You can try dataset such as google n gram dataset. Here is the breakdown of the problem statement: Input: ""I enjoy a cold glass of water on a hot day"". Output : ""water"". Split the sentences into words list. Example: [""I"", ""enjoy"", ""a"", ""cold"", ""glass"", ""of"", ""water"", ""on"", ""a"", ""hot"", ""day""] First loop in through all the word of the sentences. so let say you are at first word ""I"". Now you will look the same word ""I"" in external dataset and will look for the frequency of that word. Let say the word ""I"" in external dataset is repeated 5000000 times Repeat this task for all the word. Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data. Frequency in the below example is random value not exact value. { ""I"": 5000000, ""enjoy"": 50000, ""a"": 10000000, ""cold"": 30000, ""glass"": 100000, ""of"": 8000000, ""water"": 1200000, ""on"": 6000000, ""hot"": 700000, ""day"": 400000 } Pick the word with highest frequency. Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself",2024-12-19 10:24:04,2024-12-19 11:17:51,69,https://stackoverflow.com/questions/79293919/determining-most-popular-words-in-the-english-dictionary-within-a-dictionary-of,"Determining most popular words in the English dictionary within a dictionary of words Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do. For example: A sentence ""I enjoy a cold glass of water on a hot day"" would return ""water"" because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations. I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route Any and all help is welcome and appreciated. For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.",determine popular word english dictionary within dictionary word forgive wording awful   m try figure determine use word english language set word dictionary  ve make   ve do research nltk can not seem find function within  library matter  help need  example  sentence   enjoy cold glass water hot day  would return   water  s use word day day conversation sentence  essentially need return value frequently use word conversation  figure will likely involve ai  time  ve try use ai wind copy paste code not understand   m try avoid go route help welcome appreciate  context  decide start project would essentially guess predetermine word base character user say not computer guess ,need external dataset task  try dataset google n gram dataset  breakdown problem statement  input    enjoy cold glass water hot day   output    water   split sentence word list  example         enjoy         cold     glass         water             hot     day   first loop word sentence  let say first word     look word    external dataset look frequency word  let say word    external dataset repeat 5000000 time repeat task word  dictionary word sentence key value frequency word get external datum  frequency example random value exact value       5000000    enjoy   50000      10000000    cold   30000    glass   100000      8000000    water   1200000      6000000    hot   700000    day   400000  pick word high frequency  note  try big corpus external datum  use big corpus english word use conversation  even frequency mention create,determine popular word english dictionary within dictionary word forgive wording awful   m try figure determine use word english language set word dictionary  ve make   ve do research nltk can not seem find function within  library matter  help need  example  sentence   enjoy cold glass water hot day  would return   water  s use word day day conversation sentence  essentially need return value frequently use word conversation  figure will likely involve ai  time  ve try use ai wind copy paste code not understand   m try avoid go route help welcome appreciate  context  decide start project would essentially guess predetermine word base character user say not computer guess  need external dataset task  try dataset google n gram dataset  breakdown problem statement  input    enjoy cold glass water hot day   output    water   split sentence word list  example         enjoy         cold     glass         water             hot     day   first loop word sentence  let say first word     look word    external dataset look frequency word  let say word    external dataset repeat 5000000 time repeat task word  dictionary word sentence key value frequency word get external datum  frequency example random value exact value       5000000    enjoy   50000      10000000    cold   30000    glass   100000      8000000    water   1200000      6000000    hot   700000    day   400000  pick word high frequency  note  try big corpus external datum  use big corpus english word use conversation  even frequency mention create,Library/Tool-Based Queries
catelog sentences into 5 words that represent them,"I have dataframe with 1000 text rows. df['text'] I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in df[""word1""] , df[""word2""] and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is ""i want to eat"" and I have 2 words : food and house. so in df[""food ""] it would be higher score than in df[""house""]","['python', 'pandas', 'nlp', 'text-mining', 'similarity']",1,"You could use a pre-trained sentence transformer model from sentence_transformers : import pandas as pd from sentence_transformers import SentenceTransformer, util class SemanticSimilarityCalculator: def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None: self.model = SentenceTransformer(model_name) self.word_embeddings = None def encode_words(self, words: list[str]) -> None: self.word_embeddings = self.model.encode(words, convert_to_tensor=True) self.words = words def calculate_similarity(self, text: str) -> list[float]: if self.word_embeddings is None: raise ValueError('Words must be encoded before calculating similarity.') text_embedding = self.model.encode(text, convert_to_tensor=True) similarities = util.cos_sim(text_embedding, self.word_embeddings)[ 0 ].tolist() return similarities def add_similarity_scores_to_df( self, df: pd.DataFrame, text_column: str ) -> pd.DataFrame: if self.words is None: raise ValueError( 'Words must be encoded before adding scores to the DataFrame.' ) similarity_columns = ['word_' + word for word in self.words] df[similarity_columns] = df[text_column].apply( lambda text: pd.Series(self.calculate_similarity(text)) ) return df def main(): data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']} df = pd.DataFrame(data) words = ['food', 'house', 'sleep', 'drink', 'run'] calculator = SemanticSimilarityCalculator() calculator.encode_words(words) df_with_scores = calculator.add_similarity_scores_to_df( df, text_column='text' ) print(df_with_scores) if __name__ == '__main__': main() Output: text word_food word_house word_sleep word_drink word_run 0 I want to eat 0.592410 0.215032 0.254065 0.370329 0.259350 1 The house is big 0.243262 0.672110 0.170785 0.213780 0.119716 2 I need to sleep 0.253703 0.222462 0.725105 0.358372 0.303838",2024-12-19 10:16:47,2024-12-19 11:26:17,57,https://stackoverflow.com/questions/79293889/catelog-sentences-into-5-words-that-represent-them,"catelog sentences into 5 words that represent them I have dataframe with 1000 text rows. df['text'] I also have 5 words that I want to know for each one of them how much they represnt the text (between 0 to 1) every score will be in df[""word1""] , df[""word2""] and etc I will glad for recomendations how to do that edit represnt = the semantic distance between the word to the text. for example - lets say in row 1 the text is ""i want to eat"" and I have 2 words : food and house. so in df[""food ""] it would be higher score than in df[""house""]",catelog sentence 5 word represent dataframe 1000 text row  df   text   also 5 word want know one much represnt text  0 1  every score df    word1    df    word2   etc glad recomendation edit represnt  semantic distance word text  example  let say row 1 text   want eat  2 word  food house  df    food    would high score df    house  ,could use pre  train sentence transformer model sentencetransformer  import panda pd sentencetransformer import sentencetransformer  util class semanticsimilaritycalculator  def   init    self  modelname  str   all  minilm  l6  v2     none  selfmodel  sentencetransformer  modelname  selfwordembedding  none def encodeword  self  word  list  str     none  selfwordembedding  selfmodelencode  word  converttotensor  true  selfword  word def calculatesimilarity  self  text  str    list  float   selfwordembedding none  raise valueerror   word must encode calculate similarity    textembedding  selfmodelencode  text  converttotensor  true  similarity  utilcossim  textembedding  selfwordembedding   0  tolist   return similarity def addsimilarityscorestodf  self  df  pd  dataframe  textcolumn  str    pd  dataframe  selfword none  raise valueerror   word must encode add score dataframe    similaritycolumn    word    word word selfword  df  similaritycolumns   df  textcolumn  apply  lambda text  pd  series  selfcalculatesimilarity  text    return df def main    datum    text     want eat    the house big    need sleep    df  pd  dataframe  datum  word    food    house    sleep    drink    run   calculator  semanticsimilaritycalculator   calculatorencodeword  word  dfwithscore  calculatoraddsimilarityscorestodf  df  textcolumntext   print  dfwithscores    name        main     main   output  text wordfood wordhouse wordsleep worddrink wordrun 0 want eat 0592410 0215032 0254065 0370329 0259350 1 house big 0243262 0672110 0170785 0213780 0119716 2 need sleep 0253703 0222462 0725105 0358372 0303838,catelog sentence 5 word represent dataframe 1000 text row  df   text   also 5 word want know one much represnt text  0 1  every score df    word1    df    word2   etc glad recomendation edit represnt  semantic distance word text  example  let say row 1 text   want eat  2 word  food house  df    food    would high score df    house   could use pre  train sentence transformer model sentencetransformer  import panda pd sentencetransformer import sentencetransformer  util class semanticsimilaritycalculator  def   init    self  modelname  str   all  minilm  l6  v2     none  selfmodel  sentencetransformer  modelname  selfwordembedding  none def encodeword  self  word  list  str     none  selfwordembedding  selfmodelencode  word  converttotensor  true  selfword  word def calculatesimilarity  self  text  str    list  float   selfwordembedding none  raise valueerror   word must encode calculate similarity    textembedding  selfmodelencode  text  converttotensor  true  similarity  utilcossim  textembedding  selfwordembedding   0  tolist   return similarity def addsimilarityscorestodf  self  df  pd  dataframe  textcolumn  str    pd  dataframe  selfword none  raise valueerror   word must encode add score dataframe    similaritycolumn    word    word word selfword  df  similaritycolumns   df  textcolumn  apply  lambda text  pd  series  selfcalculatesimilarity  text    return df def main    datum    text     want eat    the house big    need sleep    df  pd  dataframe  datum  word    food    house    sleep    drink    run   calculator  semanticsimilaritycalculator   calculatorencodeword  word  dfwithscore  calculatoraddsimilarityscorestodf  df  textcolumntext   print  dfwithscores    name        main     main   output  text wordfood wordhouse wordsleep worddrink wordrun 0 want eat 0592410 0215032 0254065 0370329 0259350 1 house big 0243262 0672110 0170785 0213780 0119716 2 need sleep 0253703 0222462 0725105 0358372 0303838,Task-Specific Queries
Counting the Frequency of Some Words within some other Key Words in Text,"I have two sets of word lists - first one I called search words and the second one I called key words . My goal is to calculate the frequency of search words within 10 words of key words . For example, assume that the word - acquire - is in key words list, then I will look for the words in search words list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my search word and key word lists - search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'] key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use'] A small Example - text_dict = { 'ITEM7':[""Last year, from AVG we have acquired Alibaba Security. This year we are in the process \ of adopting Symantec. We believe these technologies will improve our access control. \ Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system.""] } df = pd.DataFrame(text_dict) My expected outcome is - ITEM7 Frequency Last year, from AVG we have acquired Alibaba S... 6 We are planning to install end-point security,... 2 For the first row in df , we see the word AVG and Alibaba Security are from search_words list and around the word acquired , the base form of which - acquire - is in the key_words list. Similarly, Symantec , Access Control , data security , diagnostic program are from search_words list and these words are within 10 words of adopting , improve , integrated from key_words list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the Frequency column of df , the value is 6. Please note that the words in key_words are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.","['python', 'pandas', 'nlp']",1,"You need to process each row of text by identifying occurrences of key_words and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique search_word found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique search_words near key_words . import pandas as pd from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords import string import re text_dict = { 'ITEM7': [ ""Last year, from AVG we have acquired Alibaba Security. This year we are in the process "" ""of adopting Symantec. We believe these technologies will improve our access control. "" ""Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system."" ] } df = pd.DataFrame(text_dict) search_words = [ 'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm' ] key_words = [ 'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use' ] def preprocess_text_no_lemmatization(text): tokens = re.findall(r'\b\w+\b', text.lower()) return tokens def calculate_final_frequency(row, search_phrases, key_phrases): text = row.lower() tokens = preprocess_text_no_lemmatization(text) search_phrases = [phrase.lower() for phrase in search_phrases] key_phrases = [phrase.lower() for phrase in key_phrases] all_matches = set() token_len = len(tokens) for idx, token in enumerate(tokens): if any(token.startswith(key) for key in key_phrases): window_start = max(0, idx - 10) window_end = min(token_len, idx + 10 + 1) window_tokens = tokens[window_start:window_end] window_text = "" "".join(window_tokens) for phrase in search_phrases: if phrase in window_text: all_matches.add(phrase) return len(all_matches) df['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words)) print(df) Which returns ITEM7 Frequency 0 Last year, from AVG we have acquired Alibaba S... 6 1 We are planning to install end-point security,... 2",2024-12-05 03:05:06,2024-12-08 17:27:28,83,https://stackoverflow.com/questions/79253283/counting-the-frequency-of-some-words-within-some-other-key-words-in-text,"Counting the Frequency of Some Words within some other Key Words in Text I have two sets of word lists - first one I called search words and the second one I called key words . My goal is to calculate the frequency of search words within 10 words of key words . For example, assume that the word - acquire - is in key words list, then I will look for the words in search words list within 10 words of acquire . Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement. Below is my search word and key word lists - search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security', 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware', 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda', 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario', 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity', 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data', 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits', 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor', 'Securion', 'security event management', 'security information and event management', 'security information management', 'SentinelOne', 'Seqrite', 'Sophos', 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'] key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource', 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand', 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend', 'upgrade', 'use'] A small Example - text_dict = { 'ITEM7':[""Last year, from AVG we have acquired Alibaba Security. This year we are in the process \ of adopting Symantec. We believe these technologies will improve our access control. \ Moreover, we also integrated data security diagnostic program."", ""We are planning to install end-point security, which will upgrade intrusion detection system.""] } df = pd.DataFrame(text_dict) My expected outcome is - ITEM7 Frequency Last year, from AVG we have acquired Alibaba S... 6 We are planning to install end-point security,... 2 For the first row in df , we see the word AVG and Alibaba Security are from search_words list and around the word acquired , the base form of which - acquire - is in the key_words list. Similarly, Symantec , Access Control , data security , diagnostic program are from search_words list and these words are within 10 words of adopting , improve , integrated from key_words list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the Frequency column of df , the value is 6. Please note that the words in key_words are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.",count frequency word within key word text two set word list  first one call search word second one call key word  goal calculate frequency search word within 10 word key word  example  assume word  acquire  key word list  look word search word list within 10 word acquire  within 10 word mean  10 word forward key word 10 word backward key word  mean forward backward movement  search word key word list  searchwords    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   small example  textdict    item7      last year  avg acquire alibaba security  year process  adopt symantec  believe technology improve access control   moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  expect outcome  item7 frequency last year  avg acquire alibaba  6 planning install end  point security   2 first row df  see word avg alibaba security searchwords list around word acquire  base form  acquire  keywords list  similarly  symantec  access control  datum security  diagnostic program searchword list word within 10 word adopt  improve  integrate keywords list   total search word 6  avgalibaba securitysymantecaccess controldata securitydiagnostic program   therefore  frequency column df  value 6  please note word keyword basically base form  variation  like adopt  adopt  count key word also ,need process row text identify occurrence keyword capture 10  word window around  within window  need check multi  word searchword  ensure match phrase  unique searchword find within window need count  avoid double  counting across row  store result frequency count row  accurately reflect number unique searchword near keyword  import panda pd nltktokenize import wordtokenize nltkstem import wordnetlemmatizer nltkcorpus import stopwords import string import textdict    item7      last year  avg acquire alibaba security  year process     adopt symantec  believe technology improve access control      moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  searchword    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   def preprocesstextnolemmatization  text   token  refindall  rbwb   textlower    return tokens def calculatefinalfrequency  row  searchphrase  keyphrase   text  rowlower   token  preprocesstextnolemmatization  text  searchphrase   phraselower   phrase searchphrases  keyphrase   phraselower   phrase keyphrase  allmatche  set   tokenlen  len  tokens  idx  token enumerate  token    tokenstartswith  key  key keyphrase   windowstart  max  0  idx  10  windowend  min  tokenlen  idx  10  1  windowtoken  token  windowstart  windowend  windowtext      join  windowtoken  phrase searchphrases  phrase windowtext  allmatchesadd  phrase  return len  allmatches  df   frequency    df   item7   apply  lambda x  calculatefinalfrequency  x  searchwords  keywords   print  df  return item7 frequency 0 last year  avg acquire alibaba  6 1 planning install end  point security   2,count frequency word within key word text two set word list  first one call search word second one call key word  goal calculate frequency search word within 10 word key word  example  assume word  acquire  key word list  look word search word list within 10 word acquire  within 10 word mean  10 word forward key word 10 word backward key word  mean forward backward movement  search word key word list  searchwords    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   small example  textdict    item7      last year  avg acquire alibaba security  year process  adopt symantec  believe technology improve access control   moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  expect outcome  item7 frequency last year  avg acquire alibaba  6 planning install end  point security   2 first row df  see word avg alibaba security searchwords list around word acquire  base form  acquire  keywords list  similarly  symantec  access control  datum security  diagnostic program searchword list word within 10 word adopt  improve  integrate keywords list   total search word 6  avgalibaba securitysymantecaccess controldata securitydiagnostic program   therefore  frequency column df  value 6  please note word keyword basically base form  variation  like adopt  adopt  count key word also  need process row text identify occurrence keyword capture 10  word window around  within window  need check multi  word searchword  ensure match phrase  unique searchword find within window need count  avoid double  counting across row  store result frequency count row  accurately reflect number unique searchword near keyword  import panda pd nltktokenize import wordtokenize nltkstem import wordnetlemmatizer nltkcorpus import stopwords import string import textdict    item7      last year  avg acquire alibaba security  year process     adopt symantec  believe technology improve access control      moreover  also integrate datum security diagnostic program       plan install end  point security  upgrade intrusion detection system     df  pd  dataframe  textdict  searchword    access control    acronis    adaware    ahnlab    ai max dev labs    alibaba security    anti  adware    anti  keylogger    anti  malware    anti  ransomware    anti  rootkit    anti  spyware    anti  subversion    anti  tamper    anti  virus    antiy    avast    avg    avira    baidu    barracuda    bitdefender    bullguard    carbon black    check point    cheetah mobile    cisco    clario    comodo    computer security    crowdstrike    cryptography    cybereason    cybersecurity    cylance    data security    diagnostic program    elastic    emsisoft    encryption    endgame    end point security    ensilo    escan    eset    fireeye    firewall    fortinet    f  secure    g data    immunet    information security    intego    intrusion detection system    k7    kaspersky    log management software    lookout    mackeeper    malwarebytes    mcafee    microsoft    network security    nod32    norton    palo alto networks    panda security    pc matic    pocketbits    qihoo    quick heal    record management    safedns    saint security    sandbox    sangfor    securion    security event management    security information event management    security information management    sentinelone    seqrite    sophos    sparkcognition    steganography    symantec    tencent    total av    total defense    trend micro    trustport    vipre    webroot    zonealarm   keyword    acquire    adopt    advance    agree    boost    capital resource    capitalize    change    commitment    complete    configure    design    develop    enhance    expand    expenditure    expense    implement    improve    increase    initiate    install    integrate    invest    lease    modernize    modify    move    obtain    plan    project    purchase    replace    spend    upgrade    use   def preprocesstextnolemmatization  text   token  refindall  rbwb   textlower    return tokens def calculatefinalfrequency  row  searchphrase  keyphrase   text  rowlower   token  preprocesstextnolemmatization  text  searchphrase   phraselower   phrase searchphrases  keyphrase   phraselower   phrase keyphrase  allmatche  set   tokenlen  len  tokens  idx  token enumerate  token    tokenstartswith  key  key keyphrase   windowstart  max  0  idx  10  windowend  min  tokenlen  idx  10  1  windowtoken  token  windowstart  windowend  windowtext      join  windowtoken  phrase searchphrases  phrase windowtext  allmatchesadd  phrase  return len  allmatches  df   frequency    df   item7   apply  lambda x  calculatefinalfrequency  x  searchwords  keywords   print  df  return item7 frequency 0 last year  avg acquire alibaba  6 1 planning install end  point security   2,Task-Specific Queries
Error in getting Captum text explanations for text classification,"I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset import pandas as pd import torch from torch.utils.data import DataLoader from transformers import BertTokenizer, BertForSequenceClassification, AdamW from sklearn.metrics import accuracy_score from captum.attr import IntegratedGradients # Loading data train_df = pd.read_csv('train_dataset.csv') test_df = pd.read_csv('test_dataset.csv') # Tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') def preprocess_data(df, tokenizer, max_len=128): inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"") labels = torch.tensor(df['label'].values) return inputs, labels train_inputs, train_labels = preprocess_data(train_df, tokenizer) test_inputs, test_labels = preprocess_data(test_df, tokenizer) # DataLoader train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels) train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels) test_loader = DataLoader(test_dataset, batch_size=16) # Model setup device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device) # Optimizer optimizer = AdamW(model.parameters(), lr=5e-5) # Training Loop model.train() for epoch in range(3): # Train for 3 epochs for batch in train_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] optimizer.zero_grad() outputs = model(input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss loss.backward() optimizer.step() print(f""Epoch {epoch+1} loss: {loss.item()}"") # Evaluation model.eval() correct_predictions = [] with torch.no_grad(): for batch in test_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] outputs = model(input_ids, attention_mask=attention_mask) preds = torch.argmax(outputs.logits, dim=1) correct_predictions.extend( (preds == labels).cpu().numpy().tolist() ) accuracy = accuracy_score(test_labels.numpy(), correct_predictions) print(f""Test Accuracy: {accuracy:.2f}"") # Integrated Gradients ig = IntegratedGradients(model) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device, dtype=torch.long) # Explicitly convert to LongTensor attention_mask = inputs['attention_mask'].to(device, dtype=torch.long) # Explicitly convert to LongTensor print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) # forward function for IG def forward_func(input_ids): outputs = model(input_ids, attention_mask=attention_mask) return outputs.logits # Applying Integrated Gradients attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) # Analysing influential words for correctly predicted texts for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words) But I am getting the following error in running the above. Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( Epoch 1 loss: 0.4719192385673523 Epoch 2 loss: 0.39585667848587036 Epoch 3 loss: 0.14659778773784637 Test Accuracy: 0.70 Input IDs shape: torch.Size([1, 8]) dtype: torch.int64 Attention mask shape: torch.Size([1, 8]) dtype: torch.int64 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-9-f047b509c98d> in <cell line: 90>() 90 for idx, correct in enumerate(correct_predictions): 91 if correct: ---> 92 influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) 93 print(f""Influential words for text: {test_df['text'].iloc[idx]}"") 94 print(influential_words) 18 frames /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 2549 # remove once script supports set_grad_enabled 2550 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -> 2551 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 2552 2553 RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)","['machine-learning', 'pytorch', 'nlp', 'huggingface-transformers', 'text-classification']",1,"You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly. I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial https://captum.ai/tutorials/Bert_SQUAD_Interpret Below please find snippet that works: from captum.attr import LayerIntegratedGradients def custom_forward(inputs): preds = predict(inputs) return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1) lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device) attention_mask = inputs['attention_mask'].to(device) # print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) # print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) attributions, delta = lig.attribute(input_ids, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) results = [] for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words)",2024-12-03 12:47:45,2024-12-03 16:12:13,89,https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification,"Error in getting Captum text explanations for text classification I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset import pandas as pd import torch from torch.utils.data import DataLoader from transformers import BertTokenizer, BertForSequenceClassification, AdamW from sklearn.metrics import accuracy_score from captum.attr import IntegratedGradients # Loading data train_df = pd.read_csv('train_dataset.csv') test_df = pd.read_csv('test_dataset.csv') # Tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') def preprocess_data(df, tokenizer, max_len=128): inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"") labels = torch.tensor(df['label'].values) return inputs, labels train_inputs, train_labels = preprocess_data(train_df, tokenizer) test_inputs, test_labels = preprocess_data(test_df, tokenizer) # DataLoader train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels) train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels) test_loader = DataLoader(test_dataset, batch_size=16) # Model setup device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device) # Optimizer optimizer = AdamW(model.parameters(), lr=5e-5) # Training Loop model.train() for epoch in range(3): # Train for 3 epochs for batch in train_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] optimizer.zero_grad() outputs = model(input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss loss.backward() optimizer.step() print(f""Epoch {epoch+1} loss: {loss.item()}"") # Evaluation model.eval() correct_predictions = [] with torch.no_grad(): for batch in test_loader: input_ids, attention_mask, labels = [x.to(device) for x in batch] outputs = model(input_ids, attention_mask=attention_mask) preds = torch.argmax(outputs.logits, dim=1) correct_predictions.extend( (preds == labels).cpu().numpy().tolist() ) accuracy = accuracy_score(test_labels.numpy(), correct_predictions) print(f""Test Accuracy: {accuracy:.2f}"") # Integrated Gradients ig = IntegratedGradients(model) def get_influential_words(input_text, model, tokenizer, ig, device): model.eval() # Tokenizing the input text inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128) input_ids = inputs['input_ids'].to(device, dtype=torch.long) # Explicitly convert to LongTensor attention_mask = inputs['attention_mask'].to(device, dtype=torch.long) # Explicitly convert to LongTensor print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype) print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype) # forward function for IG def forward_func(input_ids): outputs = model(input_ids, attention_mask=attention_mask) return outputs.logits # Applying Integrated Gradients attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True) tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist()) token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy() return list(zip(tokens, token_importances)) # Analysing influential words for correctly predicted texts for idx, correct in enumerate(correct_predictions): if correct: influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) print(f""Influential words for text: {test_df['text'].iloc[idx]}"") print(influential_words) But I am getting the following error in running the above. Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( Epoch 1 loss: 0.4719192385673523 Epoch 2 loss: 0.39585667848587036 Epoch 3 loss: 0.14659778773784637 Test Accuracy: 0.70 Input IDs shape: torch.Size([1, 8]) dtype: torch.int64 Attention mask shape: torch.Size([1, 8]) dtype: torch.int64 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-9-f047b509c98d> in <cell line: 90>() 90 for idx, correct in enumerate(correct_predictions): 91 if correct: ---> 92 influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device) 93 print(f""Influential words for text: {test_df['text'].iloc[idx]}"") 94 print(influential_words) 18 frames /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse) 2549 # remove once script supports set_grad_enabled 2550 _no_grad_embedding_renorm_(weight, input, max_norm, norm_type) -> 2551 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 2552 2553 RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset import panda pd import torch torchutilsdata import dataloader transformer import berttokenizer  bertforsequenceclassification  adamw sklearnmetric import accuracyscore captumattr import integratedgradients  loading datum traindf  pdreadcsv   traindatasetcsv   testdf  pdreadcsv   testdatasetcsv    tokenizer tokenizer  berttokenizerfrompretraine   bert  base  uncase   def preprocessdata  df  tokenizer  maxlen128   input  tokenizer  list  df   text     pad  true  truncation  true  maxlength  maxlen  returntensors  pt   label  torchtensor  df   label   values  return input  label traininput  trainlabel  preprocessdata  traindf  tokenizer  testinput  testlabel  preprocessdata  testdf  tokenizer   dataloader traindataset  torchutilsdata  tensordataset  traininput   inputids    traininput   attentionmask    trainlabel  trainloader  dataloader  traindataset  batchsize16  shuffle  true  testdataset  torchutilsdata  tensordataset  testinput   inputids    testinput   attentionmask    testlabel  testloader  dataloader  testdataset  batchsize16   model setup device  torchdevice    cuda  torchcudaisavailable   else   cpu   model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels2  to  device   optimizer optimizer  adamw  modelparameter    lr5e5   training loop modeltrain   epoch range  3    train 3 epoch batch trainloader  inputids  attentionmask  label   xto  device  x batch  optimizerzerograd   output  model  inputids  attentionmask  attentionmask  label  label  loss  outputsloss lossbackward   optimizerstep   print  f  epoch  epoch1  loss   lossitem       evaluation modeleval   correctprediction    torchnograd    batch testloader  inputid  attentionmask  label   xto  device  x batch  output  model  inputids  attentionmask  attentionmask  pred  torchargmax  outputslogit  dim1  correctpredictionsextend   pred   label  cpu   numpy   tolist    accuracy  accuracyscore  testlabelsnumpy    correctprediction  print  f  test accuracy   accuracy  2f     integrated gradients ig  integratedgradients  model  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  dtype  torchlong   explicitly convert longtensor attentionmask  input   attentionmask   to  device  dtype  torchlong   explicitly convert longtensor print    input id shape    inputidsshape    dtype    inputidsdtype  print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype   forward function ig def forwardfunc  inputids   output  model  inputids  attentionmask  attentionmask  return outputslogit  apply integrated gradient attribution  delta  igattribute  inputids  target1  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances    analyse influential word correctly predict text idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword  get follow error run  weight bertforsequenceclassification initialize model checkpoint bert  base  uncase newly initialize    classifierbia    classifierweight   probably train model down  stream task able use prediction inference  usr  local  lib  python310  dist  package  transformer  optimizationpy591  futurewarning  implementation adamw deprecate remove future version  use pytorch implementation torchoptim  adamw instead  set  nodeprecationwarne  true  disable warning warningswarn  epoch 1 loss  04719192385673523 epoch 2 loss  039585667848587036 epoch 3 loss  014659778773784637 test accuracy  070 input id shape  torch  size   1  8   dtype  torchint64 attention mask shape  torch  size   1  8   dtype  torchint64                                       runtimeerror traceback  recent call last   ipython  input9  f047b509c98d   cell line  90    90 idx  correct enumerate  correctprediction   91 correct     92 influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  93 print  f  influential word text   testdf   text   iloc  idx     94 print  influentialword  18 frame usr  local  lib  python310  dist  package  torch  nn  functionalpy embed  input  weight  paddingidx  maxnorm  normtype  scalegradbyfreq  sparse  2549  remove script support setgradenable 2550  nogradembeddingrenorm   weight  input  maxnorm  normtype    2551 return torchembedding  weight  input  paddingidx  scalegradbyfreq  sparse  2552 2553 runtimeerror  expect tensor argument  1  index  one follow scalar type  long  int  get torchcuda  floattensor instead  check argument embed ,need slightly change gradient calculation class  also  not include forwardfunc gradient class constructor  attribute method able launch stuff properly  think use layerintegratedgradient well debug bert  line tutorial   captumai  tutorial  bertsquadinterpret please find snippet work  captumattr import layerintegratedgradients def customforward  input   pred  predict  input  return torchsoftmax  pred  dim  1   0   1  unsqueeze  1  lig  layerintegratedgradients  customforward  modelbertembedding  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  attentionmask  input   attentionmask   to  device   print    input id shape    inputidsshape    dtype    inputidsdtype   print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype  attribution  delta  ligattribute  inputid  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances   result    idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword ,error get captum text explanation text classification follow code use identify influential word use correctly predict text test dataset import panda pd import torch torchutilsdata import dataloader transformer import berttokenizer  bertforsequenceclassification  adamw sklearnmetric import accuracyscore captumattr import integratedgradients  loading datum traindf  pdreadcsv   traindatasetcsv   testdf  pdreadcsv   testdatasetcsv    tokenizer tokenizer  berttokenizerfrompretraine   bert  base  uncase   def preprocessdata  df  tokenizer  maxlen128   input  tokenizer  list  df   text     pad  true  truncation  true  maxlength  maxlen  returntensors  pt   label  torchtensor  df   label   values  return input  label traininput  trainlabel  preprocessdata  traindf  tokenizer  testinput  testlabel  preprocessdata  testdf  tokenizer   dataloader traindataset  torchutilsdata  tensordataset  traininput   inputids    traininput   attentionmask    trainlabel  trainloader  dataloader  traindataset  batchsize16  shuffle  true  testdataset  torchutilsdata  tensordataset  testinput   inputids    testinput   attentionmask    testlabel  testloader  dataloader  testdataset  batchsize16   model setup device  torchdevice    cuda  torchcudaisavailable   else   cpu   model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels2  to  device   optimizer optimizer  adamw  modelparameter    lr5e5   training loop modeltrain   epoch range  3    train 3 epoch batch trainloader  inputids  attentionmask  label   xto  device  x batch  optimizerzerograd   output  model  inputids  attentionmask  attentionmask  label  label  loss  outputsloss lossbackward   optimizerstep   print  f  epoch  epoch1  loss   lossitem       evaluation modeleval   correctprediction    torchnograd    batch testloader  inputid  attentionmask  label   xto  device  x batch  output  model  inputids  attentionmask  attentionmask  pred  torchargmax  outputslogit  dim1  correctpredictionsextend   pred   label  cpu   numpy   tolist    accuracy  accuracyscore  testlabelsnumpy    correctprediction  print  f  test accuracy   accuracy  2f     integrated gradients ig  integratedgradients  model  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  dtype  torchlong   explicitly convert longtensor attentionmask  input   attentionmask   to  device  dtype  torchlong   explicitly convert longtensor print    input id shape    inputidsshape    dtype    inputidsdtype  print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype   forward function ig def forwardfunc  inputids   output  model  inputids  attentionmask  attentionmask  return outputslogit  apply integrated gradient attribution  delta  igattribute  inputids  target1  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances    analyse influential word correctly predict text idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword  get follow error run  weight bertforsequenceclassification initialize model checkpoint bert  base  uncase newly initialize    classifierbia    classifierweight   probably train model down  stream task able use prediction inference  usr  local  lib  python310  dist  package  transformer  optimizationpy591  futurewarning  implementation adamw deprecate remove future version  use pytorch implementation torchoptim  adamw instead  set  nodeprecationwarne  true  disable warning warningswarn  epoch 1 loss  04719192385673523 epoch 2 loss  039585667848587036 epoch 3 loss  014659778773784637 test accuracy  070 input id shape  torch  size   1  8   dtype  torchint64 attention mask shape  torch  size   1  8   dtype  torchint64                                       runtimeerror traceback  recent call last   ipython  input9  f047b509c98d   cell line  90    90 idx  correct enumerate  correctprediction   91 correct     92 influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  93 print  f  influential word text   testdf   text   iloc  idx     94 print  influentialword  18 frame usr  local  lib  python310  dist  package  torch  nn  functionalpy embed  input  weight  paddingidx  maxnorm  normtype  scalegradbyfreq  sparse  2549  remove script support setgradenable 2550  nogradembeddingrenorm   weight  input  maxnorm  normtype    2551 return torchembedding  weight  input  paddingidx  scalegradbyfreq  sparse  2552 2553 runtimeerror  expect tensor argument  1  index  one follow scalar type  long  int  get torchcuda  floattensor instead  check argument embed  need slightly change gradient calculation class  also  not include forwardfunc gradient class constructor  attribute method able launch stuff properly  think use layerintegratedgradient well debug bert  line tutorial   captumai  tutorial  bertsquadinterpret please find snippet work  captumattr import layerintegratedgradients def customforward  input   pred  predict  input  return torchsoftmax  pred  dim  1   0   1  unsqueeze  1  lig  layerintegratedgradients  customforward  modelbertembedding  def getinfluentialword  inputtext  model  tokenizer  ig  device   modeleval    tokenize input text input  tokenizer  inputtext  returntensors  pt   truncation  true  pad  true  maxlength128  inputids  input   inputids   to  device  attentionmask  input   attentionmask   to  device   print    input id shape    inputidsshape    dtype    inputidsdtype   print    attention mask shape    attentionmaskshape    dtype    attentionmaskdtype  attribution  delta  ligattribute  inputid  returnconvergencedelta  true  token  tokenizerconvertidstotoken  inputid  0  tolist    tokenimportance  attributionssum  dim2  squeeze  0  detach   cpu   numpy   return list  zip  token  tokenimportances   result    idx  correct enumerate  correctprediction   correct  influentialword  getinfluentialword  testdf   text   iloc  idx   model  tokenizer  ig  device  print  f  influential word text   testdf   text   iloc  idx     print  influentialword ,Basic Understanding
euclidian distance from word to sentence after doing Vectorizer,"I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word ""king"". df['king'] I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.","['pandas', 'dataframe', 'nlp', 'text-classification', 'tf-idf']",1,"I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores: import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity import numpy as np data = { 'text': [ ""The king sat on the throne with wisdom."", ""A queen ruled the kingdom alongside the king."", ""Knights were loyal to their king."", ""The empire prospered under the rule of a wise monarch."" ] } df = pd.DataFrame(data) tfidf = TfidfVectorizer() tfidf_matrix = tfidf.fit_transform(df['text']) try: king_vector = tfidf.transform([""king""]).toarray() except KeyError: print(""The word 'king' is not in the vocabulary."") king_vector = np.zeros((1, tfidf_matrix.shape[1])) similarities = cosine_similarity(tfidf_matrix, king_vector).flatten() feature_names = np.array(tfidf.get_feature_names_out()) def get_top_n_words(row_vector, top_n=5): indices = row_vector.argsort()[::-1][:top_n] return feature_names[indices] averages = [] for i in range(tfidf_matrix.shape[0]): sentence_vector = tfidf_matrix[i].toarray().flatten() top_words = get_top_n_words(sentence_vector) top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words] averages.append(np.mean(top_similarities)) df['king_similarity'] = similarities df['avg_closest_similarity'] = averages print(df) which would give you text king_similarity \ 0 The king sat on the throne with wisdom. 0.240614 1 A queen ruled the kingdom alongside the king. 0.259779 2 Knights were loyal to their king. 0.274487 3 The empire prospered under the rule of a wise ... 0.000000 avg_closest_similarity 0 0.0 1 0.0 2 0.0 3 0.0 That being said, if you absolutely want to focus on Euclidean distance, here is a method: import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer import numpy as np from scipy.spatial.distance import euclidean data = { 'text': [ ""The king sat on the throne with wisdom."", ""A queen ruled the kingdom alongside the king."", ""Knights were loyal to their king."", ""The empire prospered under the rule of a wise monarch."" ] } df = pd.DataFrame(data) tfidf = TfidfVectorizer() tfidf_matrix = tfidf.fit_transform(df['text']).toarray() feature_names = tfidf.get_feature_names_out() if ""king"" in feature_names: king_index = np.where(feature_names == ""king"")[0][0] king_vector = np.zeros_like(tfidf_matrix[0]) king_vector[king_index] = 1 else: print(""The word 'king' is not in the vocabulary."") king_vector = np.zeros_like(tfidf_matrix[0]) df['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix] print(df) which gives text king_distance 0 The king sat on the throne with wisdom. 1.232385 1 A queen ruled the kingdom alongside the king. 1.216734 2 Knights were loyal to their king. 1.204586 3 The empire prospered under the rule of a wise ... 1.414214",2024-12-03 12:25:05,2024-12-03 14:51:12,46,https://stackoverflow.com/questions/79247594/euclidian-distance-from-word-to-sentence-after-doing-vectorizer,"euclidian distance from word to sentence after doing Vectorizer I have dataframe with 1000 text rows. I did TfidfVectorizer. Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word ""king"". df['king'] I thought about taking in each sentence the 5 closet words to the word king and make average of them. I will glad to know how to do that or to hear about another method.",euclidian distance word sentence vectorizer dataframe 1000 text row  tfidfvectorizer  want create new field give distance sentence word want  let say word   king   df   king   think take sentence 5 closet word word king make average  glad know hear another method ,convince euclidean distance would optimal measure  would actually look similarity score  import panda pd sklearnfeatureextractiontext import tfidfvectorizer sklearnmetricspairwise import cosinesimilarity import numpy np datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    try  kingvector  tfidftransform     king    toarray   except keyerror  print    word  king  vocabulary    kingvector  npzeros   1  tfidfmatrixshape  1    similarity  cosinesimilarity  tfidfmatrix  kingvector  flatten   featurename  nparray  tfidfgetfeaturenamesout    def gettopnword  rowvector  topn5   index  rowvectorargsort      1    topn  return featurename  index  average    range  tfidfmatrixshape  0    sentencevector  tfidfmatrix   toarray   flatten   topword  gettopnwords  sentencevector  topsimilaritie   cosinesimilarity  tfidftransform   word    kingvector  flatten    0  word topwords  averagesappend  npmean  topsimilaritie   df   kingsimilarity    similaritie df   avgclosestsimilarity    average print  df  would give text kingsimilarity  0 king sit throne wisdom  0240614 1 queen rule kingdom alongside king  0259779 2 knights loyal king  0274487 3 empire prosper rule wise  0000000 avgclosestsimilarity 0 00 1 00 2 00 3 00 say  absolutely want focus euclidean distance  method  import panda pd sklearnfeatureextractiontext import tfidfvectorizer import numpy np scipyspatialdistance import euclidean datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    toarray   featurename  tfidfgetfeaturenamesout     king  featurename  kingindex  npwhere  featurename     king    0   0  kingvector  npzeroslike  tfidfmatrix  0   kingvector  kingindex   1 else  print    word  king  vocabulary    kingvector  npzeroslike  tfidfmatrix  0   df   kingdistance     euclidean  sentencevector  kingvector  sentencevector tfidfmatrix  print  df  give text kingdistance 0 king sit throne wisdom  1232385 1 queen rule kingdom alongside king  1216734 2 knights loyal king  1204586 3 empire prosper rule wise  1414214,euclidian distance word sentence vectorizer dataframe 1000 text row  tfidfvectorizer  want create new field give distance sentence word want  let say word   king   df   king   think take sentence 5 closet word word king make average  glad know hear another method  convince euclidean distance would optimal measure  would actually look similarity score  import panda pd sklearnfeatureextractiontext import tfidfvectorizer sklearnmetricspairwise import cosinesimilarity import numpy np datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    try  kingvector  tfidftransform     king    toarray   except keyerror  print    word  king  vocabulary    kingvector  npzeros   1  tfidfmatrixshape  1    similarity  cosinesimilarity  tfidfmatrix  kingvector  flatten   featurename  nparray  tfidfgetfeaturenamesout    def gettopnword  rowvector  topn5   index  rowvectorargsort      1    topn  return featurename  index  average    range  tfidfmatrixshape  0    sentencevector  tfidfmatrix   toarray   flatten   topword  gettopnwords  sentencevector  topsimilaritie   cosinesimilarity  tfidftransform   word    kingvector  flatten    0  word topwords  averagesappend  npmean  topsimilaritie   df   kingsimilarity    similaritie df   avgclosestsimilarity    average print  df  would give text kingsimilarity  0 king sit throne wisdom  0240614 1 queen rule kingdom alongside king  0259779 2 knights loyal king  0274487 3 empire prosper rule wise  0000000 avgclosestsimilarity 0 00 1 00 2 00 3 00 say  absolutely want focus euclidean distance  method  import panda pd sklearnfeatureextractiontext import tfidfvectorizer import numpy np scipyspatialdistance import euclidean datum    text      king sit throne wisdom       queen rule kingdom alongside king       knights loyal king       empire prosper rule wise monarch     df  pd  dataframe  datum  tfidf  tfidfvectorizer   tfidfmatrix  tfidffittransform  df   text    toarray   featurename  tfidfgetfeaturenamesout     king  featurename  kingindex  npwhere  featurename     king    0   0  kingvector  npzeroslike  tfidfmatrix  0   kingvector  kingindex   1 else  print    word  king  vocabulary    kingvector  npzeroslike  tfidfmatrix  0   df   kingdistance     euclidean  sentencevector  kingvector  sentencevector tfidfmatrix  print  df  give text kingdistance 0 king sit throne wisdom  1232385 1 queen rule kingdom alongside king  1216734 2 knights loyal king  1204586 3 empire prosper rule wise  1414214,Task-Specific Queries
Llama-3.2-1B-Instruct generate inconsistent output,"I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0 , it still generates inconsistent output. This is how my pipeline looks like: pipe = pipeline( ""text-generation"", model=model_id, torch_dtype=torch.bfloat16, device_map=""mps"", model_kwargs={""temperature"": 0.0, ""do_sample"":True, ""top_p"":0.0, ""top_k"":0,}, ) Any idea how to solve this issue?","['python', 'nlp', 'huggingface-transformers', 'large-language-model']",2,"The model inconsistent output can be due to two main factors: 1. Temperature: setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail. So the best option is to set temperature to very low values such as 0.00001 instead of zero. 2. do_sample You already set it false, and it should remain that way only.",2024-11-28 13:02:37,2024-12-03 07:18:24,689,https://stackoverflow.com/questions/79234004/llama-3-2-1b-instruct-generate-inconsistent-output,"Llama-3.2-1B-Instruct generate inconsistent output I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0 , it still generates inconsistent output. This is how my pipeline looks like: pipe = pipeline( ""text-generation"", model=model_id, torch_dtype=torch.bfloat16, device_map=""mps"", model_kwargs={""temperature"": 0.0, ""do_sample"":True, ""top_p"":0.0, ""top_k"":0,}, ) Any idea how to solve this issue?",llama32  1b  instruct generate inconsistent output want use llama32  1b  instruct model  although set   temperature   00    topp   00   topk  0  still generate inconsistent output  pipeline look like  pipe  pipeline    text  generation   model  modelid  torchdtype  torchbfloat16  devicemap  mp   modelkwargs    temperature   00    dosample   true    topp   00    topk  0     idea solve issue ,model inconsistent output due two main factor  1  temperature  set temperature zero give inconsistent result  refer opeani discussion page detail  good option set temperature low value 000001 instead zero  2  dosample already set false  remain way ,llama32  1b  instruct generate inconsistent output want use llama32  1b  instruct model  although set   temperature   00    topp   00   topk  0  still generate inconsistent output  pipeline look like  pipe  pipeline    text  generation   model  modelid  torchdtype  torchbfloat16  devicemap  mp   modelkwargs    temperature   00    dosample   true    topp   00    topk  0     idea solve issue  model inconsistent output due two main factor  1  temperature  set temperature zero give inconsistent result  refer opeani discussion page detail  good option set temperature low value 000001 instead zero  2  dosample already set false  remain way ,Basic Understanding
Using an AWS service to execute a python script that will extract keywords from text using keyBERT?,"I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.","['python', 'amazon-web-services', 'aws-lambda', 'nlp', 'large-language-model']",2,"In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do: Package your KeyBERT script in a lambda and easily deploy it with a container. Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.",2024-11-15 11:13:36,2024-11-15 12:43:31,60,https://stackoverflow.com/questions/79192130/using-an-aws-service-to-execute-a-python-script-that-will-extract-keywords-from,"Using an AWS service to execute a python script that will extract keywords from text using keyBERT? I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords. Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.",use aws service execute python script extract keyword text use keybert  simple python script give two block text  extract keyword use keybert  compare list keyword sort two list depend list share keyword  aws service would well fit need  want able esentially spin need  give block text  execute return result  not want integrate project not use python   ve attempt use lambda  m concerned potential cost run  thank ,case  would normally think two resource align good practice aws software engineering  sagemaker lambda  model  m use resource  intensive require gpu acceleration would go sagemaker otherwise lambda good solution  case  be d  package keybert script lambda easily deploy container  invoke whenever need process text block  aws lambda charge execution time   cost  efficient occasional task ,use aws service execute python script extract keyword text use keybert  simple python script give two block text  extract keyword use keybert  compare list keyword sort two list depend list share keyword  aws service would well fit need  want able esentially spin need  give block text  execute return result  not want integrate project not use python   ve attempt use lambda  m concerned potential cost run  thank  case  would normally think two resource align good practice aws software engineering  sagemaker lambda  model  m use resource  intensive require gpu acceleration would go sagemaker otherwise lambda good solution  case  be d  package keybert script lambda easily deploy container  invoke whenever need process text block  aws lambda charge execution time   cost  efficient occasional task ,Basic Understanding
Normalization of token embeddings in BERT encoder blocks,"Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?","['nlp', 'normalization', 'bert-language-model', 'attention-model']",2,I tracked down full details of layer normalization (LN) in BERT here . Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.,2024-11-11 14:30:31,2024-11-29 21:21:04,178,https://stackoverflow.com/questions/79178041/normalization-of-token-embeddings-in-bert-encoder-blocks,"Normalization of token embeddings in BERT encoder blocks Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?",normalization token embedding bert encoder block follow multi  headed attention layer bert encoder block  layer normalization done separately embed token  ie  one mean variance per token embed   concatenate vector token embedding  mean variance embedding  ,track full detail layer normalization  ln  bert  mean variance compute per token  weight bias parameter learn ln per token   per embed dimension ,normalization token embedding bert encoder block follow multi  headed attention layer bert encoder block  layer normalization done separately embed token  ie  one mean variance per token embed   concatenate vector token embedding  mean variance embedding   track full detail layer normalization  ln  bert  mean variance compute per token  weight bias parameter learn ln per token   per embed dimension ,Basic Understanding
How to convert character indices to BERT token indices,"I am working with a question-answer dataset UCLNLP/adversarial_qa . from datasets import load_dataset ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: d0 = ds['train'][0] d0 {'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95', 'title': 'Brain', 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.', 'question': 'What sare the benifts of the blood brain barrir?', 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]}, 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}} After tokenization, the answer indices are 56 and 16: from transformers import BertTokenizerFast bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61]) 'isolated from the bloodstream' I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:","['python', 'nlp', 'dataset', 'large-language-model', 'bert-language-model']",1,"You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices. The following function does the above for you: def get_token_indices(example): # Tokenize with `return_offsets_mapping=True` to get character offsets for each token encoded = tokenizer( example['question'], example['context'], return_offsets_mapping=True ) # Find character start and end from the original answer char_start = example['answers']['answer_start'][0] char_end = char_start + len(example['answers']['text'][0]) # Identify token indices for the answer start_token_idx = None end_token_idx = None for i, (start, end) in enumerate(encoded['offset_mapping']): if start <= char_start < end: start_token_idx = i if start < char_end <= end: end_token_idx = i break example['answer_start_token_idx'] = start_token_idx example['answer_end_token_idx'] = end_token_idx return example Here's how you can use and test this function: ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) tokenized_ds = ds['train'].map(get_token_indices) # Example d0_tokenized = tokenized_ds[0] print(""Tokenized start index:"", d0_tokenized['answer_start_token_idx']) print(""Tokenized end index:"", d0_tokenized['answer_end_token_idx']) answer_tokens = tokenizer.decode( tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1] ) print(""Tokenized answer:"", answer_tokens) Output: Tokenized start index: 56 Tokenized end index: 60 Tokenized answer: isolated from the bloodstream",2024-11-09 15:15:33,2024-11-10 15:18:14,37,https://stackoverflow.com/questions/79173053/how-to-convert-character-indices-to-bert-token-indices,"How to convert character indices to BERT token indices I am working with a question-answer dataset UCLNLP/adversarial_qa . from datasets import load_dataset ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"") How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset: d0 = ds['train'][0] d0 {'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95', 'title': 'Brain', 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.', 'question': 'What sare the benifts of the blood brain barrir?', 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]}, 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}} After tokenization, the answer indices are 56 and 16: from transformers import BertTokenizerFast bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True) bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61]) 'isolated from the bloodstream' I want to create a new dataset with the answer's token indices, e.g., 56 ad 60. This is from a linkedin learning class . The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:",convert character index bert token index work question  answer dataset uclnlp  adversarialqa  dataset import loaddataset ds  loaddataset    uclnlp  adversarialqa     adversarialqa   map character  base answer indice token  base index tokenize context question together use tokenizer like bert  s example row dataset  d0  ds   train    0  d0   i d    7ba1e8f4261d3170fcf42e84a81dd749116fae95    title    brain    context    another approach brain function examine consequence damage specific brain area  even though protect skull meninge  surround cerebrospinal fluid  isolated bloodstream blood  brain barrier  delicate nature brain make vulnerable numerous disease several type damage  human  effect stroke type brain damage key source information brain function  ability experimentally control nature damage  however  information often difficult interpret  animal study  commonly involve rat  possible use electrode locally inject chemical produce precise pattern damage examine consequence behavior     question    what sare benift blood brain barrir     answer     text     isolated bloodstream     answerstart    195     metadata     split    train    modelintheloop    combined    tokenization  answer indice 56 16  transformer import berttokenizerfast berttokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  berttokenizerdecode  berttokenizerencode  d0   question    d0   context     5661    isolated bloodstream  want create new dataset answer s token index  eg  56 ad 60  linkedin learn class  instructor conversion create csv file share code  expect result ,encode question context  locate token span answer within tokenized context  update dataset token  level index  follow function  def gettokenindice  example    tokenize  returnoffsetsmapping  true  get character offset token encode  tokenizer  example   question    example   context    returnoffsetsmappe  true   find character start end original answer charstart  example   answer     answerstart    0  charend  charstart  len  example   answer     text    0    identify token index answer starttokenidx  none endtokenidx  none   start  end  enumerate  encode   offsetmappe     start   charstart  end  starttokenidx  start  charend   end  endtokenidx  break example   answerstarttokenidx    starttokenidx example   answerendtokenidx    endtokenidx return example s use test function  ds  loaddataset    uclnlp  adversarialqa     adversarialqa   tokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  tokenizedds  ds   train   map  gettokenindice   example d0tokenized  tokenizedds  0  print    tokenized start index    d0tokenize   answerstarttokenidx    print    tokenized end index    d0tokenize   answerendtokenidx    answertokens  tokenizerdecode  tokenizerencode  d0tokenized   question    d0tokenize   context     d0tokenize   answerstarttokenidx    d0tokenize   answerendtokenidx   1   print    tokenized answer    answertokens  output  tokenized start index  56 tokenized end index  60 tokenized answer  isolate bloodstream,convert character index bert token index work question  answer dataset uclnlp  adversarialqa  dataset import loaddataset ds  loaddataset    uclnlp  adversarialqa     adversarialqa   map character  base answer indice token  base index tokenize context question together use tokenizer like bert  s example row dataset  d0  ds   train    0  d0   i d    7ba1e8f4261d3170fcf42e84a81dd749116fae95    title    brain    context    another approach brain function examine consequence damage specific brain area  even though protect skull meninge  surround cerebrospinal fluid  isolated bloodstream blood  brain barrier  delicate nature brain make vulnerable numerous disease several type damage  human  effect stroke type brain damage key source information brain function  ability experimentally control nature damage  however  information often difficult interpret  animal study  commonly involve rat  possible use electrode locally inject chemical produce precise pattern damage examine consequence behavior     question    what sare benift blood brain barrir     answer     text     isolated bloodstream     answerstart    195     metadata     split    train    modelintheloop    combined    tokenization  answer indice 56 16  transformer import berttokenizerfast berttokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  berttokenizerdecode  berttokenizerencode  d0   question    d0   context     5661    isolated bloodstream  want create new dataset answer s token index  eg  56 ad 60  linkedin learn class  instructor conversion create csv file share code  expect result  encode question context  locate token span answer within tokenized context  update dataset token  level index  follow function  def gettokenindice  example    tokenize  returnoffsetsmapping  true  get character offset token encode  tokenizer  example   question    example   context    returnoffsetsmappe  true   find character start end original answer charstart  example   answer     answerstart    0  charend  charstart  len  example   answer     text    0    identify token index answer starttokenidx  none endtokenidx  none   start  end  enumerate  encode   offsetmappe     start   charstart  end  starttokenidx  start  charend   end  endtokenidx  break example   answerstarttokenidx    starttokenidx example   answerendtokenidx    endtokenidx return example s use test function  ds  loaddataset    uclnlp  adversarialqa     adversarialqa   tokenizer  berttokenizerfastfrompretraine   bert  large  uncased   returntokentypeid  true  tokenizedds  ds   train   map  gettokenindice   example d0tokenized  tokenizedds  0  print    tokenized start index    d0tokenize   answerstarttokenidx    print    tokenized end index    d0tokenize   answerendtokenidx    answertokens  tokenizerdecode  tokenizerencode  d0tokenized   question    d0tokenize   context     d0tokenize   answerstarttokenidx    d0tokenize   answerendtokenidx   1   print    tokenized answer    answertokens  output  tokenized start index  56 tokenized end index  60 tokenized answer  isolate bloodstream,Implementation Issues
How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?,"I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage. I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing. Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process? Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes? If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects? Any suggestions or examples would be greatly appreciated! Thank you!","['nlp', 'multiprocessing', 'python-multiprocessing', 'spacy']",2,"I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application. Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved. An example of implementing such a microservice using FastAPI + Docker could look like this: # main.py: FastAPI service with spaCy model from fastapi import FastAPI import spacy app = FastAPI() nlp = spacy.load(""en_core_web_lg"") # Load model once @app.post(""/process/"") async def process_text(text: str): doc = nlp(text) return {""tokens"": [(token.text, token.pos_) for token in doc]} To containerize above FastAPI service: # Dockerfile for the NLP model microservice FROM python:3.9-slim COPY requirements.txt . RUN pip install -r requirements.txt && python -m spacy download en_core_web_lg COPY . /app WORKDIR /app CMD [""gunicorn"", ""-w"", ""4"", ""-k"", ""uvicorn.workers.UvicornWorker"", ""main:app""]",2024-11-05 15:49:33,2024-11-06 10:41:19,96,https://stackoverflow.com/questions/79159805/how-can-i-share-a-complex-spacy-nlp-model-across-multiple-python-processes-to-mi,"How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage? I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage. I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing. Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process? Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes? If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects? Any suggestions or examples would be greatly appreciated! Thank you!",share complex spacy nlp model across multiple python process minimize memory usage   m work multiprocesse python application multiple process need access large  pre  loaded spacy nlp model  eg  encoreweblg   since model memory  intensive  want avoid load separately process  since quickly run main memory object read  only  instead   like load share location process read without duplicate memory usage  look multiprocesse  manager multiprocessingsharedmemory  approach seem well suited numpy array  raw datum buffer simple object  complex object internal reference like nlp model  also look mpi s mpiwin  allocateshared   run issue  use redis server make rank 0 processing work mpi  since process do single rank  defeat propose use multiprocessing  efficient way share spacy model instance across multiple process python avoid reloading process  library technique specifically suited sharing complex  read  only object like nlp model memory across process  multiprocesse  manager sharedmemory viable  way improve performance reduce memory overhead work complex object  suggestion example would greatly appreciate  thank ,would strongly advise treat nlp model like python object  would always prefer load nlp model use microservice approach  align ml  software engineering good practice separate model logic main application  instead load model process  memory  intensive   model load dedicated service  setup allow model use multiple part application without duplicate memory usage  make efficient  modular  scalable  concern memory efficiency address  scalability modularity also improve  example implement microservice use fastapi  docker could look like   mainpy  fastapi service spacy model fastapi import fastapi import spacy app  fastapi   nlp  spacyload    encoreweblg    load model  apppost    process   async def processtext  text  str   doc  nlp  text  return    token     tokentext  tokenpos   token doc   containerize fastapi service   dockerfile nlp model microservice python39  slim copy requirementstxt  run pip install r requirementstxt   python m spacy download encoreweblg copy  app workdir app cmd    gunicorn     w     4     k     uvicornworker  uvicornworker     main  app  ,share complex spacy nlp model across multiple python process minimize memory usage   m work multiprocesse python application multiple process need access large  pre  loaded spacy nlp model  eg  encoreweblg   since model memory  intensive  want avoid load separately process  since quickly run main memory object read  only  instead   like load share location process read without duplicate memory usage  look multiprocesse  manager multiprocessingsharedmemory  approach seem well suited numpy array  raw datum buffer simple object  complex object internal reference like nlp model  also look mpi s mpiwin  allocateshared   run issue  use redis server make rank 0 processing work mpi  since process do single rank  defeat propose use multiprocessing  efficient way share spacy model instance across multiple process python avoid reloading process  library technique specifically suited sharing complex  read  only object like nlp model memory across process  multiprocesse  manager sharedmemory viable  way improve performance reduce memory overhead work complex object  suggestion example would greatly appreciate  thank  would strongly advise treat nlp model like python object  would always prefer load nlp model use microservice approach  align ml  software engineering good practice separate model logic main application  instead load model process  memory  intensive   model load dedicated service  setup allow model use multiple part application without duplicate memory usage  make efficient  modular  scalable  concern memory efficiency address  scalability modularity also improve  example implement microservice use fastapi  docker could look like   mainpy  fastapi service spacy model fastapi import fastapi import spacy app  fastapi   nlp  spacyload    encoreweblg    load model  apppost    process   async def processtext  text  str   doc  nlp  text  return    token     tokentext  tokenpos   token doc   containerize fastapi service   dockerfile nlp model microservice python39  slim copy requirementstxt  run pip install r requirementstxt   python m spacy download encoreweblg copy  app workdir app cmd    gunicorn     w     4     k     uvicornworker  uvicornworker     main  app  ,Library/Tool-Based Queries
"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing","When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data. https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative. Is there a way to get neutral labels for such examples in RobBERTje? from transformers import RobertaTokenizer, RobertaForSequenceClassification from transformers import pipeline import torch model_name = ""DTAI-KULeuven/robbert-v2-dutch-sentiment"" model = RobertaForSequenceClassification.from_pretrained(model_name) tokenizer = RobertaTokenizer.from_pretrained(model_name) classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer) result1 = classifier('Fhdf') result2 = classifier('Als gisteren inclusief blauw') print(result1) print(result2) Output: [{'label': 'Positive', 'score': 0.7520257234573364}] [{'label': 'Negative', 'score': 0.7538396120071411}]","['python', 'nlp', 'bert-language-model', 'roberta-language-model']",1,"This model was trained only on negative and positive labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral. what you can do is to: 1- Find other models that was trained to include neutral label. 2- Fine-tune this model on a dataset that includes neutral label. 3- Empirically define a threshold based on the confidence outputs and interpret it as neutral . The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as neutral . Here's a sample: def classify_with_neutral(text, threshold=0.5): result = classifier(text)[0] # Get the classification result if result['score'] < threshold: result['label'] = 'Neutral' # Override label to 'Neutral' return result",2024-11-04 11:36:35,2024-11-04 12:04:43,57,https://stackoverflow.com/questions/79155290/dutch-sentiment-analysis-robbertje-outputs-just-positive-negative-labels-netura,"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data. https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative. Is there a way to get neutral labels for such examples in RobBERTje? from transformers import RobertaTokenizer, RobertaForSequenceClassification from transformers import pipeline import torch model_name = ""DTAI-KULeuven/robbert-v2-dutch-sentiment"" model = RobertaForSequenceClassification.from_pretrained(model_name) tokenizer = RobertaTokenizer.from_pretrained(model_name) classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer) result1 = classifier('Fhdf') result2 = classifier('Als gisteren inclusief blauw') print(result1) print(result2) Output: [{'label': 'Positive', 'score': 0.7520257234573364}] [{'label': 'Negative', 'score': 0.7538396120071411}]",dutch sentiment analysis robbertje outputs positive  negative label  netural label miss run dutch sentiment analysis robbertje  output positive  negative label  netural label miss datum    huggingfaceco  dtai  kuleuven  robbert  v2  dutch  sentiment obvious neutral sentence  word eg   fhdf   nonsense   als gisteren inclusief blauw   neutral   evaluate positive negative  way get neutral label example robbertje  transformer import robertatokenizer  robertaforsequenceclassification transformer import pipeline import torch modelname    dtai  kuleuven  robbert  v2  dutch  sentiment  model  robertaforsequenceclassificationfrompretrained  modelname  tokenizer  robertatokenizerfrompretraine  modelname  classifier  pipeline   sentiment  analysis   model  model  tokenizer  tokenizer  result1  classifi   fhdf   result2  classifi   als gisteren inclusief blauw   print  result1  print  result2  output     label    positive    score   07520257234573364      label    negative    score   07538396120071411  ,model train negative positive label  therefore  try categorize every input positive negative  even nonsensical neutral   1 find model train include neutral label  2 fine  tune model dataset include neutral label  3 empirically define threshold base confidence output interpret neutral  first 2 choice extensive effort  would suggest go third option quick workaround  try feeding model neutral input observe range confidence score output  use threshold classify neutral  s sample  def classifywithneutral  text  threshold05   result  classifier  text   0   get classification result result   score    threshold  result   label     neutral   override label  neutral  return result,dutch sentiment analysis robbertje outputs positive  negative label  netural label miss run dutch sentiment analysis robbertje  output positive  negative label  netural label miss datum    huggingfaceco  dtai  kuleuven  robbert  v2  dutch  sentiment obvious neutral sentence  word eg   fhdf   nonsense   als gisteren inclusief blauw   neutral   evaluate positive negative  way get neutral label example robbertje  transformer import robertatokenizer  robertaforsequenceclassification transformer import pipeline import torch modelname    dtai  kuleuven  robbert  v2  dutch  sentiment  model  robertaforsequenceclassificationfrompretrained  modelname  tokenizer  robertatokenizerfrompretraine  modelname  classifier  pipeline   sentiment  analysis   model  model  tokenizer  tokenizer  result1  classifi   fhdf   result2  classifi   als gisteren inclusief blauw   print  result1  print  result2  output     label    positive    score   07520257234573364      label    negative    score   07538396120071411   model train negative positive label  therefore  try categorize every input positive negative  even nonsensical neutral   1 find model train include neutral label  2 fine  tune model dataset include neutral label  3 empirically define threshold base confidence output interpret neutral  first 2 choice extensive effort  would suggest go third option quick workaround  try feeding model neutral input observe range confidence score output  use threshold classify neutral  s sample  def classifywithneutral  text  threshold05   result  classifier  text   0   get classification result result   score    threshold  result   label     neutral   override label  neutral  return result,Basic Understanding
Finding Root Form of Verbs using Curiosity-AI/Catalyst,"I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match PartOfSpeech.VERB but I don't know how to continue from there. This is what I have so far: const string text = ""The disastrous cat runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var sentence in doc.TokensData) { foreach (var token in sentence) { if(token.Tag == PartOfSpeech.VERB) { // so here I'd like to the root form of the verb } } } Any help is greatly appreciated.","['c#', 'nlp']",1,"The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form. (I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.) //// Installed Curiosity.Library v24.10.52882 //// Installed Catalyst v1.0.51118 //// Installed Catalyst.Models.English v1.0.30952 using Catalyst; using Mosaik.Core; const string text = ""The disastrous cat quickly runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var span in doc.Spans) { foreach (var token in span.Tokens) { if (token.POS == PartOfSpeech.VERB) { Console.WriteLine($""Root of the verb '{token.Value}' is '{token.Lemma}'.""); } } } Console.WriteLine(); Console.WriteLine(""Complete; press any key.""); Console.ReadKey(); Note: For this specific sentence, I have added an adverb (""quickly"") before the verb (""runs""). Without this, the library incorrectly interprets ""runs"" as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.",2024-11-01 17:58:01,2024-11-05 17:40:24,143,https://stackoverflow.com/questions/79148979/finding-root-form-of-verbs-using-curiosity-ai-catalyst,"Finding Root Form of Verbs using Curiosity-AI/Catalyst I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match PartOfSpeech.VERB but I don't know how to continue from there. This is what I have so far: const string text = ""The disastrous cat runs after the fat field mouse.""; Catalyst.Models.English.Register(); Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory); var nlp = await Pipeline.ForAsync(Language.English); var doc = new Document(text, Language.English); nlp.ProcessSingle(doc); foreach (var sentence in doc.TokensData) { foreach (var token in sentence) { if(token.Tag == PartOfSpeech.VERB) { // so here I'd like to the root form of the verb } } } Any help is greatly appreciated.",finding root form verbs use curiosity  ai  catalyst  m try find root form verb  run text pipeline identify tokens match partofspeech  verb not know continue  far  const string text    disastrous cat run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var sentence doc  tokensdata   foreach  var token sentence    token  tag   partofspeech  verb    would like root form verb    help greatly appreciate ,follow code  target net 80  illustrate one method obtain root form verb inflect form   annonote  code comment  three nuget package  version  require  code identical original sample    installed curiosity  library v241052882  installed catalyst v1051118  installed catalyst  model  english v1030952 use catalyst  use mosaik  core  const string text    disastrous cat quickly run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var span doc  spans   foreach  var token span  tokens    token  pos   partofspeech  verb   console  writeline    root verb   token  value     token  lemma           console  writeline    console  writeline    complete  press key      console  readkey    note  specific sentence  add adverb    quickly   verb    run    without  library incorrectly interpret   run  noun  depend source text  might issue  believe separate question ask ,finding root form verbs use curiosity  ai  catalyst  m try find root form verb  run text pipeline identify tokens match partofspeech  verb not know continue  far  const string text    disastrous cat run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var sentence doc  tokensdata   foreach  var token sentence    token  tag   partofspeech  verb    would like root form verb    help greatly appreciate  follow code  target net 80  illustrate one method obtain root form verb inflect form   annonote  code comment  three nuget package  version  require  code identical original sample    installed curiosity  library v241052882  installed catalyst v1051118  installed catalyst  model  english v1030952 use catalyst  use mosaik  core  const string text    disastrous cat quickly run fat field mouse     catalyst  model  english  register    storage  current  new diskstorage  appdomain  currentdomain  basedirectory   var nlp  await pipeline  forasync  language  english   var doc  new document  text  language  english   nlp  processsingle  doc   foreach  var span doc  spans   foreach  var token span  tokens    token  pos   partofspeech  verb   console  writeline    root verb   token  value     token  lemma           console  writeline    console  writeline    complete  press key      console  readkey    note  specific sentence  add adverb    quickly   verb    run    without  library incorrectly interpret   run  noun  depend source text  might issue  believe separate question ask ,Basic Understanding
Is it possible to get embeddings from NV-Embed using Candle?,"What I want to do is a CLI program that outputs embeddings of an arbitrary input. To do that, I want to do an inference with an embeddings model, and I chose NV-Embed-v2 . My framework of choice is Candle , but I also looked at Mistral-RS . Basically, what I'm trying to do is this code fragment: https://huggingface.co/nvidia/NV-Embed-v2 but with Rust and Candle. What I tried is to start off with Mistral Candle's example because the NV-Embed's HF page says: Model Details / Base Decoder-only LLM: Mistral-7B-v0.1 . I replaced the model id in the original code with nvidia/NV-Embed-v2 , and was able to download the weights from Hugging Face, but upon loading the config, I got this: Error: missing field `vocab_size` at line 101 column 1 Then I hardcoded the values from the JSON config loaded from HF to a newly created candle_transformers::models::mistral::Config instance. And after that, Mistral::new(&config, vb) fails with: Error: cannot find tensor model.embed_tokens.weight Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?","['machine-learning', 'rust', 'nlp']",1,"candle looking for model.embed_tokens.weight whereas the original tensor name is embedding_model.embed_tokens.weight . You just have to change this line of mistral.rs in candle_transformers. // from let vb_m = vb.pp(""model""); //to let vb_m = vb.pp(""embedding_model"");",2024-10-31 15:55:49,2024-11-04 17:45:20,335,https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle,"Is it possible to get embeddings from NV-Embed using Candle? What I want to do is a CLI program that outputs embeddings of an arbitrary input. To do that, I want to do an inference with an embeddings model, and I chose NV-Embed-v2 . My framework of choice is Candle , but I also looked at Mistral-RS . Basically, what I'm trying to do is this code fragment: https://huggingface.co/nvidia/NV-Embed-v2 but with Rust and Candle. What I tried is to start off with Mistral Candle's example because the NV-Embed's HF page says: Model Details / Base Decoder-only LLM: Mistral-7B-v0.1 . I replaced the model id in the original code with nvidia/NV-Embed-v2 , and was able to download the weights from Hugging Face, but upon loading the config, I got this: Error: missing field `vocab_size` at line 101 column 1 Then I hardcoded the values from the JSON config loaded from HF to a newly created candle_transformers::models::mistral::Config instance. And after that, Mistral::new(&config, vb) fails with: Error: cannot find tensor model.embed_tokens.weight Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?",possible get embedding nv  embed use candle  want cli program output embedding arbitrary input   want inference embedding model  choose nv  embed  v2  framework choice candle  also look mistral  rs  basically   m try code fragment    huggingfaceco  nvidia  nv  embed  v2 rust candle  try start mistral candle s example nv  embed s hf page say  model details  base decoder  only llm  mistral7b  v01  replace model i d original code nvidia  nv  embed  v2  able download weight hugging face  upon load config  get  error  miss field  vocabsize  line 101 column 1 hardcode value json config load hf newly create candletransformer   model   mistral   config instance   mistral   new   config  vb  fail  error  find tensor modelembedtokensweight way around  maybe candle  base open source work could use inspiration   maybe s common mistake could easily diagnose ,candle look modelembedtokensweight whereas original tensor name embeddingmodelembedtokensweight  change line mistralrs candletransformer   let vbm  vbpp    model    to let vbm  vbpp    embeddingmodel   ,possible get embedding nv  embed use candle  want cli program output embedding arbitrary input   want inference embedding model  choose nv  embed  v2  framework choice candle  also look mistral  rs  basically   m try code fragment    huggingfaceco  nvidia  nv  embed  v2 rust candle  try start mistral candle s example nv  embed s hf page say  model details  base decoder  only llm  mistral7b  v01  replace model i d original code nvidia  nv  embed  v2  able download weight hugging face  upon load config  get  error  miss field  vocabsize  line 101 column 1 hardcode value json config load hf newly create candletransformer   model   mistral   config instance   mistral   new   config  vb  fail  error  find tensor modelembedtokensweight way around  maybe candle  base open source work could use inspiration   maybe s common mistake could easily diagnose  candle look modelembedtokensweight whereas original tensor name embeddingmodelembedtokensweight  change line mistralrs candletransformer   let vbm  vbpp    model    to let vbm  vbpp    embeddingmodel   ,Library/Tool-Based Queries
"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) I have short product descriptions that I’d like to transform into structured attributes. Example: Input: “La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml” Output: Year = 2017 Color = Red Weight = 750 Weight Unit = ml If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach: There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed. There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc. Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues. I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about. I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary. I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies. Appreciate any insight into approaches or suggested learning resources. I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.","['nlp', 'artificial-intelligence', 'large-language-model', 'named-entity-recognition']",1,"LLM would work nicely for this. I'v done similar tasks before and it worked nicely with minimal training. Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate, but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions. For you task I would use a framework like Langchain, and the following prompt (note you might need to work on your prompt a bit this just an example). When run with a model it will create an XML output which would be trivial to parse. You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me. You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components: - **Year**: The vintage year of the wine. - **Color**: The color of the wine (e.g., Red, White, Rosé). - **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500). - **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters). - **Brand**: The brand or producer of the wine. - **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot). **Instructions:** - Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components. - Be cautious of potential ambiguities. For example: - A brand name may include words like ""Red"" or ""White"" (e.g., ""Red Head Vineyard"") which should not be confused with the wine color. - Large numbers may represent weight (e.g., ""1500 ml"") rather than a year. - **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it. **Output Format:** Provide the extracted information in XML format, using the following structure: <Wine> <Year>{{Year}}</Year> <Color>{{Color}}</Color> <Weight>{{Weight}}</Weight> <WeightUnit>{{WeightUnit}}</WeightUnit> <Brand>{{Brand}}</Brand> <GrapeVariety>{{GrapeVariety}}</GrapeVariety> </Wine> **Examples:** 1. **Input:** `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml` **Output:** ```xml <Wine> <Year>2017</Year> <Color>Red</Color> <Weight>750</Weight> <WeightUnit>ml</WeightUnit> <Brand>La Lecciaia</Brand> <GrapeVariety>Cabernet Sauvignon</GrapeVariety> </Wine> ``` `Red Head Vineyard Chardonnay 2020 1.5L` **Output:** <Wine> <Year>2020</Year> <Color></Color> <Weight>1.5</Weight> <WeightUnit>L</WeightUnit> <Brand>Red Head Vineyard</Brand> <GrapeVariety>Chardonnay</GrapeVariety> </Wine> **Task:** Given the following wine description, extract the components and provide the output in XML format as specified. {win_description} Keep in mind that LLMs are not cheap to run. But for this tasks given ambiguousness of the domain it is most likely the best choice. For this particular task it would be 1/1000 of a penny per label using OpenAI service. You might find a cheaper model / provider. However when working with LLM it is very important to ensure accuracy first, then optimize for costs. The whole thing will probably take 1-2 hours to build for the intermediate LLM developer. If you are learning it may vary. But this is a perfect project to learn about LLMs",2024-10-21 20:54:56,2024-10-22 11:49:29,183,https://stackoverflow.com/questions/79111733/how-to-derive-attributes-labels-from-short-plain-text-descriptions-ner-llm,"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?) I have short product descriptions that I’d like to transform into structured attributes. Example: Input: “La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml” Output: Year = 2017 Color = Red Weight = 750 Weight Unit = ml If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach: There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed. There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc. Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues. I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about. I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary. I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies. Appreciate any insight into approaches or suggested learning resources. I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.",derive attribute  label short plain text description   ner  llm    derive attribute  label short plain text description   ner  llm    short product description  like transform structure attribute  example  input   la lecciaia cabernet sauvignon 2017  red  750ml  output  year  2017 color  red weight  750 weight unit  ml everything format would trivial write regular expression do  many different format nuance  increasingly cumbersome hard  code logic format  try create generic solution immediately run issue  basic  approach  several different data provider  format  example  another provider might use   red  2017 la lecciaia cabernet sauvignon 750 ml   even give provider  may multiple format may change time  format always strictly follow  many way express particular component  example  weight might express one   15l    1 12 liters    1500ml   etc  part description may confused target component  may white wine brand call  red head vineyard   weight  2000 ml  may confused year  etc   use wine example sake simplicity general audience product domain conceptual issue   consider  nice  would useful able parse even detail like algo would smart enough know  la lecciaia  brand  cabernet sauvignon  grape variety  assume would take front work hard get right  straightforward method would good know   like develop general  purpose function accept description format  little experience nlp  artificial intelligence suspect useful tool  algo leverage  1000  example record could potentially use train model  something run locally would prefer absolutely necessary   look specific implementation guidance anyone  work similar problem  open hybrid approach additional logic manual oversight could account initial inaccuracy  appreciate insight approach suggest learn resource   ve look online information many approach involve significant amount front work unclear will work practical sense ,llm would work nicely   v do similar task work nicely minimal training  keep mind statistical method nlp  llm  ner never 100  accurate  practical purpose find llms accurate custom soup regular expression  task would use framework like langchain  follow prompt  note might need work prompt bit example   run model create xml output would trivial parse  modify prompt create different type output   personally find xml work well  ai language model design parse wine bottle description structure datum  give wine bottle description  task extract follow component     year    vintage year wine     color    color wine  eg  red  white  ros      weight    volume wine bottle express number  eg  750  1500      weight unit    unit measurement weight  eg  ml  ml  l  liters      brand    brand producer wine     grape variety    variety grape use  eg  cabernet sauvignon  merlot     instruction     wine description may come various format may include additional confusing information  carefully analyze description accurately extract component   cautious potential ambiguity  example   brand name may include word like   red    white   eg    red head vineyard   confuse wine color   large number may represent weight  eg    1500 ml   rather year     assume information present description    component missing  may leave correspond tag empty omit    output format    provide extract information xml format  use follow structure   wine   year    year    year   color    color    color   weight    weight    weight   weightunit    weightunit    weightunit   brand    brand    brand   grapevariety    grapevariety    grapevariety   wine    example    1    input     la lecciaia cabernet sauvignon 2017  red  750ml    output       xml  wine   year  2017  year   color  red  color   weight  750  weight   weightunit  ml  weightunit   brand  la lecciaia  brand   grapevariety  cabernet sauvignon  grapevariety   wine      red head vineyard chardonnay 2020 15l    output     wine   year  2020  year   color   color   weight  15  weight   weightunit  l  weightunit   brand  red head vineyard  brand   grapevariety  chardonnay  grapevariety   wine    task    give follow wine description  extract component provide output xml format specify   windescription  keep mind llms cheap run  task give ambiguousness domain likely good choice  particular task would 11000 penny per label use openai service  might find cheap model  provider  however work llm important ensure accuracy first  optimize cost  whole thing probably take 1  2 hour build intermediate llm developer  learning may vary  perfect project learn llms,derive attribute  label short plain text description   ner  llm    derive attribute  label short plain text description   ner  llm    short product description  like transform structure attribute  example  input   la lecciaia cabernet sauvignon 2017  red  750ml  output  year  2017 color  red weight  750 weight unit  ml everything format would trivial write regular expression do  many different format nuance  increasingly cumbersome hard  code logic format  try create generic solution immediately run issue  basic  approach  several different data provider  format  example  another provider might use   red  2017 la lecciaia cabernet sauvignon 750 ml   even give provider  may multiple format may change time  format always strictly follow  many way express particular component  example  weight might express one   15l    1 12 liters    1500ml   etc  part description may confused target component  may white wine brand call  red head vineyard   weight  2000 ml  may confused year  etc   use wine example sake simplicity general audience product domain conceptual issue   consider  nice  would useful able parse even detail like algo would smart enough know  la lecciaia  brand  cabernet sauvignon  grape variety  assume would take front work hard get right  straightforward method would good know   like develop general  purpose function accept description format  little experience nlp  artificial intelligence suspect useful tool  algo leverage  1000  example record could potentially use train model  something run locally would prefer absolutely necessary   look specific implementation guidance anyone  work similar problem  open hybrid approach additional logic manual oversight could account initial inaccuracy  appreciate insight approach suggest learn resource   ve look online information many approach involve significant amount front work unclear will work practical sense  llm would work nicely   v do similar task work nicely minimal training  keep mind statistical method nlp  llm  ner never 100  accurate  practical purpose find llms accurate custom soup regular expression  task would use framework like langchain  follow prompt  note might need work prompt bit example   run model create xml output would trivial parse  modify prompt create different type output   personally find xml work well  ai language model design parse wine bottle description structure datum  give wine bottle description  task extract follow component     year    vintage year wine     color    color wine  eg  red  white  ros      weight    volume wine bottle express number  eg  750  1500      weight unit    unit measurement weight  eg  ml  ml  l  liters      brand    brand producer wine     grape variety    variety grape use  eg  cabernet sauvignon  merlot     instruction     wine description may come various format may include additional confusing information  carefully analyze description accurately extract component   cautious potential ambiguity  example   brand name may include word like   red    white   eg    red head vineyard   confuse wine color   large number may represent weight  eg    1500 ml   rather year     assume information present description    component missing  may leave correspond tag empty omit    output format    provide extract information xml format  use follow structure   wine   year    year    year   color    color    color   weight    weight    weight   weightunit    weightunit    weightunit   brand    brand    brand   grapevariety    grapevariety    grapevariety   wine    example    1    input     la lecciaia cabernet sauvignon 2017  red  750ml    output       xml  wine   year  2017  year   color  red  color   weight  750  weight   weightunit  ml  weightunit   brand  la lecciaia  brand   grapevariety  cabernet sauvignon  grapevariety   wine      red head vineyard chardonnay 2020 15l    output     wine   year  2020  year   color   color   weight  15  weight   weightunit  l  weightunit   brand  red head vineyard  brand   grapevariety  chardonnay  grapevariety   wine    task    give follow wine description  extract component provide output xml format specify   windescription  keep mind llms cheap run  task give ambiguousness domain likely good choice  particular task would 11000 penny per label use openai service  might find cheap model  provider  however work llm important ensure accuracy first  optimize cost  whole thing probably take 1  2 hour build intermediate llm developer  learning may vary  perfect project learn llms,Implementation Issues
Varying embedding dim due to changing padding in batch size,"I want to train a simple neural network, which has embedding_dim as a parameter: class BoolQNN(nn.Module): def __init__(self, embedding_dim): super(BoolQNN, self).__init__() self.fc1 = nn.Linear(embedding_dim, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 1) def forward(self, question_emb, passage_emb): combined = torch.cat((question_emb, passage_emb), dim=1) x = self.fc1(combined) x = self.relu(x) x = self.fc2(x) return torch.sigmoid(x) To load the data I used torchs DataLoader with a custom collate_fn. train_dataset = BoolQDataset(train_data, pretrained_embeddings) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd) model = BoolQNN(301) The collate_fn_padd function looks the following: def collate_fn_padd(batch): questions, passages, labels = zip(*batch) questions = [torch.tensor(q) for q in questions] passages = [torch.tensor(p) for p in passages] padded_questions = pad_sequence(questions, batch_first=True, padding_value=0) padded_passages = pad_sequence(passages, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.float32) return padded_questions, padded_passages, labels The problem: For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch). That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch. Due to that, I receive errors like that: mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64) Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?","['python', 'text', 'nlp', 'padding', 'data-preprocessing']",1,"You can add a maximum length argument set to embedding_dim to pad and truncate all the data to a fixed length: padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions] padded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]",2024-10-18 15:54:51,2024-10-19 14:05:46,49,https://stackoverflow.com/questions/79102797/varying-embedding-dim-due-to-changing-padding-in-batch-size,"Varying embedding dim due to changing padding in batch size I want to train a simple neural network, which has embedding_dim as a parameter: class BoolQNN(nn.Module): def __init__(self, embedding_dim): super(BoolQNN, self).__init__() self.fc1 = nn.Linear(embedding_dim, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 1) def forward(self, question_emb, passage_emb): combined = torch.cat((question_emb, passage_emb), dim=1) x = self.fc1(combined) x = self.relu(x) x = self.fc2(x) return torch.sigmoid(x) To load the data I used torchs DataLoader with a custom collate_fn. train_dataset = BoolQDataset(train_data, pretrained_embeddings) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd) model = BoolQNN(301) The collate_fn_padd function looks the following: def collate_fn_padd(batch): questions, passages, labels = zip(*batch) questions = [torch.tensor(q) for q in questions] passages = [torch.tensor(p) for p in passages] padded_questions = pad_sequence(questions, batch_first=True, padding_value=0) padded_passages = pad_sequence(passages, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.float32) return padded_questions, padded_passages, labels The problem: For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch). That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch. Due to that, I receive errors like that: mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64) Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?",vary embed dim due change padding batch size want train simple neural network  embeddingdim parameter  class boolqnn  nn  module   def   init    self  embeddingdim   super  boolqnn  self  init     selffc1  nn  linear  embeddingdim  64  selfrelu  nn  relu   selffc2  nn  linear  64  1  def forward  self  questionemb  passageemb   combine  torchcat   questionemb  passageemb   dim1  x  selffc1  combine  x  selfrelu  x  x  selffc2  x  return torchsigmoid  x  load datum use torchs dataloader custom collatefn  traindataset  boolqdataset  traindata  pretrainedembedding  trainloader  torchutilsdata  dataloader  traindataset  batchsize16  shuffle  true  collatefn  collatefnpadd  model  boolqnn  301  collatefnpadd function look follow  def collatefnpadd  batch   question  passage  label  zip   batch  question   torchtensor  q  q question  passage   torchtensor  p  p passage  paddedquestion  padsequence  question  batchfirst  true  paddingvalue0  paddedpassage  padsequence  passage  batchfirst  true  paddingvalue0  label  torchtensor  label  dtype  torchfloat32  return paddedquestion  paddedpassage  label problem  every batch want train model  embed text get pad differently long  take long sequence current batch   mean embed dim  input size linear layer neural network change batch batch  althoug want size every batch  due  receive error like  mat1 mat2 shape multiply  16x182 301x64  possible adjust collatefnpad function padd sequence size  independet batch size ,add maximum length argument set embeddingdim pad truncate datum fix length  paddedquestion   torchnnfunctionalpad  torchtensor  q    0  maxlength  len  q    value0    maxlength  q question  paddedpassage   torchnnfunctionalpad  torchtensor  p    0  maxlength  len  p    value0    maxlength  p passage ,vary embed dim due change padding batch size want train simple neural network  embeddingdim parameter  class boolqnn  nn  module   def   init    self  embeddingdim   super  boolqnn  self  init     selffc1  nn  linear  embeddingdim  64  selfrelu  nn  relu   selffc2  nn  linear  64  1  def forward  self  questionemb  passageemb   combine  torchcat   questionemb  passageemb   dim1  x  selffc1  combine  x  selfrelu  x  x  selffc2  x  return torchsigmoid  x  load datum use torchs dataloader custom collatefn  traindataset  boolqdataset  traindata  pretrainedembedding  trainloader  torchutilsdata  dataloader  traindataset  batchsize16  shuffle  true  collatefn  collatefnpadd  model  boolqnn  301  collatefnpadd function look follow  def collatefnpadd  batch   question  passage  label  zip   batch  question   torchtensor  q  q question  passage   torchtensor  p  p passage  paddedquestion  padsequence  question  batchfirst  true  paddingvalue0  paddedpassage  padsequence  passage  batchfirst  true  paddingvalue0  label  torchtensor  label  dtype  torchfloat32  return paddedquestion  paddedpassage  label problem  every batch want train model  embed text get pad differently long  take long sequence current batch   mean embed dim  input size linear layer neural network change batch batch  althoug want size every batch  due  receive error like  mat1 mat2 shape multiply  16x182 301x64  possible adjust collatefnpad function padd sequence size  independet batch size  add maximum length argument set embeddingdim pad truncate datum fix length  paddedquestion   torchnnfunctionalpad  torchtensor  q    0  maxlength  len  q    value0    maxlength  q question  paddedpassage   torchnnfunctionalpad  torchtensor  p    0  maxlength  len  p    value0    maxlength  p passage ,Basic Understanding
How can I adjust the performance of tokenizer?,"Working with the tokenizer from the transformers library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not. I'm wondering if I can ""adjust"" (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to. To be more specific, the type of tokenizer is transformers.XLMRobertaTokenizerFast , which is a unigram tokenizer, and the model is paraphrase-multilingual-mpnet-base-v2 .","['nlp', 'huggingface-transformers', 'huggingface', 'huggingface-tokenizers']",1,"You can change the tokenizer's vocabulary: tokenizer.add_tokens([""asadaf"", ""sdfsaf""]) model.resize_token_embeddings(len(tokenizer)) # change input embeddings size input_text = ""This is asadaf and sdfsaf"" print(tokenizer(input_text)) As a result, asadaf and sdfsaf would be tokenized as unique words.",2024-10-18 06:45:15,2024-10-20 16:50:59,49,https://stackoverflow.com/questions/79100835/how-can-i-adjust-the-performance-of-tokenizer,"How can I adjust the performance of tokenizer? Working with the tokenizer from the transformers library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not. I'm wondering if I can ""adjust"" (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to. To be more specific, the type of tokenizer is transformers.XLMRobertaTokenizerFast , which is a unigram tokenizer, and the model is paraphrase-multilingual-mpnet-base-v2 .",adjust performance tokenizer  work tokenizer transformer library hugging face  tokenizer work fine case  case    m wonder   adjust   train new tokenizer scratch  performance tokenizer handle bad case still maintain good performance case use  specific  type tokenizer transformer  xlmrobertatokenizerfast  unigram tokenizer  model paraphrase  multilingual  mpnet  base  v2 ,change tokenizer s vocabulary  tokenizeraddtokens     asadaf     sdfsaf    modelresizetokenembedding  len  tokenizer    change input embedding size inputtext    asadaf sdfsaf  print  tokenizer  inputtext   result  asadaf sdfsaf would tokenize unique word ,adjust performance tokenizer  work tokenizer transformer library hugging face  tokenizer work fine case  case    m wonder   adjust   train new tokenizer scratch  performance tokenizer handle bad case still maintain good performance case use  specific  type tokenizer transformer  xlmrobertatokenizerfast  unigram tokenizer  model paraphrase  multilingual  mpnet  base  v2  change tokenizer s vocabulary  tokenizeraddtokens     asadaf     sdfsaf    modelresizetokenembedding  len  tokenizer    change input embedding size inputtext    asadaf sdfsaf  print  tokenizer  inputtext   result  asadaf sdfsaf would tokenize unique word ,Library/Tool-Based Queries
"With spaCy, how can I get all lemmas from a string?","I have a pandas data frame with a column of text values (documents). I want to apply lemmatization on these values with the spaCy library using the pandas apply function. I've defined my to_lemma function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow. Is there a way to extract the lemmatized form of a document in spaCy? def to_lemma(text): tp = nlp(text) line = """" for word in tp: line = line + word.lemma_ + "" "" return line","['python', 'pandas', 'nlp', 'spacy', 'lemmatization']",2,"There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input. The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use nlp.pipe() with an Iterable of strings. This means it is easier to not use apply. Disable components that you do not use. For token level processing where you need the lemmas this would be 'parser' (the dependency parser) and 'ner' (the Named Entity Recognition component). Increase the batch_size (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot. Increase the number of processors used using n_process . This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an if __name__ == '__main__': wrapper. Basic example with 1. and 2.: texts = df[""column_name""] nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner']) lemmas = [] for processed_doc in nlp.pipe(texts): lemmas.append("" "".join([token.lemma_ for token in processed_doc])) df[""column_name_lemmas""] = lemmas Advanced example for all four: if __name__ == '__main__': texts = df[""column_name""] nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner']) lemmas = [] for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4): lemmas.append("" "".join([token.lemma_ for token in processed_doc])) df[""column_name_lemmas""] = lemmas",2024-10-12 21:03:21,2024-10-14 13:09:50,105,https://stackoverflow.com/questions/79081924/with-spacy-how-can-i-get-all-lemmas-from-a-string,"With spaCy, how can I get all lemmas from a string? I have a pandas data frame with a column of text values (documents). I want to apply lemmatization on these values with the spaCy library using the pandas apply function. I've defined my to_lemma function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow. Is there a way to extract the lemmatized form of a document in spaCy? def to_lemma(text): tp = nlp(text) line = """" for word in tp: line = line + word.lemma_ + "" "" return line",spacy  get lemmas string  panda datum frame column text value  document   want apply lemmatization value spacy library use panda apply function   ve define tolemma function iterate word document concatenate correspond lemmas output string  however slow  way extract lemmatize form document spacy  def tolemma  text   tp  nlp  text  line     word tp  line  line  wordlemma       return line,many way speed spacy processing  question make sense depend mostly size input  obvious one individually apply model every single row  rather use batch processing  use nlppipe   iterable string  mean easy use apply  disable component use  token level processing need lemma would  parser   dependency parser   ner   name entity recognition component   increase batchsize  object buffer  pipe    default 1000  obviously make sense touch memory increase lot  increase number processor use use nprocess  increase time take initially load model decrease processing time  experience start make sense 500k text  note also require code run   name        main     wrapper  basic example 1  2   text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemmas advanced example four    name        main     text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text  batchsize10000  nprocess4   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemma,spacy  get lemmas string  panda datum frame column text value  document   want apply lemmatization value spacy library use panda apply function   ve define tolemma function iterate word document concatenate correspond lemmas output string  however slow  way extract lemmatize form document spacy  def tolemma  text   tp  nlp  text  line     word tp  line  line  wordlemma       return line many way speed spacy processing  question make sense depend mostly size input  obvious one individually apply model every single row  rather use batch processing  use nlppipe   iterable string  mean easy use apply  disable component use  token level processing need lemma would  parser   dependency parser   ner   name entity recognition component   increase batchsize  object buffer  pipe    default 1000  obviously make sense touch memory increase lot  increase number processor use use nprocess  increase time take initially load model decrease processing time  experience start make sense 500k text  note also require code run   name        main     wrapper  basic example 1  2   text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemmas advanced example four    name        main     text  df    columnname   nlp  spacyload   encoreweblg   disable   parser    ner    lemma    processeddoc nlppipe  text  batchsize10000  nprocess4   lemmasappend      join   tokenlemma  token processeddoc    df    columnnamelemmas    lemma,Implementation Issues
Avoiding overlap in frequency and document frequency count in Quanteda,"Below is a dummy corpus of 4 documents. The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in. The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive. Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each, but are intended to be counted once, according to the dictionary. The expected overall frequency count is (extracted from the 'pattern"" column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key. Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key. I have three questions: are there any problems/issues that could affect output accuracy using this approach? is there a better/more parsimonius approach to achieve what I am trying to do? what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table? library (quanteda) library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") x <- x %>% tokens_compound(phrase(""dairy farmers""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farms""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farm""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farming""), concatenator = "" "") %>% tokens_compound(phrase(""dairy industry""), concatenator = "" "") %>% tokens_compound(phrase(""indigenous australians""), concatenator = "" "") %>% tokens_compound(phrase(""australia post""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farmer""), concatenator = "" "") x dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign- owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farmers"", ""dairy farms"",""dairy farm"",""dairy farming"",""dairy industry"", ""dairy farmer"",""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) kwicdict <- kwic(x, pattern = dict, window = 4) write.csv (kwicdict, ""D:/Output/TEST.csv"") DF <- read.csv(""D://Output/TEST.csv"",header=T) ## obtaining frequency count of KWIC table 'pattern ' values > x2 <- DF[,8] > > table (x2) x2 auspost business dairy farmers foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2","['r', 'count', 'nlp', 'overlap', 'quanteda']",1,"I don't think that kwic() is what you want here. tokens_lookup() lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.) library(quanteda) #> Package version: 4.1.0 #> Unicode version: 14.0 #> ICU version: 71.1 #> Parallel computing: 10 of 10 threads used. #> See https://quanteda.io for tutorials and examples. library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign-owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farm*"", ""dairy industry"", ""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) x <- tokens(txt) # with overlap tokens_lookup(x, dict) |> dfm() #> Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars. #> features #> docs multinat dairy auspost oz farmers foreign business indig peep industry #> doc1 0 3 0 2 5 0 0 0 0 1 #> doc2 0 2 0 1 0 0 0 0 1 1 #> doc3 1 1 0 0 1 0 1 1 1 0 #> doc4 0 0 1 2 1 1 0 0 0 0 # without overlap tokens_lookup(x, dict, nested_scope = ""dictionary"") |> dfm() #> Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars. #> features #> docs multinat dairy auspost oz farmers foreign business indig peep industry #> doc1 0 3 0 2 3 0 0 0 0 1 #> doc2 0 2 0 1 0 0 0 0 1 0 #> doc3 1 1 0 0 1 0 1 1 0 0 #> doc4 0 0 1 1 1 1 0 0 0 0 Created on 2024-10-06 with reprex v2.1.1",2024-10-05 12:43:52,2024-10-06 10:02:36,61,https://stackoverflow.com/questions/79057082/avoiding-overlap-in-frequency-and-document-frequency-count-in-quanteda,"Avoiding overlap in frequency and document frequency count in Quanteda Below is a dummy corpus of 4 documents. The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in. The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive. Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each, but are intended to be counted once, according to the dictionary. The expected overall frequency count is (extracted from the 'pattern"" column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key. Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key. I have three questions: are there any problems/issues that could affect output accuracy using this approach? is there a better/more parsimonius approach to achieve what I am trying to do? what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table? library (quanteda) library(quanteda.textstats) txt <- c(doc1 = ""A significant percent of all farms in Australia, are dairy. Although there are a lot of dairy farms in this country, it is not the biggest farm industry. The life of a farmer is not easy, a dairy farmer has to be an early riser. "", doc2 = ""Australian people like milk so a healthy dairy industry is important in our country"", doc3 = ""Dairy and sheep farms developed at the expense of Indigenous Australians. Further many companies are now foreign-owned"", doc4 = ""Some farmers are lucky to receive a service from Australia Post. Mail is sent to many foreign countries and received more quickly than delivered in some locations in Australia."") x <- x %>% tokens_compound(phrase(""dairy farmers""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farms""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farm""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farming""), concatenator = "" "") %>% tokens_compound(phrase(""dairy industry""), concatenator = "" "") %>% tokens_compound(phrase(""indigenous australians""), concatenator = "" "") %>% tokens_compound(phrase(""australia post""), concatenator = "" "") %>% tokens_compound(phrase(""dairy farmer""), concatenator = "" "") x dict <- dictionary(list(multinat = c(""offshore petroleum companies"", ""foreign- owned"", ""foreign owned"", ""foreign companies"", ""multinational"", ""multinational oil companies"", ""multinationals"", ""transnational""), dairy = c(""dairy farmers"", ""dairy farms"",""dairy farm"",""dairy farming"",""dairy industry"", ""dairy farmer"",""dairy"", ""milk""), auspost = ""australia post"", oz = c(""australia"", ""this country"", ""our country""), farmers = c(""farmers"", ""farmer"", ""farm"", ""farms""), foreign = c(""foreign"", ""foreigner"", ""foreigners""), business =c(""small business"", ""business"", ""businesses"", ""company"", ""companies""), indig = c(""aboriginal"", ""aboriginals"", ""indigenous australians"", ""torres strait""), peep = c(""australians"", ""people of australia"", ""australian people"", ""people of this nation"", ""people of this country""), industry = c(""industry"", ""industries""))) kwicdict <- kwic(x, pattern = dict, window = 4) write.csv (kwicdict, ""D:/Output/TEST.csv"") DF <- read.csv(""D://Output/TEST.csv"",header=T) ## obtaining frequency count of KWIC table 'pattern ' values > x2 <- DF[,8] > > table (x2) x2 auspost business dairy farmers foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2",avoid overlap frequency document frequency count quanteda dummy corpus 4 document  dictionary develop identify frequency word phrase corpus  well number document word phrase occur  world  australians  occur two dictionary key  peep  indig   key content intend mutually exclusive  similarly  australia   oz australia post   foreign  foreign multinat  farm  farmer  dairy farmer  occur two dictionary key  intend count  accord dictionary  expect overall frequency count  extract  pattern  column kwic table  report x2  note word industry appears allocate industry define din indig key  dairy frequency occur key  occur three document  calculate unique row kwic table  doc name  column key  three question  problem  issue could affect output accuracy use approach  well  more parsimonius approach achieve try  would well way extract equivalent tetxstat frequency count datum kwic table  library  quanteda  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    x   x    tokenscompound  phrase    dairy farmer    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farming    concatenator          tokenscompound  phrase    dairy industry    concatenator          tokenscompound  phrase    indigenous australian    concatenator          tokenscompound  phrase    australia post    concatenator          tokenscompound  phrase    dairy farmer    concatenator       x dict   dictionary  list  multinat  c    offshore petroleum company     foreign own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farmer     dairy farm    dairy farm    dairy farming    dairy industry     dairy farmer    dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     kwicdict   kwic  x  pattern  dict  window  4  writecsv  kwicdict     output  testcsv   df   readcsv     output  testcsv   header  t    obtain frequency count kwic table  pattern  value  x2   df   8    table  x2  x2 auspost business dairy farmer foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2,not think kwic   want  tokenslookup   lets specify nest scope mutually exclusive across key  within key  observe difference   note use wildcarde dairy key   library  quanteda    package version  410   unicode version  140   icu version  711   parallel computing  10 10 thread use    see   quantedaio tutorial example  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    dict   dictionary  list  multinat  c    offshore petroleum company     foreign  own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farm      dairy industry     dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     x   token  txt   overlap tokenslookup  x  dict    dfm     document  feature matrix  4 document  10 feature  5500  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 5 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 1   doc3 1 1 0 0 1 0 1 1 1 0   doc4 0 0 1 2 1 1 0 0 0 0  without overlap tokenslookup  x  dict  nestedscope    dictionary     dfm     document  feature matrix  4 document  10 feature  6000  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 3 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 0   doc3 1 1 0 0 1 0 1 1 0 0   doc4 0 0 1 1 1 1 0 0 0 0 create 2024  10  06 reprex v211,avoid overlap frequency document frequency count quanteda dummy corpus 4 document  dictionary develop identify frequency word phrase corpus  well number document word phrase occur  world  australians  occur two dictionary key  peep  indig   key content intend mutually exclusive  similarly  australia   oz australia post   foreign  foreign multinat  farm  farmer  dairy farmer  occur two dictionary key  intend count  accord dictionary  expect overall frequency count  extract  pattern  column kwic table  report x2  note word industry appears allocate industry define din indig key  dairy frequency occur key  occur three document  calculate unique row kwic table  doc name  column key  three question  problem  issue could affect output accuracy use approach  well  more parsimonius approach achieve try  would well way extract equivalent tetxstat frequency count datum kwic table  library  quanteda  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    x   x    tokenscompound  phrase    dairy farmer    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farm    concatenator          tokenscompound  phrase    dairy farming    concatenator          tokenscompound  phrase    dairy industry    concatenator          tokenscompound  phrase    indigenous australian    concatenator          tokenscompound  phrase    australia post    concatenator          tokenscompound  phrase    dairy farmer    concatenator       x dict   dictionary  list  multinat  c    offshore petroleum company     foreign own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farmer     dairy farm    dairy farm    dairy farming    dairy industry     dairy farmer    dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     kwicdict   kwic  x  pattern  dict  window  4  writecsv  kwicdict     output  testcsv   df   readcsv     output  testcsv   header  t    obtain frequency count kwic table  pattern  value  x2   df   8    table  x2  x2 auspost business dairy farmer foreign indig industry multinat oz peep 1 1 6 5 1 1 1 1 5 2 not think kwic   want  tokenslookup   lets specify nest scope mutually exclusive across key  within key  observe difference   note use wildcarde dairy key   library  quanteda    package version  410   unicode version  140   icu version  711   parallel computing  10 10 thread use    see   quantedaio tutorial example  library  quantedatextstat  txt   c  doc1    significant percent farm australia  dairy  although lot dairy farm country  big farm industry  life farmer easy  dairy farmer early riser     doc2    australian people like milk healthy dairy industry important country   doc3    dairy sheep farm develop expense indigenous australians  many company foreign  own   doc4    farmer lucky receive service australia post  mail send many foreign country receive quickly deliver location australia    dict   dictionary  list  multinat  c    offshore petroleum company     foreign  own     foreign own     foreign company     multinational     multinational oil company     multinational     transnational    dairy  c    dairy farm      dairy industry     dairy     milk    auspost    australia post   oz  c    australia     country     country    farmer  c    farmer     farmer     farm     farm    foreign  c    foreign     foreigner     foreigner    business  c    small business     business     business     company     company    indig  c    aboriginal     aboriginal     indigenous australian     torre strait    peep  c    australians     people australia     australian people     people nation     people country    industry  c    industry     industry     x   token  txt   overlap tokenslookup  x  dict    dfm     document  feature matrix  4 document  10 feature  5500  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 5 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 1   doc3 1 1 0 0 1 0 1 1 1 0   doc4 0 0 1 2 1 1 0 0 0 0  without overlap tokenslookup  x  dict  nestedscope    dictionary     dfm     document  feature matrix  4 document  10 feature  6000  sparse  0 docvar    feature   docs multinat dairy auspost oz farmer foreign business indig peep industry   doc1 0 3 0 2 3 0 0 0 0 1   doc2 0 2 0 1 0 0 0 0 1 0   doc3 1 1 0 0 1 0 1 1 0 0   doc4 0 0 1 1 1 1 0 0 0 0 create 2024  10  06 reprex v211,Basic Understanding
Seq2Seq trainer.train() keeps giving indexing error,"I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error: IndexError: Invalid key: 39463 is out of bounds for size 0. The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also. Detailed error message: Traceback (most recent call last): File ""nllbtrain.py"", line 273, in <module> print(trainer.train()) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train return inner_training_loop( File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__ data = self._next_data() File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch data = self.dataset.__getitems__(possibly_batched_index) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__ batch = self.__getitem__(keys) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__ return self._getitem(key) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem pa_subtable = query_table(self._data, key, indices=self._indices) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table _check_valid_index_key(key, size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key _check_valid_index_key(int(max(key)), size=size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key raise IndexError(f""Invalid key: {key} is out of bounds for size {size}"") IndexError: Invalid key: 39463 is out of bounds for size 0 0%| The code of the preprocessing done for the data: def preprocess_function(examples): inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]] targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]] model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length') # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) with tokenizer.as_target_tokenizer(): # labels = tokenizer(targets, max_length=max_target_length, truncation=True) labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length') model_inputs['labels'] = labels['input_ids'] return model_inputs Data after preprocessing: DatasetDict({ train: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 39729 }) val: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2210 }) test: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2214 }) }) The code of model params and training: model_path = 'facebook/nllb-200-1.3B' model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path) tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=""hin_Deva"", tgt_lang=""san_Deva"", max_length = 500) training_args = Seq2SeqTrainingArguments( evaluation_strategy=""epoch"", save_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, output_dir=""./output_dir"", weight_decay=0.01, save_total_limit=1, num_train_epochs=4, predict_with_generate=True, fp16=False, push_to_hub=False, ) trainer = Seq2SeqTrainer( model=model, tokenizer=tokenizer, args=training_args, train_dataset=dataset['train'], data_collator=data_collator, compute_metrics=compute_metrics, ) print(trainer.train()) Any idea why this error is persisting?","['python', 'nlp', 'huggingface-transformers', 'huggingface-trainer']",1,"size 0 indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this ( https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25 ) and this ( https://github.com/huggingface/datasets/issues/6535 ) thread suggests adding remove_unused_columns = False to your training_args might resolve the issue, so you could give that a try.",2024-09-20 08:43:32,2024-09-20 16:38:59,57,https://stackoverflow.com/questions/79005985/seq2seq-trainer-train-keeps-giving-indexing-error,"Seq2Seq trainer.train() keeps giving indexing error I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error: IndexError: Invalid key: 39463 is out of bounds for size 0. The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also. Detailed error message: Traceback (most recent call last): File ""nllbtrain.py"", line 273, in <module> print(trainer.train()) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train return inner_training_loop( File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop for step, inputs in enumerate(epoch_iterator): File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__ data = self._next_data() File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data data = self._dataset_fetcher.fetch(index) # may raise StopIteration File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch data = self.dataset.__getitems__(possibly_batched_index) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__ batch = self.__getitem__(keys) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__ return self._getitem(key) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem pa_subtable = query_table(self._data, key, indices=self._indices) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table _check_valid_index_key(key, size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key _check_valid_index_key(int(max(key)), size=size) File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key raise IndexError(f""Invalid key: {key} is out of bounds for size {size}"") IndexError: Invalid key: 39463 is out of bounds for size 0 0%| The code of the preprocessing done for the data: def preprocess_function(examples): inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]] targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]] model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length') # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) with tokenizer.as_target_tokenizer(): # labels = tokenizer(targets, max_length=max_target_length, truncation=True) labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length') model_inputs['labels'] = labels['input_ids'] return model_inputs Data after preprocessing: DatasetDict({ train: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 39729 }) val: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2210 }) test: Dataset({ features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'], num_rows: 2214 }) }) The code of model params and training: model_path = 'facebook/nllb-200-1.3B' model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path) tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=""hin_Deva"", tgt_lang=""san_Deva"", max_length = 500) training_args = Seq2SeqTrainingArguments( evaluation_strategy=""epoch"", save_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, output_dir=""./output_dir"", weight_decay=0.01, save_total_limit=1, num_train_epochs=4, predict_with_generate=True, fp16=False, push_to_hub=False, ) trainer = Seq2SeqTrainer( model=model, tokenizer=tokenizer, args=training_args, train_dataset=dataset['train'], data_collator=data_collator, compute_metrics=compute_metrics, ) print(trainer.train()) Any idea why this error is persisting?",seq2seq trainertrain   keep give indexing error try machine translation hindi sanskrit use nllb model  keep get error  indexerror  invalid key  39463 bound size 0  error come training pretraine nllb model  facebook  nllb200  13b input datum 40k hindi sentence  error arises try train sample datum also  detailed error message  traceback  recent call last   file   nllbtrainpy   line 273   module  print  trainertrain    file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1645  train return innertrainingloop  file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1907   innertrainingloop step  input enumerate  epochiterator   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 631    next   datum  selfnextdata   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 675   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   homeconda  envs  dict  lib  python38  site  package  torch  util  datautils  fetchpy   line 49  fetch datum  selfdatasetgetitem    possiblybatchedindex  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2814    getitem   batch  selfgetitem    key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2810    getitem   return selfgetitem  key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2794   getitem pasubtable  querytable  selfdata  key  index  selfindice  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 583  querytable  checkvalidindexkey  key  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 536   checkvalidindexkey  checkvalidindexkey  int  max  key    size  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 526   checkvalidindexkey raise indexerror  f  invalid key   key  bound size  size    indexerror  invalid key  39463 bound size 0 0   code preprocessing do datum  def preprocessfunction  example   input   example    s    f   2  slang    example example  sourcelang   target   f   2  tlang     example    s   example example  targetlang   modelinput  tokenizerbatchencodeplus  input  maxlength  maxinputlength  truncation  true  paddingmaxlength    modelinput  tokenizer  input  maxlength  maxinputlength  truncation  true  tokenizerastargettokenizer     label  tokenizer  target  maxlength  maxtargetlength  truncation  true  label  tokenizerbatchencodeplus  target  maxlength  maxinputlength  truncation  true  paddingmaxlength   modelinput   label    label   inputids   return modelinput data preprocesse  datasetdict   train  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  39729   val  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  2210   test  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrow  2214     code model param training  modelpath   facebook  nllb200  13b  model  automodelforseq2seqlmfrompretraine  pretrainedmodelnameorpath  modelpath  tokenizer  autotokenizerfrompretraine   facebook  nllb200  13b   dolowercase  false  usefast  false  truncation  true  xkeepaccent  true  srclang  hindeva   tgtlang  sandeva   maxlength  500  trainingarg  seq2seqtrainingargument  evaluationstrategy  epoch   savestrategyepoch   learningrate2e5  perdevicetrainbatchsize16  perdeviceevalbatchsize16  outputdir  outputdir   weightdecay001  savetotallimit1  numtrainepochs4  predictwithgenerate  true  fp16  false  pushtohub  false   trainer  seq2seqtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  traindataset  dataset   train    datacollator  datacollator  computemetric  computemetric   print  trainertrain    idea error persist ,size 0 indicate dataset trainer get fine  tuning start empty  look    discusshuggingfaceco  t  indexerror  invalid  key16  be  out  of  bound  for  size01429825     githubcom  huggingface  dataset  issues6535  thread suggest add removeunusedcolumns  false trainingargs might resolve issue  could give try ,seq2seq trainertrain   keep give indexing error try machine translation hindi sanskrit use nllb model  keep get error  indexerror  invalid key  39463 bound size 0  error come training pretraine nllb model  facebook  nllb200  13b input datum 40k hindi sentence  error arises try train sample datum also  detailed error message  traceback  recent call last   file   nllbtrainpy   line 273   module  print  trainertrain    file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1645  train return innertrainingloop  file   homeconda  envs  dict  lib  python38  site  package  transformer  trainerpy   line 1907   innertrainingloop step  input enumerate  epochiterator   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 631    next   datum  selfnextdata   file   homeconda  envs  dict  lib  python38  site  package  torch  util  data  dataloaderpy   line 675   nextdata datum  selfdatasetfetcherfetch  index   may raise stopiteration file   homeconda  envs  dict  lib  python38  site  package  torch  util  datautils  fetchpy   line 49  fetch datum  selfdatasetgetitem    possiblybatchedindex  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2814    getitem   batch  selfgetitem    key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2810    getitem   return selfgetitem  key  file   homeconda  envs  dict  lib  python38  site  package  dataset  arrowdatasetpy   line 2794   getitem pasubtable  querytable  selfdata  key  index  selfindice  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 583  querytable  checkvalidindexkey  key  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 536   checkvalidindexkey  checkvalidindexkey  int  max  key    size  size  file   homeconda  envs  dict  lib  python38  site  package  dataset  format  formattingpy   line 526   checkvalidindexkey raise indexerror  f  invalid key   key  bound size  size    indexerror  invalid key  39463 bound size 0 0   code preprocessing do datum  def preprocessfunction  example   input   example    s    f   2  slang    example example  sourcelang   target   f   2  tlang     example    s   example example  targetlang   modelinput  tokenizerbatchencodeplus  input  maxlength  maxinputlength  truncation  true  paddingmaxlength    modelinput  tokenizer  input  maxlength  maxinputlength  truncation  true  tokenizerastargettokenizer     label  tokenizer  target  maxlength  maxtargetlength  truncation  true  label  tokenizerbatchencodeplus  target  maxlength  maxinputlength  truncation  true  paddingmaxlength   modelinput   label    label   inputids   return modelinput data preprocesse  datasetdict   train  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  39729   val  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrows  2210   test  dataset   feature    hindi    sanskrit      indexlevel0      inputids    attentionmask    label    numrow  2214     code model param training  modelpath   facebook  nllb200  13b  model  automodelforseq2seqlmfrompretraine  pretrainedmodelnameorpath  modelpath  tokenizer  autotokenizerfrompretraine   facebook  nllb200  13b   dolowercase  false  usefast  false  truncation  true  xkeepaccent  true  srclang  hindeva   tgtlang  sandeva   maxlength  500  trainingarg  seq2seqtrainingargument  evaluationstrategy  epoch   savestrategyepoch   learningrate2e5  perdevicetrainbatchsize16  perdeviceevalbatchsize16  outputdir  outputdir   weightdecay001  savetotallimit1  numtrainepochs4  predictwithgenerate  true  fp16  false  pushtohub  false   trainer  seq2seqtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  traindataset  dataset   train    datacollator  datacollator  computemetric  computemetric   print  trainertrain    idea error persist  size 0 indicate dataset trainer get fine  tuning start empty  look    discusshuggingfaceco  t  indexerror  invalid  key16  be  out  of  bound  for  size01429825     githubcom  huggingface  dataset  issues6535  thread suggest add removeunusedcolumns  false trainingargs might resolve issue  could give try ,Basic Understanding
Alternative to device_map = &quot;auto&quot; in Huggingface Pretrained,"I have a model that I was reading from huggingface using the following code: from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model. Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model. So simply something like tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) model.device_map = ""auto""","['machine-learning', 'deep-learning', 'nlp', 'huggingface-transformers']",1,"I found out that there are actually several methods in accelerate for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model: https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map The second one is used to match your model with the devices: https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model So basically, in your case, you can use the following code: from accelerate import dispatch_model, infer_auto_device_map model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) *** ... new_model = CustomModel(model) ... *** device_map_dict = infer_auto_device_map(new_model) dispatch_model(new_model, device_map_dict) P.S. This code still needs to be tested on fine-tuning.",2024-09-14 12:42:03,2024-09-20 15:09:15,1081,https://stackoverflow.com/questions/78985137/alternative-to-device-map-auto-in-huggingface-pretrained,"Alternative to device_map = &quot;auto&quot; in Huggingface Pretrained I have a model that I was reading from huggingface using the following code: from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model. Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model. So simply something like tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True) model.device_map = ""auto""",alternative devicemap   quot  auto  quot  huggingface pretrained model read huggingface use follow code  transformer import autotokenizer  automodelforcausallm tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  read model modification internal layer add layer  start training  fine  tuning get everything model  investigation  find custom layer not distribute multi gpus original model  need something like devicemap  auto  reading model  simply something like tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  modeldevicemap    auto ,find actually several method accelerate  first one use analyze model calculate total amount available memory occupy model    huggingfaceco  docs  accelerate  en  packagereference  bigmodele  accelerateinferautodevicemap second one use match model device    huggingfaceco  docs  accelerate  en  packagereference  bigmodeling  acceleratedispatchmodel basically  case  use follow code  accelerate import dispatchmodel  inferautodevicemap model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true      newmodel  custommodel  model      devicemapdict  inferautodevicemap  newmodel  dispatchmodel  newmodel  devicemapdict  ps  code still need test fine  tuning ,alternative devicemap   quot  auto  quot  huggingface pretrained model read huggingface use follow code  transformer import autotokenizer  automodelforcausallm tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  read model modification internal layer add layer  start training  fine  tuning get everything model  investigation  find custom layer not distribute multi gpus original model  need something like devicemap  auto  reading model  simply something like tokenizer  autotokenizerfrompretraine  modelpath  model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true  modeldevicemap    auto  find actually several method accelerate  first one use analyze model calculate total amount available memory occupy model    huggingfaceco  docs  accelerate  en  packagereference  bigmodele  accelerateinferautodevicemap second one use match model device    huggingfaceco  docs  accelerate  en  packagereference  bigmodeling  acceleratedispatchmodel basically  case  use follow code  accelerate import dispatchmodel  inferautodevicemap model  automodelforcausallmfrompretraine  modelpath  devicemap  auto   trustremotecode  true      newmodel  custommodel  model      devicemapdict  inferautodevicemap  newmodel  dispatchmodel  newmodel  devicemapdict  ps  code still need test fine  tuning ,Basic Understanding
How are the weights of the Mistral models reinitialized in Huggingface?,"From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model? and https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4 there's different suggestions to reinitialize the model. When I tried this, it seems to work. from transformers import AutoModelForCausalLM, AutoConfig m = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.3"", token=""hf_*****"") c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) print(m2.model.layers[0].mlp.down_proj.state_dict()) print(m.model.layers[0].mlp.down_proj.state_dict()) [out]: OrderedDict([('weight', tensor([[ 0.0315, -0.0025, -0.0015, ..., -0.0022, 0.0168, -0.0296], [-0.0013, -0.0190, -0.0103, ..., 0.0037, 0.0021, -0.0374], [-0.0378, -0.0230, 0.0031, ..., -0.0035, 0.0099, -0.0027], ..., [-0.0029, 0.0042, -0.0041, ..., -0.0003, 0.0396, -0.0012], [-0.0487, -0.0050, -0.0068, ..., 0.0170, 0.0135, -0.0006], [ 0.0103, 0.0424, 0.0019, ..., 0.0155, 0.0254, 0.0061]]))]) OrderedDict([('weight', tensor([[-0.0027, -0.0004, -0.0007, ..., -0.0025, 0.0032, -0.0014], [ 0.0012, -0.0047, 0.0026, ..., -0.0017, 0.0015, -0.0044], [ 0.0056, -0.0084, 0.0027, ..., 0.0026, -0.0053, 0.0038], ..., [ 0.0052, 0.0017, -0.0019, ..., -0.0013, 0.0052, -0.0017], [-0.0032, 0.0029, -0.0014, ..., 0.0003, 0.0006, 0.0023], [-0.0023, -0.0045, -0.0013, ..., -0.0036, 0.0002, -0.0008]]))]) How are the layers re-initialized through the from_config function? Is it using Xaiver/He initialization or just random initialization?","['nlp', 'huggingface-transformers', 'large-language-model', 'mistral-7b']",2,"MistralConfig has a default parameter initializer_range which is set to 0.02 and described as The standard deviation of the truncated_normal_initializer for initializing all weight matrices , so one can assume they use a truncated normal distribution with a standard deviation of 0.02. If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me: import numpy as np from matplotlib import pyplot as plt from scipy.stats import truncnorm from transformers import AutoModelForCausalLM, AutoConfig # histogram of actual weights distribution c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) weights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel() plt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights') # what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like lower = -0.1 upper = 0.1 mean = 0 std = 0.02 a, b = (lower - mean) / std, (upper - mean) / std x = np.linspace(lower, upper, 1000) plt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected') plt.legend() plt.show()",2024-09-09 19:25:52,2024-09-10 13:18:59,185,https://stackoverflow.com/questions/78966943/how-are-the-weights-of-the-mistral-models-reinitialized-in-huggingface,"How are the weights of the Mistral models reinitialized in Huggingface? From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model? and https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4 there's different suggestions to reinitialize the model. When I tried this, it seems to work. from transformers import AutoModelForCausalLM, AutoConfig m = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.3"", token=""hf_*****"") c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"") m2 = AutoModelForCausalLM.from_config(c) print(m2.model.layers[0].mlp.down_proj.state_dict()) print(m.model.layers[0].mlp.down_proj.state_dict()) [out]: OrderedDict([('weight', tensor([[ 0.0315, -0.0025, -0.0015, ..., -0.0022, 0.0168, -0.0296], [-0.0013, -0.0190, -0.0103, ..., 0.0037, 0.0021, -0.0374], [-0.0378, -0.0230, 0.0031, ..., -0.0035, 0.0099, -0.0027], ..., [-0.0029, 0.0042, -0.0041, ..., -0.0003, 0.0396, -0.0012], [-0.0487, -0.0050, -0.0068, ..., 0.0170, 0.0135, -0.0006], [ 0.0103, 0.0424, 0.0019, ..., 0.0155, 0.0254, 0.0061]]))]) OrderedDict([('weight', tensor([[-0.0027, -0.0004, -0.0007, ..., -0.0025, 0.0032, -0.0014], [ 0.0012, -0.0047, 0.0026, ..., -0.0017, 0.0015, -0.0044], [ 0.0056, -0.0084, 0.0027, ..., 0.0026, -0.0053, 0.0038], ..., [ 0.0052, 0.0017, -0.0019, ..., -0.0013, 0.0052, -0.0017], [-0.0032, 0.0029, -0.0014, ..., 0.0003, 0.0006, 0.0023], [-0.0023, -0.0045, -0.0013, ..., -0.0036, 0.0002, -0.0008]]))]) How are the layers re-initialized through the from_config function? Is it using Xaiver/He initialization or just random initialization?",weight mistral model reinitialize huggingface  one reinitialize weight hugging face llama v2 model official way original model    discusshuggingfaceco  t  how  do  one  reinitialize  the  weight  of  a  hug  face  llama  v2  model  the  official  way  as  the  original  model625474 s different suggestion reinitialize model  try  seem work  transformer import automodelforcausallm  autoconfig  automodelforcausallmfrompretraine    mistralai  mistral7b  v03   token  hf         c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  print  m2modellayer  0  mlpdownprojstatedict    print  mmodellayer  0  mlpdownprojstatedict       ordereddict     weight   tensor    00315  00025  00015    00022  00168  00296    00013  00190  00103    00037  00021  00374    00378  00230  00031    00035  00099  00027      00029  00042  00041    00003  00396  00012    00487  00050  00068    00170  00135  00006    00103  00424  00019    00155  00254  00061       ordereddict     weight   tensor    00027  00004  00007    00025  00032  00014    00012  00047  00026    00017  00015  00044    00056  00084  00027    00026  00053  00038      00052  00017  00019    00013  00052  00017    00032  00029  00014    00003  00006  00023    00023  00045  00013    00036  00002  00008       layer re  initialized fromconfig function  use xaiver  he initialization random initialization ,mistralconfig default parameter initializerrange set 002 describe standard deviation truncatednormalinitializer initialize weight matrix  one assume use truncate normal distribution standard deviation 002  plot actual model weight distribution truncate normal distribution standard deviation 002 look like  seem like fit  import numpy np matplotlib import pyplot plt scipystat import truncnorm transformer import automodelforcausallm  autoconfig  histogram actual weight distribution c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  weight  m2modellayer  0  mlpdownprojstatedict     weight   ravel   plthist  weight  bin  nplinspace  01  01  100   histtypestep   density  true  labelmodel weight    truncate normal distribution mean 0 std 002 suppose look like low  01 upper  01 mean  0 std  002  b   lower  mean   std   upper  mean   std x  nplinspace  low  upper  1000  pltplot  x  truncnormpdf  x   b  loc  mean  scale  std   labelexpecte   pltlegend   pltshow  ,weight mistral model reinitialize huggingface  one reinitialize weight hugging face llama v2 model official way original model    discusshuggingfaceco  t  how  do  one  reinitialize  the  weight  of  a  hug  face  llama  v2  model  the  official  way  as  the  original  model625474 s different suggestion reinitialize model  try  seem work  transformer import automodelforcausallm  autoconfig  automodelforcausallmfrompretraine    mistralai  mistral7b  v03   token  hf         c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  print  m2modellayer  0  mlpdownprojstatedict    print  mmodellayer  0  mlpdownprojstatedict       ordereddict     weight   tensor    00315  00025  00015    00022  00168  00296    00013  00190  00103    00037  00021  00374    00378  00230  00031    00035  00099  00027      00029  00042  00041    00003  00396  00012    00487  00050  00068    00170  00135  00006    00103  00424  00019    00155  00254  00061       ordereddict     weight   tensor    00027  00004  00007    00025  00032  00014    00012  00047  00026    00017  00015  00044    00056  00084  00027    00026  00053  00038      00052  00017  00019    00013  00052  00017    00032  00029  00014    00003  00006  00023    00023  00045  00013    00036  00002  00008       layer re  initialized fromconfig function  use xaiver  he initialization random initialization  mistralconfig default parameter initializerrange set 002 describe standard deviation truncatednormalinitializer initialize weight matrix  one assume use truncate normal distribution standard deviation 002  plot actual model weight distribution truncate normal distribution standard deviation 002 look like  seem like fit  import numpy np matplotlib import pyplot plt scipystat import truncnorm transformer import automodelforcausallm  autoconfig  histogram actual weight distribution c  autoconfigfrompretrained    mistralai  mistral7b  v03   m2  automodelforcausallmfromconfig  c  weight  m2modellayer  0  mlpdownprojstatedict     weight   ravel   plthist  weight  bin  nplinspace  01  01  100   histtypestep   density  true  labelmodel weight    truncate normal distribution mean 0 std 002 suppose look like low  01 upper  01 mean  0 std  002  b   lower  mean   std   upper  mean   std x  nplinspace  low  upper  1000  pltplot  x  truncnormpdf  x   b  loc  mean  scale  std   labelexpecte   pltlegend   pltshow  ,Implementation Issues
Break after first PER sequence found with Spacy,"I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all ""PER"" tags, but I want to reduce the overhead and get only the first contiguous sequence of ""PER"" entities. Here’s the example output I get: Detected Names in Text: ['garcía', 'lópez'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama'] Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias'] But I want the result to be: Detected Names in Text: ['garcía'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez'] Detected Names in Text: ['pedro sánchez'] Here is the code I am currently using: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."" ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}] ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc, start_idx=0): per_entities = [] misc_count = 0 for ent in doc[start_idx:].ents: if ent.label_ == ""PER"": per_entities.append(ent.text) elif ent.label_ == ""MISC"" and misc_count < 1: misc_count += 1 per_entities.append(ent.text) else: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position title_start = None title_end = None for match_id, start, end in matches: title_start = start title_end = end break # If a title was found, start searching for PER entities from that position if title_start is not None: names = find_per_sequence(doc, start_idx=title_end) else: names = find_per_sequence(doc) # Output the detected names for each text print(f""Detected Names in Text: {names}"") What I'm looking for: I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of ""PER"" entities in the text, ignoring any subsequent ""PER"" entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?","['python', 'nlp', 'spacy']",1,"The issues is that doc[start_idx:].ents is only the named entities in that slice of the doc. Thus, you will never process ""habló"" for the first entry, you will just go straight from ""García"" to ""López"". To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the .ents part. Then you just wait until you see the first token with ent_type_ PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."", ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}], ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc: spacy.tokens.Doc, start_idx: int): per_entities = [] misc_count = 0 per_started = False for token in doc[start_idx:]: if token.ent_type_ == ""PER"": per_entities.append(token.text) per_started = True elif token.ent_type_ == ""MISC"" and misc_count < 1 and per_started: misc_count += 1 per_entities.append(token.text) elif per_started: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position _, _, title_end = matches[0] if matches else (None, None, None) names = find_per_sequence(doc, title_end if title_end else 0) # Output the detected names for each text print(f""Detected Names in Text: {names}"")",2024-09-06 13:14:32,2024-09-06 15:03:25,41,https://stackoverflow.com/questions/78957322/break-after-first-per-sequence-found-with-spacy,"Break after first PER sequence found with Spacy I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all ""PER"" tags, but I want to reduce the overhead and get only the first contiguous sequence of ""PER"" entities. Here’s the example output I get: Detected Names in Text: ['garcía', 'lópez'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama'] Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias'] But I want the result to be: Detected Names in Text: ['garcía'] Detected Names in Text: ['j. jesus orozco alfaro'] Detected Names in Text: ['josé guadarrama márquez'] Detected Names in Text: ['pedro sánchez'] Here is the code I am currently using: import spacy from spacy.matcher import Matcher nlp = spacy.load(""es_core_news_lg"") texts = [ ""El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez."", ""PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO"", "" -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez"", ""El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias."" ] texts = [text.lower() for text in texts] matcher = Matcher(nlp.vocab) patterns = [ [{""LOWER"": ""el""}, {""LOWER"": ""c""}], [{""LOWER"": ""el""}, {""LOWER"": ""sr""}], [{""LOWER"": ""el""}, {""LOWER"": ""sra""}] ] matcher.add(""LEGISLATIVE_TITLES"", patterns) # Function to find a sequence of PER entities allowing one MISC def find_per_sequence(doc, start_idx=0): per_entities = [] misc_count = 0 for ent in doc[start_idx:].ents: if ent.label_ == ""PER"": per_entities.append(ent.text) elif ent.label_ == ""MISC"" and misc_count < 1: misc_count += 1 per_entities.append(ent.text) else: break # Should stop if any other entity or second MISC is encountered return per_entities for text in texts: doc = nlp(text) # Find matches matches = matcher(doc) # Extract the first match and its position title_start = None title_end = None for match_id, start, end in matches: title_start = start title_end = end break # If a title was found, start searching for PER entities from that position if title_start is not None: names = find_per_sequence(doc, start_idx=title_end) else: names = find_per_sequence(doc) # Output the detected names for each text print(f""Detected Names in Text: {names}"") What I'm looking for: I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of ""PER"" entities in the text, ignoring any subsequent ""PER"" entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?",break first per sequence find spacy try extract first speaker s name list text use spacy  currently  function return   per  tag  want reduce overhead get first contiguous sequence   per  entity   example output get  detected names text    garca    lpez   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez    jos guadarrama   detect names text    pedro snchez    jos manuel albares    pablo iglesia   want result  detected names text    garca   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez   detect names text    pedro snchez   code currently use  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias    text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra     matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  startidx0   perentitie    misccount  0 ent doc  startidx   ents  entlabel      per   perentitiesappend  enttext  elif entlabel      misc  misccount  1  misccount   1 perentitiesappend  enttext  else  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position titlestart  none titleend  none matchid  start  end match  titlestart  start titleend  end break  title find  start search per entity position titlestart none  name  findpersequence  doc  startidx  titleend  else  name  findpersequence  doc   output detect name text print  f  detect names text   name     m look  want modify findpersequence function return first contiguous sequence   per  entity text  ignore subsequent   per  entity encounter different type entity  provide function return multiple name partial name  need way ensure first name sequence include  achieve ,issue doc  startidx   ents name entity slice doc  thus  never process   habl  first entry  go straight   garca    lpez   actually iterate token see per sequence end  leave ent part  wait see first token enttype  per start append  break one condition meet  end refactore code little debug  s edit version program produce desire output  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias      text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra      matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  spacytoken  doc  startidx  int   perentitie    misccount  0 perstarte  false token doc  startidx    tokenenttype      per   perentitiesappend  tokentext  perstarte  true elif tokenenttype      misc  misccount  1 perstarte  misccount   1 perentitiesappend  tokentext  elif perstarte  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position     titleend  match  0  match else  none  none  none  name  findpersequence  doc  titleend titleend else 0   output detect name text print  f  detect names text   name   ,break first per sequence find spacy try extract first speaker s name list text use spacy  currently  function return   per  tag  want reduce overhead get first contiguous sequence   per  entity   example output get  detected names text    garca    lpez   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez    jos guadarrama   detect names text    pedro snchez    jos manuel albares    pablo iglesia   want result  detected names text    garca   detect names text    j  jesus orozco alfaro   detect names text    jos guadarrama mrquez   detect names text    pedro snchez   code currently use  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias    text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra     matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  startidx0   perentitie    misccount  0 ent doc  startidx   ents  entlabel      per   perentitiesappend  enttext  elif entlabel      misc  misccount  1  misccount   1 perentitiesappend  enttext  else  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position titlestart  none titleend  none matchid  start  end match  titlestart  start titleend  end break  title find  start search per entity position titlestart none  name  findpersequence  doc  startidx  titleend  else  name  findpersequence  doc   output detect name text print  f  detect names text   name     m look  want modify findpersequence function return first contiguous sequence   per  entity text  ignore subsequent   per  entity encounter different type entity  provide function return multiple name partial name  need way ensure first name sequence include  achieve  issue doc  startidx   ents name entity slice doc  thus  never process   habl  first entry  go straight   garca    lpez   actually iterate token see per sequence end  leave ent part  wait see first token enttype  per start append  break one condition meet  end refactore code little debug  s edit version program produce desire output  import spacy spacymatcher import matcher nlp  spacyload    escorenewslg   text     el sr  garca habl en la sesin  tambin estuvo presente el senador lpez la diputada martnez       presidencia del c senador j jesus orozco alfaro     er c jos guadarrama mrquez  el contrabando del dia  jos guadarrama mrquez     el presidente pedro snchez el ministro de asuntos exteriores jos manuel albares se reunieron con el senador pablo iglesias      text   textlower   text text  matcher  matcher  nlpvocab  pattern       lower     el       lower     c         lower     el       lower     sr         lower     el       lower     sra      matcheradd    legislativetitles   pattern   function find sequence per entity allow one misc def findpersequence  doc  spacytoken  doc  startidx  int   perentitie    misccount  0 perstarte  false token doc  startidx    tokenenttype      per   perentitiesappend  tokentext  perstarte  true elif tokenenttype      misc  misccount  1 perstarte  misccount   1 perentitiesappend  tokentext  elif perstarte  break  stop entity second misc encounter return perentitie text text  doc  nlp  text   find match match  matcher  doc   extract first match position     titleend  match  0  match else  none  none  none  name  findpersequence  doc  titleend titleend else 0   output detect name text print  f  detect names text   name   ,Library/Tool-Based Queries
Trainer huggingface - RuntimeError: cannot pin &#39;torch.cuda.FloatTensor&#39; only dense CPU tensors can be pinned,"I recently got the following error: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned when doing LoRA on a small LLM. I saw on a discord someone saying: The issue likely stems from the fact that you are manually placing your inputs on the GPU (with to(model.device)), but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally. I can't find anything of the sort written in the Trainer documentation of huggingface https://huggingface.co/docs/transformers/en/main_classes/trainer . Is it true? If not, how can I get rid of that error? MRE: import torch from torch.utils.data import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from transformers import TrainingArguments from transformers import Trainer from peft import LoraConfig, get_peft_model model_name = ""croissantllm/CroissantLLMBase"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"") texts = [ ""The first sentence for fine-tuning. </s>"", ""The second sentence for fine-tuning. </s>"" ] inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts] lora_config = LoraConfig( r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""q_proj"", ""v_proj""], ) model = get_peft_model(model, lora_config) class CustomDataset(Dataset): def __init__(self, input_list): self.input_list = input_list def __len__(self): return len(self.input_list) def __getitem__(self, idx): input_ids = self.input_list[idx]['input_ids'].squeeze() labels = input_ids.clone() return {""input_ids"": input_ids, ""labels"": labels} train_dataset = CustomDataset(inputs) training_args = TrainingArguments( output_dir=""./lora_croissantllm"", per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, ) trainer.train() The issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).","['nlp', 'huggingface-transformers']",1,"Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting dataloader_pin_memory to False for TrainingArguments training_args = TrainingArguments( output_dir=""./lora_croissantllm"", dataloader_pin_memory=False, per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, )",2024-09-04 16:09:18,2024-10-22 01:40:51,1512,https://stackoverflow.com/questions/78949607/trainer-huggingface-runtimeerror-cannot-pin-torch-cuda-floattensor-only-den,"Trainer huggingface - RuntimeError: cannot pin &#39;torch.cuda.FloatTensor&#39; only dense CPU tensors can be pinned I recently got the following error: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned when doing LoRA on a small LLM. I saw on a discord someone saying: The issue likely stems from the fact that you are manually placing your inputs on the GPU (with to(model.device)), but the Trainer expects data to be on the CPU and will handle the transfer to the GPU internally. I can't find anything of the sort written in the Trainer documentation of huggingface https://huggingface.co/docs/transformers/en/main_classes/trainer . Is it true? If not, how can I get rid of that error? MRE: import torch from torch.utils.data import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from transformers import TrainingArguments from transformers import Trainer from peft import LoraConfig, get_peft_model model_name = ""croissantllm/CroissantLLMBase"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"") texts = [ ""The first sentence for fine-tuning. </s>"", ""The second sentence for fine-tuning. </s>"" ] inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts] lora_config = LoraConfig( r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""q_proj"", ""v_proj""], ) model = get_peft_model(model, lora_config) class CustomDataset(Dataset): def __init__(self, input_list): self.input_list = input_list def __len__(self): return len(self.input_list) def __getitem__(self, idx): input_ids = self.input_list[idx]['input_ids'].squeeze() labels = input_ids.clone() return {""input_ids"": input_ids, ""labels"": labels} train_dataset = CustomDataset(inputs) training_args = TrainingArguments( output_dir=""./lora_croissantllm"", per_device_train_batch_size=1, num_train_epochs=1, save_steps=10, save_total_limit=2, logging_dir=""./logs"", logging_steps=10, ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, ) trainer.train() The issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).",trainer huggingface  runtimeerror  pin   39  torchcuda  floattensor   39  dense cpu tensor pin recently get follow error  runtimeerror  pin  torchcuda  floattensor  dense cpu tensor pin lora small llm  see discord someone say  issue likely stem fact manually place input gpu   modeldevice    trainer expect datum cpu handle transfer gpu internally  can not find anything sort write trainer documentation huggingface   huggingfaceco  doc  transformer  en  mainclasse  trainer  true   get rid error  mre  import torch torchutilsdata import dataset transformer import automodelforcausallm  autotokenizer transformer import trainingarguments transformer import trainer peft import loraconfig  getpeftmodel modelname    croissantllm  croissantllmbase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforcausallmfrompretraine  modelname  torchdtype  torchfloat16  devicemap  auto   text     first sentence fine  tuning   s      second sentence fine  tuning   s    input   tokenizer  text  returntensors  pt   to  modeldevice  text text  loraconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    qproj     vproj     model  getpeftmodel  model  loraconfig  class customdataset  dataset   def   init    self  inputlist   selfinputlist  inputlist def   len    self   return len  selfinputlist  def   getitem    self  idx   inputids  selfinputlist  idx    inputids   squeeze   label  inputidsclone   return    inputids   inputid    label   label  traindataset  customdataset  input  trainingargs  trainingarguments  outputdir  loracroissantllm   perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10   trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset   trainertrain   issue fairly easy reproduce directly colab  run  pip install  upgrade torch transformer peft first cell  ,since pin memory available cpu gpu  run gpu colab  disable set dataloaderpinmemory false trainingarguments trainingargs  trainingarguments  outputdir  loracroissantllm   dataloaderpinmemory  false  perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10  ,trainer huggingface  runtimeerror  pin   39  torchcuda  floattensor   39  dense cpu tensor pin recently get follow error  runtimeerror  pin  torchcuda  floattensor  dense cpu tensor pin lora small llm  see discord someone say  issue likely stem fact manually place input gpu   modeldevice    trainer expect datum cpu handle transfer gpu internally  can not find anything sort write trainer documentation huggingface   huggingfaceco  doc  transformer  en  mainclasse  trainer  true   get rid error  mre  import torch torchutilsdata import dataset transformer import automodelforcausallm  autotokenizer transformer import trainingarguments transformer import trainer peft import loraconfig  getpeftmodel modelname    croissantllm  croissantllmbase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforcausallmfrompretraine  modelname  torchdtype  torchfloat16  devicemap  auto   text     first sentence fine  tuning   s      second sentence fine  tuning   s    input   tokenizer  text  returntensors  pt   to  modeldevice  text text  loraconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    qproj     vproj     model  getpeftmodel  model  loraconfig  class customdataset  dataset   def   init    self  inputlist   selfinputlist  inputlist def   len    self   return len  selfinputlist  def   getitem    self  idx   inputids  selfinputlist  idx    inputids   squeeze   label  inputidsclone   return    inputids   inputid    label   label  traindataset  customdataset  input  trainingargs  trainingarguments  outputdir  loracroissantllm   perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10   trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset   trainertrain   issue fairly easy reproduce directly colab  run  pip install  upgrade torch transformer peft first cell   since pin memory available cpu gpu  run gpu colab  disable set dataloaderpinmemory false trainingarguments trainingargs  trainingarguments  outputdir  loracroissantllm   dataloaderpinmemory  false  perdevicetrainbatchsize1  numtrainepochs1  savesteps10  savetotallimit2  loggingdir  logs   loggingsteps10  ,Basic Understanding
Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot;,"I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem? Here is a minimal example: import os from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast import torch from torch.cuda.amp import GradScaler, autocast model_name = ""facebook/opt-1.3b"" cache_dir = './models' os.environ[""CUDA_VISIBLE_DEVICES""] = ""7"" quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.float16 ) pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config) tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name, cache_dir=cache_dir) optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4) scaler = GradScaler() input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0) labels = torch.LongTensor([[1, 2, 3, 4]]).to(0) with torch.autocast(device_type='cuda'): out = pretrained_model(input_ids=input_ids, labels=labels) loss = out.loss scaler.scale(out.loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() print(f'End') At the line scaler.step(optimizer) , an error occurs: Exception has occurred: ValueError: Attempting to unscale FP16 gradients.","['python', 'pytorch', 'nlp', 'huggingface-transformers', 'fine-tuning']",1,"You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32. You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training. What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details here",2024-09-03 08:38:23,2024-09-03 17:22:57,517,https://stackoverflow.com/questions/78943401/fine-tuning-a-pretrained-model-with-quantization-and-amp-scaler-error-attempti,"Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot; I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem? Here is a minimal example: import os from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast import torch from torch.cuda.amp import GradScaler, autocast model_name = ""facebook/opt-1.3b"" cache_dir = './models' os.environ[""CUDA_VISIBLE_DEVICES""] = ""7"" quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.float16 ) pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config) tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name, cache_dir=cache_dir) optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4) scaler = GradScaler() input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0) labels = torch.LongTensor([[1, 2, 3, 4]]).to(0) with torch.autocast(device_type='cuda'): out = pretrained_model(input_ids=input_ids, labels=labels) loss = out.loss scaler.scale(out.loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() print(f'End') At the line scaler.step(optimizer) , an error occurs: Exception has occurred: ValueError: Attempting to unscale FP16 gradients.",fine  tuning pretrained model quantization amp  scaler error  quot  attempt unscale fp16 gradients  quot  try fine  tune pretraine model limited vram  achieve  use quantization automatic mixed precision  amp   however  encounter issue can not seem resolve  could please help identify problem  minimal example  import os transformer import bitsandbytesconfig  optforcausallm  gpt2tokenizerfast import torch torchcudaamp import gradscaler  autocast modelname    facebook  opt13b  cachedir   model  osenviron    cudavisibledevice      7  quantizationconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchfloat16  pretrainedmodel  optforcausallm  optforcausallmfrompretrained  modelname  cachedir  cachedir  quantizationconfig  quantizationconfig  tokenizer  gpt2tokenizerfast  gpt2tokenizerfastfrompretrained  modelname  cachedir  cachedir  optimizer  torchoptim  adamw  pretrainedmodelparameters    lr1e4  scaler  gradscaler   inputids  torch  longtensor    0  1  2  3    to  0  label  torch  longtensor    1  2  3  4    to  0  torchautocast  devicetypecuda     pretrainedmodel  inputid  inputid  label  label  loss  outloss scalerscale  outloss  backward   scalerstep  optimizer  scalerupdate   optimizerzerograd   print  fend   line scalerstep  optimizer   error occur  exception occur  valueerror  attempt unscale fp16 gradient ,can not fine  tune fp16  uint8 model amp  amp use fp32 parameter  param autocast fp16 forward pass  amp expect master set parameter fp32  also not fine  tune quantize model first place  quantization cause sort numerical issue instability training  suppose keep quantize model static train adapter top quantize model  find detail,fine  tuning pretrained model quantization amp  scaler error  quot  attempt unscale fp16 gradients  quot  try fine  tune pretraine model limited vram  achieve  use quantization automatic mixed precision  amp   however  encounter issue can not seem resolve  could please help identify problem  minimal example  import os transformer import bitsandbytesconfig  optforcausallm  gpt2tokenizerfast import torch torchcudaamp import gradscaler  autocast modelname    facebook  opt13b  cachedir   model  osenviron    cudavisibledevice      7  quantizationconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchfloat16  pretrainedmodel  optforcausallm  optforcausallmfrompretrained  modelname  cachedir  cachedir  quantizationconfig  quantizationconfig  tokenizer  gpt2tokenizerfast  gpt2tokenizerfastfrompretrained  modelname  cachedir  cachedir  optimizer  torchoptim  adamw  pretrainedmodelparameters    lr1e4  scaler  gradscaler   inputids  torch  longtensor    0  1  2  3    to  0  label  torch  longtensor    1  2  3  4    to  0  torchautocast  devicetypecuda     pretrainedmodel  inputid  inputid  label  label  loss  outloss scalerscale  outloss  backward   scalerstep  optimizer  scalerupdate   optimizerzerograd   print  fend   line scalerstep  optimizer   error occur  exception occur  valueerror  attempt unscale fp16 gradient  can not fine  tune fp16  uint8 model amp  amp use fp32 parameter  param autocast fp16 forward pass  amp expect master set parameter fp32  also not fine  tune quantize model first place  quantization cause sort numerical issue instability training  suppose keep quantize model static train adapter top quantize model  find detail,Implementation Issues
Keep training pytorch model on new data,"I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps: Load and process the text. Use a TF-IDF Vectorizer. Build the neural network and save the TF-IDF Vectorizer and model to predict new data. However, every day I need to classify new comments and correct any wrong classifications. Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct). Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen. So the mains questions are: How could i check if the propossed way to solve this problem work as i expect? What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer? Here its the full training process: import torch from torch import nn from torch.utils.data import Dataset, DataLoader, random_split from sklearn.preprocessing import LabelEncoder import polars as pl from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer import joblib set1 = ( pl .read_csv( ""set1.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) ) # since the dateset its unbalanced, im going to force to have more balance fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df]) """""" The text its already clean, so im going to change the labels to numeric and then split it on train, test ,val """""" label_mapping = { ""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3 } train_mapped = ( train_df .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) train_set, pre_Test = train_test_split(train_mapped, test_size=0.4, random_state=42, stratify=train_mapped[""label""]) test_set, val_set = train_test_split(pre_Test, test_size=0.5, random_state=42, stratify=pre_Test[""label""]) # Vectorize text data using TF-IDF vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2)) X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray() X_val_tfidf = vectorizer.transform(val_set['text']).toarray() X_test_tfidf = vectorizer.transform(test_set['text']).toarray() y_train = train_set['label'] y_val = val_set['label'] y_test = test_set['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label train_dataset = TextDataset(X_train_tfidf, y_train) val_dataset = TextDataset(X_val_tfidf, y_val) test_dataset = TextDataset(X_test_tfidf, y_test) batch_size = 32 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size) test_loader = DataLoader(test_dataset, batch_size=batch_size) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x input_dim = X_train_tfidf.shape[1] model = TextClassificationModel(input_dim, 4) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) # Training loop num_epochs = 17 best_val_acc = 0.0 best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() # Validation model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in val_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() val_acc = correct / total if val_acc > best_val_acc: best_val_acc = val_acc torch.save(model.state_dict(), best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}') # Load the best model model.load_state_dict(torch.load(best_model_path)) # Load the best model model.load_state_dict(torch.load(best_model_path)) # Test the model model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in test_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() test_acc = correct / total print(f'Test Acc: {test_acc:.3f}') # Save the TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" joblib.dump(vectorizer, vectorizer_path) # Save the PyTorch model model_path = ""text_classification_model.pth"" torch.save(model.state_dict(), model_path) Proposed code: import torch import joblib import polars as pl from sklearn.model_selection import train_test_split from torch import nn from torch.utils.data import Dataset, DataLoader # Load the saved TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" vectorizer = joblib.load(vectorizer_path) input_dim = len(vectorizer.get_feature_names_out()) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x # Load the saved PyTorch model model_path = ""text_classification_model.pth"" model = TextClassificationModel(input_dim, 4) model.load_state_dict(torch.load(model_path)) # Map labels to numeric values label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} sentiments = [""fear"",""joy"",""sadness"",""anger""] new_data = ( pl .read_csv( ""set2.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) .filter(pl.col(""label"").is_in(sentiments)) .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) # Vectorize the new text data using the loaded TF-IDF vectorizer X_new = vectorizer.transform(new_data['text']).toarray() y_new = new_data['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label batch_size = 10 # Create DataLoader for the new training data new_train_dataset = TextDataset(X_new, y_new) new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) num_epochs = 5 new_best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in new_train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() torch.save(model.state_dict(), new_best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # Save the PyTorch model new_best_model_path = ""new_moedl.pth"" torch.save(model.state_dict(), new_best_model_path) The dataset can be found here","['python', 'scikit-learn', 'pytorch', 'nlp', 'python-polars']",2,"use pre-trained word embeddings like BertForSequenceClassification. These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words. Model Training with BERT import torch from torch import nn, optim from torch.utils.data import DataLoader, Dataset from transformers import BertTokenizer, BertModel, BertForSequenceClassification from transformers import Trainer, TrainingArguments from sklearn.model_selection import train_test_split import polars as pl # Load and prepare data set1 = pl.read_csv(""set1.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""]) # Balance dataset fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df, joy_df, sadness_df, anger_df]) label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} train_df = train_df.with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)) # Split dataset train_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[""label""]) test_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[""label""]) # Dataset class class TextDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_length=128): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_length = max_length def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt' ) return { 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.long) } # Initialize tokenizer and datasets tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') train_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer) val_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer) test_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer) # Initialize BERT model for classification model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4) # Training arguments training_args = TrainingArguments( output_dir='./results', num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, evaluation_strategy='epoch', save_strategy='epoch', logging_dir='./logs', learning_rate=2e-5, load_best_model_at_end=True ) # Define Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset ) # Train model trainer.train() # Evaluate model results = trainer.evaluate(test_dataset) print(f""Test Accuracy: {results['eval_accuracy']:.4f}"") # Save the model and tokenizer model.save_pretrained(""saved_model"") tokenizer.save_pretrained(""saved_tokenizer"") Incremental training with least effort # Load the saved model and tokenizer model = BertForSequenceClassification.from_pretrained(""saved_model"") tokenizer = BertTokenizer.from_pretrained(""saved_tokenizer"") # Load new data new_data = ( pl.read_csv(""set2.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""]) .filter(pl.col(""label"").is_in([""fear"", ""joy"", ""sadness"", ""anger""])) .with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)) ) # Create new dataset new_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer) # Update training arguments for incremental training new_training_args = TrainingArguments( output_dir='./results_incremental', num_train_epochs=2, # Fewer epochs since it's incremental per_device_train_batch_size=16, evaluation_strategy='epoch', logging_dir='./logs_incremental', learning_rate=2e-5, load_best_model_at_end=True ) # Define new trainer new_trainer = Trainer( model=model, args=new_training_args, train_dataset=new_dataset, eval_dataset=val_dataset # Validate on previous validation set ) # Train on new data new_trainer.train() # Evaluate after retraining new_results = new_trainer.evaluate(test_dataset) print(f""Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}"") # Save the updated model model.save_pretrained(""saved_model_incremental"")",2024-08-30 17:47:59,2024-08-31 02:50:05,283,https://stackoverflow.com/questions/78933232/keep-training-pytorch-model-on-new-data,"Keep training pytorch model on new data I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps: Load and process the text. Use a TF-IDF Vectorizer. Build the neural network and save the TF-IDF Vectorizer and model to predict new data. However, every day I need to classify new comments and correct any wrong classifications. Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct). Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen. So the mains questions are: How could i check if the propossed way to solve this problem work as i expect? What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer? Here its the full training process: import torch from torch import nn from torch.utils.data import Dataset, DataLoader, random_split from sklearn.preprocessing import LabelEncoder import polars as pl from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer import joblib set1 = ( pl .read_csv( ""set1.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) ) # since the dateset its unbalanced, im going to force to have more balance fear_df = set1.filter(pl.col(""label"") == ""fear"") joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500) sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500) anger_df = set1.filter(pl.col(""label"") == ""anger"") train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df]) """""" The text its already clean, so im going to change the labels to numeric and then split it on train, test ,val """""" label_mapping = { ""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3 } train_mapped = ( train_df .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) train_set, pre_Test = train_test_split(train_mapped, test_size=0.4, random_state=42, stratify=train_mapped[""label""]) test_set, val_set = train_test_split(pre_Test, test_size=0.5, random_state=42, stratify=pre_Test[""label""]) # Vectorize text data using TF-IDF vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2)) X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray() X_val_tfidf = vectorizer.transform(val_set['text']).toarray() X_test_tfidf = vectorizer.transform(test_set['text']).toarray() y_train = train_set['label'] y_val = val_set['label'] y_test = test_set['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label train_dataset = TextDataset(X_train_tfidf, y_train) val_dataset = TextDataset(X_val_tfidf, y_val) test_dataset = TextDataset(X_test_tfidf, y_test) batch_size = 32 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size) test_loader = DataLoader(test_dataset, batch_size=batch_size) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x input_dim = X_train_tfidf.shape[1] model = TextClassificationModel(input_dim, 4) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) # Training loop num_epochs = 17 best_val_acc = 0.0 best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() # Validation model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in val_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() val_acc = correct / total if val_acc > best_val_acc: best_val_acc = val_acc torch.save(model.state_dict(), best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}') # Load the best model model.load_state_dict(torch.load(best_model_path)) # Load the best model model.load_state_dict(torch.load(best_model_path)) # Test the model model.eval() correct, total = 0, 0 with torch.no_grad(): for texts, labels in test_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() test_acc = correct / total print(f'Test Acc: {test_acc:.3f}') # Save the TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" joblib.dump(vectorizer, vectorizer_path) # Save the PyTorch model model_path = ""text_classification_model.pth"" torch.save(model.state_dict(), model_path) Proposed code: import torch import joblib import polars as pl from sklearn.model_selection import train_test_split from torch import nn from torch.utils.data import Dataset, DataLoader # Load the saved TF-IDF vectorizer vectorizer_path = ""tfidf_vectorizer.pkl"" vectorizer = joblib.load(vectorizer_path) input_dim = len(vectorizer.get_feature_names_out()) class TextClassificationModel(nn.Module): def __init__(self, input_dim, num_classes): super(TextClassificationModel, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.dropout1 = nn.Dropout(0.5) self.fc2 = nn.Linear(64, 32) self.dropout2 = nn.Dropout(0.5) self.fc3 = nn.Linear(32, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout1(x) x = torch.relu(self.fc2(x)) x = self.dropout2(x) x = torch.softmax(self.fc3(x), dim=1) return x # Load the saved PyTorch model model_path = ""text_classification_model.pth"" model = TextClassificationModel(input_dim, 4) model.load_state_dict(torch.load(model_path)) # Map labels to numeric values label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3} sentiments = [""fear"",""joy"",""sadness"",""anger""] new_data = ( pl .read_csv( ""set2.txt"", separator="";"", has_header=False, new_columns=[""text"",""label""] ) .filter(pl.col(""label"").is_in(sentiments)) .with_columns( pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16) ) ) # Vectorize the new text data using the loaded TF-IDF vectorizer X_new = vectorizer.transform(new_data['text']).toarray() y_new = new_data['label'] class TextDataset(Dataset): def __init__(self, texts, labels): self.texts = texts self.labels = labels def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] return text, label batch_size = 10 # Create DataLoader for the new training data new_train_dataset = TextDataset(X_new, y_new) new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adamax(model.parameters()) num_epochs = 5 new_best_model_path = ""modelbest.pth"" for epoch in range(num_epochs): model.train() for texts, labels in new_train_loader: texts, labels = texts.float(), labels.long() outputs = model(texts) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() torch.save(model.state_dict(), new_best_model_path) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # Save the PyTorch model new_best_model_path = ""new_moedl.pth"" torch.save(model.state_dict(), new_best_model_path) The dataset can be found here",keep training pytorch model new datum  m work text classification task decide use pytorch model purpose  process mainly involve follow step  load process text  use tf  idf vectorizer  build neural network save tf  idf vectorizer model predict new datum  however  every day need classify new comment correct wrong classification  currently  approach add new comment correct classification dataset retrain entire model  process time  consume  new comment lose validation  would like create new dataset newly classify text continue train new datum  new comment classify manually  label correct   use gpt online code  write desire process  however  i m sure working expect  i m make silly mistake happen  main question  could check proposse way solve problem work expect  vectorizer face new token  fittransform   would loose original vectorizer  full training process  import torch torch import nn torchutilsdata import dataset  dataloader  randomsplit sklearnpreprocesse import labelencoder import polar pl sklearnmodelselection import traintestsplit sklearnfeatureextractiontext import tfidfvectorizer import joblib set1   pl readcsv    set1txt   separator     hasheader  false  newcolumns    text    label      since dateset unbalanced  i m go force balance feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf       text already clean  i m go change label numeric split train  test  val     labelmappe     anger   0    fear   1    joy   2    sadness   3  trainmapped   traindf withcolumn  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    trainset  pretest  traintestsplit  trainmapped  testsize04  randomstate42  stratify  trainmapped    label    testset  valset  traintestsplit  pretest  testsize05  randomstate42  stratify  pret    label     vectorize text datum use tf  idf vectorizer  tfidfvectorizer  maxfeatures30000  ngramrange  1  2   xtraintfidf  vectorizerfittransform  trainset   text    toarray   xvaltfidf  vectorizertransform  valset   text    toarray   xtesttfidf  vectorizertransform  testset   text    toarray   ytrain  trainset   label   yval  valset   label   yt  testset   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label traindataset  textdataset  xtraintfidf  ytrain  valdataset  textdataset  xvaltfidf  yval  testdataset  textdataset  xtesttfidf  ytest  batchsize  32 trainloader  dataloader  traindataset  batchsize  batchsize  shuffle  true  valloader  dataloader  valdataset  batchsize  batchsize  testloader  dataloader  testdataset  batchsize  batchsize  class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x inputdim  xtraintfidfshape  1  model  textclassificationmodel  inputdim  4   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter     training loop numepochs  17 bestvalacc  00 bestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label trainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep    validation modeleval   correct  total  0  0 torchnograd    text  label valloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   valacc  correct  total valacc  bestvalacc  bestvalacc  valacc torchsave  modelstatedict    bestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f   val acc   valacc  4f     load good model modelloadstatedict  torchload  bestmodelpath    load good model modelloadstatedict  torchload  bestmodelpath    test model modeleval   correct  total  0  0 torchnograd    text  label testloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   testacc  correct  total print  ft acc   testacc  3f     save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  joblibdump  vectorizer  vectorizerpath   save pytorch model modelpath    textclassificationmodelpth  torchsave  modelstatedict    modelpath  propose code  import torch import joblib import polar pl sklearnmodelselection import traintestsplit torch import nn torchutilsdata import dataset  dataloader  load save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  vectorizer  joblibload  vectorizerpath  inputdim  len  vectorizergetfeaturenamesout    class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x  load save pytorch model modelpath    textclassificationmodelpth  model  textclassificationmodel  inputdim  4  modelloadstatedict  torchload  modelpath    map label numeric value labelmapping     anger   0    fear   1    joy   2    sadness   3  sentiment     fear    joy    sadness    anger   newdata   pl readcsv    set2txt   separator     hasheader  false  newcolumns    text    label    filter  plcol    label   isin  sentiment   withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     vectorize new text datum use load tf  idf vectorizer xnew  vectorizertransform  newdata   text    toarray   ynew  newdata   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label batchsize  10  create dataloader new training datum newtraindataset  textdataset  xnew  ynew  newtrainloader  dataloader  newtraindataset  batchsize  batchsize  shuffle  true   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter    numepochs  5 newbestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label newtrainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep   torchsave  modelstatedict    newbestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f     save pytorch model newbestmodelpath    newmoedlpth  torchsave  modelstatedict    newbestmodelpath  dataset find,use pre  train word embedding like bertforsequenceclassification  embedding handle unseen token gracefully since map word continuous vector base semantic meaning  reduce impact unseen word  model training bert import torch torch import nn  optim torchutilsdata import dataloader  dataset transformer import berttokenizer  bertmodel  bertforsequenceclassification transformer import trainer  trainingarguments sklearnmodelselection import traintestsplit import polar pl  load prepare datum set1  plreadcsv    set1txt   separator     hasheader  false  newcolumns    text     label     balance dataset feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf   labelmapping     anger   0    fear   1    joy   2    sadness   3  traindf  traindfwithcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    split dataset trainset  testvalset  traintestsplit  traindf  testsize04  randomstate42  stratify  traindf    label    testset  valset  traintestsplit  testvalset  testsize05  randomstate42  stratify  testvalset    label     dataset class class textdataset  dataset   def   init    self  text  label  tokenizer  maxlength128   selftext  text selflabels  label selftokenizer  tokenizer selfmaxlength  maxlength def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  encode  selftokenizerencodeplus  text  addspecialtoken  true  maxlength  selfmaxlength  paddingmaxlength   truncation  true  returntensorspt   return   inputids   encode   inputids   flatten     attentionmask   encode   attentionmask   flatten     label   torchtensor  label  dtype  torchlong    initialize tokenizer dataset tokenizer  berttokenizerfrompretraine   bert  base  uncase   traindataset  textdataset  trainset   text    trainset   label    tokenizer  valdataset  textdataset  valset   text    valset   label    tokenizer  testdataset  textdataset  testset   text    testset   label    tokenizer   initialize bert model classification model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels4   training argument trainingargs  trainingarguments  outputdirresults   numtrainepochs3  perdevicetrainbatchsize16  perdeviceevalbatchsize16  evaluationstrategyepoch   savestrategyepoch   loggingdirlogs   learningrate2e5  loadbestmodelatend  true   define trainer trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset   train model trainertrain    evaluate model result  trainerevaluate  testdataset  print  f  test accuracy   result   evalaccuracy    4f     save model tokenizer modelsavepretraine    savedmodel   tokenizersavepretraine    savedtokenizer   incremental training least effort  load save model tokenizer model  bertforsequenceclassificationfrompretraine    savedmodel   tokenizer  berttokenizerfrompretraine    savedtokenizer    load new datum newdata   plreadcsv    set2txt   separator     hasheader  false  newcolumns    text     label    filter  plcol    label   isin     fear     joy     sadness     anger     withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     create new dataset newdataset  textdataset  newdata   text    newdata   label    tokenizer   update training argument incremental training newtrainingarg  trainingarguments  outputdirresultsincremental   numtrainepochs2   few epoch since s incremental perdevicetrainbatchsize16  evaluationstrategyepoch   loggingdirlogsincremental   learningrate2e5  loadbestmodelatend  true   define new trainer newtrainer  trainer  model  model  arg  newtrainingargs  traindataset  newdataset  evaldataset  valdataset  validate previous validation set   train new datum newtrainertrain    evaluate retrain newresult  newtrainerevaluate  testdataset  print  f  test accuracy incremental training   newresult   evalaccuracy    4f     save update model modelsavepretraine    savedmodelincremental  ,keep training pytorch model new datum  m work text classification task decide use pytorch model purpose  process mainly involve follow step  load process text  use tf  idf vectorizer  build neural network save tf  idf vectorizer model predict new datum  however  every day need classify new comment correct wrong classification  currently  approach add new comment correct classification dataset retrain entire model  process time  consume  new comment lose validation  would like create new dataset newly classify text continue train new datum  new comment classify manually  label correct   use gpt online code  write desire process  however  i m sure working expect  i m make silly mistake happen  main question  could check proposse way solve problem work expect  vectorizer face new token  fittransform   would loose original vectorizer  full training process  import torch torch import nn torchutilsdata import dataset  dataloader  randomsplit sklearnpreprocesse import labelencoder import polar pl sklearnmodelselection import traintestsplit sklearnfeatureextractiontext import tfidfvectorizer import joblib set1   pl readcsv    set1txt   separator     hasheader  false  newcolumns    text    label      since dateset unbalanced  i m go force balance feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf       text already clean  i m go change label numeric split train  test  val     labelmappe     anger   0    fear   1    joy   2    sadness   3  trainmapped   traindf withcolumn  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    trainset  pretest  traintestsplit  trainmapped  testsize04  randomstate42  stratify  trainmapped    label    testset  valset  traintestsplit  pretest  testsize05  randomstate42  stratify  pret    label     vectorize text datum use tf  idf vectorizer  tfidfvectorizer  maxfeatures30000  ngramrange  1  2   xtraintfidf  vectorizerfittransform  trainset   text    toarray   xvaltfidf  vectorizertransform  valset   text    toarray   xtesttfidf  vectorizertransform  testset   text    toarray   ytrain  trainset   label   yval  valset   label   yt  testset   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label traindataset  textdataset  xtraintfidf  ytrain  valdataset  textdataset  xvaltfidf  yval  testdataset  textdataset  xtesttfidf  ytest  batchsize  32 trainloader  dataloader  traindataset  batchsize  batchsize  shuffle  true  valloader  dataloader  valdataset  batchsize  batchsize  testloader  dataloader  testdataset  batchsize  batchsize  class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x inputdim  xtraintfidfshape  1  model  textclassificationmodel  inputdim  4   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter     training loop numepochs  17 bestvalacc  00 bestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label trainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep    validation modeleval   correct  total  0  0 torchnograd    text  label valloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   valacc  correct  total valacc  bestvalacc  bestvalacc  valacc torchsave  modelstatedict    bestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f   val acc   valacc  4f     load good model modelloadstatedict  torchload  bestmodelpath    load good model modelloadstatedict  torchload  bestmodelpath    test model modeleval   correct  total  0  0 torchnograd    text  label testloader  text  label  textsfloat    labelslong   output  model  text    predict  torchmax  outputsdata  1  total   labelssize  0  correct    predict   label  sum   item   testacc  correct  total print  ft acc   testacc  3f     save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  joblibdump  vectorizer  vectorizerpath   save pytorch model modelpath    textclassificationmodelpth  torchsave  modelstatedict    modelpath  propose code  import torch import joblib import polar pl sklearnmodelselection import traintestsplit torch import nn torchutilsdata import dataset  dataloader  load save tf  idf vectorizer vectorizerpath    tfidfvectorizerpkl  vectorizer  joblibload  vectorizerpath  inputdim  len  vectorizergetfeaturenamesout    class textclassificationmodel  nn  module   def   init    self  inputdim  numclasse   super  textclassificationmodel  self  init     selffc1  nn  linear  inputdim  64  selfdropout1  nn  dropout  05  selffc2  nn  linear  64  32  selfdropout2  nn  dropout  05  selffc3  nn  linear  32  numclasse  def forward  self  x   x  torchrelu  selffc1  x   x  selfdropout1  x  x  torchrelu  selffc2  x   x  selfdropout2  x  x  torchsoftmax  selffc3  x   dim1  return x  load save pytorch model modelpath    textclassificationmodelpth  model  textclassificationmodel  inputdim  4  modelloadstatedict  torchload  modelpath    map label numeric value labelmapping     anger   0    fear   1    joy   2    sadness   3  sentiment     fear    joy    sadness    anger   newdata   pl readcsv    set2txt   separator     hasheader  false  newcolumns    text    label    filter  plcol    label   isin  sentiment   withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     vectorize new text datum use load tf  idf vectorizer xnew  vectorizertransform  newdata   text    toarray   ynew  newdata   label   class textdataset  dataset   def   init    self  text  label   selftext  text selflabel  label def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  return text  label batchsize  10  create dataloader new training datum newtraindataset  textdataset  xnew  ynew  newtrainloader  dataloader  newtraindataset  batchsize  batchsize  shuffle  true   define loss optimizer criterion  nn  crossentropyloss   optimizer  torchoptim  adamax  modelparameter    numepochs  5 newbestmodelpath    modelbestpth  epoch range  numepochs   modeltrain   text  label newtrainloader  text  label  textsfloat    labelslong   output  model  text  loss  criterion  output  label  optimizerzerograd   lossbackward   optimizerstep   torchsave  modelstatedict    newbestmodelpath  print  fepoch   epoch1    numepochs    loss   lossitem    4f     save pytorch model newbestmodelpath    newmoedlpth  torchsave  modelstatedict    newbestmodelpath  dataset find use pre  train word embedding like bertforsequenceclassification  embedding handle unseen token gracefully since map word continuous vector base semantic meaning  reduce impact unseen word  model training bert import torch torch import nn  optim torchutilsdata import dataloader  dataset transformer import berttokenizer  bertmodel  bertforsequenceclassification transformer import trainer  trainingarguments sklearnmodelselection import traintestsplit import polar pl  load prepare datum set1  plreadcsv    set1txt   separator     hasheader  false  newcolumns    text     label     balance dataset feardf  set1filter  plcol    label       fear   joydf  set1filter  plcol    label       joy   sample  n2500  sadnessdf  set1filter  plcol    label       sadness   sample  n2500  angerdf  set1filter  plcol    label       anger   traindf  plconcat   feardf  joydf  sadnessdf  angerdf   labelmapping     anger   0    fear   1    joy   2    sadness   3  traindf  traindfwithcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16    split dataset trainset  testvalset  traintestsplit  traindf  testsize04  randomstate42  stratify  traindf    label    testset  valset  traintestsplit  testvalset  testsize05  randomstate42  stratify  testvalset    label     dataset class class textdataset  dataset   def   init    self  text  label  tokenizer  maxlength128   selftext  text selflabels  label selftokenizer  tokenizer selfmaxlength  maxlength def   len    self   return len  selftexts  def   getitem    self  idx   text  selftexts  idx  label  selflabel  idx  encode  selftokenizerencodeplus  text  addspecialtoken  true  maxlength  selfmaxlength  paddingmaxlength   truncation  true  returntensorspt   return   inputids   encode   inputids   flatten     attentionmask   encode   attentionmask   flatten     label   torchtensor  label  dtype  torchlong    initialize tokenizer dataset tokenizer  berttokenizerfrompretraine   bert  base  uncase   traindataset  textdataset  trainset   text    trainset   label    tokenizer  valdataset  textdataset  valset   text    valset   label    tokenizer  testdataset  textdataset  testset   text    testset   label    tokenizer   initialize bert model classification model  bertforsequenceclassificationfrompretraine   bert  base  uncased   numlabels4   training argument trainingargs  trainingarguments  outputdirresults   numtrainepochs3  perdevicetrainbatchsize16  perdeviceevalbatchsize16  evaluationstrategyepoch   savestrategyepoch   loggingdirlogs   learningrate2e5  loadbestmodelatend  true   define trainer trainer  trainer  model  model  arg  trainingargs  traindataset  traindataset  evaldataset  valdataset   train model trainertrain    evaluate model result  trainerevaluate  testdataset  print  f  test accuracy   result   evalaccuracy    4f     save model tokenizer modelsavepretraine    savedmodel   tokenizersavepretraine    savedtokenizer   incremental training least effort  load save model tokenizer model  bertforsequenceclassificationfrompretraine    savedmodel   tokenizer  berttokenizerfrompretraine    savedtokenizer    load new datum newdata   plreadcsv    set2txt   separator     hasheader  false  newcolumns    text     label    filter  plcol    label   isin     fear     joy     sadness     anger     withcolumns  plcol    label   replacestrict  labelmapping  default    cast  pl  int16     create new dataset newdataset  textdataset  newdata   text    newdata   label    tokenizer   update training argument incremental training newtrainingarg  trainingarguments  outputdirresultsincremental   numtrainepochs2   few epoch since s incremental perdevicetrainbatchsize16  evaluationstrategyepoch   loggingdirlogsincremental   learningrate2e5  loadbestmodelatend  true   define new trainer newtrainer  trainer  model  model  arg  newtrainingargs  traindataset  newdataset  evaldataset  valdataset  validate previous validation set   train new datum newtrainertrain    evaluate retrain newresult  newtrainerevaluate  testdataset  print  f  test accuracy incremental training   newresult   evalaccuracy    4f     save update model modelsavepretraine    savedmodelincremental  ,Implementation Issues
Capitalized words in sentiment analysis,"I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive . A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and distinction between sentiments as good , very good than just positive to include the importance of this upper case words . this is my current code : from itertools import chain def is_upper_case(text): return [word for word in text.split() if word.isupper() and word != 'I'] unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case))) print(unique_upper_words)","['nlp', 'sentiment-analysis', 'bert-language-model', 'data-preprocessing']",1,"If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data. If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis. If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out this repository as an example.",2024-08-30 13:49:56,2024-08-30 17:50:30,134,https://stackoverflow.com/questions/78932356/capitalized-words-in-sentiment-analysis,"Capitalized words in sentiment analysis I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive . A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and distinction between sentiments as good , very good than just positive to include the importance of this upper case words . this is my current code : from itertools import chain def is_upper_case(text): return [word for word in text.split() if word.isupper() and word != 'I'] unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case))) print(unique_upper_words)",capitalize word sentiment analysis  m currently work datum customer review product sephora  task classify sentiment  negative  neutral  positive  common technique text preprocesse low case word  situation upper case word like  amazing  hide significant emotion behind turn word low case cause information loss  would happy opinion subject still low case word  personally think create class distinction sentiment good  good positive include importance upper case word  current code  itertool import chain def isuppercase  text   return  word word textsplit   wordisupper   word      uniqueupperword  set  chainfromiterable  allreviews   reviewtext   apply  isuppercase    print  uniqueupperwords ,use bert  base model  llm  actual classification would recommend use preprocessing  least come capitalization   model pre  train non  preprocesse datum  want kind analysis result label sentence could lowercase everything group n  gram simplify analysis  think multiple class well distinction prediction  think would make sense switch sentiment regression instead classification  predict value continuous range  come somewhat natural fine  tune language model normal classification would take continuous output model map categorical class use something like softmax  need skip last step directly use model output  many python ml framework fine  tuning use language model class regression task  check repository example ,capitalize word sentiment analysis  m currently work datum customer review product sephora  task classify sentiment  negative  neutral  positive  common technique text preprocesse low case word  situation upper case word like  amazing  hide significant emotion behind turn word low case cause information loss  would happy opinion subject still low case word  personally think create class distinction sentiment good  good positive include importance upper case word  current code  itertool import chain def isuppercase  text   return  word word textsplit   wordisupper   word      uniqueupperword  set  chainfromiterable  allreviews   reviewtext   apply  isuppercase    print  uniqueupperwords  use bert  base model  llm  actual classification would recommend use preprocessing  least come capitalization   model pre  train non  preprocesse datum  want kind analysis result label sentence could lowercase everything group n  gram simplify analysis  think multiple class well distinction prediction  think would make sense switch sentiment regression instead classification  predict value continuous range  come somewhat natural fine  tune language model normal classification would take continuous output model map categorical class use something like softmax  need skip last step directly use model output  many python ml framework fine  tuning use language model class regression task  check repository example ,Basic Understanding
cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39;,"I've been using LLAMA 2 for research for a few months now and I import as follows: from transformers import AutoModelForCausalLM, AutoTokenizer device = torch.device(""cuda"") tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"") model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=True) It has always worked. However, today it is showing the following error: RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): Failed to import transformers.generation.utils because of the following error (look up to see its traceback): cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/ init .py) Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.","['python', 'nlp', 'huggingface-transformers', 'transformer-model', 'llama']",1,"The error you're encountering is due to the split_torch_state_dict_into_shards function not being available in huggingface-hub version < 0.23.0 . This function is included starting from version 0.23.0 . To resolve this issue, update the huggingface-hub library to version 0.23.0 or later Also please install accelerate: pip install accelerate==0.31.0 here is a git link: https://github.com/run-llama/llama_index/discussions/14605",2024-08-27 17:20:42,2024-08-27 17:24:43,6132,https://stackoverflow.com/questions/78920095/cannot-import-name-split-torch-state-dict-into-shards-from-huggingface-hub,"cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39; I've been using LLAMA 2 for research for a few months now and I import as follows: from transformers import AutoModelForCausalLM, AutoTokenizer device = torch.device(""cuda"") tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"") model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=True) It has always worked. However, today it is showing the following error: RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): Failed to import transformers.generation.utils because of the following error (look up to see its traceback): cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/ init .py) Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.",import name   39  splittorchstatedictintoshard   39    39  huggingfacehub   39   ve use llama 2 research month import follow  transformer import automodelforcausallm  autotokenizer device  torchdevice    cuda   tokenizer  autotokenizerfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   model  automodelforcausallmfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   loadin4bit  true  always work  however  today show follow error  runtimeerror  fail import transformersmodelsllamamodelingllama follow error  look see traceback   fail import transformersgenerationutil follow error  look see traceback   import name  splittorchstatedictintoshards   huggingfacehub   opt  conda  lib  python310  site  package  huggingfacehub init py  recreate hugging face token  not work  use google colab kaggle notebook ,error be encounter due splittorchstatedictintoshards function available huggingface  hub version  0230  function include start version 0230  resolve issue  update huggingface  hub library version 0230 later also please install accelerate  pip install accelerate0310 git link    githubcom  run  llama  llamaindex  discussions14605,import name   39  splittorchstatedictintoshard   39    39  huggingfacehub   39   ve use llama 2 research month import follow  transformer import automodelforcausallm  autotokenizer device  torchdevice    cuda   tokenizer  autotokenizerfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   model  automodelforcausallmfrompretraine    meta  llama  llama2  7b  chat  hf   token    tokenkey   torchdtype  auto   loadin4bit  true  always work  however  today show follow error  runtimeerror  fail import transformersmodelsllamamodelingllama follow error  look see traceback   fail import transformersgenerationutil follow error  look see traceback   import name  splittorchstatedictintoshards   huggingfacehub   opt  conda  lib  python310  site  package  huggingfacehub init py  recreate hugging face token  not work  use google colab kaggle notebook  error be encounter due splittorchstatedictintoshards function available huggingface  hub version  0230  function include start version 0230  resolve issue  update huggingface  hub library version 0230 later also please install accelerate  pip install accelerate0310 git link    githubcom  run  llama  llamaindex  discussions14605,Implementation Issues
How to Process Data on GPU Instead of RAM for This Python Code?,"I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance. my code : def prepare_dataset(batch): audio = batch[""audio""] batch[""input_features""] = feature_extractor( audio[""array""], sampling_rate=audio[""sampling_rate""] ).input_features[0] batch[""labels""] = tokenizer(batch[""sentence""]).input_ids return batch common_voice = common_voice.map( prepare_dataset, remove_columns=common_voice.column_names[""train""], num_proc=1 ) How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!","['nlp', 'gpu', 'torch', 'openai-whisper']",1,"you can using the following code to process audio data on GPU import torch device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") print(device) def prepare_dataset(batch): audio = batch[""audio""] input_features = feature_extractor(audio[""array""], sampling_rate=audio[""sampling_rate""]).input_features[0] batch[""input_features""] = torch.tensor(input_features).to(device) labels = tokenizer(batch[""sentence""]).input_ids batch[""labels""] = torch.tensor(labels).to(device) return batch common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[""train""])",2024-08-27 08:03:28,2024-08-27 12:21:29,64,https://stackoverflow.com/questions/78917743/how-to-process-data-on-gpu-instead-of-ram-for-this-python-code,"How to Process Data on GPU Instead of RAM for This Python Code? I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance. my code : def prepare_dataset(batch): audio = batch[""audio""] batch[""input_features""] = feature_extractor( audio[""array""], sampling_rate=audio[""sampling_rate""] ).input_features[0] batch[""labels""] = tokenizer(batch[""sentence""]).input_ids return batch common_voice = common_voice.map( prepare_dataset, remove_columns=common_voice.column_names[""train""], num_proc=1 ) How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!",process data gpu instead ram python code   m currently use follow code process audio datum  run ram  want offload processing gpu improve performance  code  def preparedataset  batch   audio  batch    audio   batch    inputfeature    featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    label    tokenizer  batch    sentence    inputids return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train    numproc1  modify code utilize gpu processing instead ram  guidance specific change much appreciate ,use follow code process audio datum gpu import torch device  torchdevice    cuda  torchcudaisavailable   else   cpu   print  device  def preparedataset  batch   audio  batch    audio   inputfeature  featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    inputfeature    torchtensor  inputfeature  to  device  label  tokenizer  batch    sentence    inputids batch    label    torchtensor  label  to  device  return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train   ,process data gpu instead ram python code   m currently use follow code process audio datum  run ram  want offload processing gpu improve performance  code  def preparedataset  batch   audio  batch    audio   batch    inputfeature    featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    label    tokenizer  batch    sentence    inputids return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train    numproc1  modify code utilize gpu processing instead ram  guidance specific change much appreciate  use follow code process audio datum gpu import torch device  torchdevice    cuda  torchcudaisavailable   else   cpu   print  device  def preparedataset  batch   audio  batch    audio   inputfeature  featureextractor  audio    array    samplingrate  audio    samplingrate    inputfeature  0  batch    inputfeature    torchtensor  inputfeature  to  device  label  tokenizer  batch    sentence    inputids batch    label    torchtensor  label  to  device  return batch commonvoice  commonvoicemap  preparedataset  removecolumn  commonvoicecolumnname    train   ,Basic Understanding
How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation,"I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this. What I’ve Tried: Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me. Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation: # VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET) from transformers import MarianMTModel, MarianTokenizer import torch import matplotlib.pyplot as plt from torch.nn import functional as F model_name = ""Helsinki-NLP/opus-mt-en-de"" tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) model.eval() keys = {} queries = {} def get_key(layer): def hook(module, input, output): key, = input keys[layer] = key return hook def get_query(layer): def hook(module, input, output): query, = input queries[layer] = query return hook def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() hooks = [] for i, layer in enumerate(model.model.decoder.layers): hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i))) hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i))) input_text = ""Please translate this to German."" inputs = tokenizer(input_text, return_tensors=""pt"") translated_tokens = model.generate(**inputs, use_cache=False) translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True) input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0]) output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0]) attentions = [] for layer in range(len(keys)): K, Q = keys[layer], queries[layer] M = Q @ K.transpose(-2, -1) attentions.append(F.softmax(M, dim=-1)) attentions = torch.stack(attentions, dim=0) print(""layers, heads, output tokens, input tokens"") print(attentions.shape) plt.figure(figsize=(10, 8)) plt.imshow(attentions[0, 0], cmap='viridis') plt.colorbar() plt.xticks(range(len(input_tokens)), input_tokens, rotation=90) plt.yticks(range(len(output_tokens)), output_tokens) plt.xlabel(""Input Tokens"") plt.ylabel(""Output Tokens"") plt.title(""Cross-Attention Matrix"") plt.show() This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation. My Question Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8? I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.","['python', 'pytorch', 'nlp', 'huggingface-transformers']",1,"Huggingface has built in methods to return attention weights translated_tokens = model.generate(**inputs, output_attentions=True, return_dict_in_generate=True ) print(translated_tokens.keys()) > odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values']) With return_dict_in_generate=True , model.generate returns a dict-like object. With output_attentions=True , the output dict will contain all attention weights. For this model, it will include encoder attentions, decoder attentions and cross attentions.",2024-08-25 20:13:54,2024-08-26 16:38:28,402,https://stackoverflow.com/questions/78912171/how-to-visualize-cross-attention-matrices-in-marianmtmodel-during-output-generat,"How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this. What I’ve Tried: Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me. Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation: # VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET) from transformers import MarianMTModel, MarianTokenizer import torch import matplotlib.pyplot as plt from torch.nn import functional as F model_name = ""Helsinki-NLP/opus-mt-en-de"" tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) model.eval() keys = {} queries = {} def get_key(layer): def hook(module, input, output): key, = input keys[layer] = key return hook def get_query(layer): def hook(module, input, output): query, = input queries[layer] = query return hook def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() hooks = [] for i, layer in enumerate(model.model.decoder.layers): hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i))) hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i))) input_text = ""Please translate this to German."" inputs = tokenizer(input_text, return_tensors=""pt"") translated_tokens = model.generate(**inputs, use_cache=False) translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True) input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0]) output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0]) attentions = [] for layer in range(len(keys)): K, Q = keys[layer], queries[layer] M = Q @ K.transpose(-2, -1) attentions.append(F.softmax(M, dim=-1)) attentions = torch.stack(attentions, dim=0) print(""layers, heads, output tokens, input tokens"") print(attentions.shape) plt.figure(figsize=(10, 8)) plt.imshow(attentions[0, 0], cmap='viridis') plt.colorbar() plt.xticks(range(len(input_tokens)), input_tokens, rotation=90) plt.yticks(range(len(output_tokens)), output_tokens) plt.xlabel(""Input Tokens"") plt.ylabel(""Output Tokens"") plt.title(""Cross-Attention Matrix"") plt.show() This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation. My Question Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8? I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.",visualize cross  attention matrices marianmtmodel output generation work machine translation task use marianmtmodel hugging face transformer library  specifically  want visualize cross  attention matrix model s translation process  however  encounter difficulty achieve   try  initial attempt  notice cross  attention matrix directly return model generate translation  example find involved feeding source text translation model  however  goal access cross  attention matrix model generate output  translation give  use forward hooks  achieve  implement forward hook key query projection attention mechanism  disable key  value cache  usecache  false  capture full matrix last step   implementation   visualize cros attention translation task  working yet  transformer import marianmtmodel  mariantokenizer import torch import matplotlibpyplot plt torchnn import functional f modelname    helsinki  nlp  opus  mt  en  de  tokenizer  mariantokenizerfrompretraine  modelname  model  marianmtmodelfrompretrained  modelname  modeleval   key    query    def getkey  layer   def hook  module  input  output   key   input key  layer   key return hook def getquery  layer   def hook  module  input  output   query   input query  layer   query return hook def  shape  self  tensor  torch  tensor  seqlen  int  bsz  int   return tensorview  bsz  seqlen  selfnumhead  selfheaddim  transpose  1  2  contiguous   hook     layer enumerate  modelmodeldecoderlayer   hooksappend  layerencoderattnkprojregisterforwardhook  getkey     hooksappend  layerencoderattnqprojregisterforwardhook  getquery     inputtext    please translate german   input  tokenizer  inputtext  returntensors  pt   translatedtoken  modelgenerate    input  usecache  false  translatedtext  tokenizerdecode  translatedtokens  0   skipspecialtoken  true  inputtokens  tokenizerconvertidstotoken  input    inputids    0   outputtoken  tokenizerconvertidstotoken  translatedtokens  0   attention    layer range  len  key    k  q  key  layer   query  layer   q  ktranspose  2  1  attentionsappend  fsoftmax   dim1   attention  torchstack  attention  dim0  print    layer  head  output token  input token   print  attentionsshape  pltfigure  figsize  10  8   pltimshow  attention  0  0   cmapviridis   pltcolorbar   pltxtick  range  len  inputtokens    inputtoken  rotation90  pltytick  range  len  outputtokens    outputtokens  pltxlabel    input tokens   pltylabel    output tokens   plttitle    cross  attention matrix   pltshow   approach seem work capture cross  attention matrix  however  observe matrix 4 attention head instead expect 8  make question correctness implementation  question give issue  encounter  reliable method extract visualize cross  attention matrix translation process  additionally  current approach fundamentally okay  resolve issue capture 4 attention head instead 8  suspect issue might relate  m currently reshape key  k  query  q  tensor head dimension multiplication  want ask advice case  easy effective way ,huggingface build method return attention weight translatedtoken  modelgenerate    input  outputattention  true  returndictingenerate  true  print  translatedtokenskeys     odictkey    sequence    encoderattention    decoderattention    crossattention    pastkeyvalue    returndictingenerate  true  modelgenerate return dict  like object  outputattention  true  output dict contain attention weight  model  include encoder attention  decoder attention cross attention ,visualize cross  attention matrices marianmtmodel output generation work machine translation task use marianmtmodel hugging face transformer library  specifically  want visualize cross  attention matrix model s translation process  however  encounter difficulty achieve   try  initial attempt  notice cross  attention matrix directly return model generate translation  example find involved feeding source text translation model  however  goal access cross  attention matrix model generate output  translation give  use forward hooks  achieve  implement forward hook key query projection attention mechanism  disable key  value cache  usecache  false  capture full matrix last step   implementation   visualize cros attention translation task  working yet  transformer import marianmtmodel  mariantokenizer import torch import matplotlibpyplot plt torchnn import functional f modelname    helsinki  nlp  opus  mt  en  de  tokenizer  mariantokenizerfrompretraine  modelname  model  marianmtmodelfrompretrained  modelname  modeleval   key    query    def getkey  layer   def hook  module  input  output   key   input key  layer   key return hook def getquery  layer   def hook  module  input  output   query   input query  layer   query return hook def  shape  self  tensor  torch  tensor  seqlen  int  bsz  int   return tensorview  bsz  seqlen  selfnumhead  selfheaddim  transpose  1  2  contiguous   hook     layer enumerate  modelmodeldecoderlayer   hooksappend  layerencoderattnkprojregisterforwardhook  getkey     hooksappend  layerencoderattnqprojregisterforwardhook  getquery     inputtext    please translate german   input  tokenizer  inputtext  returntensors  pt   translatedtoken  modelgenerate    input  usecache  false  translatedtext  tokenizerdecode  translatedtokens  0   skipspecialtoken  true  inputtokens  tokenizerconvertidstotoken  input    inputids    0   outputtoken  tokenizerconvertidstotoken  translatedtokens  0   attention    layer range  len  key    k  q  key  layer   query  layer   q  ktranspose  2  1  attentionsappend  fsoftmax   dim1   attention  torchstack  attention  dim0  print    layer  head  output token  input token   print  attentionsshape  pltfigure  figsize  10  8   pltimshow  attention  0  0   cmapviridis   pltcolorbar   pltxtick  range  len  inputtokens    inputtoken  rotation90  pltytick  range  len  outputtokens    outputtokens  pltxlabel    input tokens   pltylabel    output tokens   plttitle    cross  attention matrix   pltshow   approach seem work capture cross  attention matrix  however  observe matrix 4 attention head instead expect 8  make question correctness implementation  question give issue  encounter  reliable method extract visualize cross  attention matrix translation process  additionally  current approach fundamentally okay  resolve issue capture 4 attention head instead 8  suspect issue might relate  m currently reshape key  k  query  q  tensor head dimension multiplication  want ask advice case  easy effective way  huggingface build method return attention weight translatedtoken  modelgenerate    input  outputattention  true  returndictingenerate  true  print  translatedtokenskeys     odictkey    sequence    encoderattention    decoderattention    crossattention    pastkeyvalue    returndictingenerate  true  modelgenerate return dict  like object  outputattention  true  output dict contain attention weight  model  include encoder attention  decoder attention cross attention ,Implementation Issues
Why doesn&#39;t permuting positional encodings in BERT affect the output as expected?,"I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another question I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results. What I expected to happen: No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary. Permuting only the input IDs should return distribution B. Permuting only the positional embeddings should return distribution B. Permuting both the input IDs and positional embeddings should return distribution A. What actually happens: Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result. My question is: Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding? For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in masked_prediction . import torch import ipywidgets as widgets from IPython.display import display from transformers import BertForMaskedLM, AutoTokenizer import matplotlib.pyplot as plt import torch.nn.functional as F # surpress renaming warnings logging.getLogger(""transformers.modeling_utils"").setLevel(logging.ERROR) warnings.simplefilter(""ignore"", FutureWarning) tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") input_ids = torch.Tensor([[]]) tokens = [] permutation = [] output = widgets.Output() def permute_columns(matrix, permutation=None): n = len(permutation) first_n_columns = matrix[:, :n] permuted_columns = first_n_columns[:, permutation] remaining_columns = matrix[:, n:] new_matrix = torch.hstack((permuted_columns, remaining_columns)) return new_matrix def update_permutation(ordered_tags): global permutation fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]] permutation = [tokens.index(tag) for tag in fixed_tokens] def tokenize(text): global input_ids, tokens input_ids = tokenizer(text, return_tensors=""pt"").input_ids tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]] if len(tokens) > 2: reorderable_tokens = tokens[1:-1] else: reorderable_tokens = [] with output: output.clear_output(wait=True) tags_input.allowed_tags = reorderable_tokens tags_input.value = reorderable_tokens update_permutation(tags_input.value) def on_tags_change(change): if len(change['new']) != len(tags_input.allowed_tags): tags_input.value = tags_input.allowed_tags # Restore original value def masked_prediction(input_ids, permutation, permute_input, permute_encoding): with output: output.clear_output(wait=True) # Clear previous outputs if input_ids.numel() == 0: print(""You can't use an empty sequence for prediction"") return model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") if permute_encoding: model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T if permute_input: input_ids = permute_columns(input_ids, permutation) decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False) with torch.no_grad(): outputs = model(input_ids) logits = outputs.logits top_k = 5 mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1] print(decoded_text, mask_token_indices, permutation) num_masks = len(mask_token_indices) if num_masks == 0: print(""You need to include a [MASK] token for prediction"") return fig, axs = plt.subplots(1, num_masks, figsize=(15, 6)) if num_masks == 1: axs = [axs] for i, idx in enumerate(mask_token_indices): mask_token_logits = logits[0, idx, :] softmax_probs = F.softmax(mask_token_logits, dim=0) top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0) predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids] predicted_confidences = top_token_probs.tolist() axs[i].bar(predicted_tokens, predicted_confidences, color='blue') axs[i].set_xlabel('Predicted Tokens') axs[i].set_ylabel('Confidence') axs[i].set_title(f'Masked Token at Position {idx.item()}') axs[i].set_ylim(0, 1) plt.show() def on_predict_button_click(b): masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value) text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:') text_input.observe(lambda change: tokenize(change['new']), names='value') tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False) # Observe changes in tags order to update the permutation and prevent deletion tags_input.observe(on_tags_change, names='value') tags_input.observe(lambda change: update_permutation(change['new']), names='value') # Create checkboxes for permute_input and permute_encoding permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs') permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings') # Create a button to trigger the prediction predict_button = widgets.Button(description=""Run Prediction"") predict_button.on_click(on_predict_button_click) # Display the widgets display(text_input) display(tags_input) display(permute_input_checkbox) display(permute_encoding_checkbox) display(predict_button) display(output)","['python', 'pytorch', 'nlp', 'huggingface-transformers']",1,"The model inputs have token ids and position ids. There are four scenarios to consider: Baseline. Correct order for tokens and positions Permute position ids only Permute token ids only Permute position ids and token ids You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider: # Given: tokens = [0, 1, 2] positions = [0, 1, 2] permutation = [2, 0, 1] # Ex1: Permute tokens but not positions [2, 0, 1] # permuted tokens [0, 1, 2] # standard positions # Ex2: Permute positions but not tokens [0, 1, 2] # standard tokens [2, 0, 1] # permuted positions In Ex1 , the model is told that token 2 occurs at position 0 . In Ex2 , the model is told that token 2 occurs at position 1 . Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs. The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results. It is simple to test this. Huggingface models take a position_ids input parameter. We can use this to test permutations of the input ids without messing with the weight matrices. To test this, we'll create input data, permute as needed, compute logits and compare logits. When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token i in scenario 1 is permuted to token j in scenario 3, we want to compare logits i from scenario 1 to logits j in scenario 3. import torch from transformers import BertForMaskedLM, AutoTokenizer def get_logits(inputs): with torch.no_grad(): outputs = model(**inputs) logits = outputs.logits return logits def permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True): outputs = {} for k,v in inputs.items(): if k=='position_ids' and permute_positions: outputs[k] = v[permutation] elif k!='position_ids' and permute_ids: outputs[k] = v[:,permutation] else: outputs[k] = v return outputs # load tokenizer/model tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") model.eval() # remember to set model to eval # create input ids and position ids inputs = tokenizer('input text test sequence', return_tensors='pt') inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1]))) # create permutation tensor permutation = torch.randperm(inputs['input_ids'].shape[1]) # compute scenario data data = { 's1' : { # scenario 1 - baseline 'inputs' : inputs, 'permuted_ids' : False }, 's2' : { # scenario 2 - permute positions only 'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True), 'permuted_ids' : False }, 's3' : { # scenario 3 - permute token ids only 'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False), 'permuted_ids' : True }, 's4' : { # scenario 4 - permute tokens and positions 'inputs' : permute_inputs(inputs, permutation), 'permuted_ids' : True } } # compute logits for k,v in data.items(): v['logits'] = get_logits(v['inputs']) comparisons = [ ['s1', 's2'], ['s1', 's3'], ['s1', 's4'], ['s2', 's3'], ['s2', 's4'], ['s3', 's4'], ] # compare scenarios for sa, sb in comparisons: data_a = data[sa] data_b = data[sb] logits_a = data_a['logits'] logits_b = data_b['logits'] if data_a['permuted_ids'] == data_b['permuted_ids']: # either both logits are permuted or both logits are unpermuted # so we can compare directly val = (logits_a - logits_b).abs().mean() elif data_a['permuted_ids'] and (not data_b['permuted_ids']): # if `a` is permuted but `b` is not, we permute `b` to make tokens line up val = (logits_a - logits_b[:,permutation]).abs().mean() else: # otherwise we permute `b` to make tokens line up val = (logits_a[:,permutation] - logits_b).abs().mean() print(f""Comparison {sa}, {sb}: {val.item():.6f}"") The code should produce an output like: Comparison s1, s2: 1.407895 Comparison s1, s3: 1.583560 Comparison s1, s4: 0.000003 Comparison s2, s3: 1.750883 Comparison s2, s4: 1.407894 Comparison s3, s4: 1.583560 Run the code a bunch of times. You will find that the S1, S4 comparison always has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues. You will find the S2, S3 comparison generally has a large deviation, but sometimes has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.",2024-08-23 11:12:08,2024-08-23 17:03:05,80,https://stackoverflow.com/questions/78905614/why-doesnt-permuting-positional-encodings-in-bert-affect-the-output-as-expected,"Why doesn&#39;t permuting positional encodings in BERT affect the output as expected? I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another question I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results. What I expected to happen: No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary. Permuting only the input IDs should return distribution B. Permuting only the positional embeddings should return distribution B. Permuting both the input IDs and positional embeddings should return distribution A. What actually happens: Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result. My question is: Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding? For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in masked_prediction . import torch import ipywidgets as widgets from IPython.display import display from transformers import BertForMaskedLM, AutoTokenizer import matplotlib.pyplot as plt import torch.nn.functional as F # surpress renaming warnings logging.getLogger(""transformers.modeling_utils"").setLevel(logging.ERROR) warnings.simplefilter(""ignore"", FutureWarning) tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") input_ids = torch.Tensor([[]]) tokens = [] permutation = [] output = widgets.Output() def permute_columns(matrix, permutation=None): n = len(permutation) first_n_columns = matrix[:, :n] permuted_columns = first_n_columns[:, permutation] remaining_columns = matrix[:, n:] new_matrix = torch.hstack((permuted_columns, remaining_columns)) return new_matrix def update_permutation(ordered_tags): global permutation fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]] permutation = [tokens.index(tag) for tag in fixed_tokens] def tokenize(text): global input_ids, tokens input_ids = tokenizer(text, return_tensors=""pt"").input_ids tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]] if len(tokens) > 2: reorderable_tokens = tokens[1:-1] else: reorderable_tokens = [] with output: output.clear_output(wait=True) tags_input.allowed_tags = reorderable_tokens tags_input.value = reorderable_tokens update_permutation(tags_input.value) def on_tags_change(change): if len(change['new']) != len(tags_input.allowed_tags): tags_input.value = tags_input.allowed_tags # Restore original value def masked_prediction(input_ids, permutation, permute_input, permute_encoding): with output: output.clear_output(wait=True) # Clear previous outputs if input_ids.numel() == 0: print(""You can't use an empty sequence for prediction"") return model = BertForMaskedLM.from_pretrained(""bert-base-uncased"") if permute_encoding: model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T if permute_input: input_ids = permute_columns(input_ids, permutation) decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False) with torch.no_grad(): outputs = model(input_ids) logits = outputs.logits top_k = 5 mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1] print(decoded_text, mask_token_indices, permutation) num_masks = len(mask_token_indices) if num_masks == 0: print(""You need to include a [MASK] token for prediction"") return fig, axs = plt.subplots(1, num_masks, figsize=(15, 6)) if num_masks == 1: axs = [axs] for i, idx in enumerate(mask_token_indices): mask_token_logits = logits[0, idx, :] softmax_probs = F.softmax(mask_token_logits, dim=0) top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0) predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids] predicted_confidences = top_token_probs.tolist() axs[i].bar(predicted_tokens, predicted_confidences, color='blue') axs[i].set_xlabel('Predicted Tokens') axs[i].set_ylabel('Confidence') axs[i].set_title(f'Masked Token at Position {idx.item()}') axs[i].set_ylim(0, 1) plt.show() def on_predict_button_click(b): masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value) text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:') text_input.observe(lambda change: tokenize(change['new']), names='value') tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False) # Observe changes in tags order to update the permutation and prevent deletion tags_input.observe(on_tags_change, names='value') tags_input.observe(lambda change: update_permutation(change['new']), names='value') # Create checkboxes for permute_input and permute_encoding permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs') permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings') # Create a button to trigger the prediction predict_button = widgets.Button(description=""Run Prediction"") predict_button.on_click(on_predict_button_click) # Display the widgets display(text_input) display(tags_input) display(permute_input_checkbox) display(permute_encoding_checkbox) display(predict_button) display(output)",  39  permute positional encoding bert affect output expect  work jupyter notebook transformers  section positional encoding  want demonstrate transformer rely entirely positional encode understand order sequence  previously learn another question post concept apply model not use mask attention  like gpt2  however  attempt approach bert model  use cross  attention  predict  mask  token  encounter unexpected result  expect happen  permutation cause model predict different token  ie  distribution consistent vocabulary  permute input id return distribution b permuting positional embedding return distribution b permuting input id positional embedding return distribution  actually happen  sometimes result align expectation  time  permute one aspect  either input id positional embedding  lead different outcome  even though occasionally  produce result  question  something else hug face s bert model might influence position  beyond positional encoding  completeness  include full code part notebook  try directly  important part happen maskedprediction  import torch import ipywidget widget ipythondisplay import display transformer import bertformaskedlm  autotokenizer import matplotlibpyplot plt import torchnnfunctional f  surpress rename warning logginggetlogger    transformersmodelingutil   setlevel  log  error  warningssimplefilter    ignore   futurewarning  tokenizer  autotokenizerfrompretraine    bert  base  uncased   inputid  torch  tensor       token    permutation    output  widget  output   def permutecolumn  matrix  permutation  none   n  len  permutation  firstncolumns  matrix     n  permutedcolumn  firstncolumns    permutation  remainingcolumn  matrix    n   newmatrix  torchhstack   permutedcolumn  remainingcolumns   return newmatrix def updatepermutation  orderedtags   global permutation fixedtokens   token  0    orderedtag   token  1   permutation   tokensindex  tag  tag fixedtokens  def tokenize  text   global inputid  token inputids  tokenizer  text  returntensors  pt   inputids token   tokenizerdecode   tokenid   strip   tokenid inputids  0   len  tokens   2  reorderabletokens  token  1  1  else  reorderabletokens    output  outputclearoutput  wait  true  tagsinputallowedtag  reorderabletokens tagsinputvalue  reorderabletokens updatepermutation  tagsinputvalue  def ontagschange  change   len  change   new      len  tagsinputallowedtag   tagsinputvalue  tagsinputallowedtag  restore original value def maskedprediction  inputid  permutation  permuteinput  permuteencoding   output  outputclearoutput  wait  true   clear previous output inputidsnumel     0  print    can not use empty sequence prediction   return model  bertformaskedlmfrompretraine    bert  base  uncased   permuteencoding  modelbertembeddingspositionembeddingsweightdata  permutecolumns  modelbertembeddingspositionembeddingsweight  t  permutation  t permuteinput  inputids  permutecolumns  inputids  permutation  decodedtext  tokenizerdecode  inputid  0   skipspecialtoken  false  torchnograd    output  model  inputids  logit  outputslogit topk  5 masktokenindice  torchwhere  inputid   tokenizermasktokenid   1  print  decodedtext  masktokenindice  permutation  nummask  len  masktokenindice  nummask   0  print    need include  mask  token prediction   return fig  axs  pltsubplot  1  nummask  figsize  15  6   nummask   1  axs   axs   idx enumerate  masktokenindice   masktokenlogit  logit  0  idx    softmaxprob  fsoftmax  masktokenlogits  dim0  toptokenprobs  toptokenid  torchtopk  softmaxprob  topk  dim0  predictedtoken   tokenizerdecode   tokenid   strip   tokenid toptokenids  predictedconfidences  toptokenprobstolist   axs   bar  predictedtokens  predictedconfidences  colorblue   axs   setxlabel   predict tokens   axs   setylabel   confidence   axs   settitle  fmaske token position  idxitem      axs   setylim  0  1  pltshow   def onpredictbuttonclick  b   maskedprediction  inputid  permutation  permuteinputcheckboxvalue  permuteencodingcheckboxvalue  textinput  widget  text  placeholderwrite text encode    descriptioninput    textinputobserve  lambda change  tokenize  change   new     namesvalue   tagsinput  widget  tagsinput  value    allowedtags    allowduplicate  false   observe change tag order update permutation prevent deletion tagsinputobserve  ontagschange  namesvalue   tagsinputobserve  lambda change  updatepermutation  change   new     namesvalue    create checkboxe permuteinput permuteencoding permuteinputcheckbox  widget  checkbox  value  false  descriptionpermute inputs   permuteencodingcheckbox  widget  checkbox  value  false  descriptionpermute encodings    create button trigger prediction predictbutton  widget  button  description  run prediction   predictbuttononclick  onpredictbuttonclick   display widget display  textinput  display  tagsinput  display  permuteinputcheckbox  display  permuteencodingcheckbox  display  predictbutton  display  output ,model input token id position id  four scenario consider  baseline  correct order token position permute position id permute token id permute position id token ids correct scenario 1 4 produce result  however incorrect assuming permute tokens position separately give result  consider   give  token   0  1  2  position   0  1  2  permutation   2  0  1   ex1  permute token position  2  0  1   permute token  0  1  2   standard position  ex2  permute position token  0  1  2   standard token  2  0  1   permute position ex1  model tell token 2 occur position 0  ex2  model tell token 2 occur position 1  even though use permutation  mapping token position different  result different model output  reason sometimes see result line  random chance  sample permutation result token  position embedding line way  mostly way  permute one  luck  average case produce different result  simple test  huggingface model take positionids input parameter  use test permutation input id without mess weight matrix  test  will create input datum  permute need  compute logit compare logit  compare logit  permute depermute need compare token token basis  example token scenario 1 permute token j scenario 3  want compare logits scenario 1 logit j scenario 3  import torch transformer import bertformaskedlm  autotokenizer def getlogit  input   torchnograd    output  model    input  logit  outputslogit return logit def permuteinput  input  permutation  permuteid  true  permuteposition  true   output    k  v inputsitem    kpositionid  permuteposition  output  k   v  permutation  elif k    positionid  permuteid  output  k   v    permutation  else  output  k   v return output  load tokenizer  model tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  bertformaskedlmfrompretraine    bert  base  uncased   modeleval    remember set model eval  create input ids position ids input  tokenizer   input text test sequence   returntensorspt   input   positionid    torchtensor  list  range  input   inputids   shape  1      create permutation tensor permutation  torchrandperm  input   inputids   shape  1    compute scenario data datum    s1     scenario 1  baseline  input   input   permutedid   false    s2     scenario 2  permute position  input   permuteinput  input  permutation  permuteid  false  permuteposition  true    permutedid   false    s3     scenario 3  permute token id  input   permuteinput  input  permutation  permuteid  true  permuteposition  false    permutedid   true    s4     scenario 4  permute token position  input   permuteinput  input  permutation    permutedid   true    compute logits k  v dataitem    v   logit    getlogit  v   input    comparison     s1    s2      s1    s3      s1    s4      s2    s3      s2    s4      s3    s4      compare scenario sa  sb comparison  dataa  datum  sa  datab  datum  sb  logitsa  dataa   logit   logitsb  datab   logit   dataa   permutedid     datab   permutedid     either logit permute logit unpermuted  compare directly val   logitsa  logitsb  abs   mean   elif dataa   permutedid    datab   permutedid        permuted  b   permute  b  make token line val   logitsa  logitsb    permutation   abs   mean   else   otherwise permute  b  make token line val   logitsa    permutation   logitsb  abs   mean   print  f  comparison  sa    sb    valitem    6f    code produce output like  comparison s1  s2  1407895 comparison s1  s3  1583560 comparison s1  s4  0000003 comparison s2  s3  1750883 comparison s2  s4  1407894 comparison s3  s4  1583560 run code bunch time  find s1  s4 comparison always small deviation  permute token position together always produce result  ignore small deviation cause numeric issue  find s2  s3 comparison generally large deviation  sometimes small deviation  discuss  due get lucky permutation position id mostly line ,  39  permute positional encoding bert affect output expect  work jupyter notebook transformers  section positional encoding  want demonstrate transformer rely entirely positional encode understand order sequence  previously learn another question post concept apply model not use mask attention  like gpt2  however  attempt approach bert model  use cross  attention  predict  mask  token  encounter unexpected result  expect happen  permutation cause model predict different token  ie  distribution consistent vocabulary  permute input id return distribution b permuting positional embedding return distribution b permuting input id positional embedding return distribution  actually happen  sometimes result align expectation  time  permute one aspect  either input id positional embedding  lead different outcome  even though occasionally  produce result  question  something else hug face s bert model might influence position  beyond positional encoding  completeness  include full code part notebook  try directly  important part happen maskedprediction  import torch import ipywidget widget ipythondisplay import display transformer import bertformaskedlm  autotokenizer import matplotlibpyplot plt import torchnnfunctional f  surpress rename warning logginggetlogger    transformersmodelingutil   setlevel  log  error  warningssimplefilter    ignore   futurewarning  tokenizer  autotokenizerfrompretraine    bert  base  uncased   inputid  torch  tensor       token    permutation    output  widget  output   def permutecolumn  matrix  permutation  none   n  len  permutation  firstncolumns  matrix     n  permutedcolumn  firstncolumns    permutation  remainingcolumn  matrix    n   newmatrix  torchhstack   permutedcolumn  remainingcolumns   return newmatrix def updatepermutation  orderedtags   global permutation fixedtokens   token  0    orderedtag   token  1   permutation   tokensindex  tag  tag fixedtokens  def tokenize  text   global inputid  token inputids  tokenizer  text  returntensors  pt   inputids token   tokenizerdecode   tokenid   strip   tokenid inputids  0   len  tokens   2  reorderabletokens  token  1  1  else  reorderabletokens    output  outputclearoutput  wait  true  tagsinputallowedtag  reorderabletokens tagsinputvalue  reorderabletokens updatepermutation  tagsinputvalue  def ontagschange  change   len  change   new      len  tagsinputallowedtag   tagsinputvalue  tagsinputallowedtag  restore original value def maskedprediction  inputid  permutation  permuteinput  permuteencoding   output  outputclearoutput  wait  true   clear previous output inputidsnumel     0  print    can not use empty sequence prediction   return model  bertformaskedlmfrompretraine    bert  base  uncased   permuteencoding  modelbertembeddingspositionembeddingsweightdata  permutecolumns  modelbertembeddingspositionembeddingsweight  t  permutation  t permuteinput  inputids  permutecolumns  inputids  permutation  decodedtext  tokenizerdecode  inputid  0   skipspecialtoken  false  torchnograd    output  model  inputids  logit  outputslogit topk  5 masktokenindice  torchwhere  inputid   tokenizermasktokenid   1  print  decodedtext  masktokenindice  permutation  nummask  len  masktokenindice  nummask   0  print    need include  mask  token prediction   return fig  axs  pltsubplot  1  nummask  figsize  15  6   nummask   1  axs   axs   idx enumerate  masktokenindice   masktokenlogit  logit  0  idx    softmaxprob  fsoftmax  masktokenlogits  dim0  toptokenprobs  toptokenid  torchtopk  softmaxprob  topk  dim0  predictedtoken   tokenizerdecode   tokenid   strip   tokenid toptokenids  predictedconfidences  toptokenprobstolist   axs   bar  predictedtokens  predictedconfidences  colorblue   axs   setxlabel   predict tokens   axs   setylabel   confidence   axs   settitle  fmaske token position  idxitem      axs   setylim  0  1  pltshow   def onpredictbuttonclick  b   maskedprediction  inputid  permutation  permuteinputcheckboxvalue  permuteencodingcheckboxvalue  textinput  widget  text  placeholderwrite text encode    descriptioninput    textinputobserve  lambda change  tokenize  change   new     namesvalue   tagsinput  widget  tagsinput  value    allowedtags    allowduplicate  false   observe change tag order update permutation prevent deletion tagsinputobserve  ontagschange  namesvalue   tagsinputobserve  lambda change  updatepermutation  change   new     namesvalue    create checkboxe permuteinput permuteencoding permuteinputcheckbox  widget  checkbox  value  false  descriptionpermute inputs   permuteencodingcheckbox  widget  checkbox  value  false  descriptionpermute encodings    create button trigger prediction predictbutton  widget  button  description  run prediction   predictbuttononclick  onpredictbuttonclick   display widget display  textinput  display  tagsinput  display  permuteinputcheckbox  display  permuteencodingcheckbox  display  predictbutton  display  output  model input token id position id  four scenario consider  baseline  correct order token position permute position id permute token id permute position id token ids correct scenario 1 4 produce result  however incorrect assuming permute tokens position separately give result  consider   give  token   0  1  2  position   0  1  2  permutation   2  0  1   ex1  permute token position  2  0  1   permute token  0  1  2   standard position  ex2  permute position token  0  1  2   standard token  2  0  1   permute position ex1  model tell token 2 occur position 0  ex2  model tell token 2 occur position 1  even though use permutation  mapping token position different  result different model output  reason sometimes see result line  random chance  sample permutation result token  position embedding line way  mostly way  permute one  luck  average case produce different result  simple test  huggingface model take positionids input parameter  use test permutation input id without mess weight matrix  test  will create input datum  permute need  compute logit compare logit  compare logit  permute depermute need compare token token basis  example token scenario 1 permute token j scenario 3  want compare logits scenario 1 logit j scenario 3  import torch transformer import bertformaskedlm  autotokenizer def getlogit  input   torchnograd    output  model    input  logit  outputslogit return logit def permuteinput  input  permutation  permuteid  true  permuteposition  true   output    k  v inputsitem    kpositionid  permuteposition  output  k   v  permutation  elif k    positionid  permuteid  output  k   v    permutation  else  output  k   v return output  load tokenizer  model tokenizer  autotokenizerfrompretraine    bert  base  uncased   model  bertformaskedlmfrompretraine    bert  base  uncased   modeleval    remember set model eval  create input ids position ids input  tokenizer   input text test sequence   returntensorspt   input   positionid    torchtensor  list  range  input   inputids   shape  1      create permutation tensor permutation  torchrandperm  input   inputids   shape  1    compute scenario data datum    s1     scenario 1  baseline  input   input   permutedid   false    s2     scenario 2  permute position  input   permuteinput  input  permutation  permuteid  false  permuteposition  true    permutedid   false    s3     scenario 3  permute token id  input   permuteinput  input  permutation  permuteid  true  permuteposition  false    permutedid   true    s4     scenario 4  permute token position  input   permuteinput  input  permutation    permutedid   true    compute logits k  v dataitem    v   logit    getlogit  v   input    comparison     s1    s2      s1    s3      s1    s4      s2    s3      s2    s4      s3    s4      compare scenario sa  sb comparison  dataa  datum  sa  datab  datum  sb  logitsa  dataa   logit   logitsb  datab   logit   dataa   permutedid     datab   permutedid     either logit permute logit unpermuted  compare directly val   logitsa  logitsb  abs   mean   elif dataa   permutedid    datab   permutedid        permuted  b   permute  b  make token line val   logitsa  logitsb    permutation   abs   mean   else   otherwise permute  b  make token line val   logitsa    permutation   logitsb  abs   mean   print  f  comparison  sa    sb    valitem    6f    code produce output like  comparison s1  s2  1407895 comparison s1  s3  1583560 comparison s1  s4  0000003 comparison s2  s3  1750883 comparison s2  s4  1407894 comparison s3  s4  1583560 run code bunch time  find s1  s4 comparison always small deviation  permute token position together always produce result  ignore small deviation cause numeric issue  find s2  s3 comparison generally large deviation  sometimes small deviation  discuss  due get lucky permutation position id mostly line ,Implementation Issues
How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?,"I'm working with the OpenAIEmbeddings() class from OpenAI , which uses the text-embedding-3-small model. According to the documentation , it generates a 1536-dimensional vector for any input text. However, I'm a bit confused about how this works: Is the 1536-dimensional vector generated for the entire input text? If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? I was expecting this: If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536. But the output is a single vector of size 1536 for the whole input text. Why I expected this? Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings? I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input. Any insights or examples would be greatly appreciated!","['deep-learning', 'nlp', 'openai-api', 'openaiembeddings']",1,"Everything you described is 100% expected. Q: Is the 1536-dimensional vector generated for the entire input text? A: Yes. Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., ""a""), but it doesn't make sense to calculate an embedding vector out of it since ""a"" doesn't semantically mean anything to us humans. Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you use them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes! This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences). There's an excellent Stack Overflow blog post about this topic you should read (pay attention to the bolded text because this is the best explanation): With RAG, you create text embeddings of the pieces of data that you want to draw from and retrieve. That allows you to place a piece of the source text within the semantic space that LLMs use to create responses. /.../ When it comes to RAG systems, you’ll need to pay special attention to how big the individual pieces of data are. How you divide your data up is called chunking, and it’s more complex than embedding whole documents. /.../ The size of the chunked data is going to make a huge difference in what information comes up in a search. When you embed a piece of data, the whole thing is converted into a vector. Include too much in a chunk and the vector loses the ability to be specific to anything it discusses. Include too little and you lose the context of the data.",2024-08-22 14:09:25,2024-08-22 14:37:28,619,https://stackoverflow.com/questions/78901998/how-does-openaiembeddings-work-is-it-creating-a-single-vector-of-size-1536-fo,"How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus? I'm working with the OpenAIEmbeddings() class from OpenAI , which uses the text-embedding-3-small model. According to the documentation , it generates a 1536-dimensional vector for any input text. However, I'm a bit confused about how this works: Is the 1536-dimensional vector generated for the entire input text? If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs? I was expecting this: If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536. But the output is a single vector of size 1536 for the whole input text. Why I expected this? Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings? I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input. Any insights or examples would be greatly appreciated!",openaiembedding   work  create single vector size 1536 whole text corpus   m work openaiembeddings   class openai  use text  embedding3  small model  accord documentation  generate 1536  dimensional vector input text  however   m bit confused work  1536  dimensional vector generate entire input text  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph  expect  100 word input text  expect openaiembeddings   would output 100 vector  size 1536  output single vector size 1536 whole input text  expect  learn   ve understand embedding like word2vec glove provide vector word corpus  differ approach take openaiembeddings   m try understand whether s way extract embedding individual word use model output always single vector represent whole input  insight example would greatly appreciate ,everything describe 100  expect  q  1536  dimensional vector generate entire input text   yes  q  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph   first  openai embeddings model not handle single word different long text  model  s input  input even single character  eg       not make sense calculate embed vector since    not semantically mean anything we human  second  probably mean question happen similarity search embedding  word  happen use  happen use embedding word  sentence  paragraph  whole text  matter  yes  call chunk  decision chunk text depend use case  good thing probably simply try see  get meaningful result similarity search  mean chunk appropriate  even mean chunk whole text   not get meaningful result similarity search  mean chunk not appropriate  eg  instead chunk paragraph  try chunk sentence   s excellent stack overflow blog post topic read  pay attention bolde text good explanation   rag  create text embedding piece datum want draw retrieve  allow place piece source text within semantic space llm use create response     come rag system   need pay special attention big individual piece datum  divide datum call chunk   complex embed whole document     size chunk datum go make huge difference information come search  embed piece datum  whole thing convert vector  include much chunk vector lose ability specific anything discuss  include little lose context datum ,openaiembedding   work  create single vector size 1536 whole text corpus   m work openaiembeddings   class openai  use text  embedding3  small model  accord documentation  generate 1536  dimensional vector input text  however   m bit confused work  1536  dimensional vector generate entire input text  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph  expect  100 word input text  expect openaiembeddings   would output 100 vector  size 1536  output single vector size 1536 whole input text  expect  learn   ve understand embedding like word2vec glove provide vector word corpus  differ approach take openaiembeddings   m try understand whether s way extract embedding individual word use model output always single vector represent whole input  insight example would greatly appreciate  everything describe 100  expect  q  1536  dimensional vector generate entire input text   yes  q  1536  dimensional vector represent entire input text  model handle individual word versus long text like sentence paragraph   first  openai embeddings model not handle single word different long text  model  s input  input even single character  eg       not make sense calculate embed vector since    not semantically mean anything we human  second  probably mean question happen similarity search embedding  word  happen use  happen use embedding word  sentence  paragraph  whole text  matter  yes  call chunk  decision chunk text depend use case  good thing probably simply try see  get meaningful result similarity search  mean chunk appropriate  even mean chunk whole text   not get meaningful result similarity search  mean chunk not appropriate  eg  instead chunk paragraph  try chunk sentence   s excellent stack overflow blog post topic read  pay attention bolde text good explanation   rag  create text embedding piece datum want draw retrieve  allow place piece source text within semantic space llm use create response     come rag system   need pay special attention big individual piece datum  divide datum call chunk   complex embed whole document     size chunk datum go make huge difference information come search  embed piece datum  whole thing convert vector  include much chunk vector lose ability specific anything discuss  include little lose context datum ,Implementation Issues
"NER versus LLM to extract name, gender, role and company from text","I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons. I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them. Is there another, smaller LLM that might be good at this while using fewer processing resources? Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.) Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass? Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction. Thanks in advance!","['nlp', 'large-language-model', 'named-entity-recognition']",2,"Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. NER is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use NLTK or SpaCy for this matter. My personal choice is SpaCy , however a gender as you defined is not a known named entity. you can see a list of named entities in this doc . I guess what you mean by gender is the possible gender associated with the names of a PERSON mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use gender-guesser package . A possible solution would be like this: import spacy import gender_guesser.detector as gender nlp = spacy.load(""en_core_web_sm"") def extract_info(text): doc = nlp(text) gender_detector = gender.Detector() for ent in doc.ents: if ent.label_ == ""PERSON"": name = ent.text name_gender = gender_detector.get_gender(name) return doc.ents, name_gender Note that en_core_web_sm is the small model available via spaCy, you can use the large model by specifying en_core_web_lg , just make sure that the model is downloaded before running your code. here's how you can download the model: python -m spacy download en_core_web_sm",2024-08-21 07:39:13,2024-08-21 09:13:54,1628,https://stackoverflow.com/questions/78895710/ner-versus-llm-to-extract-name-gender-role-and-company-from-text,"NER versus LLM to extract name, gender, role and company from text I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons. I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them. Is there another, smaller LLM that might be good at this while using fewer processing resources? Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.) Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass? Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction. Thanks in advance!",ner versus llm extract name  gender  role company text need extract name  gender  job title employer  company name newspaper article  running process local hardware  cloud allow  due copyright reason   ve play around llama 31  m finding not get useable result model small 70b parameter  size model run much slowly good hardware throw  another  small llm might good use few processing resource  ner use extract datum  ner  ve look extract name gender   not know extract datum gender showstopper   alternatively  approach take first pass ner  pass name llm together original newspaper article extract datum  get well result  fast single llm pass  answer training model  good model use starting point   m much begin machine learn journey would love point right direction  thank advance ,apart limitation  would not recommend use llms like llamma 31 task  ner one classic task nlp small language model tool incorporate achieve goal  use nltk spacy matter  personal choice spacy  however gender define know name entity  see list name entity doc  guess mean gender possible gender associate name person mention article  python package use lookup gender  however  note ambiguous substantial tolerance error  use gender  guesser package  possible solution would like  import spacy import genderguesserdetector gender nlp  spacyload    encorewebsm   def extractinfo  text   doc  nlp  text  genderdetector  gender  detector   ent docent  entlabel      person   name  enttext namegender  genderdetectorgetgender  name  return docent  namegender note encorewebsm small model available via spacy  use large model specify encoreweblg  make sure model download run code  s download model  python m spacy download encorewebsm,ner versus llm extract name  gender  role company text need extract name  gender  job title employer  company name newspaper article  running process local hardware  cloud allow  due copyright reason   ve play around llama 31  m finding not get useable result model small 70b parameter  size model run much slowly good hardware throw  another  small llm might good use few processing resource  ner use extract datum  ner  ve look extract name gender   not know extract datum gender showstopper   alternatively  approach take first pass ner  pass name llm together original newspaper article extract datum  get well result  fast single llm pass  answer training model  good model use starting point   m much begin machine learn journey would love point right direction  thank advance  apart limitation  would not recommend use llms like llamma 31 task  ner one classic task nlp small language model tool incorporate achieve goal  use nltk spacy matter  personal choice spacy  however gender define know name entity  see list name entity doc  guess mean gender possible gender associate name person mention article  python package use lookup gender  however  note ambiguous substantial tolerance error  use gender  guesser package  possible solution would like  import spacy import genderguesserdetector gender nlp  spacyload    encorewebsm   def extractinfo  text   doc  nlp  text  genderdetector  gender  detector   ent docent  entlabel      person   name  enttext namegender  genderdetectorgetgender  name  return docent  namegender note encorewebsm small model available via spacy  use large model specify encoreweblg  make sure model download run code  s download model  python m spacy download encorewebsm,Implementation Issues
Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?,"In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%). I am curious about the following: When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively? Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact? Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%? Thank you for your insights!","['pytorch', 'nlp', 'huggingface-transformers', 'transformer-model']",1,"Attention is computed on a tensor of shape (batch_size, sequence_length, embedding_dimension) . The compute and memory requirements scale with the size of those dimensions. For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size. With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length 8 and 10 sequences of length 128 . Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of 128 . If you sort by length before batching, you get one batch with sequence length of 8 and another with length 128 . The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128). That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to ""skip over"" padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.",2024-08-19 11:49:06,2024-08-20 02:25:26,544,https://stackoverflow.com/questions/78887743/does-padding-in-a-batch-of-sequences-affect-performance-how-effective-is-the-at,"Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask? In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%). I am curious about the following: When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively? Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact? Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%? Thank you for your insights!",padding batch sequences affect performance  effective attention mask  transformer model  sequence variable length typically pad maximum length batch  however  sequence length vary significantly  batch may contain substantial amount padding  potentially 50    curious following  pytorch compute transformer  pad token impact calculation speed negatively  presence attention mask allow model effectively skip padding token  result minimal performance impact  overall  effective attention mask  sparse attention mask 10  non  zero value  computation effectively reduce approximately 10   thank insight ,attention compute tensor shape  batchsize  sequencelength  embeddingdimension   compute memory requirement scale size dimension  input fix size  percent padding impact performance  minor overhead apply padding mask  ie padding mask save one mask fill operation   x  padding  padding be go see difference  overall compute requirement set tensor size  respect batching sequence  add inefficiency batch together sequence wildly different length  say 10 sequence length 8 10 sequence length 128  pad batch sequence two batch  mix length evenly  get two batch sequence length 128  sort length batching  get one batch sequence length 8 another length 128  first case  two batch sequence length 128  require overall compute compare second case  one batch 8  one 128   say  fix input size  not go see performance change percent padding  way attention operation   skip  pad token  conditional control flow require sort approach not work well way gpu execute operation parallel  effect padding mask assign 0 attention weight padding token ,padding batch sequences affect performance  effective attention mask  transformer model  sequence variable length typically pad maximum length batch  however  sequence length vary significantly  batch may contain substantial amount padding  potentially 50    curious following  pytorch compute transformer  pad token impact calculation speed negatively  presence attention mask allow model effectively skip padding token  result minimal performance impact  overall  effective attention mask  sparse attention mask 10  non  zero value  computation effectively reduce approximately 10   thank insight  attention compute tensor shape  batchsize  sequencelength  embeddingdimension   compute memory requirement scale size dimension  input fix size  percent padding impact performance  minor overhead apply padding mask  ie padding mask save one mask fill operation   x  padding  padding be go see difference  overall compute requirement set tensor size  respect batching sequence  add inefficiency batch together sequence wildly different length  say 10 sequence length 8 10 sequence length 128  pad batch sequence two batch  mix length evenly  get two batch sequence length 128  sort length batching  get one batch sequence length 8 another length 128  first case  two batch sequence length 128  require overall compute compare second case  one batch 8  one 128   say  fix input size  not go see performance change percent padding  way attention operation   skip  pad token  conditional control flow require sort approach not work well way gpu execute operation parallel  effect padding mask assign 0 attention weight padding token ,Implementation Issues
SpaCy Matcher with optional suffix in pattern reports multiple matches on same text,"Using the following Matcher rule: {'label': 'R-1', 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}], 'greedy': 'LONGEST', } on the text: 'MyLabel: Some Value' I get two matches: 'MyLabel' and 'MyLabel:' For me, that was quite surprising - I was expecting a single match on 'MyLabel:'. Adding the new greedy flag didn't make any difference. Is this the intended behavior or is it a bug? How should I determine that the second match really is just a subset of the first match? Will the shorter match always be reported before the longer match? SpaCy version 3.7.5","['nlp', 'spacy', 'matcher']",1,"i will say that the behavior you're observing with the SpaCy Matcher is expected, and it is not a bug. When you use the {'TEXT': ':', 'OP': '?'} pattern, the OP: '?' operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen. Explanation: Pattern : {'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'} . Text : 'MyLabel: Some Value' . So for this pattern, SpaCy will try to match: 'MyLabel' alone (because the colon is optional). 'MyLabel:' (because the colon can be included). Therefore, you will get two matches: 'MyLabel' and 'MyLabel:' . Now to Answer Your Questions: Is this the intended behavior or is it a bug? This is intended behavior. The OP: '?' operator allows the colon to be optionally matched, leading to multiple matches. How should I determine that the second match really is just a subset of the first match? To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below pip show spacy Name: spacy Version: 3.7.5 Summary: Industrial-strength Natural Language Processing (NLP) in Python Home-page: https://spacy.io Author: Explosion Author-email: contact@explosion.ai License: MIT Location: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel Required-by: en-core-web-sm Now Example in code: import spacy from spacy.matcher import Matcher nlp = spacy.load(""en_core_web_sm"") doc = nlp(""MyLabel: Some Value"") matcher = Matcher(nlp.vocab) pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}] matcher.add(""R-1"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: span = doc[start:end] print(f""Match: {span.text}, Start: {start}, End: {end}"") # Now, we Determine if one match is a subset of another matches.sort(key=lambda x: (x[1], -x[2])) # Sort by start index, then by end index descending filtered_matches = [] last_end = -1 for match_id, start, end in matches: if start >= last_end: # This is for Avoiding adding subsets filtered_matches.append((match_id, start, end)) last_end = end for match_id, start, end in filtered_matches: span = doc[start:end] print(f""Filtered Match: {span.text}"") Now, This code will filter out the shorter match and your output will be Match: MyLabel, Start: 0, End: 1 Match: MyLabel:, Start: 0, End: 2 Filtered Match: MyLabel: , you can see MYLabel: with the colon symbol there Now Will the shorter match always be reported before the longer match? I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches. Another Alternative Solution: If you want to ensure that only the longest match is returned, you can change the way you define the pattern: pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}] note that the greedy flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings. Now back to the Summary of what i explained: The behavior you're seeing is by design, due to the optional OP: '?' operator. in addition, you can filter out the shorter match by comparing start and end indices of the matches. furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.",2024-08-13 09:37:23,2024-08-14 12:27:48,37,https://stackoverflow.com/questions/78865486/spacy-matcher-with-optional-suffix-in-pattern-reports-multiple-matches-on-same-t,"SpaCy Matcher with optional suffix in pattern reports multiple matches on same text Using the following Matcher rule: {'label': 'R-1', 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}], 'greedy': 'LONGEST', } on the text: 'MyLabel: Some Value' I get two matches: 'MyLabel' and 'MyLabel:' For me, that was quite surprising - I was expecting a single match on 'MyLabel:'. Adding the new greedy flag didn't make any difference. Is this the intended behavior or is it a bug? How should I determine that the second match really is just a subset of the first match? Will the shorter match always be reported before the longer match? SpaCy version 3.7.5",spacy matcher optional suffix pattern report multiple match text use follow matcher rule    label    r1    pattern      text    mylabel      text        op          greedy    longest    text   mylabel  value  get two match   mylabel   mylabel    quite surprising  expect single match  mylabel    add new greedy flag not make difference  intend behavior bug  determine second match really subset first match  short match always report long match  spacy version 375,say behavior be observe spacy matcher expect  bug  use   text        op       pattern  op     operator mean colon optional  matcher generate shorter long match   ve see  explanation  pattern    text    mylabel      text        op        text   mylabel  value   pattern  spacy try match   mylabel  alone  colon optional    mylabel    colon include   therefore  get two match   mylabel   mylabel    answer questions  intend behavior bug  intend behavior  op     operator allow colon optionally match  lead multiple match  determine second match really subset first match  determine one match subset another  compare start end index match  long match start index different end index  write code even use spacy version 375  see detail pip show spacy name  spacy version  375 summary  industrial  strength natural language processing  nlp  python home  page    spacyio author  explosion author  email  contact  explosionai license  mit location  home  adesoji  download  visis  backend  assessment  adesoji  visisenv  lib  python311  site  package require  catalogue  cymem  jinja2  langcode  murmurhash  numpy  packaging  preshed  pydantic  request  setuptool  spacy  legacy  spacy  logger  srsly  thinc  tqdm  typer  wasabi  weasel required  by  en  core  web  sm example code  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   doc  nlp    mylabel  value   matcher  matcher  nlpvocab  pattern     text    mylabel      text        op        matcheradd    r1    pattern   match  matcher  doc  matchid  start  end match  span  doc  start  end  print  f  match   spantext   start   start   end   end      determine one match subset another matchessort  key  lambda x   x  1   x  2     sort start index  end index descend filteredmatche    lastend  1 matchid  start  end match  start   lastend   avoid add subset filteredmatchesappend   matchid  start  end   lastend  end matchid  start  end filteredmatches  span  doc  start  end  print  f  filtered match   spantext     code filter short match output match  mylabel  start  0  end  1 match  mylabel   start  0  end  2 filter match  mylabel   see mylabel  colon symbol short match always report long match  not think match guarantee report specific order  handle  sort match start end index show code example above  now  sort  filter match subset long match  another alternative solution  want ensure long match return  change way define pattern  pattern     text    mylabel      text        op        greedy    longest    note greedy flag not change behavior match rather influence overlaps handle certain custom setting  back summary explain  behavior be see design  due optional op     operator  addition  filter short match compare start end index match  furthermore  sort match start end index allow keep long  non  overlapping match ,spacy matcher optional suffix pattern report multiple match text use follow matcher rule    label    r1    pattern      text    mylabel      text        op          greedy    longest    text   mylabel  value  get two match   mylabel   mylabel    quite surprising  expect single match  mylabel    add new greedy flag not make difference  intend behavior bug  determine second match really subset first match  short match always report long match  spacy version 375 say behavior be observe spacy matcher expect  bug  use   text        op       pattern  op     operator mean colon optional  matcher generate shorter long match   ve see  explanation  pattern    text    mylabel      text        op        text   mylabel  value   pattern  spacy try match   mylabel  alone  colon optional    mylabel    colon include   therefore  get two match   mylabel   mylabel    answer questions  intend behavior bug  intend behavior  op     operator allow colon optionally match  lead multiple match  determine second match really subset first match  determine one match subset another  compare start end index match  long match start index different end index  write code even use spacy version 375  see detail pip show spacy name  spacy version  375 summary  industrial  strength natural language processing  nlp  python home  page    spacyio author  explosion author  email  contact  explosionai license  mit location  home  adesoji  download  visis  backend  assessment  adesoji  visisenv  lib  python311  site  package require  catalogue  cymem  jinja2  langcode  murmurhash  numpy  packaging  preshed  pydantic  request  setuptool  spacy  legacy  spacy  logger  srsly  thinc  tqdm  typer  wasabi  weasel required  by  en  core  web  sm example code  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   doc  nlp    mylabel  value   matcher  matcher  nlpvocab  pattern     text    mylabel      text        op        matcheradd    r1    pattern   match  matcher  doc  matchid  start  end match  span  doc  start  end  print  f  match   spantext   start   start   end   end      determine one match subset another matchessort  key  lambda x   x  1   x  2     sort start index  end index descend filteredmatche    lastend  1 matchid  start  end match  start   lastend   avoid add subset filteredmatchesappend   matchid  start  end   lastend  end matchid  start  end filteredmatches  span  doc  start  end  print  f  filtered match   spantext     code filter short match output match  mylabel  start  0  end  1 match  mylabel   start  0  end  2 filter match  mylabel   see mylabel  colon symbol short match always report long match  not think match guarantee report specific order  handle  sort match start end index show code example above  now  sort  filter match subset long match  another alternative solution  want ensure long match return  change way define pattern  pattern     text    mylabel      text        op        greedy    longest    note greedy flag not change behavior match rather influence overlaps handle certain custom setting  back summary explain  behavior be see design  due optional op     operator  addition  filter short match compare start end index match  furthermore  sort match start end index allow keep long  non  overlapping match ,Implementation Issues
`mlflow.transformers.log_model()` does not finish,"Problem I want to use mlflow.transformers.log_model() to log a finetuned huggingface model. However, when the mlflow.transformers.log_model method is running, it simply does not finish - runs forever - throws no errors. I suspect my configuration is not right, the model is too big? The output says Skipping saving pretrained model weights to disk so that should not be the problem. Any ideas how to do this properly? Example This is more or less how my setup looks like, you cannot run this, it includes some pseudocode... I am on python 3.11.9 with transformers = ""^4.41.2"" & mlflow = ""^2.15.1"" . import mlflow import torch from peft import LoraConfig from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, ) from trl import SFTTrainer, setup_chat_format train_dataset = ... eval_dataset = ... model_id = ""LeoLM/leo-hessianai-7b-chat-bilingual"" # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( model_id, device_map=""auto"", torch_dtype=torch.bfloat16, quantization_config=bnb_config, ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True) model, tokenizer = setup_chat_format(model, tokenizer) peft_config = LoraConfig(...) args = TrainingArguments(...) # Define Trainer trainer = SFTTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, peft_config=peft_config, tokenizer=tokenizer, packing=True, ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! This function all does not finish... !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", ) The last output I get in the console is: INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead. Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'} /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning. warnings.warn( 2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead. /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils. warnings.warn(""Setuptools is replacing distutils."")","['python', 'nlp', 'huggingface-transformers', 'mlflow', 'mlops']",1,"Before defining the trainer, the model has be turned into a Peft model object via get_peft_model , then the mlflow.transformers.log_model works: from peft import LoraConfig, get_peft_model model = ... peft_config = LoraConfig(...) args = TrainingArguments(...) peft_model = get_peft_model(model, peft_config) trainer = SFTTrainer( model=peft_model, args=args, ... ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! Now the logginig of the model works, we can find it in the artifacts !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", )",2024-08-12 16:27:32,2024-08-16 07:14:17,353,https://stackoverflow.com/questions/78862691/mlflow-transformers-log-model-does-not-finish,"`mlflow.transformers.log_model()` does not finish Problem I want to use mlflow.transformers.log_model() to log a finetuned huggingface model. However, when the mlflow.transformers.log_model method is running, it simply does not finish - runs forever - throws no errors. I suspect my configuration is not right, the model is too big? The output says Skipping saving pretrained model weights to disk so that should not be the problem. Any ideas how to do this properly? Example This is more or less how my setup looks like, you cannot run this, it includes some pseudocode... I am on python 3.11.9 with transformers = ""^4.41.2"" & mlflow = ""^2.15.1"" . import mlflow import torch from peft import LoraConfig from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, ) from trl import SFTTrainer, setup_chat_format train_dataset = ... eval_dataset = ... model_id = ""LeoLM/leo-hessianai-7b-chat-bilingual"" # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( model_id, device_map=""auto"", torch_dtype=torch.bfloat16, quantization_config=bnb_config, ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True) model, tokenizer = setup_chat_format(model, tokenizer) peft_config = LoraConfig(...) args = TrainingArguments(...) # Define Trainer trainer = SFTTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, peft_config=peft_config, tokenizer=tokenizer, packing=True, ) # mlflow mlflow.set_experiment(""my_experiment"") with mlflow.start_run() as run: mlflow.transformers.autolog() trainer.train() components = { ""model"": trainer.model, ""tokenizer"": tokenizer_no_pad, } # !!! This function all does not finish... !!! mlflow.transformers.log_model( transformers_model=components, artifact_path=""model"", ) The last output I get in the console is: INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead. Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'} /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning. warnings.warn( 2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead. /mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils. warnings.warn(""Setuptools is replacing distutils."")", mlflowtransformerslogmodel    finish problem want use mlflowtransformerslogmodel   log finetune huggingface model  however  mlflowtransformerslogmodel method run  simply finish  run forever  throw error  suspect configuration right  model big  output say skipping save pretraine model weights disk problem  idea properly  example less setup look like  run  include pseudocode  python 3119 transformer    4412   mlflow    2151   import mlflow import torch peft import loraconfig transformer import  automodelforcausallm  autotokenizer  bitsandbytesconfig  trainingarguments   trl import sfttrainer  setupchatformat traindataset   evaldataset   modelid    leolm  leo  hessianai7b  chat  bilingual   load model tokenizer model  automodelforcausallmfrompretraine  modelid  devicemap  auto   torchdtype  torchbfloat16  quantizationconfig  bnbconfig   tokenizer  autotokenizerfrompretraine  modelid  tokenizernopad  autotokenizerfrompretraine  modelid  addbostoken  true  model  tokenizer  setupchatformat  model  tokenizer  peftconfig  loraconfig    args  trainingarguments     define trainer trainer  sfttrainer  model  model  arg  args  traindataset  traindataset  evaldataset  evaldataset  peftconfig  peftconfig  tokenizer  tokenizer  pack  true    mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       function finish     mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model    last output get console  info mlflowtransformer  overriding savepretrained false peft model  follow transformers behavior  peft adaptor config save  base model weights reference huggingface hub repository log instead  unrecognized key  ropescaling   ropetypelinear     type   mypath  llm4pa  open  sourcevenv  lib  python311  site  package  peft  util  saveandloadpy209  userwarning  set  saveembeddinglayer   true  embed layer resize finetune  warningswarn  20240812 182114 info mlflowtransformer  skip save pretraine model weights disk savepretrained set false  reference huggingface hub repository leolm  leo  hessianai7b  chat  bilingual log instead  mypath  llm4pa  open  sourcevenv  lib  python311  site  packagesdistutilshackinitpy26  userwarning  setuptool replace distutil  warningswarn    setuptool replace distutil   ,define trainer  model turn peft model object via getpeftmodel  mlflowtransformerslogmodel work  peft import loraconfig  getpeftmodel model   peftconfig  loraconfig    args  trainingarguments    peftmodel  getpeftmodel  model  peftconfig  trainer  sfttrainer  model  peftmodel  arg  args     mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       logginig model work  find artifact    mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model   , mlflowtransformerslogmodel    finish problem want use mlflowtransformerslogmodel   log finetune huggingface model  however  mlflowtransformerslogmodel method run  simply finish  run forever  throw error  suspect configuration right  model big  output say skipping save pretraine model weights disk problem  idea properly  example less setup look like  run  include pseudocode  python 3119 transformer    4412   mlflow    2151   import mlflow import torch peft import loraconfig transformer import  automodelforcausallm  autotokenizer  bitsandbytesconfig  trainingarguments   trl import sfttrainer  setupchatformat traindataset   evaldataset   modelid    leolm  leo  hessianai7b  chat  bilingual   load model tokenizer model  automodelforcausallmfrompretraine  modelid  devicemap  auto   torchdtype  torchbfloat16  quantizationconfig  bnbconfig   tokenizer  autotokenizerfrompretraine  modelid  tokenizernopad  autotokenizerfrompretraine  modelid  addbostoken  true  model  tokenizer  setupchatformat  model  tokenizer  peftconfig  loraconfig    args  trainingarguments     define trainer trainer  sfttrainer  model  model  arg  args  traindataset  traindataset  evaldataset  evaldataset  peftconfig  peftconfig  tokenizer  tokenizer  pack  true    mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       function finish     mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model    last output get console  info mlflowtransformer  overriding savepretrained false peft model  follow transformers behavior  peft adaptor config save  base model weights reference huggingface hub repository log instead  unrecognized key  ropescaling   ropetypelinear     type   mypath  llm4pa  open  sourcevenv  lib  python311  site  package  peft  util  saveandloadpy209  userwarning  set  saveembeddinglayer   true  embed layer resize finetune  warningswarn  20240812 182114 info mlflowtransformer  skip save pretraine model weights disk savepretrained set false  reference huggingface hub repository leolm  leo  hessianai7b  chat  bilingual log instead  mypath  llm4pa  open  sourcevenv  lib  python311  site  packagesdistutilshackinitpy26  userwarning  setuptool replace distutil  warningswarn    setuptool replace distutil    define trainer  model turn peft model object via getpeftmodel  mlflowtransformerslogmodel work  peft import loraconfig  getpeftmodel model   peftconfig  loraconfig    args  trainingarguments    peftmodel  getpeftmodel  model  peftconfig  trainer  sfttrainer  model  peftmodel  arg  args     mlflow mlflowsetexperiment    myexperiment   mlflowstartrun   run  mlflowtransformersautolog   trainertrain   component     model   trainermodel    tokenizer   tokenizernopad       logginig model work  find artifact    mlflowtransformerslogmodel  transformersmodel  component  artifactpath  model   ,Implementation Issues
NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation),"I'm attempting to fine-tune the NLLB model ""facebook/nllb-200-distilled-600M"" for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb. Documentation: link This is the code block which is giving error: DATA_CONFIG = ""/content/sample_data/data_config.json"" OUTPUT_DIR = ""/content/outputs"" MODEL_FOLDER = ""/content/drive/MyDrive/Thesis/nllb-checkpoints"" DROP = 0.1 SRC = ""eng_Latn"" TGT = ""deu_Latn"" !python /content/fairseq/examples/nllb/modeling/train/train_script.py \ cfg=nllb200_dense3.3B_finetune_on_fbseed \ cfg/dataset=default \ cfg.dataset.lang_pairs=""$SRC-$TGT"" \ cfg.fairseq_root=$(pwd) \ cfg.output_dir=$OUTPUT_DIR \ cfg.dropout=$DROP \ cfg.warmup=10 \ cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt This is the error: /content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: The version_base parameter is not specified. Please specify a compatability version level, or None. Will assume defaults for version 1.1 @hydra.main(config_path=""conf"", config_name=""base_config"") /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default. See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information. ret = run_job( TRAINING DIR: /content/outputs Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt'] Traceback (most recent call last): File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main train_module = TrainModule(config) File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__ assert cluster_name in cfg.dataset.data_prefix omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct full_key: cfg.dataset.data_prefix object_type=dict Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace. So far, I understand there is a Missing data_prefix configuration . I created a demo custom data_config.json. Which looks like this: { ""data_prefix"": ""/content/sample_data"", ""train_data"": ""train_demo.json"", ""test_data"": ""test_demo.json"", ""lang_pairs"": ""eng_Latn-deu_Latn"" } While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?","['python', 'nlp', 'machine-translation', 'fine-tuning', 'fairseq']",1,"While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the transformers library, specifically using the Seq2SeqTrainer . I did this before for multiple models, including NLLB, check out this repository: https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/ This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so: tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=""eng_Latn"", tgt_lang=""deu_Latn"") and generate translations like this: model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(""deu_Latn"")[1], max_length=512)",2024-08-09 14:46:54,2024-08-09 21:05:47,159,https://stackoverflow.com/questions/78853409/nllb-fine-tuning-error-missing-data-prefix-configuration-english-german-transl,"NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation) I'm attempting to fine-tune the NLLB model ""facebook/nllb-200-distilled-600M"" for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb. Documentation: link This is the code block which is giving error: DATA_CONFIG = ""/content/sample_data/data_config.json"" OUTPUT_DIR = ""/content/outputs"" MODEL_FOLDER = ""/content/drive/MyDrive/Thesis/nllb-checkpoints"" DROP = 0.1 SRC = ""eng_Latn"" TGT = ""deu_Latn"" !python /content/fairseq/examples/nllb/modeling/train/train_script.py \ cfg=nllb200_dense3.3B_finetune_on_fbseed \ cfg/dataset=default \ cfg.dataset.lang_pairs=""$SRC-$TGT"" \ cfg.fairseq_root=$(pwd) \ cfg.output_dir=$OUTPUT_DIR \ cfg.dropout=$DROP \ cfg.warmup=10 \ cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt This is the error: /content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: The version_base parameter is not specified. Please specify a compatability version level, or None. Will assume defaults for version 1.1 @hydra.main(config_path=""conf"", config_name=""base_config"") /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default. See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information. ret = run_job( TRAINING DIR: /content/outputs Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt'] Traceback (most recent call last): File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main train_module = TrainModule(config) File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__ assert cluster_name in cfg.dataset.data_prefix omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct full_key: cfg.dataset.data_prefix object_type=dict Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace. So far, I understand there is a Missing data_prefix configuration . I created a demo custom data_config.json. Which looks like this: { ""data_prefix"": ""/content/sample_data"", ""train_data"": ""train_demo.json"", ""test_data"": ""test_demo.json"", ""lang_pairs"": ""eng_Latn-deu_Latn"" } While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?",nllb fine  tuning error  miss dataprefix configuration  english  german translation   m attempt fine  tune nllb model   facebook  nllb200  distilled600 m  scientific translation task english  englatn  german  deulatn   follow official guideline fine  tune author nllb  documentation  link code block give error  dataconfig    content  sampledata  dataconfigjson  outputdir    content  output  modelfolder    content  drive  mydrive  thesis  nllb  checkpoint  drop  01 src    englatn  tgt    deulatn   python content  fairseq  example  nllb  modeling  train  trainscriptpy  cfg  nllb200dense33bfinetuneonfbseed  cfg  dataset  default  cfgdatasetlangpairs   src  tgt   cfgfairseqroot   pwd   cfgoutputdir  outputdir  cfgdropout  drop  cfgwarmup10  cfgfinetunefrommodel  modelfolder  checkpointpt error  content  fairseq  example  nllb  modeling  train  trainscriptpy287  userwarning  versionbase parameter specify  please specify compatability version level  none  assume default version 11  hydramain  configpath  conf   configname  baseconfig   usr  local  lib  python310  dist  package  hydrainternal  hydrapy119  userwarning  future hydra version long change work directory job runtime default  see   hydracc  docs12  upgrades11to12  changestojobworkingdir information  ret  runjob  training dir  content  output error executing job override    cfg  nllb200dense33bfinetuneonfbseed    cfg  dataset  default    cfgdatasetlangpair  englatn  deulatn    cfgfairseqrootcontent    cfgoutputdircontent  output    cfgdropout01    cfgwarmup10    cfgfinetunefrommodelcontent  drive  mydrive  lasskgdata  thesis  nllb  checkpoint  checkpointpt   traceback  recent call last   file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 289  main trainmodule  trainmodule  config  file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 122    init   assert clustername cfgdatasetdataprefix omegaconferror  configattributeerror  key  dataprefix  struct fullkey  cfgdatasetdataprefix objecttype  dict set environment variable hydrafullerror1 complete stack trace  far  understand miss dataprefix configuration  create demo custom dataconfigjson  look like     dataprefix     content  sampledata     traindata     traindemojson     testdata     testdemojson     langpairs     englatn  deulatn   official documentation provide information   m encounter difficulty apply specific use case  someone share detailed guide point helpful resource fine  tune nllb ,can not help concrete error message get  guess would issue structure provide json file   personal recommendation would fine  tune nllb transformer library  specifically use seq2seqtrainer  multiple model  include nllb  check repository    githubcom  eliask93  transformer  model  for  domain  specific  machine  translation way fine  tune inference process nllb model bilingual model  find guide easiely   exception load tokenizer like  tokenizer  nllbtokenizerfrompretraine  modelpath  srclang  englatn   tgtlang  deulatn   generate translation like  modelgenerate  tokenizedchunkinputid  forcedbostokenid  tokenizerencode    deulatn    1   maxlength512 ,nllb fine  tuning error  miss dataprefix configuration  english  german translation   m attempt fine  tune nllb model   facebook  nllb200  distilled600 m  scientific translation task english  englatn  german  deulatn   follow official guideline fine  tune author nllb  documentation  link code block give error  dataconfig    content  sampledata  dataconfigjson  outputdir    content  output  modelfolder    content  drive  mydrive  thesis  nllb  checkpoint  drop  01 src    englatn  tgt    deulatn   python content  fairseq  example  nllb  modeling  train  trainscriptpy  cfg  nllb200dense33bfinetuneonfbseed  cfg  dataset  default  cfgdatasetlangpairs   src  tgt   cfgfairseqroot   pwd   cfgoutputdir  outputdir  cfgdropout  drop  cfgwarmup10  cfgfinetunefrommodel  modelfolder  checkpointpt error  content  fairseq  example  nllb  modeling  train  trainscriptpy287  userwarning  versionbase parameter specify  please specify compatability version level  none  assume default version 11  hydramain  configpath  conf   configname  baseconfig   usr  local  lib  python310  dist  package  hydrainternal  hydrapy119  userwarning  future hydra version long change work directory job runtime default  see   hydracc  docs12  upgrades11to12  changestojobworkingdir information  ret  runjob  training dir  content  output error executing job override    cfg  nllb200dense33bfinetuneonfbseed    cfg  dataset  default    cfgdatasetlangpair  englatn  deulatn    cfgfairseqrootcontent    cfgoutputdircontent  output    cfgdropout01    cfgwarmup10    cfgfinetunefrommodelcontent  drive  mydrive  lasskgdata  thesis  nllb  checkpoint  checkpointpt   traceback  recent call last   file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 289  main trainmodule  trainmodule  config  file   content  fairseq  example  nllb  modeling  train  trainscriptpy   line 122    init   assert clustername cfgdatasetdataprefix omegaconferror  configattributeerror  key  dataprefix  struct fullkey  cfgdatasetdataprefix objecttype  dict set environment variable hydrafullerror1 complete stack trace  far  understand miss dataprefix configuration  create demo custom dataconfigjson  look like     dataprefix     content  sampledata     traindata     traindemojson     testdata     testdemojson     langpairs     englatn  deulatn   official documentation provide information   m encounter difficulty apply specific use case  someone share detailed guide point helpful resource fine  tune nllb  can not help concrete error message get  guess would issue structure provide json file   personal recommendation would fine  tune nllb transformer library  specifically use seq2seqtrainer  multiple model  include nllb  check repository    githubcom  eliask93  transformer  model  for  domain  specific  machine  translation way fine  tune inference process nllb model bilingual model  find guide easiely   exception load tokenizer like  tokenizer  nllbtokenizerfrompretraine  modelpath  srclang  englatn   tgtlang  deulatn   generate translation like  modelgenerate  tokenizedchunkinputid  forcedbostokenid  tokenizerencode    deulatn    1   maxlength512 ,Basic Understanding
How can I use structured_output with Azure OpenAI with the openai Python library?,"I want to use structured output with Azure OpenAI. I tried the following code, based on the code given in https://openai.com/index/introducing-structured-outputs-in-the-api/ : from pydantic import BaseModel from openai import AzureOpenAI class Step(BaseModel): explanation: str output: str class MathResponse(BaseModel): steps: list[Step] final_answer: str client = AzureOpenAI(api_key='[redacted]', api_version='2024-05-01-preview', azure_endpoint='[redacted]') completion = client.beta.chat.completions.parse( model=""gpt-4omini-2024-07-18-name"", messages=[ {""role"": ""system"", ""content"": ""You are a helpful math tutor.""}, {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""}, ], response_format=MathResponse, ) message = completion.choices[0].message if message.parsed: print(message.parsed.steps) print(message.parsed.final_answer) else: print(message.refusal) I get the error: openai.BadRequestError: Error code: 400: { ""error"": { ""message"": ""Invalid parameter: response_format must be one of json_object, text."", ""type"": ""invalid_request_error"", ""param"": ""response_format"", ""code"": ""None"" } } How to fix it? I ran pip install -U openai : I use openai==1.40.1 and Python 3.11. I also tried https://cookbook.openai.com/examples/structured_outputs_intro using using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message: from openai import AzureOpenAI # Replace these variables with your Azure OpenAI endpoint and API key endpoint = ""https://<your-resource-name>.openai.azure.com"" api_key = ""<your-api-key>"" deployment_name = ""<your-deployment-name>"" # Replace with your deployment name MODEL = deployment_name # API endpoint for the completion request api_url = f""{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01"" client = AzureOpenAI(api_key='[redacted]', api_version='2024-07-01-preview', azure_endpoint='https://[redacted].openai.azure.com/') math_tutor_prompt = ''' You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning. ''' def get_math_solution(question): response = client.chat.completions.create( model=MODEL, messages=[ { ""role"": ""system"", ""content"": math_tutor_prompt }, { ""role"": ""user"", ""content"": question } ], response_format={ ""type"": ""json_schema"", ""json_schema"": { ""name"": ""math_reasoning"", ""schema"": { ""type"": ""object"", ""properties"": { ""steps"": { ""type"": ""array"", ""items"": { ""type"": ""object"", ""properties"": { ""explanation"": {""type"": ""string""}, ""output"": {""type"": ""string""} }, ""required"": [""explanation"", ""output""], ""additionalProperties"": False } }, ""final_answer"": {""type"": ""string""} }, ""required"": [""steps"", ""final_answer""], ""additionalProperties"": False }, ""strict"": True } } ) return response.choices[0].message # Testing with an example question question = ""how can I solve 8x + 7 = -23"" result = get_math_solution(question) print(result.content)","['python', 'nlp', 'azure-openai', 'gpt-4']",2,"Using gpt-4o-2024-08-06 , which finally got deployed today (2024-09-03) on Azure, made it work. Code example from learn.microsoft.com : from pydantic import BaseModel from openai import AzureOpenAI endpoint = ""https://your-azure-openai-endpoint.com"" api_key = ""your-azure-openai-key"" deployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name client = AzureOpenAI(api_key=api_key, api_version='2024-08-01-preview', azure_endpoint=endpoint) class CalendarEvent(BaseModel): name: str date: str participants: list[str] completion = client.beta.chat.completions.parse( model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment messages=[ {""role"": ""system"", ""content"": ""Extract the event information.""}, {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""}, ], response_format=CalendarEvent, ) event = completion.choices[0].message.parsed print(event) print(completion.model_dump_json(indent=2)) output: name='Science Fair' date='Friday' participants=['Alice', 'Bob'] { ""id"": ""chatcmpl-A3XDRVolXpjeAAQIGddswI990weid"", ""choices"": [ { ""finish_reason"": ""stop"", ""index"": 0, ""logprobs"": null, ""message"": { ""content"": ""{\""name\"":\""Science Fair\"",\""date\"":\""Friday\"",\""participants\"":[\""Alice\"",\""Bob\""]}"", ""refusal"": null, ""role"": ""assistant"", ""function_call"": null, ""tool_calls"": [], ""parsed"": { ""name"": ""Science Fair"", ""date"": ""Friday"", ""participants"": [ ""Alice"", ""Bob"" ] } }, ""content_filter_results"": { ""hate"": { ""filtered"": false, ""severity"": ""safe"" }, ""self_harm"": { ""filtered"": false, ""severity"": ""safe"" }, ""sexual"": { ""filtered"": false, ""severity"": ""safe"" }, ""violence"": { ""filtered"": false, ""severity"": ""safe"" } } } ], ""created"": 1725406029, ""model"": ""gpt-4o-2024-08-06"", ""object"": ""chat.completion"", ""service_tier"": null, ""system_fingerprint"": ""fp_b2ffeb31ff"", ""usage"": { ""completion_tokens"": 17, ""prompt_tokens"": 32, ""total_tokens"": 49 }, ""prompt_filter_results"": [ { ""prompt_index"": 0, ""content_filter_results"": { ""hate"": { ""filtered"": false, ""severity"": ""safe"" }, ""self_harm"": { ""filtered"": false, ""severity"": ""safe"" }, ""sexual"": { ""filtered"": false, ""severity"": ""safe"" }, ""violence"": { ""filtered"": false, ""severity"": ""safe"" } } } ] } Tested with Python 3.11.7 and openai==1.43.0.",2024-08-07 23:14:19,2024-09-03 23:28:44,1226,https://stackoverflow.com/questions/78846004/how-can-i-use-structured-output-with-azure-openai-with-the-openai-python-library,"How can I use structured_output with Azure OpenAI with the openai Python library? I want to use structured output with Azure OpenAI. I tried the following code, based on the code given in https://openai.com/index/introducing-structured-outputs-in-the-api/ : from pydantic import BaseModel from openai import AzureOpenAI class Step(BaseModel): explanation: str output: str class MathResponse(BaseModel): steps: list[Step] final_answer: str client = AzureOpenAI(api_key='[redacted]', api_version='2024-05-01-preview', azure_endpoint='[redacted]') completion = client.beta.chat.completions.parse( model=""gpt-4omini-2024-07-18-name"", messages=[ {""role"": ""system"", ""content"": ""You are a helpful math tutor.""}, {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""}, ], response_format=MathResponse, ) message = completion.choices[0].message if message.parsed: print(message.parsed.steps) print(message.parsed.final_answer) else: print(message.refusal) I get the error: openai.BadRequestError: Error code: 400: { ""error"": { ""message"": ""Invalid parameter: response_format must be one of json_object, text."", ""type"": ""invalid_request_error"", ""param"": ""response_format"", ""code"": ""None"" } } How to fix it? I ran pip install -U openai : I use openai==1.40.1 and Python 3.11. I also tried https://cookbook.openai.com/examples/structured_outputs_intro using using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message: from openai import AzureOpenAI # Replace these variables with your Azure OpenAI endpoint and API key endpoint = ""https://<your-resource-name>.openai.azure.com"" api_key = ""<your-api-key>"" deployment_name = ""<your-deployment-name>"" # Replace with your deployment name MODEL = deployment_name # API endpoint for the completion request api_url = f""{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01"" client = AzureOpenAI(api_key='[redacted]', api_version='2024-07-01-preview', azure_endpoint='https://[redacted].openai.azure.com/') math_tutor_prompt = ''' You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning. ''' def get_math_solution(question): response = client.chat.completions.create( model=MODEL, messages=[ { ""role"": ""system"", ""content"": math_tutor_prompt }, { ""role"": ""user"", ""content"": question } ], response_format={ ""type"": ""json_schema"", ""json_schema"": { ""name"": ""math_reasoning"", ""schema"": { ""type"": ""object"", ""properties"": { ""steps"": { ""type"": ""array"", ""items"": { ""type"": ""object"", ""properties"": { ""explanation"": {""type"": ""string""}, ""output"": {""type"": ""string""} }, ""required"": [""explanation"", ""output""], ""additionalProperties"": False } }, ""final_answer"": {""type"": ""string""} }, ""required"": [""steps"", ""final_answer""], ""additionalProperties"": False }, ""strict"": True } } ) return response.choices[0].message # Testing with an example question question = ""how can I solve 8x + 7 = -23"" result = get_math_solution(question) print(result.content)",use structuredoutput azure openai openai python library  want use structured output azure openai  try follow code  base code give   openaicom  index  introducing  structure  outputs  in  the  api  pydantic import basemodel openai import azureopenai class step  basemodel   explanation  str output  str class mathresponse  basemodel   step  list  step  finalanswer  str client  azureopenai  apikey   redact    apiversion2024  05  01  preview   azureendpoint   redact    completion  clientbetachatcompletionsparse  model  gpt4omini2024  07  18  name   messages     role     system     content     helpful math tutor         role     user     content     solve 8x  31  2      responseformat  mathresponse   message  completionchoice  0  message messageparse  print  messageparsedstep  print  messageparsedfinalanswer  else  print  messagerefusal  get error  openai  badrequesterror  error code  400     error      message     invalid parameter  responseformat must one jsonobject  text       type     invalidrequesterror     param     responseformat     code     none    fix  run pip install u openai  use openai1401 python 311  also try   cookbookopenaicom  example  structuredoutputsintro use use azure gpt4o mini  2024  07  18   not work either  error message  openai import azureopenai  replace variable azure openai endpoint api key endpoint        your  resource  name  openaiazurecom  apikey     your  api  key   deploymentname     your  deployment  name    replace deployment name model  deploymentname  api endpoint completion request apiurl  f   endpoint  openai  deployments  deploymentname  chat  completion  api  version2024  06  01  client  azureopenai  apikey   redact    apiversion2024  07  01  preview   azureendpoint    redact  openaiazurecom   mathtutorprompt     helpful math tutor  provide math problem  goal output step step solution  along final answer  step  provide output equation use explanation field detail reasoning     def getmathsolution  question   response  clientchatcompletionscreate  model  model  messages     role     system     content   mathtutorprompt      role     user     content   question    responseformat    type     jsonschema     jsonschema      name     mathreasoning     schema      type     object     property      step      type     array     item      type     object     property      explanation      type     string      output      type     string       require      explanation     output      additionalproperties   false      finalanswer      type     string       require      step     finalanswer      additionalproperties   false     strict   true    return responsechoice  0  message  testing example question question    solve 8x  7  23  result  getmathsolution  question  print  resultcontent ,use gpt4o2024  08  06  finally get deploy today  2024  09  03  azure  make work  code example learnmicrosoftcom  pydantic import basemodel openai import azureopenai endpoint      your  azure  openai  endpointcom  apikey    your  azure  openai  key  deploymentname   deployment name   replace gpt4o 2024  08  06 deployment name client  azureopenai  apikey  apikey  apiversion2024  08  01  preview   azureendpoint  endpoint  class calendarevent  basemodel   name  str date  str participant  list  str  completion  clientbetachatcompletionsparse  model  deploymentname   replace model deployment name gpt4o 2024  08  06 deployment messages     role     system     content     extract event information         role     user     content     alice bob go science fair friday        responseformat  calendarevent   event  completionchoice  0  messageparse print  event  print  completionmodeldumpjson  indent2   output  namescience fair  datefriday  participants   alice    bob      i d     chatcmpl  a3xdrvolxpjeaaqigddswi990weid     choice       finishreason     stop     index   0    logprob   null    message      content        name     science fair     date     friday     participants      alice     bob        refusal   null    role     assistant     functioncall   null    toolcalls        parse      name     science fair     date     friday     participant      alice     bob        contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe         create   1725406029    model     gpt4o2024  08  06     object     chatcompletion     servicetier   null    systemfingerprint     fpb2ffeb31ff     usage      completiontokens   17    prompttoken   32    totaltoken   49     promptfilterresult       promptindex   0    contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe       test python 3117 openai1430 ,use structuredoutput azure openai openai python library  want use structured output azure openai  try follow code  base code give   openaicom  index  introducing  structure  outputs  in  the  api  pydantic import basemodel openai import azureopenai class step  basemodel   explanation  str output  str class mathresponse  basemodel   step  list  step  finalanswer  str client  azureopenai  apikey   redact    apiversion2024  05  01  preview   azureendpoint   redact    completion  clientbetachatcompletionsparse  model  gpt4omini2024  07  18  name   messages     role     system     content     helpful math tutor         role     user     content     solve 8x  31  2      responseformat  mathresponse   message  completionchoice  0  message messageparse  print  messageparsedstep  print  messageparsedfinalanswer  else  print  messagerefusal  get error  openai  badrequesterror  error code  400     error      message     invalid parameter  responseformat must one jsonobject  text       type     invalidrequesterror     param     responseformat     code     none    fix  run pip install u openai  use openai1401 python 311  also try   cookbookopenaicom  example  structuredoutputsintro use use azure gpt4o mini  2024  07  18   not work either  error message  openai import azureopenai  replace variable azure openai endpoint api key endpoint        your  resource  name  openaiazurecom  apikey     your  api  key   deploymentname     your  deployment  name    replace deployment name model  deploymentname  api endpoint completion request apiurl  f   endpoint  openai  deployments  deploymentname  chat  completion  api  version2024  06  01  client  azureopenai  apikey   redact    apiversion2024  07  01  preview   azureendpoint    redact  openaiazurecom   mathtutorprompt     helpful math tutor  provide math problem  goal output step step solution  along final answer  step  provide output equation use explanation field detail reasoning     def getmathsolution  question   response  clientchatcompletionscreate  model  model  messages     role     system     content   mathtutorprompt      role     user     content   question    responseformat    type     jsonschema     jsonschema      name     mathreasoning     schema      type     object     property      step      type     array     item      type     object     property      explanation      type     string      output      type     string       require      explanation     output      additionalproperties   false      finalanswer      type     string       require      step     finalanswer      additionalproperties   false     strict   true    return responsechoice  0  message  testing example question question    solve 8x  7  23  result  getmathsolution  question  print  resultcontent  use gpt4o2024  08  06  finally get deploy today  2024  09  03  azure  make work  code example learnmicrosoftcom  pydantic import basemodel openai import azureopenai endpoint      your  azure  openai  endpointcom  apikey    your  azure  openai  key  deploymentname   deployment name   replace gpt4o 2024  08  06 deployment name client  azureopenai  apikey  apikey  apiversion2024  08  01  preview   azureendpoint  endpoint  class calendarevent  basemodel   name  str date  str participant  list  str  completion  clientbetachatcompletionsparse  model  deploymentname   replace model deployment name gpt4o 2024  08  06 deployment messages     role     system     content     extract event information         role     user     content     alice bob go science fair friday        responseformat  calendarevent   event  completionchoice  0  messageparse print  event  print  completionmodeldumpjson  indent2   output  namescience fair  datefriday  participants   alice    bob      i d     chatcmpl  a3xdrvolxpjeaaqigddswi990weid     choice       finishreason     stop     index   0    logprob   null    message      content        name     science fair     date     friday     participants      alice     bob        refusal   null    role     assistant     functioncall   null    toolcalls        parse      name     science fair     date     friday     participant      alice     bob        contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe         create   1725406029    model     gpt4o2024  08  06     object     chatcompletion     servicetier   null    systemfingerprint     fpb2ffeb31ff     usage      completiontokens   17    prompttoken   32    totaltoken   49     promptfilterresult       promptindex   0    contentfilterresult      hate      filter   false    severity     safe      selfharm      filter   false    severity     safe      sexual      filter   false    severity     safe      violence      filter   false    severity     safe       test python 3117 openai1430 ,Basic Understanding
Removing bi-grams after tokenization for TfidfVectorizer,"I'm attempting to remove bi-grams that are created by TfidfVectorizer . I'm using text.TfidfVectorizer so that I can use my own preprocessor function. Test strings and preprocessor function: doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 'another that has aa aa and start date and hurricane hitting south carolina'] def remove_bigrams(doc): gram_2 = ['past performance', 'start date', 'aa aa'] res = [] for record in doc: the_string = record for phrase in gram_2: the_string = the_string.replace(phrase, """") res.append(the_string) return res remove_bigrams(doc2) My TfidfVectorizer instantiation and fit_transform : from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import text custom_stop_words = [i for i in stop_words] vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range=(2, 2), preprocessor=remove_bigrams, ) features = vec.fit_transform(doc2) Here is my error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Input In [49], in <cell line: 5>() 3 #t3_cv = CountVectorizer(t2, stop_words = stop_words) 4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams) ----> 5 features = vec.fit_transform(doc2) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y) 2072 self._check_params() 2073 self._tfidf = TfidfTransformer( 2074 norm=self.norm, 2075 use_idf=self.use_idf, 2076 smooth_idf=self.smooth_idf, 2077 sublinear_tf=self.sublinear_tf, 2078 ) -> 2079 X = super().fit_transform(raw_documents) 2080 self._tfidf.fit(X) 2081 # X is already a transformed view of raw_documents so 2082 # we set copy to False File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y) 1330 warnings.warn( 1331 ""Upper case characters found in"" 1332 "" vocabulary while 'lowercase'"" 1333 "" is True. These entries will not"" 1334 "" be matched with any documents"" 1335 ) 1336 break -> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_) 1340 if self.binary: 1341 X.data.fill(1) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab) 1207 for doc in raw_documents: 1208 feature_counter = {} -> 1209 for feature in analyze(doc): 1210 try: 1211 feature_idx = vocabulary[feature] File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words) 111 doc = preprocessor(doc) 112 if tokenizer is not None: --> 113 doc = tokenizer(doc) 114 if ngrams is not None: 115 if stop_words is not None: TypeError: expected string or bytes-like object How to resolve it?","['python', 'scikit-learn', 'nlp', 'preprocessor', 'tfidfvectorizer']",1,"The preprocessor should handle documents, not the whole corpus. (The clues are the ""expected string"" in the error, and the fact that the TfidfVectorizer docs refer to ""the preprocessing (string transformation) stage"". The docs could definitely be clearer.) This should fix it: def remove_bigrams(doc: str) -> str: """"""Remove certain bi-grams from a document."""""" gram_2 = ['past performance', 'start date', 'aa aa'] for phrase in gram_2: doc = doc.replace(phrase, """") return doc",2024-08-05 19:46:40,2024-08-06 06:57:17,41,https://stackoverflow.com/questions/78836208/removing-bi-grams-after-tokenization-for-tfidfvectorizer,"Removing bi-grams after tokenization for TfidfVectorizer I'm attempting to remove bi-grams that are created by TfidfVectorizer . I'm using text.TfidfVectorizer so that I can use my own preprocessor function. Test strings and preprocessor function: doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 'another that has aa aa and start date and hurricane hitting south carolina'] def remove_bigrams(doc): gram_2 = ['past performance', 'start date', 'aa aa'] res = [] for record in doc: the_string = record for phrase in gram_2: the_string = the_string.replace(phrase, """") res.append(the_string) return res remove_bigrams(doc2) My TfidfVectorizer instantiation and fit_transform : from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import text custom_stop_words = [i for i in stop_words] vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range=(2, 2), preprocessor=remove_bigrams, ) features = vec.fit_transform(doc2) Here is my error: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Input In [49], in <cell line: 5>() 3 #t3_cv = CountVectorizer(t2, stop_words = stop_words) 4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams) ----> 5 features = vec.fit_transform(doc2) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y) 2072 self._check_params() 2073 self._tfidf = TfidfTransformer( 2074 norm=self.norm, 2075 use_idf=self.use_idf, 2076 smooth_idf=self.smooth_idf, 2077 sublinear_tf=self.sublinear_tf, 2078 ) -> 2079 X = super().fit_transform(raw_documents) 2080 self._tfidf.fit(X) 2081 # X is already a transformed view of raw_documents so 2082 # we set copy to False File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y) 1330 warnings.warn( 1331 ""Upper case characters found in"" 1332 "" vocabulary while 'lowercase'"" 1333 "" is True. These entries will not"" 1334 "" be matched with any documents"" 1335 ) 1336 break -> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_) 1340 if self.binary: 1341 X.data.fill(1) File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab) 1207 for doc in raw_documents: 1208 feature_counter = {} -> 1209 for feature in analyze(doc): 1210 try: 1211 feature_idx = vocabulary[feature] File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words) 111 doc = preprocessor(doc) 112 if tokenizer is not None: --> 113 doc = tokenizer(doc) 114 if ngrams is not None: 115 if stop_words is not None: TypeError: expected string or bytes-like object How to resolve it?",remove bi  grams tokenization tfidfvectorizer  m attempt remove bi  gram create tfidfvectorizer   m use text  tfidfvectorizer use preprocessor function  test string preprocessor function  doc2    this test past performance another aa aa add buile cat dog horse hurricane    another aa aa start date hurricane hit south carolina   def removebigram  doc   gram2    past performance    start date    aa aa   re    record doc  thestring  record phrase gram2  thestre  thestringreplace  phrase      resappend  thestring  return re removebigrams  doc2  tfidfvectorizer instantiation fittransform  sklearnfeatureextractiontext import englishstopwords stopwords sklearnfeatureextractiontext import tfidfvectorizer sklearnfeatureextraction import text customstopwords   stopwords  vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange  2  2   preprocessor  removebigram   feature  vecfittransform  doc2  error                                        typeerror traceback  recent call last  input  49    cell line  5    3  t3cv  countvectorizer  t2  stopword  stopwords  4 vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange   22   preprocessor  removebigrams     5 feature  vecfittransform  doc2  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy2079  tfidfvectorizerfittransform  self  rawdocument   2072 selfcheckparams   2073 selftfidf  tfidftransformer  2074 norm  selfnorm  2075 useidf  selfuseidf  2076 smoothidf  selfsmoothidf  2077 sublineartf  selfsublineartf  2078    2079 x  super   fittransform  rawdocument  2080 selftfidffit  x  2081  x already transform view rawdocument 2082  set copy false file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1338  countvectorizerfittransform  self  rawdocument   1330 warningswarn  1331   upper case character find  1332   vocabulary  lowercase   1333   true  entry  1334   match document  1335  1336 break   1338 vocabulary  x  selfcountvocab  rawdocument  selffixedvocabulary   1340 selfbinary  1341 xdatafill  1  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1209  countvectorizercountvocab  self  rawdocument  fixedvocab  1207 doc rawdocument  1208 featurecounter      1209 feature analyze  doc   1210 try  1211 featureidx  vocabulary  feature  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy113   analyze  doc  analyzer  tokenizer  ngram  preprocessor  decoder  stopwords  111 doc  preprocessor  doc  112 tokenizer none    113 doc  tokenizer  doc  114 ngram none  115 stopwords none  typeerror  expect string byte  like object resolve ,preprocessor handle document  whole corpus   clue   expect string  error  fact tfidfvectorizer docs refer   preprocesse  string transformation  stage   doc could definitely clear   fix  def removebigram  doc  str    str      remove certain bi  gram document     gram2    past performance    start date    aa aa   phrase gram2  doc  docreplace  phrase      return doc,remove bi  grams tokenization tfidfvectorizer  m attempt remove bi  gram create tfidfvectorizer   m use text  tfidfvectorizer use preprocessor function  test string preprocessor function  doc2    this test past performance another aa aa add buile cat dog horse hurricane    another aa aa start date hurricane hit south carolina   def removebigram  doc   gram2    past performance    start date    aa aa   re    record doc  thestring  record phrase gram2  thestre  thestringreplace  phrase      resappend  thestring  return re removebigrams  doc2  tfidfvectorizer instantiation fittransform  sklearnfeatureextractiontext import englishstopwords stopwords sklearnfeatureextractiontext import tfidfvectorizer sklearnfeatureextraction import text customstopwords   stopwords  vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange  2  2   preprocessor  removebigram   feature  vecfittransform  doc2  error                                        typeerror traceback  recent call last  input  49    cell line  5    3  t3cv  countvectorizer  t2  stopword  stopwords  4 vec  text  tfidfvectorizer  stopword  customstopwords  analyzerword   ngramrange   22   preprocessor  removebigrams     5 feature  vecfittransform  doc2  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy2079  tfidfvectorizerfittransform  self  rawdocument   2072 selfcheckparams   2073 selftfidf  tfidftransformer  2074 norm  selfnorm  2075 useidf  selfuseidf  2076 smoothidf  selfsmoothidf  2077 sublineartf  selfsublineartf  2078    2079 x  super   fittransform  rawdocument  2080 selftfidffit  x  2081  x already transform view rawdocument 2082  set copy false file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1338  countvectorizerfittransform  self  rawdocument   1330 warningswarn  1331   upper case character find  1332   vocabulary  lowercase   1333   true  entry  1334   match document  1335  1336 break   1338 vocabulary  x  selfcountvocab  rawdocument  selffixedvocabulary   1340 selfbinary  1341 xdatafill  1  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy1209  countvectorizercountvocab  self  rawdocument  fixedvocab  1207 doc rawdocument  1208 featurecounter      1209 feature analyze  doc   1210 try  1211 featureidx  vocabulary  feature  file c  developmentsolutionssandboxsbvelibsite  packagessklearnfeatureextractiontextpy113   analyze  doc  analyzer  tokenizer  ngram  preprocessor  decoder  stopwords  111 doc  preprocessor  doc  112 tokenizer none    113 doc  tokenizer  doc  114 ngram none  115 stopwords none  typeerror  expect string byte  like object resolve  preprocessor handle document  whole corpus   clue   expect string  error  fact tfidfvectorizer docs refer   preprocesse  string transformation  stage   doc could definitely clear   fix  def removebigram  doc  str    str      remove certain bi  gram document     gram2    past performance    start date    aa aa   phrase gram2  doc  docreplace  phrase      return doc,Task-Specific Queries
HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long,"Context : I tried to create an evaluation pipeline for a text summary task using HuggingFace evaluate packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this. Here is what I did: My code : from transformers import pipeline from evaluate import evaluator from datasets import load_dataset # Load data: booksum = load_dataset(""kmfoda/booksum"", split=""validation[:1000]"") # Load pipeline pipe = pipeline( task=""summarization"", model=""pszemraj/led-base-book-summary"", device=""mps"" ) # Setup Evaluate task using Rouge task_evaluator = evaluator(""summarization"") # The code that yield issue: eval_results = task_evaluator.compute( model_or_pipeline=pipe, data=booksum, metric=""rouge"", input_column=""chapter"", label_column=""summary_text"" ) This gives me the value error below: Short Error message: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Full Error message: --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 eval_results = task_evaluator.compute( 2 model_or_pipeline=pipe, 3 data=booksum, 4 metric=""rouge"", 6 input_column=""chapter"", 7 label_column=""summary_text"" 8 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 166 @add_start_docstrings( 167 EVALUTOR_COMPUTE_START_DOCSTRING, 168 TASK_DOCUMENTATION_KWARGS, (...) 189 generation_kwargs: dict = None, 190 ) -> Tuple[Dict[str, float], Any]: --> 191 result = super().compute( 192 model_or_pipeline=model_or_pipeline, 193 data=data, 194 subset=subset, 195 split=split, 196 metric=metric, 197 tokenizer=tokenizer, 198 strategy=strategy, 199 confidence_level=confidence_level, 200 n_resamples=n_resamples, 201 device=device, 202 random_state=random_state, 203 input_column=input_column, 204 label_column=label_column, 205 generation_kwargs=generation_kwargs, 206 ) 208 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 130 if generation_kwargs is not None: 131 self.PIPELINE_KWARGS.update(generation_kwargs) --> 133 result = super().compute( 134 model_or_pipeline=model_or_pipeline, 135 data=data, 136 subset=subset, 137 split=split, 138 metric=metric, 139 tokenizer=tokenizer, 140 strategy=strategy, 141 confidence_level=confidence_level, 142 n_resamples=n_resamples, 143 device=device, 144 random_state=random_state, 145 input_column=input_column, 146 label_column=label_column, 147 ) 149 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping) 252 metric = self.prepare_metric(metric) 254 # Compute predictions --> 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs) 256 predictions = self.predictions_processor(predictions, label_mapping) 258 metric_inputs.update(predictions) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs) 511 def call_pipeline(self, pipe, *args, **kwargs): 512 start_time = perf_counter() --> 513 pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS) 514 end_time = perf_counter() 515 return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output)) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs) 245 def __call__(self, *args, **kwargs): 246 r"""""" 247 Summarize the text(s) given as inputs. 248 (...) 267 ids of the summary. 268 """""" --> 269 return super().__call__(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs) 138 def __call__(self, *args, **kwargs): 139 r"""""" 140 Generate the output text(s) using text(s) given as inputs. 141 (...) 164 ids of the generated text. 165 """""" --> 167 result = super().__call__(*args, **kwargs) 168 if ( 169 isinstance(args[0], list) 170 and all(isinstance(el, str) for el in args[0]) 171 and all(len(res) == 1 for res in result) 172 ): 173 return [res[0] for res in result] File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs) 1231 if can_use_iterator: 1232 final_iterator = self.get_iterator( 1233 inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params 1234 ) -> 1235 outputs = list(final_iterator) 1236 return outputs 1237 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self) 121 return self.loader_batch_item() 123 # We're out of items within a batch --> 124 item = next(self.iterator) 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self) 123 # We're out of items within a batch 124 item = next(self.iterator) --> 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". 127 if self.loader_batch_size is not None: 128 # Try to infer the size of the batch File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params) 1159 with inference_context(): 1160 model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device) -> 1161 model_outputs = self._forward(model_inputs, **forward_params) 1162 model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu"")) 1163 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs) 184 in_b, input_length = tf.shape(model_inputs[""input_ids""]).numpy() 186 self.check_inputs( 187 input_length, 188 generate_kwargs.get(""min_length"", self.model.config.min_length), 189 generate_kwargs.get(""max_length"", self.model.config.max_length), 190 ) --> 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs) 192 out_b = output_ids.shape[0] 193 if self.framework == ""pt"": File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs) 113 @functools.wraps(func) 114 def decorate_context(*args, **kwargs): 115 with ctx_factory(): --> 116 return func(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs) 2020 input_ids, model_kwargs = self._expand_inputs_for_generation( 2021 input_ids=input_ids, 2022 expand_size=generation_config.num_beams, 2023 is_encoder_decoder=self.config.is_encoder_decoder, 2024 **model_kwargs, 2025 ) 2027 # 14. run beam sample -> 2028 result = self._beam_search( 2029 input_ids, 2030 beam_scorer, 2031 logits_processor=prepared_logits_processor, 2032 logits_warper=prepared_logits_warper, 2033 stopping_criteria=prepared_stopping_criteria, 2034 generation_config=generation_config, 2035 synced_gpus=synced_gpus, 2036 **model_kwargs, 2037 ) 2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH: 2040 # 11. prepare beam search scorer 2041 beam_scorer = BeamSearchScorer( 2042 batch_size=batch_size, 2043 num_beams=generation_config.num_beams, (...) 2049 max_length=generation_config.max_length, 2050 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs) 3195 next_token_logits = outputs.logits[:, -1, :].clone() 3196 next_token_scores = nn.functional.log_softmax( 3197 next_token_logits, dim=-1 3198 ) # (batch_size * num_beams, vocab_size) -> 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores) 3201 if do_sample: 3202 next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs) 96 scores = processor(input_ids, scores, **kwargs) 97 else: ---> 98 scores = processor(input_ids, scores) 100 return scores File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Notes : I did try to add device=""mps"" to the task_evaluator.compute but it gave me another error of ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.","['pytorch', 'nlp', 'apple-m1', 'huggingface', 'huggingface-evaluate']",1,I ran into a similar issue trying to run Facebook's nougat OCR tool. The error message mentions macOS 14 (Sonoma) and I was on macOS 13 (Ventura). Upgrading to macOS 14 fixed the issue for me.,2024-08-01 21:19:16,2024-08-15 13:01:25,859,https://stackoverflow.com/questions/78823069/huggingface-llm-evaluate-runtimeerror-isin-tensor-tensor-out-only-works-on-flo,"HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Context : I tried to create an evaluation pipeline for a text summary task using HuggingFace evaluate packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this. Here is what I did: My code : from transformers import pipeline from evaluate import evaluator from datasets import load_dataset # Load data: booksum = load_dataset(""kmfoda/booksum"", split=""validation[:1000]"") # Load pipeline pipe = pipeline( task=""summarization"", model=""pszemraj/led-base-book-summary"", device=""mps"" ) # Setup Evaluate task using Rouge task_evaluator = evaluator(""summarization"") # The code that yield issue: eval_results = task_evaluator.compute( model_or_pipeline=pipe, data=booksum, metric=""rouge"", input_column=""chapter"", label_column=""summary_text"" ) This gives me the value error below: Short Error message: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Full Error message: --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 eval_results = task_evaluator.compute( 2 model_or_pipeline=pipe, 3 data=booksum, 4 metric=""rouge"", 6 input_column=""chapter"", 7 label_column=""summary_text"" 8 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 166 @add_start_docstrings( 167 EVALUTOR_COMPUTE_START_DOCSTRING, 168 TASK_DOCUMENTATION_KWARGS, (...) 189 generation_kwargs: dict = None, 190 ) -> Tuple[Dict[str, float], Any]: --> 191 result = super().compute( 192 model_or_pipeline=model_or_pipeline, 193 data=data, 194 subset=subset, 195 split=split, 196 metric=metric, 197 tokenizer=tokenizer, 198 strategy=strategy, 199 confidence_level=confidence_level, 200 n_resamples=n_resamples, 201 device=device, 202 random_state=random_state, 203 input_column=input_column, 204 label_column=label_column, 205 generation_kwargs=generation_kwargs, 206 ) 208 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs) 130 if generation_kwargs is not None: 131 self.PIPELINE_KWARGS.update(generation_kwargs) --> 133 result = super().compute( 134 model_or_pipeline=model_or_pipeline, 135 data=data, 136 subset=subset, 137 split=split, 138 metric=metric, 139 tokenizer=tokenizer, 140 strategy=strategy, 141 confidence_level=confidence_level, 142 n_resamples=n_resamples, 143 device=device, 144 random_state=random_state, 145 input_column=input_column, 146 label_column=label_column, 147 ) 149 return result File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping) 252 metric = self.prepare_metric(metric) 254 # Compute predictions --> 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs) 256 predictions = self.predictions_processor(predictions, label_mapping) 258 metric_inputs.update(predictions) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs) 511 def call_pipeline(self, pipe, *args, **kwargs): 512 start_time = perf_counter() --> 513 pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS) 514 end_time = perf_counter() 515 return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output)) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs) 245 def __call__(self, *args, **kwargs): 246 r"""""" 247 Summarize the text(s) given as inputs. 248 (...) 267 ids of the summary. 268 """""" --> 269 return super().__call__(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs) 138 def __call__(self, *args, **kwargs): 139 r"""""" 140 Generate the output text(s) using text(s) given as inputs. 141 (...) 164 ids of the generated text. 165 """""" --> 167 result = super().__call__(*args, **kwargs) 168 if ( 169 isinstance(args[0], list) 170 and all(isinstance(el, str) for el in args[0]) 171 and all(len(res) == 1 for res in result) 172 ): 173 return [res[0] for res in result] File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs) 1231 if can_use_iterator: 1232 final_iterator = self.get_iterator( 1233 inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params 1234 ) -> 1235 outputs = list(final_iterator) 1236 return outputs 1237 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self) 121 return self.loader_batch_item() 123 # We're out of items within a batch --> 124 item = next(self.iterator) 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self) 123 # We're out of items within a batch 124 item = next(self.iterator) --> 125 processed = self.infer(item, **self.params) 126 # We now have a batch of ""inferred things"". 127 if self.loader_batch_size is not None: 128 # Try to infer the size of the batch File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params) 1159 with inference_context(): 1160 model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device) -> 1161 model_outputs = self._forward(model_inputs, **forward_params) 1162 model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu"")) 1163 else: File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs) 184 in_b, input_length = tf.shape(model_inputs[""input_ids""]).numpy() 186 self.check_inputs( 187 input_length, 188 generate_kwargs.get(""min_length"", self.model.config.min_length), 189 generate_kwargs.get(""max_length"", self.model.config.max_length), 190 ) --> 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs) 192 out_b = output_ids.shape[0] 193 if self.framework == ""pt"": File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs) 113 @functools.wraps(func) 114 def decorate_context(*args, **kwargs): 115 with ctx_factory(): --> 116 return func(*args, **kwargs) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs) 2020 input_ids, model_kwargs = self._expand_inputs_for_generation( 2021 input_ids=input_ids, 2022 expand_size=generation_config.num_beams, 2023 is_encoder_decoder=self.config.is_encoder_decoder, 2024 **model_kwargs, 2025 ) 2027 # 14. run beam sample -> 2028 result = self._beam_search( 2029 input_ids, 2030 beam_scorer, 2031 logits_processor=prepared_logits_processor, 2032 logits_warper=prepared_logits_warper, 2033 stopping_criteria=prepared_stopping_criteria, 2034 generation_config=generation_config, 2035 synced_gpus=synced_gpus, 2036 **model_kwargs, 2037 ) 2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH: 2040 # 11. prepare beam search scorer 2041 beam_scorer = BeamSearchScorer( 2042 batch_size=batch_size, 2043 num_beams=generation_config.num_beams, (...) 2049 max_length=generation_config.max_length, 2050 ) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs) 3195 next_token_logits = outputs.logits[:, -1, :].clone() 3196 next_token_scores = nn.functional.log_softmax( 3197 next_token_logits, dim=-1 3198 ) # (batch_size * num_beams, vocab_size) -> 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores) 3201 if do_sample: 3202 next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed) File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs) 96 scores = processor(input_ids, scores, **kwargs) 97 else: ---> 98 scores = processor(input_ids, scores) 100 return scores File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores) 154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING) 155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor: 156 vocab_tensor = torch.arange(scores.shape[-1], device=scores.device) --> 157 eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id) 158 scores_processed = scores.clone() 159 if input_ids.shape[-1] < self.min_length: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long Notes : I did try to add device=""mps"" to the task_evaluator.compute but it gave me another error of ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.",huggingface llm evaluate  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long context  try create evaluation pipeline text summary task use huggingface evaluate package  get issue receive dtype long tensor  feed long type two column specify evaluate pipeline text  investigation look like issue root torch version mac  m1    m sure proceed   code  transformer import pipeline evaluate import evaluator dataset import loaddataset  load datum  booksum  loaddataset    kmfoda  booksum   split  validation   1000     load pipeline pipe  pipeline  task  summarization   model  pszemraj  lead  base  book  summary   device  mp    setup evaluate task use rouge taskevaluator  evaluator    summarization    code yield issue  evalresult  taskevaluatorcompute  modelorpipeline  pipe  datum  booksum  metric  rouge   inputcolumn  chapter   labelcolumn  summarytext   give value error  short error message  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long full error message                                        runtimeerror traceback  recent call last  cell  10   line 1    1 evalresult  taskevaluatorcompute  2 modelorpipeline  pipe  3 datum  booksum  4 metric  rouge   6 inputcolumn  chapter   7 labelcolumn  summarytext  8  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy191  summarizationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  166  addstartdocstring  167 evalutorcomputestartdocstring  168 taskdocumentationkwargs     189 generationkwargs  dict  none  190    tuple  dict  str  float       191 result  super   compute  192 modelorpipeline  modelorpipeline  193 datum  datum  194 subset  subset  195 split  split  196 metric  metric  197 tokenizer  tokenizer  198 strategy  strategy  199 confidencelevel  confidencelevel  200 nresample  nresample  201 device  device  202 randomstate  randomstate  203 inputcolumn  inputcolumn  204 labelcolumn  labelcolumn  205 generationkwarg  generationkwargs  206  208 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy133  text2textgenerationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  130 generationkwargs none  131 selfpipelinekwargsupdate  generationkwargs    133 result  super   compute  134 modelorpipeline  modelorpipeline  135 datum  datum  136 subset  subset  137 split  split  138 metric  metric  139 tokenizer  tokenizer  140 strategy  strategy  141 confidencelevel  confidencelevel  142 nresample  nresample  143 device  device  144 randomstate  randomstate  145 inputcolumn  inputcolumn  146 labelcolumn  labelcolumn  147  149 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy255  evaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  featureextractor  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  labelmapping  252 metric  selfpreparemetric  metric  254  compute prediction   255 prediction  perfresult  selfcallpipeline  pipe  pipeinput  256 prediction  selfpredictionsprocessor  prediction  labelmapping  258 metricinputsupdate  prediction  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy513  evaluatorcallpipeline  self  pipe   args    kwargs  511 def callpipeline  self  pipe   args    kwargs   512 starttime  perfcounter     513 pipeoutput  pipe   args    kwarg    self  pipelinekwargs  514 endtime  perfcounter   515 return pipeoutput  selfcomputetimeperf  starttime  endtime  len  pipeoutput   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy269  summarizationpipelinecall    self   args    kwargs  245 def   call    self   args    kwargs   246 r    247 summarize text   give input  248    267 id summary  268       269 return super   call     args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy167  text2textgenerationpipelinecall    self   args    kwargs  138 def   call    self   args    kwargs   139 r    140 generate output text   use text   give input  141    164 id generate text  165       167 result  super   call     args    kwargs  168  169 isinstance  args  0   list  170  isinstance  el  str  el args  0   171  len  re    1 re result  172   173 return  re  0  re result  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1235  pipelinecall    self  input  numworker  batchsize   args    kwargs  1231 canuseiterator  1232 finaliterator  selfgetiterator  1233 input  numworker  batchsize  preprocessparam  forwardparams  postprocessparams 1234    1235 output  list  finaliterator  1236 return output 1237 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy124  pipelineiteratornext    self  121 return selfloaderbatchitem   123  be item within batch   124 item  next  selfiterator  125 process  selfinf  item    selfparam  126  batch   infer thing   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy125  pipelineiteratornext    self  123  be item within batch 124 item  next  selfiterator    125 process  selfinf  item    selfparam  126  batch   infer thing   127 selfloaderbatchsize none  128  try infer size batch file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1161  pipelineforward  self  modelinput    forwardparams  1159 inferencecontext    1160 modelinput  selfensuretensorondevice  modelinput  device  selfdevice    1161 modeloutputs  selfforward  modelinput    forwardparams  1162 modeloutputs  selfensuretensorondevice  modeloutput  device  torchdevice    cpu    1163 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy191  text2textgenerationpipelineforward  self  modelinput    generatekwargs  184 inb  inputlength  tfshape  modelinput    inputids    numpy   186 selfcheckinput  187 inputlength  188 generatekwargsget    minlength   selfmodelconfigminlength   189 generatekwargsget    maxlength   selfmodelconfigmaxlength   190    191 outputid  selfmodelgenerate    modelinput    generatekwargs  192 outb  outputidsshape  0  193 selfframework     pt   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  torch  utilscontextlibpy116  contextdecorator   local  decoratecontext   args    kwargs  113  functoolswraps  func  114 def decoratecontext   args    kwargs   115 ctxfactory      116 return func   args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy2028  generationmixingenerate  self  input  generationconfig  logitsprocessor  stoppingcriteria  prefixallowedtokensfn  syncedgpus  assistantmodel  streamer  negativepromptid  negativepromptattentionmask    kwargs  2020 inputids  modelkwarg  selfexpandinputsforgeneration  2021 inputid  inputids  2022 expandsize  generationconfignumbeams  2023 isencoderdecoder  selfconfigisencoderdecoder  2024   modelkwarg  2025  2027  14  run beam sample   2028 result  selfbeamsearch  2029 inputids  2030 beamscorer  2031 logitsprocessor  preparedlogitsprocessor  2032 logitswarper  preparedlogitswarper  2033 stoppingcriteria  preparedstoppingcriteria  2034 generationconfig  generationconfig  2035 syncedgpus  syncedgpus  2036   modelkwarg  2037  2039 elif generationmode   generationmode  groupbeamsearch  2040  11  prepare beam search scorer 2041 beamscorer  beamsearchscorer  2042 batchsize  batchsize  2043 numbeam  generationconfignumbeams     2049 maxlength  generationconfigmaxlength  2050  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy3200  generationmixinbeamsearch  self  inputid  beamscorer  logitsprocessor  stoppingcriteria  generationconfig  syncedgpus  logitswarper    modelkwarg  3195 nexttokenlogits  outputslogits    1    clone   3196 nexttokenscores  nnfunctionallogsoftmax  3197 nexttokenlogits  dim1 3198    batchsize  numbeams  vocabsize    3200 nexttokenscoresprocesse  logitsprocessor  inputids  nexttokenscores  3201 dosample  3202 nexttokenscoresprocesse  logitswarper  inputids  nexttokenscoresprocesse  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy98  logitsprocessorlistcall    self  inputid  score    kwargs  96 score  processor  inputids  score    kwargs  97 else     98 score  processor  inputids  score  100 return score file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long note  try add device  mp  taskevaluatorcompute give another error valueerror  pipeline instantiate device none device  mp pass  compute  ,run similar issue try run facebook s nougat ocr tool  error message mention macos 14  sonoma  macos 13  ventura   upgrade macos 14 fix issue ,huggingface llm evaluate  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long context  try create evaluation pipeline text summary task use huggingface evaluate package  get issue receive dtype long tensor  feed long type two column specify evaluate pipeline text  investigation look like issue root torch version mac  m1    m sure proceed   code  transformer import pipeline evaluate import evaluator dataset import loaddataset  load datum  booksum  loaddataset    kmfoda  booksum   split  validation   1000     load pipeline pipe  pipeline  task  summarization   model  pszemraj  lead  base  book  summary   device  mp    setup evaluate task use rouge taskevaluator  evaluator    summarization    code yield issue  evalresult  taskevaluatorcompute  modelorpipeline  pipe  datum  booksum  metric  rouge   inputcolumn  chapter   labelcolumn  summarytext   give value error  short error message  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long full error message                                        runtimeerror traceback  recent call last  cell  10   line 1    1 evalresult  taskevaluatorcompute  2 modelorpipeline  pipe  3 datum  booksum  4 metric  rouge   6 inputcolumn  chapter   7 labelcolumn  summarytext  8  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy191  summarizationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  166  addstartdocstring  167 evalutorcomputestartdocstring  168 taskdocumentationkwargs     189 generationkwargs  dict  none  190    tuple  dict  str  float       191 result  super   compute  192 modelorpipeline  modelorpipeline  193 datum  datum  194 subset  subset  195 split  split  196 metric  metric  197 tokenizer  tokenizer  198 strategy  strategy  199 confidencelevel  confidencelevel  200 nresample  nresample  201 device  device  202 randomstate  randomstate  203 inputcolumn  inputcolumn  204 labelcolumn  labelcolumn  205 generationkwarg  generationkwargs  206  208 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  text2textgenerationpy133  text2textgenerationevaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  generationkwargs  130 generationkwargs none  131 selfpipelinekwargsupdate  generationkwargs    133 result  super   compute  134 modelorpipeline  modelorpipeline  135 datum  datum  136 subset  subset  137 split  split  138 metric  metric  139 tokenizer  tokenizer  140 strategy  strategy  141 confidencelevel  confidencelevel  142 nresample  nresample  143 device  device  144 randomstate  randomstate  145 inputcolumn  inputcolumn  146 labelcolumn  labelcolumn  147  149 return result file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy255  evaluatorcompute  self  modelorpipeline  datum  subset  split  metric  tokenizer  featureextractor  strategy  confidencelevel  nresample  device  randomstate  inputcolumn  labelcolumn  labelmapping  252 metric  selfpreparemetric  metric  254  compute prediction   255 prediction  perfresult  selfcallpipeline  pipe  pipeinput  256 prediction  selfpredictionsprocessor  prediction  labelmapping  258 metricinputsupdate  prediction  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  evaluate  evaluator  basepy513  evaluatorcallpipeline  self  pipe   args    kwargs  511 def callpipeline  self  pipe   args    kwargs   512 starttime  perfcounter     513 pipeoutput  pipe   args    kwarg    self  pipelinekwargs  514 endtime  perfcounter   515 return pipeoutput  selfcomputetimeperf  starttime  endtime  len  pipeoutput   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy269  summarizationpipelinecall    self   args    kwargs  245 def   call    self   args    kwargs   246 r    247 summarize text   give input  248    267 id summary  268       269 return super   call     args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy167  text2textgenerationpipelinecall    self   args    kwargs  138 def   call    self   args    kwargs   139 r    140 generate output text   use text   give input  141    164 id generate text  165       167 result  super   call     args    kwargs  168  169 isinstance  args  0   list  170  isinstance  el  str  el args  0   171  len  re    1 re result  172   173 return  re  0  re result  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1235  pipelinecall    self  input  numworker  batchsize   args    kwargs  1231 canuseiterator  1232 finaliterator  selfgetiterator  1233 input  numworker  batchsize  preprocessparam  forwardparams  postprocessparams 1234    1235 output  list  finaliterator  1236 return output 1237 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy124  pipelineiteratornext    self  121 return selfloaderbatchitem   123  be item within batch   124 item  next  selfiterator  125 process  selfinf  item    selfparam  126  batch   infer thing   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  ptutilspy125  pipelineiteratornext    self  123  be item within batch 124 item  next  selfiterator    125 process  selfinf  item    selfparam  126  batch   infer thing   127 selfloaderbatchsize none  128  try infer size batch file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  basepy1161  pipelineforward  self  modelinput    forwardparams  1159 inferencecontext    1160 modelinput  selfensuretensorondevice  modelinput  device  selfdevice    1161 modeloutputs  selfforward  modelinput    forwardparams  1162 modeloutputs  selfensuretensorondevice  modeloutput  device  torchdevice    cpu    1163 else  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  pipeline  text2textgenerationpy191  text2textgenerationpipelineforward  self  modelinput    generatekwargs  184 inb  inputlength  tfshape  modelinput    inputids    numpy   186 selfcheckinput  187 inputlength  188 generatekwargsget    minlength   selfmodelconfigminlength   189 generatekwargsget    maxlength   selfmodelconfigmaxlength   190    191 outputid  selfmodelgenerate    modelinput    generatekwargs  192 outb  outputidsshape  0  193 selfframework     pt   file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  torch  utilscontextlibpy116  contextdecorator   local  decoratecontext   args    kwargs  113  functoolswraps  func  114 def decoratecontext   args    kwargs   115 ctxfactory      116 return func   args    kwargs  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy2028  generationmixingenerate  self  input  generationconfig  logitsprocessor  stoppingcriteria  prefixallowedtokensfn  syncedgpus  assistantmodel  streamer  negativepromptid  negativepromptattentionmask    kwargs  2020 inputids  modelkwarg  selfexpandinputsforgeneration  2021 inputid  inputids  2022 expandsize  generationconfignumbeams  2023 isencoderdecoder  selfconfigisencoderdecoder  2024   modelkwarg  2025  2027  14  run beam sample   2028 result  selfbeamsearch  2029 inputids  2030 beamscorer  2031 logitsprocessor  preparedlogitsprocessor  2032 logitswarper  preparedlogitswarper  2033 stoppingcriteria  preparedstoppingcriteria  2034 generationconfig  generationconfig  2035 syncedgpus  syncedgpus  2036   modelkwarg  2037  2039 elif generationmode   generationmode  groupbeamsearch  2040  11  prepare beam search scorer 2041 beamscorer  beamsearchscorer  2042 batchsize  batchsize  2043 numbeam  generationconfignumbeams     2049 maxlength  generationconfigmaxlength  2050  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  utilspy3200  generationmixinbeamsearch  self  inputid  beamscorer  logitsprocessor  stoppingcriteria  generationconfig  syncedgpus  logitswarper    modelkwarg  3195 nexttokenlogits  outputslogits    1    clone   3196 nexttokenscores  nnfunctionallogsoftmax  3197 nexttokenlogits  dim1 3198    batchsize  numbeams  vocabsize    3200 nexttokenscoresprocesse  logitsprocessor  inputids  nexttokenscores  3201 dosample  3202 nexttokenscoresprocesse  logitswarper  inputids  nexttokenscoresprocesse  file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy98  logitsprocessorlistcall    self  inputid  score    kwargs  96 score  processor  inputids  score    kwargs  97 else     98 score  processor  inputids  score  100 return score file pyenv  versions3120  envs  llm  aug  lib  python312  site  package  transformer  generation  logitsprocesspy157  minlengthlogitsprocessorcall    self  inputid  score  154  addstartdocstring  logitsprocessorinputsdocstring  155 def   call    self  inputid  torch  longtensor  score  torch  floattensor    torch  floattensor  156 vocabtensor  torcharange  scoresshape  1   device  scoresdevice    157 eostokenmask  torchisin  vocabtensor  selfeostokenid  158 scoresprocesse  scoresclone   159 inputidsshape  1   selfminlength  runtimeerror  isintensortensorout work float type mps pre macos140  receive dtype  long note  try add device  mp  taskevaluatorcompute give another error valueerror  pipeline instantiate device none device  mp pass  compute   run similar issue try run facebook s nougat ocr tool  error message mention macos 14  sonoma  macos 13  ventura   upgrade macos 14 fix issue ,Basic Understanding
ImportError: cannot import name &#39;pre_init&#39; from &#39;langchain_core.utils&#39;,"When I want to install langchain libraries from requirements.txt I'm getting ImportError: cannot import name 'pre_init' from 'langchain_core.utils' I've tried to install libraries from terminal using these commands : pip install gigachain pip install gigachat pip install -U langchain-community and it's working, so I used pip freeze And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help","['python', 'pip', 'nlp', 'langchain', 'large-language-model']",1,"Your import error suggests that the langchain-core module has not been installed. You can confirm whether this is the case by checking the output from the following command: pip show langchain-core If it hasn't been installed, ensure it is installed via the following command: pip install langchain-core As for using a requirements.txt file, simply creating it doesn't automatically install the packages listed within. You still have to run the following command in your terminal to install the listed packages: python -m pip install -r /path/to/requirements.txt Finally, you don't have to manually create the requirements.txt file by coping and pasting the output from pip freeze . You can simply run: python -m pip freeze > /path/to/requirements.txt You can read more about requirement files under the requirements files section of the pip user guide.",2024-07-29 22:59:13,2024-07-29 23:00:42,1812,https://stackoverflow.com/questions/78809281/importerror-cannot-import-name-pre-init-from-langchain-core-utils,"ImportError: cannot import name &#39;pre_init&#39; from &#39;langchain_core.utils&#39; When I want to install langchain libraries from requirements.txt I'm getting ImportError: cannot import name 'pre_init' from 'langchain_core.utils' I've tried to install libraries from terminal using these commands : pip install gigachain pip install gigachat pip install -U langchain-community and it's working, so I used pip freeze And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help",importerror  import name   39  preinit   39    39  langchaincoreutils   39  want install langchain library requirementstxt  m get importerror  import name  preinit   langchaincoreutils   ve try install library terminal use command  pip install gigachain pip install gigachat pip install u langchain  community s working  use pip freeze pasted library terminal requirementstxt  not work  would nice someone could help,import error suggest langchain  core module instal  confirm whether case check output follow command  pip show langchain  core not instal  ensure instal via follow command  pip install langchain  core use requirementstxt file  simply creating not automatically install package list within  still run follow command terminal install list package  python m pip install r path  to  requirementstxt finally  not manually create requirementstxt file cope paste output pip freeze  simply run  python m pip freeze  path  to  requirementstxt read requirement file requirement file section pip user guide ,importerror  import name   39  preinit   39    39  langchaincoreutils   39  want install langchain library requirementstxt  m get importerror  import name  preinit   langchaincoreutils   ve try install library terminal use command  pip install gigachain pip install gigachat pip install u langchain  community s working  use pip freeze pasted library terminal requirementstxt  not work  would nice someone could help import error suggest langchain  core module instal  confirm whether case check output follow command  pip show langchain  core not instal  ensure instal via follow command  pip install langchain  core use requirementstxt file  simply creating not automatically install package list within  still run follow command terminal install list package  python m pip install r path  to  requirementstxt finally  not manually create requirementstxt file cope paste output pip freeze  simply run  python m pip freeze  path  to  requirementstxt read requirement file requirement file section pip user guide ,Implementation Issues
Do I need to use Named Entity Recognition (NER) in tokenization?,"I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the documentation , I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching. The thing I am trying to understand is how to embody it ( if I should ) in my tokenization process. I am giving an example. text = ""Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it. A significant portion of Apple's user base wasn't able to use it even if they wanted to. As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased."" sentence = sp(text) # sp = spacy.load('en_core_web_sm') for word in sentence: print(word.text) # Let # 's # not # forget # that # Apple # Pay # in # etc... for word in sentence.ents: print(word.text + "" _ "" + word.label_ + "" _ "" + str(spacy.explain(word.label_))) # Apple Pay _ ORG _ Companies, agencies, institutions, etc. # 2014 _ DATE _ Absolute or relative dates or periods # iPhone _ ORG _ Companies, agencies, institutions, etc. # Apple _ ORG _ Companies, agencies, institutions, etc. # iPhones _ ORG _ Companies, agencies, institutions, etc. The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) ""type"" of tokenization? My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').","['python', 'python-3.x', 'nlp', 'spacy', 'named-entity-recognition']",1,"Tokenization typically is the splitting of a sentence into words or even subwords. I am not sure what you later plan to do with the data, but it is a convention in NLP to stick to either the document level, sentence level or word/token level. Having some mix of token and n-gram level (like [""Apple Pay"", ""required"", ""an"", ""iPhone"", ""to"", ""use"", ""it"", "".""] in my opinion will not help you in most later use cases. If you later train a classifier (assuming you're talking about fine-tuning a transformer based language model on a token classification task) would then use something like the IOB format to handle n-grams, e.g. like so: Token Label Apple B Pay I required O an O iPhone B to O use O it O . O Of course this depends on your application and directly merging to n-grams might work well for you. If you have some application where you are searching for frequent n-grams, you could use collocation metrics to extract those n-grams, e.g. using NLTK's CollocationFinder . Or as you mentioned use SpaCy either for noun chunk extraction or named entity recognition . For the latter one, you could access the token level ent_type_ and ent_iob_ attributes to iterate over the tokens in the processed docs once and then merge these n-grams together based on their IOB-tags.",2024-07-22 13:28:14,2024-07-22 21:24:34,384,https://stackoverflow.com/questions/78778988/do-i-need-to-use-named-entity-recognition-ner-in-tokenization,"Do I need to use Named Entity Recognition (NER) in tokenization? I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the documentation , I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching. The thing I am trying to understand is how to embody it ( if I should ) in my tokenization process. I am giving an example. text = ""Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it. A significant portion of Apple's user base wasn't able to use it even if they wanted to. As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased."" sentence = sp(text) # sp = spacy.load('en_core_web_sm') for word in sentence: print(word.text) # Let # 's # not # forget # that # Apple # Pay # in # etc... for word in sentence.ents: print(word.text + "" _ "" + word.label_ + "" _ "" + str(spacy.explain(word.label_))) # Apple Pay _ ORG _ Companies, agencies, institutions, etc. # 2014 _ DATE _ Absolute or relative dates or periods # iPhone _ ORG _ Companies, agencies, institutions, etc. # Apple _ ORG _ Companies, agencies, institutions, etc. # iPhones _ ORG _ Companies, agencies, institutions, etc. The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) ""type"" of tokenization? My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').",need use name entity recognition  ner  tokenization  work nlp project sentiment analysis  use spacy tokenize sentence  read documentation  learn ner   ve read use extract entity text aid user s search  thing try understand embody   tokenization process  give example  text    let us forget apple pay 2014 require brand new iphone order use  significant portion apple s user base not able use even want  successive iphone incorporate technology old iphone replace number people could use technology increase   sentence  sp  text   sp  spacyload   encorewebsm   word sentence  print  wordtext   let  s   forget   apple  pay   etc  word sentenceent  print  wordtext        wordlabel         str  spacyexplain  wordlabel      apple pay  org  company  agency  institution  etc   2014  date  absolute relative date period  iphone  org  company  agency  institution  etc   apple  org  company  agency  institution  etc   iphones  org  company  agency  institution  etc  first loops show  apple   pay  different token  printing discover entity second loop  understand  apply pay  org  yes  could achieve  let us say    type  tokenization  think  not  apple   pay  tokenized single word together  create classifier recognize entity recognize fruit   apple   verb   pay   ,tokenization typically split sentence word even subword  sure later plan datum  convention nlp stick either document level  sentence level word  token level  mix token n  gram level  like    apple pay     require         iphone         use            opinion help later use case  later train classifier  assume be talk fine  tuning transformer base language model token classification task  would use something like iob format handle n  gram  eg  like  token label apple b pay require iphone b use  course depend application directly merge n  gram might work well  application search frequent n  gram  could use collocation metric extract n  gram  eg  use nltk s collocationfinder  mention use spacy either noun chunk extraction name entity recognition  latter one  could access token level enttype  entiob  attribute iterate token process docs merge n  gram together base iob  tag ,need use name entity recognition  ner  tokenization  work nlp project sentiment analysis  use spacy tokenize sentence  read documentation  learn ner   ve read use extract entity text aid user s search  thing try understand embody   tokenization process  give example  text    let us forget apple pay 2014 require brand new iphone order use  significant portion apple s user base not able use even want  successive iphone incorporate technology old iphone replace number people could use technology increase   sentence  sp  text   sp  spacyload   encorewebsm   word sentence  print  wordtext   let  s   forget   apple  pay   etc  word sentenceent  print  wordtext        wordlabel         str  spacyexplain  wordlabel      apple pay  org  company  agency  institution  etc   2014  date  absolute relative date period  iphone  org  company  agency  institution  etc   apple  org  company  agency  institution  etc   iphones  org  company  agency  institution  etc  first loops show  apple   pay  different token  printing discover entity second loop  understand  apply pay  org  yes  could achieve  let us say    type  tokenization  think  not  apple   pay  tokenized single word together  create classifier recognize entity recognize fruit   apple   verb   pay    tokenization typically split sentence word even subword  sure later plan datum  convention nlp stick either document level  sentence level word  token level  mix token n  gram level  like    apple pay     require         iphone         use            opinion help later use case  later train classifier  assume be talk fine  tuning transformer base language model token classification task  would use something like iob format handle n  gram  eg  like  token label apple b pay require iphone b use  course depend application directly merge n  gram might work well  application search frequent n  gram  could use collocation metric extract n  gram  eg  use nltk s collocationfinder  mention use spacy either noun chunk extraction name entity recognition  latter one  could access token level enttype  entiob  attribute iterate token process docs merge n  gram together base iob  tag ,Implementation Issues
"IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface","i am trying to learn on how to fine tune a pretrained model and use it. this is my code from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) now, i am trying to make a prediction on the fine tuned model, like this inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"") predictions = trainer.predict(test_dataset=inputs) i am getting this error when i am trying to make a prediction, IndexError Traceback (most recent call last) Cell In[8], line 7 3 inputs=tokenizer(dataset[0][‘text’], padding=“max_length”, truncation=True,return_tensors=“pt”) 6 # Make predictions ----> 7 predictions = trainer.predict(test_dataset=inputs) File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix) 3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305 output = eval_loop( 3306 test_dataloader, description=“Prediction”, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 ) 3308 total_batch_size = self.args.eval_batch_size * self.args.world_size 3309 if f""{metric_key_prefix}_jit_compilation_time"" in output.metrics: File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix) 3406 observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step, inputs in enumerate(dataloader): 3409 # Update the observed num examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if observed_batch_size is not None: File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check when we are at the end 453 try: → 454 current_batch = next(dataloader_iter) 455 except StopIteration: 456 yield File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in _BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] → 631 data = self._next_data() 632 self._num_yielded += 1 633 if self._dataset_kind == _DatasetKind.Iterable and 634 self._IterableDataset_len_called is not None and 635 self._num_yielded self._IterableDataset_len_called: File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in _SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 676 if self._pin_memory: 677 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device) File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in (.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255, in BatchEncoding.getitem(self, item) 253 return self.data[item] 254 elif self._encodings is not None: → 255 return self._encodings[item] 256 elif isinstance(item, slice): 257 return {key: self.data[key][item] for key in self.data.keys()} IndexError: list index out of range","['nlp', 'huggingface-transformers', 'huggingface', 'fine-tuning']",1,"The error you are encountering is because the trainer.predict method expects a dataset as input, but you are passing a single example that has been tokenized into tensors. To perform predictions on a single input, you need to prepare it similarly to how the dataset was prepared before training, and then use the model directly for prediction. Here's how you can modify your code to make predictions on a single input: Prepare the input correctly Use the model directly for prediction Here's the revised code: from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) # Make a prediction on a single input inputs = tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True, return_tensors=""pt"") model.eval() # Set the model to evaluation mode with torch.no_grad(): # Disable gradient calculation outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=-1) print(f""Predicted label: {predictions.item()}"")",2024-07-20 19:44:43,2024-07-21 11:40:54,115,https://stackoverflow.com/questions/78773758/indexerror-list-index-out-of-range-when-trying-to-predict-from-the-fine-tuned,"IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface i am trying to learn on how to fine tune a pretrained model and use it. this is my code from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer from datasets import load_dataset import numpy as np import torch # Define a simple accuracy metric def compute_metrics(p): predictions, labels = p preds = np.argmax(predictions, axis=1) return {""accuracy"": (preds == labels).mean()} # Load the dataset dataset = load_dataset(""imdb"", split='train[:1%]') small_train_dataset = dataset.train_test_split(test_size=0.1)['train'] small_eval_dataset = dataset.train_test_split(test_size=0.1)['test'] # Load the tokenizer and model model_name = ""bert-base-uncased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize the dataset def tokenize_function(examples): return tokenizer(examples['text'], padding=""max_length"", truncation=True) small_train_dataset = small_train_dataset.map(tokenize_function, batched=True) small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True) small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"") small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"") small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""]) # Define training arguments training_args = TrainingArguments( output_dir=""test_trainer"", evaluation_strategy=""epoch"", per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01 ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics ) # Train the model trainer.train() # Evaluate the model validation_results = trainer.evaluate() print(validation_results) now, i am trying to make a prediction on the fine tuned model, like this inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"") predictions = trainer.predict(test_dataset=inputs) i am getting this error when i am trying to make a prediction, IndexError Traceback (most recent call last) Cell In[8], line 7 3 inputs=tokenizer(dataset[0][‘text’], padding=“max_length”, truncation=True,return_tensors=“pt”) 6 # Make predictions ----> 7 predictions = trainer.predict(test_dataset=inputs) File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix) 3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305 output = eval_loop( 3306 test_dataloader, description=“Prediction”, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 ) 3308 total_batch_size = self.args.eval_batch_size * self.args.world_size 3309 if f""{metric_key_prefix}_jit_compilation_time"" in output.metrics: File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix) 3406 observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step, inputs in enumerate(dataloader): 3409 # Update the observed num examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if observed_batch_size is not None: File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check when we are at the end 453 try: → 454 current_batch = next(dataloader_iter) 455 except StopIteration: 456 yield File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in _BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] → 631 data = self._next_data() 632 self._num_yielded += 1 633 if self._dataset_kind == _DatasetKind.Iterable and 634 self._IterableDataset_len_called is not None and 635 self._num_yielded self._IterableDataset_len_called: File C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in _SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 676 if self._pin_memory: 677 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device) File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in (.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data = self.dataset[possibly_batched_index] File C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255, in BatchEncoding.getitem(self, item) 253 return self.data[item] 254 elif self._encodings is not None: → 255 return self._encodings[item] 256 elif isinstance(item, slice): 257 return {key: self.data[key][item] for key in self.data.keys()} IndexError: list index out of range",indexerror  list index range  try predict fine tuned model use hugginface try learn fine tune pretraine model use  code transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   try make prediction fine tune model  like input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   prediction  trainerpredict  testdataset  input  get error try make prediction  indexerror traceback  recent call last  cell  8   line 7 3 input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   6  make prediction    7 prediction  trainerpredict  testdataset  input  file c  python311libsite  packagestransformerstrainerpy3305  trainerpredict  self  testdataset  ignorekeys  metrickeyprefix  3302 starttime  timetime   3304 evalloop  selfpredictionloop selfargsuselegacypredictionloop else selfevaluationloop  3305 output  evalloop  3306 testdataloader  description  prediction   ignorekey  ignorekey  metrickeyprefix  metrickeyprefix 3307  3308 totalbatchsize  selfargsevalbatchsize  selfargsworldsize 3309 f   metrickeyprefix   jitcompilationtime  outputmetric  file c  python311libsite  packagestransformerstrainerpy3408  trainerevaluationloop  self  dataloader  description  predictionlossonly  ignorekey  metrickeyprefix  3406 observednumexample  0 3407  main evaluation loop  3408 step  input enumerate  dataloader   3409  update observe num example 3410 observedbatchsize  findbatchsize  input  3411 observedbatchsize none  file c  python311libsite  packagesacceleratedataloaderpy454  dataloadersharditer  self  452  iterate one batch ahead check end 453 try   454 currentbatch  next  dataloaderiter  455 except stopiteration  456 yield file c  python311libsite  packagestorchutilsdatadataloaderpy631   basedataloaderiternext  self  628 selfsamplerit none  629  todo  bug dataloader iterator find mypy  issue  76750  pytorch  pytorch  github  630 selfreset    type  ignore  call  arg   631 datum  selfnextdata   632 selfnumyielde   1 633 selfdatasetkind    datasetkind  iterable 634 selfiterabledatasetlencalle none 635 selfnumyielded selfiterabledatasetlencalle  file c  python311libsite  packagestorchutilsdatadataloaderpy675   singleprocessdataloaderiternextdata  self  673 def  nextdata  self   674 index  selfnextindex    may raise stopiteration  675 datum  selfdatasetfetcherfetch  index   may raise stopiteration 676 selfpinmemory  677 datum   utilspinmemorypinmemory  datum  selfpinmemorydevice  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   mapdatasetfetcherfetch  self  possiblybatchedindex  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   0  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestransformerstokenizationutilsbasepy255  batchencodinggetitem  self  item  253 return selfdata  item  254 elif selfencoding none   255 return selfencoding  item  256 elif isinstance  item  slice   257 return  key  selfdata  key   item  key selfdatakey    indexerror  list index range,error encounter trainerpredict method expect dataset input  pass single example tokenized tensor  perform prediction single input  need prepare similarly dataset prepared training  use model directly prediction  s modify code make prediction single input  prepare input correctly use model directly prediction s revise code  transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   make prediction single input input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   modeleval    set model evaluation mode torchnograd     disable gradient calculation output  model    input  prediction  torchargmax  outputslogit  dim1  print  f  predict label   predictionsitem     ,indexerror  list index range  try predict fine tuned model use hugginface try learn fine tune pretraine model use  code transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   try make prediction fine tune model  like input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   prediction  trainerpredict  testdataset  input  get error try make prediction  indexerror traceback  recent call last  cell  8   line 7 3 input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   6  make prediction    7 prediction  trainerpredict  testdataset  input  file c  python311libsite  packagestransformerstrainerpy3305  trainerpredict  self  testdataset  ignorekeys  metrickeyprefix  3302 starttime  timetime   3304 evalloop  selfpredictionloop selfargsuselegacypredictionloop else selfevaluationloop  3305 output  evalloop  3306 testdataloader  description  prediction   ignorekey  ignorekey  metrickeyprefix  metrickeyprefix 3307  3308 totalbatchsize  selfargsevalbatchsize  selfargsworldsize 3309 f   metrickeyprefix   jitcompilationtime  outputmetric  file c  python311libsite  packagestransformerstrainerpy3408  trainerevaluationloop  self  dataloader  description  predictionlossonly  ignorekey  metrickeyprefix  3406 observednumexample  0 3407  main evaluation loop  3408 step  input enumerate  dataloader   3409  update observe num example 3410 observedbatchsize  findbatchsize  input  3411 observedbatchsize none  file c  python311libsite  packagesacceleratedataloaderpy454  dataloadersharditer  self  452  iterate one batch ahead check end 453 try   454 currentbatch  next  dataloaderiter  455 except stopiteration  456 yield file c  python311libsite  packagestorchutilsdatadataloaderpy631   basedataloaderiternext  self  628 selfsamplerit none  629  todo  bug dataloader iterator find mypy  issue  76750  pytorch  pytorch  github  630 selfreset    type  ignore  call  arg   631 datum  selfnextdata   632 selfnumyielde   1 633 selfdatasetkind    datasetkind  iterable 634 selfiterabledatasetlencalle none 635 selfnumyielded selfiterabledatasetlencalle  file c  python311libsite  packagestorchutilsdatadataloaderpy675   singleprocessdataloaderiternextdata  self  673 def  nextdata  self   674 index  selfnextindex    may raise stopiteration  675 datum  selfdatasetfetcherfetch  index   may raise stopiteration 676 selfpinmemory  677 datum   utilspinmemorypinmemory  datum  selfpinmemorydevice  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   mapdatasetfetcherfetch  self  possiblybatchedindex  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestorchutilsdatautilsfetchpy51   0  49 datum  selfdatasetgetitem  possiblybatchedindex  50 else    51 datum   selfdataset  idx  idx possiblybatchedindex  52 else  53 datum  selfdataset  possiblybatchedindex  file c  python311libsite  packagestransformerstokenizationutilsbasepy255  batchencodinggetitem  self  item  253 return selfdata  item  254 elif selfencoding none   255 return selfencoding  item  256 elif isinstance  item  slice   257 return  key  selfdata  key   item  key selfdatakey    indexerror  list index range error encounter trainerpredict method expect dataset input  pass single example tokenized tensor  perform prediction single input  need prepare similarly dataset prepared training  use model directly prediction  s modify code make prediction single input  prepare input correctly use model directly prediction s revise code  transformer import automodelforsequenceclassification  autotokenizer  trainingarguments  trainer dataset import loaddataset import numpy np import torch  define simple accuracy metric def computemetric  p   prediction  label  p pred  npargmax  prediction  axis1  return    accuracy    pred   label  mean     load dataset dataset  loaddataset    imdb   splittrain  1     smalltraindataset  datasettraintestsplit  testsize01    train   smallevaldataset  datasettraintestsplit  testsize01    test    load tokenizer model modelname    bert  base  uncase  tokenizer  autotokenizerfrompretraine  modelname  model  automodelforsequenceclassificationfrompretrained  modelname  numlabels2   tokenize dataset def tokenizefunction  example   return tokenizer  example   text    padding  maxlength   truncation  true  smalltraindataset  smalltraindatasetmap  tokenizefunction  batch  true  smallevaldataset  smallevaldatasetmap  tokenizefunction  batch  true  smalltraindataset  smalltraindatasetrenamecolumn    label     label   smallevaldataset  smallevaldatasetrenamecolumn    label     label   smalltraindatasetsetformat    torch   columns    inputids     attentionmask     label    smallevaldatasetsetformat    torch   columns    inputids     attentionmask     label     define training argument trainingargs  trainingarguments  outputdir  testtrainer   evaluationstrategy  epoch   perdevicetrainbatchsize8  perdeviceevalbatchsize8  numtrainepochs3  weightdecay001   initialize trainer trainer  trainer  model  model  arg  trainingargs  traindataset  smalltraindataset  evaldataset  smallevaldataset  computemetric  computemetric   train model trainertrain    evaluate model validationresult  trainerevaluate   print  validationresult   make prediction single input input  tokenizer  dataset  0    text    padding  maxlength   truncation  true  returntensors  pt   modeleval    set model evaluation mode torchnograd     disable gradient calculation output  model    input  prediction  torchargmax  outputslogit  dim1  print  f  predict label   predictionsitem     ,Task-Specific Queries
not able END function using &quot;add_conditional_edges&quot; in lang graph,"this is my code: import os from dotenv import load_dotenv load_ dotenv() from langchain_openai import ChatOpenAI from langgraph.graph import StateGraph, END from langgraph.graph import Graph, MessagesState from typing import Annotated, Any, Dict, Optional, List,Sequence, TypedDict from langgraph.graph.message import add_messages class AgentState(TypedDict): # The `add_messages` function within the annotation defines # *how* updates should be merged into the state. messages: Annotated[list, add_messages] def function1(state): return {""messages"": ""Hi""} def function2(state): return {""messages"": ""Hello""} def my_condition(state): return ""end"" workflow=StateGraph(AgentState) workflow.add_node(""agent"", function1) workflow.add_node(""tool"", function2) workflow.add_edge('agent','tool') workflow.set_entry_point(""agent"") workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) app=workflow.compile() print(app.invoke({""messages"": ""tell me about you""})) In above code I want to END function at ""function1"" and get this result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463')]} But I am getting this below result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463'), HumanMessage(content='Hello', id='7ea9ab2a-635f-46eb-8f17-d9a6af79688e')]}","['nlp', 'langchain', 'langgraph']",1,"Edges tell lang graph where to look after a node. If it's ""conditional edges,"" it will go to different next nodes in different conditions. If it's just a normal edge, it is a direct line from that node to the next node. In your code, you are adding both conditional edges and a normal edge from the same ""agent"" node. workflow.add_edge('agent','tool') ... workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) You need to decide, after the ""agent"" function is called, which thing do you want to happen next? If you want to reach END, you could put the conditional edges from ""tool"" instead of ""agent,"" which already has an edge, and all of the functions will be called.",2024-07-05 07:29:18,2024-07-06 02:56:30,1990,https://stackoverflow.com/questions/78710185/not-able-end-function-using-add-conditional-edges-in-lang-graph,"not able END function using &quot;add_conditional_edges&quot; in lang graph this is my code: import os from dotenv import load_dotenv load_ dotenv() from langchain_openai import ChatOpenAI from langgraph.graph import StateGraph, END from langgraph.graph import Graph, MessagesState from typing import Annotated, Any, Dict, Optional, List,Sequence, TypedDict from langgraph.graph.message import add_messages class AgentState(TypedDict): # The `add_messages` function within the annotation defines # *how* updates should be merged into the state. messages: Annotated[list, add_messages] def function1(state): return {""messages"": ""Hi""} def function2(state): return {""messages"": ""Hello""} def my_condition(state): return ""end"" workflow=StateGraph(AgentState) workflow.add_node(""agent"", function1) workflow.add_node(""tool"", function2) workflow.add_edge('agent','tool') workflow.set_entry_point(""agent"") workflow.add_conditional_edges(""agent"", my_condition,{ ""end"": END}) app=workflow.compile() print(app.invoke({""messages"": ""tell me about you""})) In above code I want to END function at ""function1"" and get this result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463')]} But I am getting this below result: {'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463'), HumanMessage(content='Hello', id='7ea9ab2a-635f-46eb-8f17-d9a6af79688e')]}",able end function use  quot  addconditionaledge  quot  lang graph code  import os dotenv import loaddotenv load  dotenv   langchainopenai import chatopenai langgraphgraph import stategraph  end langgraphgraph import graph  messagesstate type import annotated   dict  optional  list  sequence  typeddict langgraphgraphmessage import addmessage class agentstate  typeddict     addmessage  function within annotation define    update merge state  message  annotated  list  addmessage  def function1  state   return    message     hi   def function2  state   return    message     hello   def mycondition  state   return   end  workflow  stategraph  agentstate  workflowaddnode    agent   function1  workflowaddnode    tool   function2  workflowaddedge   agent    tool   workflowsetentrypoint    agent   workflowaddconditionaledge    agent   mycondition     end   end   app  workflowcompile   print  appinvoke     message     tell     code want end function   function1  get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463     get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463    humanmessage  contenthello   id7ea9ab2a635f46eb8f17  d9a6af79688e    ,edge tell lang graph look node  s   conditional edge   go different next node different condition  s normal edge  direct line node next node  code  add conditional edge normal edge   agent  node  workflowaddedge   agent    tool    workflowaddconditionaledge    agent   mycondition     end   end   need decide    agent  function call  thing want happen next  want reach end  could put conditional edge   tool  instead   agent   already edge  function call ,able end function use  quot  addconditionaledge  quot  lang graph code  import os dotenv import loaddotenv load  dotenv   langchainopenai import chatopenai langgraphgraph import stategraph  end langgraphgraph import graph  messagesstate type import annotated   dict  optional  list  sequence  typeddict langgraphgraphmessage import addmessage class agentstate  typeddict     addmessage  function within annotation define    update merge state  message  annotated  list  addmessage  def function1  state   return    message     hi   def function2  state   return    message     hello   def mycondition  state   return   end  workflow  stategraph  agentstate  workflowaddnode    agent   function1  workflowaddnode    tool   function2  workflowaddedge   agent    tool   workflowsetentrypoint    agent   workflowaddconditionaledge    agent   mycondition     end   end   app  workflowcompile   print  appinvoke     message     tell     code want end function   function1  get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463     get result    message    humanmessage  contenttell   id70a7cb55  4cb2  4d0b9623  79cb06bcabf3    humanmessage  contenthi   idd95bd56d93b6  44b1  ae05  3449472d8463    humanmessage  contenthello   id7ea9ab2a635f46eb8f17  d9a6af79688e     edge tell lang graph look node  s   conditional edge   go different next node different condition  s normal edge  direct line node next node  code  add conditional edge normal edge   agent  node  workflowaddedge   agent    tool    workflowaddconditionaledge    agent   mycondition     end   end   need decide    agent  function call  thing want happen next  want reach end  could put conditional edge   tool  instead   agent   already edge  function call ,Basic Understanding
How to make dynamic API calls based on user input in a Gemini application python nlp?,"I'm working on a Gemini application where I need to make dynamic API calls based on user input. Specifically, I want to perform different API requests depending on the user's query. For example, if the user asks for the latest news, the application should make an API call to a news service. Similarly, if the user wants to know the current weather, the application should fetch data from a weather API. Here's a basic outline of what I'm trying to achieve: Capture the user's input. Determine the type of request based on the input (e.g., news or weather). Make the appropriate API call and return the data to the user. I want to avoid using third-party libraries like RAG for this purpose. How can I implement this functionality in a clean and efficient way within my Gemini application? And i dont want to use a approach like this ' def handle_user_input(user_input): if ""news"" in user_input: # Call news API pass elif ""weather"" in user_input: # Call weather API pass else: return ""I can't handle that request.""","['python', 'nlp', 'request', 'google-cloud-vertex-ai', 'google-gemini']",1,"I suggest using a dictionary, basically what you u are looking to do is something called a ""strategy pattern"". You want to choose a different ""strategy"" in your case api based on different input. so the way it will look is you have a dictionary {""news"": ""news-url"", ""weather"": ""weather-url""} and then your code will be very simple user_input = """" url = api_dict[user_input] # make api call and at the start of your program you need to initialize your dict. having it in this structure makes it so that you could even write a json file, parse it and have it used as your dictionary meaning you dont even need to edit code to update your available API's At the end of the day when making a dynamic program, the options will always need to be inputted somehow, worst case is within the code like the if that you successfully understood is bad, as it makes the code long, unreadable and hard to modify. An improvement like I suggested is creating that dynamic options map as a file or a python dictionary on the start of the program and using that. the most dynamic option would be if the code is able to generate different responses to different input purely by the input. such an example would be gemini and chat-gpt where you can ask the some api route both ""what is the weather in france"" and ""how much is 4 + 4"" and it will provide a fitting answer (although not always correct) EDIT (based on last comment) Now I understand your question better. I cant say I understand your reasoning but the implementation is very simple, here is a chat I had with: chatGPT here are a few api routes http://test/weather http://test/stocks http://test/facts in the next message I will put user input and based on the input you need to output the appropriate url ChatGPT Got it! Please provide the user input in your next message, and I'll give you the appropriate URL. what should I wear tomorrow ChatGPT For the query ""what should I wear tomorrow,"" the appropriate URL is: http://test/weather you will need chatGPT to alaways know the context for your URLs, meaning always provide or from a longer session have it available. then you query with the user input and get your URL as a response key issues: chat gpt makes stuff up, it can easily make stuff up, best case it will provide no url result for such a case, but im sure some edge cases will make it generate a url that doesnt exist in your context expansive, using chat gpt to process user input simply to make a http request is expansive, it means a lot of requests, a lot of queries, a big context window (tokens) depending on your url count and input length. whatever you are building will be quite expansive to run",2024-07-04 15:11:18,2024-07-04 16:47:22,458,https://stackoverflow.com/questions/78707861/how-to-make-dynamic-api-calls-based-on-user-input-in-a-gemini-application-python,"How to make dynamic API calls based on user input in a Gemini application python nlp? I'm working on a Gemini application where I need to make dynamic API calls based on user input. Specifically, I want to perform different API requests depending on the user's query. For example, if the user asks for the latest news, the application should make an API call to a news service. Similarly, if the user wants to know the current weather, the application should fetch data from a weather API. Here's a basic outline of what I'm trying to achieve: Capture the user's input. Determine the type of request based on the input (e.g., news or weather). Make the appropriate API call and return the data to the user. I want to avoid using third-party libraries like RAG for this purpose. How can I implement this functionality in a clean and efficient way within my Gemini application? And i dont want to use a approach like this ' def handle_user_input(user_input): if ""news"" in user_input: # Call news API pass elif ""weather"" in user_input: # Call weather API pass else: return ""I can't handle that request.""",make dynamic api call base user input gemini application python nlp   m work gemini application need make dynamic api call base user input  specifically  want perform different api request depend user s query  example  user ask late news  application make api call news service  similarly  user want know current weather  application fetch datum weather api  s basic outline  m try achieve  capture user s input  determine type request base input  eg  news weather   make appropriate api call return datum user  want avoid use third  party library like rag purpose  implement functionality clean efficient way within gemini application  do not want use approach like  def handleuserinput  userinput     news  userinput   call news api pass elif   weather  userinput   call weather api pass else  return   can not handle request  ,suggest use dictionary  basically u look something call   strategy pattern   want choose different   strategy  case api base different input  way look dictionary    news     news  url     weather     weather  url   code simple userinput     url  apidict  userinput   make api call start program need initialize dict  structure make could even write json file  parse use dictionary meaning do not even need edit code update available api s end day make dynamic program  option always need inputte somehow  bad case within code like successfully understand bad  make code long  unreadable hard modify  improvement like suggest create dynamic option map file python dictionary start program use  dynamic option would code able generate different response different input purely input  example would gemini chat  gpt ask api route   weather france    much 4  4  provide fitting answer  although always correct  edit  base last comment  understand question well  can not say understand reasoning implementation simple  chat  chatgpt api route http  test  weather http  test  stock http  test  fact next message put user input base input need output appropriate url chatgpt got  please provide user input next message  will give appropriate url  wear tomorrow chatgpt query   wear tomorrow   appropriate url  http  test  weather need chatgpt alaway know context url  meaning always provide long session available  query user input get url response key issue  chat gpt make stuff  easily make stuff  good case provide url result case  i m sure edge case make generate url do not exist context expansive  use chat gpt process user input simply make http request expansive  mean lot request  lot query  big context window  tokens  depend url count input length  whatever build quite expansive run,make dynamic api call base user input gemini application python nlp   m work gemini application need make dynamic api call base user input  specifically  want perform different api request depend user s query  example  user ask late news  application make api call news service  similarly  user want know current weather  application fetch datum weather api  s basic outline  m try achieve  capture user s input  determine type request base input  eg  news weather   make appropriate api call return datum user  want avoid use third  party library like rag purpose  implement functionality clean efficient way within gemini application  do not want use approach like  def handleuserinput  userinput     news  userinput   call news api pass elif   weather  userinput   call weather api pass else  return   can not handle request   suggest use dictionary  basically u look something call   strategy pattern   want choose different   strategy  case api base different input  way look dictionary    news     news  url     weather     weather  url   code simple userinput     url  apidict  userinput   make api call start program need initialize dict  structure make could even write json file  parse use dictionary meaning do not even need edit code update available api s end day make dynamic program  option always need inputte somehow  bad case within code like successfully understand bad  make code long  unreadable hard modify  improvement like suggest create dynamic option map file python dictionary start program use  dynamic option would code able generate different response different input purely input  example would gemini chat  gpt ask api route   weather france    much 4  4  provide fitting answer  although always correct  edit  base last comment  understand question well  can not say understand reasoning implementation simple  chat  chatgpt api route http  test  weather http  test  stock http  test  fact next message put user input base input need output appropriate url chatgpt got  please provide user input next message  will give appropriate url  wear tomorrow chatgpt query   wear tomorrow   appropriate url  http  test  weather need chatgpt alaway know context url  meaning always provide long session available  query user input get url response key issue  chat gpt make stuff  easily make stuff  good case provide url result case  i m sure edge case make generate url do not exist context expansive  use chat gpt process user input simply make http request expansive  mean lot request  lot query  big context window  tokens  depend url count input length  whatever build quite expansive run,Implementation Issues
Alternative to Receptive field in Transformers and what factors impact it,"I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting? In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images. Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?","['nlp', 'huggingface-transformers', 'receptive-field']",1,"How do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties? For modern Transformer-based Language Models (LMs), there are some empirical ""scaling laws,"" such as the Chinchilla scaling laws ( Wikipedia ), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. So far, most LMs seem to roughly follow Chinchilla scaling. There is another kind of scaling, which is closer to a ""receptive field"", that I talk about below. Do you guys have something similar in NLP? Kind of. Transformer-based LMs can be thought to have a ""receptive field"" similar to CNN layers, as the attention mechanism in the Transformer operates on a pre-defined ""context window"" or ""context length"", which is the maximum number of tokens the layer can look at (""attend to"") at any given time, similar to a CNN kernel. However, with the introduction of new positional encoding (PE) approaches, such as Rotary Positional Encoding (RoPE) , and modified attention architectures, like Sliding Window Attention (SWA) , this is not strictly accurate. Scaling in terms of ""context length"" is of much interest, but usually, it is very difficult to scale Transformers this way, because of attention being a ($\mathcal{O}(N^2)$) (O(N^2)) operation. So, usually, researchers go towards deeper architectures with more parameters (""over-parameterization"") that can allow the model to ""memorize"" as much of the large training corpus as it can (""overfitting""), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).",2024-06-29 04:58:30,2024-06-29 20:38:23,147,https://stackoverflow.com/questions/78685093/alternative-to-receptive-field-in-transformers-and-what-factors-impact-it,"Alternative to Receptive field in Transformers and what factors impact it I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting? In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images. Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?",alternative receptive field transformers factor impact two transformer network  one 3 head per attention 15 layer total second one 5 head per layer 30 layer total  give arbitrary set document  2048 token per   find  network go well use less prone overfitting  computer vision concept call    receptive field   allow we understand big small network need use  instance  cnn 120 layer cnn 70 layer  calculate receptive field understand one go perform well particular dataset image  guy something similar nlp  understand whether one architecture optimal use versus anotherhave set text document unique property ,understand whether one architecture optimal use versus another  set text document unique property  modern transformer  base language models  lms   empirical   scale law   chinchilla scale law  wikipedia   essentially say large  deep  model layer  ie  parameter tend perform well  far  lms seem roughly follow chinchilla scaling  another kind scaling  close   receptive field   talk  guy something similar nlp  kind  transformer  base lm think   receptive field  similar cnn layer  attention mechanism transformer operate pre  define   context window    context length   maximum number token layer look    attend   give time  similar cnn kernel  however  introduction new positional encoding  pe  approach  rotary positional encoding  rope   modify attention architecture  like sliding window attention  swa   strictly accurate  scale term   context length  much interest  usually  difficult scale transformers way  attention   mathcal    n2      n2   operation   usually  researcher go towards deep architecture parameter    over  parameterization   allow model   memorize  much large training corpus    overfitte    perform reasonably well  fine  tune down  stream task  least representative example training corpus  ,alternative receptive field transformers factor impact two transformer network  one 3 head per attention 15 layer total second one 5 head per layer 30 layer total  give arbitrary set document  2048 token per   find  network go well use less prone overfitting  computer vision concept call    receptive field   allow we understand big small network need use  instance  cnn 120 layer cnn 70 layer  calculate receptive field understand one go perform well particular dataset image  guy something similar nlp  understand whether one architecture optimal use versus anotherhave set text document unique property  understand whether one architecture optimal use versus another  set text document unique property  modern transformer  base language models  lms   empirical   scale law   chinchilla scale law  wikipedia   essentially say large  deep  model layer  ie  parameter tend perform well  far  lms seem roughly follow chinchilla scaling  another kind scaling  close   receptive field   talk  guy something similar nlp  kind  transformer  base lm think   receptive field  similar cnn layer  attention mechanism transformer operate pre  define   context window    context length   maximum number token layer look    attend   give time  similar cnn kernel  however  introduction new positional encoding  pe  approach  rotary positional encoding  rope   modify attention architecture  like sliding window attention  swa   strictly accurate  scale term   context length  much interest  usually  difficult scale transformers way  attention   mathcal    n2      n2   operation   usually  researcher go towards deep architecture parameter    over  parameterization   allow model   memorize  much large training corpus    overfitte    perform reasonably well  fine  tune down  stream task  least representative example training corpus  ,Implementation Issues
implement a search engine chain using tavily in langchain,"I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts: Tavily search platform Parser with the aim of converting search output data into standard LangChai documents. I wrote this code but I don't know how to change tavily output format into standard form of document: from langchain_core.documents.base import Document from langchain_community.tools.tavily_search import TavilySearchResults search = TavilySearchResults(max_results=5) class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") search_parser = PydanticOutputParser(pydantic_object=ParsedDocument) search_engine_chain = search | search_parser I would be grateful if you could help me how to change this code.","['python', 'nlp', 'search-engine', 'langchain', 'chain']",1,"I finally found the answer: class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") # Define a custom parser def custom_parser(search_results): parsed_documents = [] for result in search_results: # Adjust this line based on the actual structure of search_results parsed_document = ParsedDocument(content=result['content'], url=result['url']) document = Document(page_content=parsed_document.content, metadata={'url': parsed_document.url}) parsed_documents.append(document) return parsed_documents search_engine_chain = search | custom_parser",2024-06-24 10:01:06,2024-08-25 06:24:37,517,https://stackoverflow.com/questions/78661728/implement-a-search-engine-chain-using-tavily-in-langchain,"implement a search engine chain using tavily in langchain I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts: Tavily search platform Parser with the aim of converting search output data into standard LangChai documents. I wrote this code but I don't know how to change tavily output format into standard form of document: from langchain_core.documents.base import Document from langchain_community.tools.tavily_search import TavilySearchResults search = TavilySearchResults(max_results=5) class ParsedDocument(BaseModel): content: str = Field(description=""This refers to the content of the search."") url: str = Field(description=""This refers to the url of the search."") search_parser = PydanticOutputParser(pydantic_object=ParsedDocument) search_engine_chain = search | search_parser I would be grateful if you could help me how to change this code.",implement search engine chain use tavily langchain want implement search engine chain use tavily langchain  chain give user s query input return 5 relate document  retrieve document must content document pagecontent url correspond site metadata definition langchain documents  must use langchaincoredocumentsbase  document class define document  chain two main part  tavily search platform parser aim convert search output datum standard langchai document  write code not know change tavily output format standard form document  langchaincoredocumentsbase import document langchaincommunitytoolstavilysearch import tavilysearchresult search  tavilysearchresult  maxresults5  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search    searchparser  pydanticoutputparser  pydanticobject  parseddocument  searchenginechain  search  searchparser would grateful could help change code ,finally find answer  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search     define custom parser def custompars  searchresults   parseddocument    result searchresult   adjust line base actual structure searchresult parseddocument  parseddocument  content  result   content    url  result   url    document  document  pagecontent  parseddocumentcontent  metadata   url   parseddocumenturl   parseddocumentsappend  document  return parseddocument searchenginechain  search  customparser,implement search engine chain use tavily langchain want implement search engine chain use tavily langchain  chain give user s query input return 5 relate document  retrieve document must content document pagecontent url correspond site metadata definition langchain documents  must use langchaincoredocumentsbase  document class define document  chain two main part  tavily search platform parser aim convert search output datum standard langchai document  write code not know change tavily output format standard form document  langchaincoredocumentsbase import document langchaincommunitytoolstavilysearch import tavilysearchresult search  tavilysearchresult  maxresults5  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search    searchparser  pydanticoutputparser  pydanticobject  parseddocument  searchenginechain  search  searchparser would grateful could help change code  finally find answer  class parseddocument  basemodel   content  str  field  description  refer content search    url  str  field  description  refer url search     define custom parser def custompars  searchresults   parseddocument    result searchresult   adjust line base actual structure searchresult parseddocument  parseddocument  content  result   content    url  result   url    document  document  pagecontent  parseddocumentcontent  metadata   url   parseddocumenturl   parseddocumentsappend  document  return parseddocument searchenginechain  search  customparser,Basic Understanding
Understanding the results of Transformers Learn In Context with Gradient Descent,"I'm trying to implement this paper: https://arxiv.org/pdf/2212.07677 (Here's their code): https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually. As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?","['machine-learning', 'nlp', 'large-language-model', 'transformer-model', 'meta-learning']",1,"What data are you using in your replication? As far as I can tell this paper does not mention explicitly the parameters of the data used for the particular result you are trying to replicate. Indeed, it tests a variety of alpha values for the distributions used in figure 6. It is feasible for the loss to be low even after one step of GD if the alpha value is low. If you find the same trends in relative behavior of GD and transformer layers, I don't think it's important to match the exact loss values.",2024-06-18 20:43:45,2024-06-20 01:45:25,136,https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent,"Understanding the results of Transformers Learn In Context with Gradient Descent I'm trying to implement this paper: https://arxiv.org/pdf/2212.07677 (Here's their code): https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually. As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?",understand result transformer learn context gradient descent  m try implement paper    arxivorg  pdf221207677  s code     githubcom  google  research  self  organise  system  tree  master  transformerslearniclbygd  m struggle match experimental result  specifically  simple gd model  single layer single head softmax   obtain constant low loss roughly 020 test datum  not quite understand case  conceptually  understand  model single iteration gradient descent datum  would reach low loss  would loss constant  near constant training step  not train learning rate gd model ,datum use replication  far tell paper mention explicitly parameter datum use particular result try replicate  indeed  test variety alpha value distribution use figure 6  feasible loss low even one step gd alpha value low  find trend relative behavior gd transformer layer  not think s important match exact loss value ,understand result transformer learn context gradient descent  m try implement paper    arxivorg  pdf221207677  s code     githubcom  google  research  self  organise  system  tree  master  transformerslearniclbygd  m struggle match experimental result  specifically  simple gd model  single layer single head softmax   obtain constant low loss roughly 020 test datum  not quite understand case  conceptually  understand  model single iteration gradient descent datum  would reach low loss  would loss constant  near constant training step  not train learning rate gd model  datum use replication  far tell paper mention explicitly parameter datum use particular result try replicate  indeed  test variety alpha value distribution use figure 6  feasible loss low even one step gd alpha value low  find trend relative behavior gd transformer layer  not think s important match exact loss value ,Library/Tool-Based Queries
Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline,"I'm currently working on a project using spaCy with the German trained pipeline de_dep_news_trf . Unfortunately, I'm having issues with named entity recognition (NER). When I run a simple sentence like ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" , no entities are detected. I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project: python.exe -m pip install --upgrade pip pip install -U pip setuptools wheel pip install -U spacy python -m spacy download de_dep_news_trf --timeout 600 pip install spacy[transformers] Here is a snippet of my code: import spacy def process_text_with_spacy(text_to_process): doc = nlp(text_to_process) data = { ""text"": text_to_process, ""sentences"": [] } for sent in doc.sents: process_sentence_data = { ""sentence"": sent.text, ""entities"": [] } for ent in sent.ents: process_sentence_data[""entities""].append({ ""text"": ent.text, ""start"": ent.start_char, ""end"": ent.end_char, ""label"": ent.label_ }) data[""sentences""].append(process_sentence_data) return data nlp = spacy.load('de_dep_news_trf') sample_text = ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" processed_data = process_text_with_spacy(sample_text) print(""Text:"", sample_text) for sentence_data in processed_data[""sentences""]: print(""Sentence:"", sentence_data[""sentence""]) print(""Entities:"", sentence_data[""entities""]) Output: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [] When using de_core_news_lg , the output for each sentence is: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}] However, when I use de_dep_news_trf , the results are empty. Model de_dep_news_trf is selected based on ""accuracy"" from the SpaCy website. Could someone explain why de_dep_news_trf does not return the same result? Is there a specific reason or setting that could cause this difference? Thank you for your help!","['python', 'nlp', 'spacy']",1,"Problem is because this model doesn't have function to recognize entities. See documentation for de_dep_news_trf - it has components transformer, tagger, morphologizer, parser, lemmatizer, attribute_ruler but no ner for EntityRecognizer So it may need to use one of other models : de_core_news_sm de_core_news_md de_core_news_lg",2024-06-17 09:14:29,2024-06-17 11:34:37,201,https://stackoverflow.com/questions/78631769/problems-with-named-entity-recognition-in-spacy-using-german-de-dep-news-trf-pip,"Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline I'm currently working on a project using spaCy with the German trained pipeline de_dep_news_trf . Unfortunately, I'm having issues with named entity recognition (NER). When I run a simple sentence like ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" , no entities are detected. I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project: python.exe -m pip install --upgrade pip pip install -U pip setuptools wheel pip install -U spacy python -m spacy download de_dep_news_trf --timeout 600 pip install spacy[transformers] Here is a snippet of my code: import spacy def process_text_with_spacy(text_to_process): doc = nlp(text_to_process) data = { ""text"": text_to_process, ""sentences"": [] } for sent in doc.sents: process_sentence_data = { ""sentence"": sent.text, ""entities"": [] } for ent in sent.ents: process_sentence_data[""entities""].append({ ""text"": ent.text, ""start"": ent.start_char, ""end"": ent.end_char, ""label"": ent.label_ }) data[""sentences""].append(process_sentence_data) return data nlp = spacy.load('de_dep_news_trf') sample_text = ""Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin."" processed_data = process_text_with_spacy(sample_text) print(""Text:"", sample_text) for sentence_data in processed_data[""sentences""]: print(""Sentence:"", sentence_data[""sentence""]) print(""Entities:"", sentence_data[""entities""]) Output: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [] When using de_core_news_lg , the output for each sentence is: Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin. Sentence: Berlin ist die Hauptstadt von Deutschland. Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}] Sentence: Angela Merkel war die Bundeskanzlerin. Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}] However, when I use de_dep_news_trf , the results are empty. Model de_dep_news_trf is selected based on ""accuracy"" from the SpaCy website. Could someone explain why de_dep_news_trf does not return the same result? Is there a specific reason or setting that could cause this difference? Thank you for your help!",problem name entity recognition spacy use german dedepnewstrf pipeline  m currently work project use spacy german train pipeline dedepnewstrf  unfortunately   m issue name entity recognition  ner   run simple sentence like   berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin    entity detect   ve follow step set python environment  312   windows  pycharm community project  pythonexe m pip install  upgrade pip pip install u pip setuptools wheel pip install u spacy python m spacy download dedepnewstrf  timeout 600 pip install spacy  transformer  snippet code  import spacy def processtextwithspacy  texttoprocess   doc  nlp  texttoprocess  datum     text   texttoprocess    sentence      send docsent  processsentencedata     sentence   senttext    entity      ent sentent  processsentencedata    entity   append     text   enttext    start   entstartchar    end   entendchar    label   entlabel    datum    sentence   append  processsentencedata  return datum nlp  spacyload   dedepnewstrf   sampletext    berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin   processeddata  processtextwithspacy  sampletext  print    text    sampletext  sentencedata processeddata    sentence    print    sentence    sentencedata    sentence    print    entities    sentencedata    entity    output  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity    sentence  angela merkel war die bundeskanzlerin  entity    use decorenewslg  output sentence  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity     text    berlin    start   0   end   6   label    loc      text    deutschland    start   30   end   41   label    loc    sentence  angela merkel war die bundeskanzlerin  entity     text    angela merkel    start   43   end   56   label    per    however  use dedepnewstrf  result empty  model dedepnewstrf select base   accuracy  spacy website  could someone explain dedepnewstrf return result  specific reason setting could cause difference  thank help ,problem model not function recognize entity  see documentation dedepnewstrf  component transformer  tagger  morphologizer  parser  lemmatizer  attributeruler ner entityrecognizer may need use one model  decorenewssm decorenewsmd decorenewslg,problem name entity recognition spacy use german dedepnewstrf pipeline  m currently work project use spacy german train pipeline dedepnewstrf  unfortunately   m issue name entity recognition  ner   run simple sentence like   berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin    entity detect   ve follow step set python environment  312   windows  pycharm community project  pythonexe m pip install  upgrade pip pip install u pip setuptools wheel pip install u spacy python m spacy download dedepnewstrf  timeout 600 pip install spacy  transformer  snippet code  import spacy def processtextwithspacy  texttoprocess   doc  nlp  texttoprocess  datum     text   texttoprocess    sentence      send docsent  processsentencedata     sentence   senttext    entity      ent sentent  processsentencedata    entity   append     text   enttext    start   entstartchar    end   entendchar    label   entlabel    datum    sentence   append  processsentencedata  return datum nlp  spacyload   dedepnewstrf   sampletext    berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin   processeddata  processtextwithspacy  sampletext  print    text    sampletext  sentencedata processeddata    sentence    print    sentence    sentencedata    sentence    print    entities    sentencedata    entity    output  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity    sentence  angela merkel war die bundeskanzlerin  entity    use decorenewslg  output sentence  text  berlin ist die hauptstadt von deutschland  angela merkel war die bundeskanzlerin  sentence  berlin ist die hauptstadt von deutschland  entity     text    berlin    start   0   end   6   label    loc      text    deutschland    start   30   end   41   label    loc    sentence  angela merkel war die bundeskanzlerin  entity     text    angela merkel    start   43   end   56   label    per    however  use dedepnewstrf  result empty  model dedepnewstrf select base   accuracy  spacy website  could someone explain dedepnewstrf return result  specific reason setting could cause difference  thank help  problem model not function recognize entity  see documentation dedepnewstrf  component transformer  tagger  morphologizer  parser  lemmatizer  attributeruler ner entityrecognizer may need use one model  decorenewssm decorenewsmd decorenewslg,Implementation Issues
How do we add/modify the normalizer in a pretrained Huggingface tokenizer?,"Given a Huggingface tokenizer that already have a normalizer, e.g. ""mistralai/Mistral-7B-v0.1"" , we can do this to modify the normalizer import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.1"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) assert old_tok.backend_tokenizer.normalizer != None new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) [out]: >>> print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) <s> I foo you < br > hello world >>> print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) <s> I bar you hello world But when this hot-plug normalizer modification don't always work, if we change it to ""mistralai/Mistral-7B-v0.3"" , it fails to work: import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.3"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) [out]: <s> I foo you < br > hello world <s> I foo you < br > hello world How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Can any normalizer from a pretrained tokenizer be modified or just specific ones? If the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?","['python', 'nlp', 'large-language-model', 'huggingface-tokenizers']",1,"This looks like a bug. The v0.1 tokenizer has a normalizer by default, which can be seen by looking at the mistral-78-v0.1/tokenizer.json file: { ... ""normalizer"": { ""type"": ""Sequence"", ""normalizers"": [ { ""type"": ""Prepend"", ""prepend"": ""▁"" }, { ""type"": ""Replace"", ""pattern"": { ""String"": "" "" }, ""content"": ""▁"" } ] }, ... } After modifying the .backend_tokenizer.normalizer object, the modification are saved to the tokenizer.json file. In the v0.3 version, the mistral-78-v0.1/tokenizer.json file has no value for the normalizer: { ... ""normalizer"": null, ... } Modifying the normalizer and saving the model does write the changes to the JSON file, but it is not getting picked up on reload using AutoTokenizer.from_pretrained . I am not sure why, but it is entirely possible the tokenizer.model file indicates no normalizer is the default and it simply does not load it. However, you can get the tokenizer to load correctly - with the custom normalizer - by instantiating the matched tokenizer class explicitly and passing in the tokenizer.model and tokenizer.json paths along with the values from the tokenizer_config.json file. In this case it is the LlamaTokenizerFast class. from transformers import AutoTokenizer, LlamaTokenizerFast, AddedToken from tokenizers.normalizers import Sequence, Replace, Prepend ### load, modify, and save tok = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.3"") tok.backend_tokenizer.normalizer = Sequence([ Prepend('_'), Replace('_', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n') ]) tok.save_pretrained(""mistral-7B-v0.3-custom/"") ### read in config, construct the AddedToken objects with open('mistral-7B-v0.3-custom/tokenizer_config.json') as fp: config = json.load(fp) config['added_tokens_decoder'] = { int(k): AddedToken(**v) for k, v in config.pop('added_tokens_decoder').items() } ### load from saved files tok_custom = LlamaTokenizerFast( 'mistral-7B-v0.3-custom/tokenizer.model', 'mistral-7B-v0.3-custom/tokenizer.json', **config, ) test_str = ""I foo you<br>hello world"" print(' '.join(tok_custom.batch_decode(tok_custom(test_str)['input_ids']))) # prints: #<s> I bar you # hello world If you don't want to specify the tokenizer class explicitly, you can load the model the the AutoTokenizer , and then load it again using from the resulting class. It is a hacky work-around. tok_path = ""path/to/mistral-7B-v0.3-custom/"" with open(f'{tok_path}/tokenizer_config.json') as fp: config = json.load(fp) config['added_tokens_decoder'] = { int(k): AddedToken(**v) for k, v in config.pop('added_tokens_decoder').items() } tok = AutoTokenizer.from_pretrained(tok_path).__class__( f'{tok_path}/tokenizer.model', f'{tok_path}/tokenizer.json', **config, )",2024-06-12 11:03:59,2024-06-14 17:18:38,311,https://stackoverflow.com/questions/78612251/how-do-we-add-modify-the-normalizer-in-a-pretrained-huggingface-tokenizer,"How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Given a Huggingface tokenizer that already have a normalizer, e.g. ""mistralai/Mistral-7B-v0.1"" , we can do this to modify the normalizer import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.1"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) assert old_tok.backend_tokenizer.normalizer != None new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) [out]: >>> print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) <s> I foo you < br > hello world >>> print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) <s> I bar you hello world But when this hot-plug normalizer modification don't always work, if we change it to ""mistralai/Mistral-7B-v0.3"" , it fails to work: import json from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend tokenizer_name = ""mistralai/Mistral-7B-v0.3"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_normalizer = Sequence( [Prepend('▁'), Replace('▁', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] ) old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name) old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) [out]: <s> I foo you < br > hello world <s> I foo you < br > hello world How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Can any normalizer from a pretrained tokenizer be modified or just specific ones? If the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?",add  modify normalizer pretraine huggingface tokenizer  give huggingface tokenizer already normalizer  eg    mistralai  mistral7b  v01   modify normalizer import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v01  oldtok  autotokenizerfrompretraine  tokenizername  assert oldtokbackendtokenizernormalizer   none newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname        print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids        foo  br  hello world    print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids        bar hello world hot  plug normalizer modification not always work  change   mistralai  mistral7b  v03   fail work  import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v03  oldtok  autotokenizerfrompretraine  tokenizername  newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname  print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids      print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids           foo  br  hello world   foo  br  hello world add  modify normalizer pretraine huggingface tokenizer  normalizer pretraine tokenizer modify specific one  latter  know pretraine tokenizer s normalizer extend modify ,look like bug  v01 tokenizer normalizer default  see look mistral78  v01  tokenizerjson file      normalizer      type     sequence     normalizer       type     prepend     prepend            type     replace     pattern      string           content             modify backendtokenizernormalizer object  modification save tokenizerjson file  v03 version  mistral78  v01  tokenizerjson file value normalizer      normalizer   null    modify normalizer saving model write change json file  getting pick reload use autotokenizerfrompretrained  sure  entirely possible tokenizermodel file indicate normalizer default simply load  however  get tokenizer load correctly  custom normalizer  instantiate match tokenizer class explicitly pass tokenizermodel tokenizerjson path along value tokenizerconfigjson file  case llamatokenizerfast class  transformer import autotokenizer  llamatokenizerfast  addedtoken tokenizersnormalizer import sequence  replace  prepend    load  modify  save tok  autotokenizerfrompretraine    mistralai  mistral7b  v03   tokbackendtokenizernormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     toksavepretraine    mistral7b  v03  custom      read config  construct addedtoken object open   mistral7b  v03  custom  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items       load save file tokcustom  llamatokenizerfast   mistral7b  v03  custom  tokenizermodel    mistral7b  v03  custom  tokenizerjson     config   teststr    foo  br  hello world  print    join  tokcustombatchdecode  tokcustom  teststr    inputids       print     bar  hello world not want specify tokenizer class explicitly  load model autotokenizer  load use result class  hacky work  around  tokpath    path  to  mistral7b  v03  custom  open  f   tokpath  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items    tok  autotokenizerfrompretraine  tokpath  class    f   tokpath  tokenizermodel   f   tokpath  tokenizerjson     config  ,add  modify normalizer pretraine huggingface tokenizer  give huggingface tokenizer already normalizer  eg    mistralai  mistral7b  v01   modify normalizer import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v01  oldtok  autotokenizerfrompretraine  tokenizername  assert oldtokbackendtokenizernormalizer   none newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname        print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids        foo  br  hello world    print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids        bar hello world hot  plug normalizer modification not always work  change   mistralai  mistral7b  v03   fail work  import json transformer import autotokenizer tokenizersnormalizer import sequence  replace  prepend tokenizername    mistralai  mistral7b  v03  oldtok  autotokenizerfrompretraine  tokenizername  newnormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     oldtokbackendtokenizernormalizer  newnormalizer newtokenizdrname  f  newtokenizer  tokenizername   oldtoksavepretraine  newtokenizdrname  oldtok  autotokenizerfrompretraine  tokenizername  newtok  autotokenizerfrompretraine  newtokenizdrname  print    join  oldtokbatchdecode  oldtok    foo  br  hello world     inputids      print    join  newtokbatchdecode  newtok    foo  br  hello world     inputids           foo  br  hello world   foo  br  hello world add  modify normalizer pretraine huggingface tokenizer  normalizer pretraine tokenizer modify specific one  latter  know pretraine tokenizer s normalizer extend modify  look like bug  v01 tokenizer normalizer default  see look mistral78  v01  tokenizerjson file      normalizer      type     sequence     normalizer       type     prepend     prepend            type     replace     pattern      string           content             modify backendtokenizernormalizer object  modification save tokenizerjson file  v03 version  mistral78  v01  tokenizerjson file value normalizer      normalizer   null    modify normalizer saving model write change json file  getting pick reload use autotokenizerfrompretrained  sure  entirely possible tokenizermodel file indicate normalizer default simply load  however  get tokenizer load correctly  custom normalizer  instantiate match tokenizer class explicitly pass tokenizermodel tokenizerjson path along value tokenizerconfigjson file  case llamatokenizerfast class  transformer import autotokenizer  llamatokenizerfast  addedtoken tokenizersnormalizer import sequence  replace  prepend    load  modify  save tok  autotokenizerfrompretraine    mistralai  mistral7b  v03   tokbackendtokenizernormalizer  sequence   prepend       replace          replace    foo     bar    replace    br     n     toksavepretraine    mistral7b  v03  custom      read config  construct addedtoken object open   mistral7b  v03  custom  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items       load save file tokcustom  llamatokenizerfast   mistral7b  v03  custom  tokenizermodel    mistral7b  v03  custom  tokenizerjson     config   teststr    foo  br  hello world  print    join  tokcustombatchdecode  tokcustom  teststr    inputids       print     bar  hello world not want specify tokenizer class explicitly  load model autotokenizer  load use result class  hacky work  around  tokpath    path  to  mistral7b  v03  custom  open  f   tokpath  tokenizerconfigjson   fp  config  jsonload  fp  config   addedtokensdecoder     int  k   addedtoken    v  k  v configpop   addedtokensdecoder   items    tok  autotokenizerfrompretraine  tokpath  class    f   tokpath  tokenizermodel   f   tokpath  tokenizerjson     config  ,Implementation Issues
HuggingFace pipeline - Debug prompt,"I've defined a pipeline using Huggingface transformer library. pipe = pipeline( ""text-generation"", model=myllm, tokenizer=tokenizer, max_new_tokens=512, ) I'd like to test it: result = pipe(""Some input prompt for the LLM"") How can I debug the prompt actually sent to the LLM? I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?","['python', 'nlp', 'huggingface-transformers']",1,"you may use preprocess method and check generated token_ids. Generally would suggest to more closely look on the code of the method, it will explain what is happening with the prompt before model forward pass. params = pipe._preprocess_params pipe.preprocess(""I can't believe you did such a "", **params) # Returns: # {'input_ids': tensor([[ 40, 460, 470, 1975, 345, 750, 884, 257, 220]]), # 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), # 'prompt_text': ""I can't believe you did such a ""} Internally preprocess is calling either for chats tokenizer.chat_template for simple text prompts tokenizer(prompt_text) . For example for ""gpt-2"" model default tokenizer outputs token_ids and masks : pipe.tokenizer(""I can't believe you did such a "") # Returns: # {'input_ids': [40, 460, 470, 1975, 345, 750, 884, 257, 220], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} Another thing to consider during prompts debug is to look what will happen if you'll invert token ids: pipe = pipeline( ""text-generation"", model=""openai-community/gpt2"" ) inputs = pipe.tokenizer(""I can't believe you did such a "") pipe.tokenizer.convert_ids_to_tokens(inputs['input_ids']) # ['I', 'Ġcan', ""'t"", 'Ġbelieve', 'Ġyou', 'Ġdid', 'Ġsuch', 'Ġa', 'Ġ']",2024-06-06 12:36:47,2024-06-06 14:21:59,384,https://stackoverflow.com/questions/78586621/huggingface-pipeline-debug-prompt,"HuggingFace pipeline - Debug prompt I've defined a pipeline using Huggingface transformer library. pipe = pipeline( ""text-generation"", model=myllm, tokenizer=tokenizer, max_new_tokens=512, ) I'd like to test it: result = pipe(""Some input prompt for the LLM"") How can I debug the prompt actually sent to the LLM? I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?",huggingface pipeline  debug prompt  ve define pipeline use huggingface transformer library  pipe  pipeline    text  generation   model  myllm  tokenizer  tokenizer  maxnewtokens512   would like test  result  pipe    input prompt llm   debug prompt actually send llm  expect pipeline apply prompt template  tokenizerdefaultchattemplate  verify prompt template apply ,may use preprocess method check generate tokenids  generally would suggest closely look code method  explain happen prompt model forward pass  param  pipepreprocessparams pipepreprocess    can not believe      param   return     inputids   tensor    40  460  470  1975  345  750  884  257  220       attentionmask   tensor    1  1  1  1  1  1  1  1  1       prompttext     can not believe    internally preprocess call either chat tokenizerchattemplate simple text prompt tokenizer  prompttext   example   gpt2  model default tokenizer output tokenids mask  pipetokenizer    can not believe     return     inputids    40  460  470  1975  345  750  884  257  220     attentionmask    1  1  1  1  1  1  1  1  1   another thing consider prompt debug look happen will invert token id  pipe  pipeline    text  generation   model  openai  community  gpt2   input  pipetokenizer    can not believe    pipetokenizerconvertidstotoken  input   inputids          can      t    believe    you    did    such    a      ,huggingface pipeline  debug prompt  ve define pipeline use huggingface transformer library  pipe  pipeline    text  generation   model  myllm  tokenizer  tokenizer  maxnewtokens512   would like test  result  pipe    input prompt llm   debug prompt actually send llm  expect pipeline apply prompt template  tokenizerdefaultchattemplate  verify prompt template apply  may use preprocess method check generate tokenids  generally would suggest closely look code method  explain happen prompt model forward pass  param  pipepreprocessparams pipepreprocess    can not believe      param   return     inputids   tensor    40  460  470  1975  345  750  884  257  220       attentionmask   tensor    1  1  1  1  1  1  1  1  1       prompttext     can not believe    internally preprocess call either chat tokenizerchattemplate simple text prompt tokenizer  prompttext   example   gpt2  model default tokenizer output tokenids mask  pipetokenizer    can not believe     return     inputids    40  460  470  1975  345  750  884  257  220     attentionmask    1  1  1  1  1  1  1  1  1   another thing consider prompt debug look happen will invert token id  pipe  pipeline    text  generation   model  openai  community  gpt2   input  pipetokenizer    can not believe    pipetokenizerconvertidstotoken  input   inputids          can      t    believe    you    did    such    a      ,Basic Understanding
AttributeError: &#39;TrainingArguments&#39; object has no attribute &#39;model_init_kwargs&#39;,"While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Code: Loading the libraries from enum import Enum from functools import partial import pandas as pd import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig from datasets import load_dataset from trl import SFTTrainer from peft import get_peft_model, LoraConfig, TaskType seed = 42 set_seed(seed) Loading the dataset and preprocess it. model_name = ""gg-hf/gemma-2b-it"" dataset_name = ""FinGPT/fingpt-fiqa_qa"" tokenizer = AutoTokenizer.from_pretrained(model_name) template = """"""{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"""""" tokenizer.chat_template = template def preprocess(samples): batch = [] for system_prompt, input, output in zip(samples[""instruction""], samples[""input""], samples[""output""]): conversation = [{""content"": system_prompt, ""role"": ""system""}, {""content"": input, ""role"": ""user""}, {""content"": output, ""role"": ""assistant""}] batch.append(tokenizer.apply_chat_template(conversation, tokenize=False)) return {""content"": batch} dataset = load_dataset(dataset_name) dataset = dataset.map( preprocess, batched=True, remove_columns=dataset[""train""].column_names ) dataset = dataset[""train""].train_test_split(0.1) print(dataset) print(dataset[""train""][0]) Create PEFT configurations peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""gate_proj"",""q_proj"",""lm_head"",""o_proj"",""k_proj"",""embed_tokens"",""down_proj"",""up_proj"",""v_proj""], task_type=TaskType.CAUSAL_LM) Create Quantization configurations bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, ) Load the model and tokenizer class ChatmlSpecialTokens(str, Enum): user = ""<|im_start|>user"" assistant = ""<|im_start|>assistant"" system = ""<|im_start|>system"" eos_token = ""<|im_end|>"" bos_token = ""<s>"" pad_token = ""<pad>"" @classmethod def list(cls): return [c.value for c in cls] tokenizer = AutoTokenizer.from_pretrained( model_name, pad_token=ChatmlSpecialTokens.pad_token.value, bos_token=ChatmlSpecialTokens.bos_token.value, eos_token=ChatmlSpecialTokens.eos_token.value, additional_special_tokens=ChatmlSpecialTokens.list(), trust_remote_code=True ) tokenizer.chat_template = template model = AutoModelForCausalLM.from_pretrained(model_name) model.resize_token_embeddings(len(tokenizer)) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # cast non-trainable params in fp16 for p in model.parameters(): if not p.requires_grad: p.data = p.to(torch.float16) Training Configurations output_dir = ""Gemma2B_finetune_QLoRA"" per_device_train_batch_size = 1 per_device_eval_batch_size = 1 gradient_accumulation_steps = 8 logging_steps = 5 learning_rate = 5e-4 max_grad_norm = 1.0 max_steps = 250 num_train_epochs=10 warmup_ratio = 0.1 lr_scheduler_type = ""cosine"" max_seq_length = 2048 training_arguments = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=per_device_train_batch_size, per_device_eval_batch_size=per_device_eval_batch_size, gradient_accumulation_steps=gradient_accumulation_steps, save_strategy=""no"", evaluation_strategy=""epoch"", logging_steps=logging_steps, learning_rate=learning_rate, max_grad_norm=max_grad_norm, weight_decay=0.1, warmup_ratio=warmup_ratio, lr_scheduler_type=lr_scheduler_type, fp16=True, report_to=[""tensorboard"", ""wandb""], hub_private_repo=True, push_to_hub=True, num_train_epochs=num_train_epochs, gradient_checkpointing=True, gradient_checkpointing_kwargs={""use_reentrant"": False} ) Create trainer trainer = SFTTrainer( model=model, args=training_arguments, train_dataset=dataset[""train""], eval_dataset=dataset[""test""], tokenizer=tokenizer, packing=True, dataset_text_field=""content"", max_seq_length=max_seq_length, peft_config=peft_config, dataset_kwargs={ ""append_concat_token"": False, ""add_special_tokens"": False, }, ) The error I'm getting is like :- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 trainer = SFTTrainer( 2 model=model, 3 args=training_arguments, 4 train_dataset=dataset[""train""], 5 eval_dataset=dataset[""test""], 6 tokenizer=tokenizer, 7 packing=True, 8 dataset_text_field=""content"", 9 max_seq_length=max_seq_length, 10 peft_config=peft_config, 11 dataset_kwargs={ 12 ""append_concat_token"": False, 13 ""add_special_tokens"": False, 14 }, 15 ) File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f(*args, **kwargs) 99 message += ""\n\n"" + custom_message 100 warnings.warn(message, FutureWarning) --> 101 return f(*args, **kwargs) File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing) 150 warnings.warn( 151 ""You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`."" 152 ) 153 args.model_init_kwargs = model_init_kwargs --> 154 if args.model_init_kwargs is None: 155 model_init_kwargs = {} 156 elif not isinstance(model, str): AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Do let me know if there's any solution for this? Thanks.","['python', 'nlp', 'huggingface-transformers', 'large-language-model', 'peft']",1,"Just replace your TrainingArguments constructor with SFTConfig constructor, and pass this to SFTTrainer. from trl import SFTConfig training_arguments = SFTConfig(your training args ...) trainer = SFTTrainer(args=training_arguments, rest of the args...)",2024-06-04 12:06:34,2024-06-05 05:02:36,4587,https://stackoverflow.com/questions/78575305/attributeerror-trainingarguments-object-has-no-attribute-model-init-kwargs,"AttributeError: &#39;TrainingArguments&#39; object has no attribute &#39;model_init_kwargs&#39; While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Code: Loading the libraries from enum import Enum from functools import partial import pandas as pd import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig from datasets import load_dataset from trl import SFTTrainer from peft import get_peft_model, LoraConfig, TaskType seed = 42 set_seed(seed) Loading the dataset and preprocess it. model_name = ""gg-hf/gemma-2b-it"" dataset_name = ""FinGPT/fingpt-fiqa_qa"" tokenizer = AutoTokenizer.from_pretrained(model_name) template = """"""{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"""""" tokenizer.chat_template = template def preprocess(samples): batch = [] for system_prompt, input, output in zip(samples[""instruction""], samples[""input""], samples[""output""]): conversation = [{""content"": system_prompt, ""role"": ""system""}, {""content"": input, ""role"": ""user""}, {""content"": output, ""role"": ""assistant""}] batch.append(tokenizer.apply_chat_template(conversation, tokenize=False)) return {""content"": batch} dataset = load_dataset(dataset_name) dataset = dataset.map( preprocess, batched=True, remove_columns=dataset[""train""].column_names ) dataset = dataset[""train""].train_test_split(0.1) print(dataset) print(dataset[""train""][0]) Create PEFT configurations peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[""gate_proj"",""q_proj"",""lm_head"",""o_proj"",""k_proj"",""embed_tokens"",""down_proj"",""up_proj"",""v_proj""], task_type=TaskType.CAUSAL_LM) Create Quantization configurations bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, ) Load the model and tokenizer class ChatmlSpecialTokens(str, Enum): user = ""<|im_start|>user"" assistant = ""<|im_start|>assistant"" system = ""<|im_start|>system"" eos_token = ""<|im_end|>"" bos_token = ""<s>"" pad_token = ""<pad>"" @classmethod def list(cls): return [c.value for c in cls] tokenizer = AutoTokenizer.from_pretrained( model_name, pad_token=ChatmlSpecialTokens.pad_token.value, bos_token=ChatmlSpecialTokens.bos_token.value, eos_token=ChatmlSpecialTokens.eos_token.value, additional_special_tokens=ChatmlSpecialTokens.list(), trust_remote_code=True ) tokenizer.chat_template = template model = AutoModelForCausalLM.from_pretrained(model_name) model.resize_token_embeddings(len(tokenizer)) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # cast non-trainable params in fp16 for p in model.parameters(): if not p.requires_grad: p.data = p.to(torch.float16) Training Configurations output_dir = ""Gemma2B_finetune_QLoRA"" per_device_train_batch_size = 1 per_device_eval_batch_size = 1 gradient_accumulation_steps = 8 logging_steps = 5 learning_rate = 5e-4 max_grad_norm = 1.0 max_steps = 250 num_train_epochs=10 warmup_ratio = 0.1 lr_scheduler_type = ""cosine"" max_seq_length = 2048 training_arguments = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=per_device_train_batch_size, per_device_eval_batch_size=per_device_eval_batch_size, gradient_accumulation_steps=gradient_accumulation_steps, save_strategy=""no"", evaluation_strategy=""epoch"", logging_steps=logging_steps, learning_rate=learning_rate, max_grad_norm=max_grad_norm, weight_decay=0.1, warmup_ratio=warmup_ratio, lr_scheduler_type=lr_scheduler_type, fp16=True, report_to=[""tensorboard"", ""wandb""], hub_private_repo=True, push_to_hub=True, num_train_epochs=num_train_epochs, gradient_checkpointing=True, gradient_checkpointing_kwargs={""use_reentrant"": False} ) Create trainer trainer = SFTTrainer( model=model, args=training_arguments, train_dataset=dataset[""train""], eval_dataset=dataset[""test""], tokenizer=tokenizer, packing=True, dataset_text_field=""content"", max_seq_length=max_seq_length, peft_config=peft_config, dataset_kwargs={ ""append_concat_token"": False, ""add_special_tokens"": False, }, ) The error I'm getting is like :- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[10], line 1 ----> 1 trainer = SFTTrainer( 2 model=model, 3 args=training_arguments, 4 train_dataset=dataset[""train""], 5 eval_dataset=dataset[""test""], 6 tokenizer=tokenizer, 7 packing=True, 8 dataset_text_field=""content"", 9 max_seq_length=max_seq_length, 10 peft_config=peft_config, 11 dataset_kwargs={ 12 ""append_concat_token"": False, 13 ""add_special_tokens"": False, 14 }, 15 ) File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f(*args, **kwargs) 99 message += ""\n\n"" + custom_message 100 warnings.warn(message, FutureWarning) --> 101 return f(*args, **kwargs) File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing) 150 warnings.warn( 151 ""You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`."" 152 ) 153 args.model_init_kwargs = model_init_kwargs --> 154 if args.model_init_kwargs is None: 155 model_init_kwargs = {} 156 elif not isinstance(model, str): AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs' Do let me know if there's any solution for this? Thanks.",attributeerror    39  trainingarguments   39  object attribute   39  modelinitkwargs   39  finetune gemma2b model use qlora  m get error attributeerror   trainingarguments  object attribute  modelinitkwargs  code  loading library enum import enum functool import partial import panda pd import torch transformer import autotokenizer  automodelforcausallm  trainingarguments  setseed  bitsandbytesconfig dataset import loaddataset trl import sfttrainer peft import getpeftmodel  loraconfig  tasktype seed  42 setseed  seed  loading dataset preprocess  modelname    gg  hf  gemma2b  it  datasetname    fingpt  fingpt  fiqaqa  tokenizer  autotokenizerfrompretraine  modelname  template        message message   n     imstart    message   role     n   message   content      imend     n      looplast addgenerationprompt       imstart  assistantn      endif     endfor      tokenizerchattemplate  template def preprocess  sample   batch    systemprompt  input  output zip  sample    instruction    sample    input    sample    output     conversation      content   systemprompt    role     system       content   input    role     user       content   output    role     assistant    batchappend  tokenizerapplychattemplate  conversation  tokenize  false   return    content   batch  dataset  loaddataset  datasetname  dataset  datasetmap  preprocess  batch  true  removecolumn  dataset    train   columnnames  dataset  dataset    train   traintestsplit  01  print  dataset  print  dataset    train    0   create peft configuration peftconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    gateproj    qproj    lmhead    oproj    kproj    embedtoken    downproj    upproj    vproj    tasktype  tasktype  causallm  create quantization configuration bnbconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchbfloat16  bnb4bitusedoublequant  true   load model tokenizer class chatmlspecialtokens  str  enum   user     imstart  user  assistant     imstart  assistant  system     imstart  system  eostoken     imend   bostoken       padtoken     pad    classmethod def list  cls   return  cvalue c cls  tokenizer  autotokenizerfrompretraine  modelname  padtoken  chatmlspecialtokenspadtokenvalue  bostoken  chatmlspecialtokensbostokenvalue  eostoken  chatmlspecialtokenseostokenvalue  additionalspecialtokens  chatmlspecialtokenslist    trustremotecode  true  tokenizerchattemplate  template model  automodelforcausallmfrompretraine  modelname  modelresizetokenembeddings  len  tokenizer   model  getpeftmodel  model  peftconfig  modelprinttrainableparameters    cast non  trainable param fp16 p modelparameter    prequiresgrad  pdata  pto  torchfloat16  training configurations outputdir    gemma2bfinetuneqlora  perdevicetrainbatchsize  1 perdeviceevalbatchsize  1 gradientaccumulationsteps  8 loggingstep  5 learningrate  5e4 maxgradnorm  10 maxstep  250 numtrainepochs10 warmupratio  01 lrschedulertype    cosine  maxseqlength  2048 trainingargument  trainingarguments  outputdir  outputdir  perdevicetrainbatchsize  perdevicetrainbatchsize  perdeviceevalbatchsize  perdeviceevalbatchsize  gradientaccumulationsteps  gradientaccumulationsteps  savestrategy    evaluationstrategy  epoch   loggingstep  loggingsteps  learningrate  learningrate  maxgradnorm  maxgradnorm  weightdecay01  warmupratio  warmupratio  lrschedulertype  lrschedulertype  fp16  true  reportto    tensorboard     wandb    hubprivaterepo  true  pushtohub  true  numtrainepochs  numtrainepochs  gradientcheckpointing  true  gradientcheckpointingkwargs    usereentrant   false   create trainer trainer  sfttrainer  model  model  arg  trainingarguments  traindataset  dataset    train    evaldataset  dataset    test    tokenizer  tokenizer  pack  true  datasettextfield  content   maxseqlength  maxseqlength  peftconfig  peftconfig  datasetkwargs    appendconcattoken   false    addspecialtoken   false     error  m get like                                         attributeerror traceback  recent call last  cell  10   line 1    1 trainer  sfttrainer  2 model  model  3 arg  trainingarguments  4 traindataset  dataset    train    5 evaldataset  dataset    test    6 tokenizer  tokenizer  7 pack  true  8 datasettextfield  content   9 maxseqlength  maxseqlength  10 peftconfig  peftconfig  11 datasetkwargs  12   appendconcattoken   false  13   addspecialtoken   false  14   15  file usr  local  lib  python310  dist  package  huggingfacehub  utilsdeprecationpy101   deprecatearguments   local  innerdeprecatepositionalargs   local  innerf   args    kwargs  99 message     nn   custommessage 100 warningswarn  message  futurewarning    101 return f   args    kwargs  file usr  local  lib  python310  dist  package  trl  trainer  sfttrainerpy154  sfttrainerinit    self  model  args  datacollator  traindataset  evaldataset  tokenizer  modelinit  computemetric  callback  optimizer  preprocesslogitsformetric  peftconfig  datasettextfield  packing  formattingfunc  maxseqlength  infinite  numofsequence  charspertoken  datasetnumproc  datasetbatchsize  neftunenoisealpha  modelinitkwargs  datasetkwargs  evalpacke  150 warningswarn  151   pass  modelinitkwargs  sfttrainer  value pass override one  sftconfig    152  153 argsmodelinitkwargs  modelinitkwargs   154 argsmodelinitkwargs none  155 modelinitkwargs    156 elif isinstance  model  str   attributeerror   trainingarguments  object attribute  modelinitkwargs  let know s solution  thank ,replace trainingarguments constructor sftconfig constructor  pass sfttrainer  trl import sftconfig trainingargument  sftconfig  training args   trainer  sfttrainer  arg  trainingargument  rest args  ,attributeerror    39  trainingarguments   39  object attribute   39  modelinitkwargs   39  finetune gemma2b model use qlora  m get error attributeerror   trainingarguments  object attribute  modelinitkwargs  code  loading library enum import enum functool import partial import panda pd import torch transformer import autotokenizer  automodelforcausallm  trainingarguments  setseed  bitsandbytesconfig dataset import loaddataset trl import sfttrainer peft import getpeftmodel  loraconfig  tasktype seed  42 setseed  seed  loading dataset preprocess  modelname    gg  hf  gemma2b  it  datasetname    fingpt  fingpt  fiqaqa  tokenizer  autotokenizerfrompretraine  modelname  template        message message   n     imstart    message   role     n   message   content      imend     n      looplast addgenerationprompt       imstart  assistantn      endif     endfor      tokenizerchattemplate  template def preprocess  sample   batch    systemprompt  input  output zip  sample    instruction    sample    input    sample    output     conversation      content   systemprompt    role     system       content   input    role     user       content   output    role     assistant    batchappend  tokenizerapplychattemplate  conversation  tokenize  false   return    content   batch  dataset  loaddataset  datasetname  dataset  datasetmap  preprocess  batch  true  removecolumn  dataset    train   columnnames  dataset  dataset    train   traintestsplit  01  print  dataset  print  dataset    train    0   create peft configuration peftconfig  loraconfig  r8  loraalpha16  loradropout01  targetmodules    gateproj    qproj    lmhead    oproj    kproj    embedtoken    downproj    upproj    vproj    tasktype  tasktype  causallm  create quantization configuration bnbconfig  bitsandbytesconfig  loadin4bit  true  bnb4bitquanttype  nf4   bnb4bitcomputedtype  torchbfloat16  bnb4bitusedoublequant  true   load model tokenizer class chatmlspecialtokens  str  enum   user     imstart  user  assistant     imstart  assistant  system     imstart  system  eostoken     imend   bostoken       padtoken     pad    classmethod def list  cls   return  cvalue c cls  tokenizer  autotokenizerfrompretraine  modelname  padtoken  chatmlspecialtokenspadtokenvalue  bostoken  chatmlspecialtokensbostokenvalue  eostoken  chatmlspecialtokenseostokenvalue  additionalspecialtokens  chatmlspecialtokenslist    trustremotecode  true  tokenizerchattemplate  template model  automodelforcausallmfrompretraine  modelname  modelresizetokenembeddings  len  tokenizer   model  getpeftmodel  model  peftconfig  modelprinttrainableparameters    cast non  trainable param fp16 p modelparameter    prequiresgrad  pdata  pto  torchfloat16  training configurations outputdir    gemma2bfinetuneqlora  perdevicetrainbatchsize  1 perdeviceevalbatchsize  1 gradientaccumulationsteps  8 loggingstep  5 learningrate  5e4 maxgradnorm  10 maxstep  250 numtrainepochs10 warmupratio  01 lrschedulertype    cosine  maxseqlength  2048 trainingargument  trainingarguments  outputdir  outputdir  perdevicetrainbatchsize  perdevicetrainbatchsize  perdeviceevalbatchsize  perdeviceevalbatchsize  gradientaccumulationsteps  gradientaccumulationsteps  savestrategy    evaluationstrategy  epoch   loggingstep  loggingsteps  learningrate  learningrate  maxgradnorm  maxgradnorm  weightdecay01  warmupratio  warmupratio  lrschedulertype  lrschedulertype  fp16  true  reportto    tensorboard     wandb    hubprivaterepo  true  pushtohub  true  numtrainepochs  numtrainepochs  gradientcheckpointing  true  gradientcheckpointingkwargs    usereentrant   false   create trainer trainer  sfttrainer  model  model  arg  trainingarguments  traindataset  dataset    train    evaldataset  dataset    test    tokenizer  tokenizer  pack  true  datasettextfield  content   maxseqlength  maxseqlength  peftconfig  peftconfig  datasetkwargs    appendconcattoken   false    addspecialtoken   false     error  m get like                                         attributeerror traceback  recent call last  cell  10   line 1    1 trainer  sfttrainer  2 model  model  3 arg  trainingarguments  4 traindataset  dataset    train    5 evaldataset  dataset    test    6 tokenizer  tokenizer  7 pack  true  8 datasettextfield  content   9 maxseqlength  maxseqlength  10 peftconfig  peftconfig  11 datasetkwargs  12   appendconcattoken   false  13   addspecialtoken   false  14   15  file usr  local  lib  python310  dist  package  huggingfacehub  utilsdeprecationpy101   deprecatearguments   local  innerdeprecatepositionalargs   local  innerf   args    kwargs  99 message     nn   custommessage 100 warningswarn  message  futurewarning    101 return f   args    kwargs  file usr  local  lib  python310  dist  package  trl  trainer  sfttrainerpy154  sfttrainerinit    self  model  args  datacollator  traindataset  evaldataset  tokenizer  modelinit  computemetric  callback  optimizer  preprocesslogitsformetric  peftconfig  datasettextfield  packing  formattingfunc  maxseqlength  infinite  numofsequence  charspertoken  datasetnumproc  datasetbatchsize  neftunenoisealpha  modelinitkwargs  datasetkwargs  evalpacke  150 warningswarn  151   pass  modelinitkwargs  sfttrainer  value pass override one  sftconfig    152  153 argsmodelinitkwargs  modelinitkwargs   154 argsmodelinitkwargs none  155 modelinitkwargs    156 elif isinstance  model  str   attributeerror   trainingarguments  object attribute  modelinitkwargs  let know s solution  thank  replace trainingarguments constructor sftconfig constructor  pass sfttrainer  trl import sftconfig trainingargument  sftconfig  training args   trainer  sfttrainer  arg  trainingargument  rest args  ,Library/Tool-Based Queries
Unable to fit new documents without running out of memory in STM topic modeling,"I'm trying to label new texts based on a previous topic model using the fitNewDocuments() function form the stm package in R. I've tried fitting 10 new documents based on topic models trained on 20000, 10000 and 3000 documents, and the function always ends up using way too much memory (from 20gb to even 50gb), crashing the R session. I'm not finding anything online about using the fitNewDocuments() properly. I'm following the documentation to the letter, but the process just never finishes. I've only noticed that the documentation says the origData argument should be out$meta , but it returns an error if I supply that, and I have to supply just out instead. That being said, I'm able to reproduce the example in the documentation using Gadarian data. But it fails with my own data. I could share code, but it would be useless without access to the data, which sadly I can't provide.","['r', 'nlp', 'lda', 'stm']",1,"After trying a million things, somehow I was able to fix this by removing the prevalencePrior = ""Covariate"" argument from fitNewDocuments() , and the new documents were properly fit based on the models.",2024-06-04 03:41:55,2024-06-04 03:58:53,25,https://stackoverflow.com/questions/78573280/unable-to-fit-new-documents-without-running-out-of-memory-in-stm-topic-modeling,"Unable to fit new documents without running out of memory in STM topic modeling I'm trying to label new texts based on a previous topic model using the fitNewDocuments() function form the stm package in R. I've tried fitting 10 new documents based on topic models trained on 20000, 10000 and 3000 documents, and the function always ends up using way too much memory (from 20gb to even 50gb), crashing the R session. I'm not finding anything online about using the fitNewDocuments() properly. I'm following the documentation to the letter, but the process just never finishes. I've only noticed that the documentation says the origData argument should be out$meta , but it returns an error if I supply that, and I have to supply just out instead. That being said, I'm able to reproduce the example in the documentation using Gadarian data. But it fails with my own data. I could share code, but it would be useless without access to the data, which sadly I can't provide.",unable fit new document without run memory stm topic modeling  m try label new text base previous topic model use fitnewdocument   function form stm package r  ve try fit 10 new document base topic model train 20000  10000 3000 document  function always end use way much memory  20 gb even 50 gb   crash r session   m find anything online use fitnewdocument   properly   m follow documentation letter  process never finish   ve notice documentation say origdata argument  meta  return error supply  supply instead  say   m able reproduce example documentation use gadarian datum  fail datum  could share code  would useless without access datum  sadly can not provide ,try million thing  somehow able fix remove prevalenceprior    covariate  argument fitnewdocument    new document properly fit base model ,unable fit new document without run memory stm topic modeling  m try label new text base previous topic model use fitnewdocument   function form stm package r  ve try fit 10 new document base topic model train 20000  10000 3000 document  function always end use way much memory  20 gb even 50 gb   crash r session   m find anything online use fitnewdocument   properly   m follow documentation letter  process never finish   ve notice documentation say origdata argument  meta  return error supply  supply instead  say   m able reproduce example documentation use gadarian datum  fail datum  could share code  would useless without access datum  sadly can not provide  try million thing  somehow able fix remove prevalenceprior    covariate  argument fitnewdocument    new document properly fit base model ,Implementation Issues
How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model?,"Preface I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace. The directories are filled with these files: - .gitattributes - adapter_config.json - adapter_model.safetensors - special_tokens_map.json - tokenizer.json - tokenizer_config.json - training_args.bin Implementation I am trying to load this model through this: model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, ) But it shows the error OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files. So then I am trying to load the config.json from the original model which is meta-llama/Meta-Llama-3-8B : original_model = ""meta-llama/Meta-Llama-3-8B"" model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) original_config = AutoConfig.from_pretrained(original_model) original_config.save_pretrained(model_id_1) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, config = original_config ) But still, it shows another error OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3. Questions How to load the fine-tuned model properly?","['pytorch', 'nlp', 'huggingface-transformers', 'large-language-model', 'peft']",1,"Your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. I assume you have used the save_pretrained method from peft. This method only saves the adapter weights and config (I use a smaller model for my answer and a different task type!): from peft import LoraConfig, TaskType, get_peft_model, PeftModel from transformers import AutoModelForTokenClassification from pathlib import Path # ferguso/llama-8b-pcl-v3 in your case adapter_path = 'bla' # meta-llama/Meta-Llama-3-8B in your case base_model_id = ""distilbert/distilbert-base-uncased"" peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, target_modules=""all-linear"") # AutoModelForCausalLM in your case model = AutoModelForTokenClassification.from_pretrained(base_model_id) model = get_peft_model(model, peft_config) model.save_pretrained(adapter_path) print(*list(Path(adapter_path).iterdir()), sep='\n') Output: bla/adapter_config.json bla/README.md bla/adapter_model.safetensors To load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter: model = AutoModelForTokenClassification.from_pretrained(base_model_id) model = PeftModel.from_pretrained(model, adapter_path) You can also merge the adapter weights back with merge_and_unload and save it: model.merge_and_unload().save_pretrained('bla2') print(*list(Path('bla2').iterdir()), sep='\n') Output: bla2/config.json bla2/model.safetensors This way you will be able to load the model without peft and only transformers as you tried in the example code of your question.",2024-05-30 02:36:56,2024-06-01 15:31:59,3057,https://stackoverflow.com/questions/78552651/how-to-fix-error-oserror-model-does-not-appear-to-have-a-file-named-config-j,"How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model? Preface I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace. The directories are filled with these files: - .gitattributes - adapter_config.json - adapter_model.safetensors - special_tokens_map.json - tokenizer.json - tokenizer_config.json - training_args.bin Implementation I am trying to load this model through this: model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, ) But it shows the error OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files. So then I am trying to load the config.json from the original model which is meta-llama/Meta-Llama-3-8B : original_model = ""meta-llama/Meta-Llama-3-8B"" model_id_1 = ""ferguso/llama-8b-pcl-v3"" tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1) quantization_config = BitsAndBytesConfig( load_in_8bit=True, ) original_config = AutoConfig.from_pretrained(original_model) original_config.save_pretrained(model_id_1) model_1 = AutoModelForCausalLM.from_pretrained( model_id_1, quantization_config=quantization_config, config = original_config ) But still, it shows another error OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3. Questions How to load the fine-tuned model properly?",fix error  oserror   lt  model  gt  appear file name configjson   load custom fine  tune model  preface new implement nlp model  successfully fine  tune llama 3  8b variant qlora upload huggingface  directory fill file   gitattributes  adapterconfigjson  adaptermodelsafetensors  specialtokensmapjson  tokenizerjson  tokenizerconfigjson  trainingargsbin implementation try load model  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig   show error oserror  ferguso  llama8b  pcl  v3 appear file name configjson  checkout    huggingfaceco  ferguso  llama8b  pcl  v3  tree  main  available file  try load configjson original model meta  llama  meta  llama3  8b  originalmodel    meta  llama  meta  llama3  8b  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   originalconfig  autoconfigfrompretrained  originalmodel  originalconfigsavepretraine  modelid1  model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig  config  originalconfig  still  show another error oserror  error file name pytorchmodelbin  modelsafetensor  tfmodelh5  modelckptindex flaxmodelmsgpack find directory ferguso  llama8b  pcl  v3  question load fine  tuned model properly ,directory contain file peft  adapter file require load tokenizer  base model weight missing  assume use savepretrained method peft  method save adapter weight config  use small model answer different task type    peft import loraconfig  tasktype  getpeftmodel  peftmodel transformer import automodelfortokenclassification pathlib import path  ferguso  llama8b  pcl  v3 case adapterpath   bla   meta  llama  meta  llama3  8b case basemodelid    distilbert  distilbert  base  uncased  peftconfig  loraconfig  tasktype  tasktype  tokencls  targetmodules  all  linear    automodelforcausallm case model  automodelfortokenclassificationfrompretraine  basemodelid  model  getpeftmodel  model  peftconfig  modelsavepretraine  adapterpath  print   list  path  adapterpath  iterdir     sepn   output  bla  adapterconfigjson bla  readmemd bla  adaptermodelsafetensors load pretraine model successfully  need load basemodel weight well use peft model class load adapter  model  automodelfortokenclassificationfrompretraine  basemodelid  model  peftmodelfrompretraine  model  adapterpath  also merge adapter weight back mergeandunload save  modelmergeandunload   savepretrained   bla2   print   list  path   bla2   iterdir     sepn   output  bla2  configjson bla2  modelsafetensor way able load model without peft transformer try example code question ,fix error  oserror   lt  model  gt  appear file name configjson   load custom fine  tune model  preface new implement nlp model  successfully fine  tune llama 3  8b variant qlora upload huggingface  directory fill file   gitattributes  adapterconfigjson  adaptermodelsafetensors  specialtokensmapjson  tokenizerjson  tokenizerconfigjson  trainingargsbin implementation try load model  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig   show error oserror  ferguso  llama8b  pcl  v3 appear file name configjson  checkout    huggingfaceco  ferguso  llama8b  pcl  v3  tree  main  available file  try load configjson original model meta  llama  meta  llama3  8b  originalmodel    meta  llama  meta  llama3  8b  modelid1    ferguso  llama8b  pcl  v3  tokenizer1  autotokenizerfrompretraine  modelid1  quantizationconfig  bitsandbytesconfig  loadin8bit  true   originalconfig  autoconfigfrompretrained  originalmodel  originalconfigsavepretraine  modelid1  model1  automodelforcausallmfrompretraine  modelid1  quantizationconfig  quantizationconfig  config  originalconfig  still  show another error oserror  error file name pytorchmodelbin  modelsafetensor  tfmodelh5  modelckptindex flaxmodelmsgpack find directory ferguso  llama8b  pcl  v3  question load fine  tuned model properly  directory contain file peft  adapter file require load tokenizer  base model weight missing  assume use savepretrained method peft  method save adapter weight config  use small model answer different task type    peft import loraconfig  tasktype  getpeftmodel  peftmodel transformer import automodelfortokenclassification pathlib import path  ferguso  llama8b  pcl  v3 case adapterpath   bla   meta  llama  meta  llama3  8b case basemodelid    distilbert  distilbert  base  uncased  peftconfig  loraconfig  tasktype  tasktype  tokencls  targetmodules  all  linear    automodelforcausallm case model  automodelfortokenclassificationfrompretraine  basemodelid  model  getpeftmodel  model  peftconfig  modelsavepretraine  adapterpath  print   list  path  adapterpath  iterdir     sepn   output  bla  adapterconfigjson bla  readmemd bla  adaptermodelsafetensors load pretraine model successfully  need load basemodel weight well use peft model class load adapter  model  automodelfortokenclassificationfrompretraine  basemodelid  model  peftmodelfrompretraine  model  adapterpath  also merge adapter weight back mergeandunload save  modelmergeandunload   savepretrained   bla2   print   list  path   bla2   iterdir     sepn   output  bla2  configjson bla2  modelsafetensor way able load model without peft transformer try example code question ,Implementation Issues
Not able to install spacy==2.3.5 version,"I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error. Using Python 3.12.3 Also it gives a E053 config file error when running the program regarding pyresparser: ""OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg"" `(.venv) PS C:\Smart_Resume_Analyser_App-master> pip install spacy==2.3.5 Getting requirements to build wheel did not run successfully. exit code: 1 [267 lines of output] Error compiling Cython file: ------------------------------------------------------------ ... len_t* widths int i int nr_layer int batch_size __init__(len_t* widths, int nr_layer, int batch_size) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this._nr_feat = <len_t*>calloc(batch_size, sizeof(len_t)) this._is_valid = <int*>calloc(batch_size * widths[nr_layer-1], sizeof(int)) this._costs = <weight_t*>calloc(batch_size * widths[nr_layer-1], sizeof(weight_t)) this.signatures = <uint64_t*>calloc(batch_size, sizeof(uint64_t)) __dealloc__() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... free(this._nr_feat) free(this._is_valid) free(this._costs) free(this.signatures) void reset() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.i): free(this._feats[i]) this._feats[i] = NULL this.i = 0 int nr_in() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this.i = 0 int nr_in() nogil: return this.widths[0] int nr_out() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.widths[0] int nr_out() nogil: return this.widths[this.nr_layer - 1] int push_back(const FeatureC* feats, int nr_feat, ^ ------------------------------------------------------------ thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.nr_out()): this.is_valid(this.i)[i] = 1 this.i += 1 return this.i >= this.batch_size FeatureC* features(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.i >= this.batch_size FeatureC* features(int i) nogil: return this._feats[i] int nr_feat(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._feats[i] int nr_feat(int i) nogil: return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out()) int best(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline' warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 Compiling thinc/linalg.pyx because it changed. Compiling thinc/structs.pyx because it changed. Compiling thinc/typedefs.pyx because it changed. Compiling thinc/linear/avgtron.pyx because it changed. Compiling thinc/linear/features.pyx because it changed. Compiling thinc/linear/serialize.pyx because it changed. Compiling thinc/linear/sparse.pyx because it changed. Compiling thinc/linear/linear.pyx because it changed. Compiling thinc/neural/optimizers.pyx because it changed. Compiling thinc/neural/ops.pyx because it changed. Compiling thinc/neural/_aligned_alloc.pyx because it changed. Compiling thinc/extra/eg.pyx because it changed. Compiling thinc/extra/mb.pyx because it changed. Compiling thinc/extra/search.pyx because it changed. Compiling thinc/extra/cache.pyx because it changed. [ 1/15] Cythonizing thinc/extra/cache.pyx [ 2/15] Cythonizing thinc/extra/eg.pyx Traceback (most recent call last): File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module> main() File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 325, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 295, in _get_build_requires self.run_setup() File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 311, in run_setup exec(code, locals()) File ""<string>"", line 258, in <module> File ""<string>"", line 195, in setup_package File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1154, in cythonize cythonize_one(*args) File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1321, in cythonize_one raise CompileError(None, pyx_file) Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error Getting requirements to build wheel did not run successfully. exit code: 1 See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. `","['python', 'parsing', 'pip', 'nlp', 'spacy']",1,try: pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz or pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz,2024-05-27 11:20:09,2024-05-28 00:47:10,213,https://stackoverflow.com/questions/78538749/not-able-to-install-spacy-2-3-5-version,"Not able to install spacy==2.3.5 version I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error. Using Python 3.12.3 Also it gives a E053 config file error when running the program regarding pyresparser: ""OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg"" `(.venv) PS C:\Smart_Resume_Analyser_App-master> pip install spacy==2.3.5 Getting requirements to build wheel did not run successfully. exit code: 1 [267 lines of output] Error compiling Cython file: ------------------------------------------------------------ ... len_t* widths int i int nr_layer int batch_size __init__(len_t* widths, int nr_layer, int batch_size) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this._nr_feat = <len_t*>calloc(batch_size, sizeof(len_t)) this._is_valid = <int*>calloc(batch_size * widths[nr_layer-1], sizeof(int)) this._costs = <weight_t*>calloc(batch_size * widths[nr_layer-1], sizeof(weight_t)) this.signatures = <uint64_t*>calloc(batch_size, sizeof(uint64_t)) __dealloc__() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... free(this._nr_feat) free(this._is_valid) free(this._costs) free(this.signatures) void reset() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.i): free(this._feats[i]) this._feats[i] = NULL this.i = 0 int nr_in() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... this.i = 0 int nr_in() nogil: return this.widths[0] int nr_out() nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.widths[0] int nr_out() nogil: return this.widths[this.nr_layer - 1] int push_back(const FeatureC* feats, int nr_feat, ^ ------------------------------------------------------------ thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... for i in range(this.nr_out()): this.is_valid(this.i)[i] = 1 this.i += 1 return this.i >= this.batch_size FeatureC* features(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.i >= this.batch_size FeatureC* features(int i) nogil: return this._feats[i] int nr_feat(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._feats[i] int nr_feat(int i) nogil: return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._nr_feat[i] weight_t* fwd(int i, int j) nogil: return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._fwd[i] + (j * this.widths[i]) weight_t* bwd(int i, int j) nogil: return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._bwd[i] + (j * this.widths[i]) weight_t* scores(int i) nogil: return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.fwd(this.nr_layer-1, i) weight_t* losses(int i) nogil: return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this.bwd(this.nr_layer-1, i) weight_t* costs(int i) nogil: return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._costs + (i * this.nr_out()) int* is_valid(int i) nogil: return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline' Error compiling Cython file: ------------------------------------------------------------ ... return this._is_valid + (i * this.nr_out()) int guess(int i) nogil: return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out()) int best(int i) nogil: ^ ------------------------------------------------------------ thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline' warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310 Compiling thinc/linalg.pyx because it changed. Compiling thinc/structs.pyx because it changed. Compiling thinc/typedefs.pyx because it changed. Compiling thinc/linear/avgtron.pyx because it changed. Compiling thinc/linear/features.pyx because it changed. Compiling thinc/linear/serialize.pyx because it changed. Compiling thinc/linear/sparse.pyx because it changed. Compiling thinc/linear/linear.pyx because it changed. Compiling thinc/neural/optimizers.pyx because it changed. Compiling thinc/neural/ops.pyx because it changed. Compiling thinc/neural/_aligned_alloc.pyx because it changed. Compiling thinc/extra/eg.pyx because it changed. Compiling thinc/extra/mb.pyx because it changed. Compiling thinc/extra/search.pyx because it changed. Compiling thinc/extra/cache.pyx because it changed. [ 1/15] Cythonizing thinc/extra/cache.pyx [ 2/15] Cythonizing thinc/extra/eg.pyx Traceback (most recent call last): File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module> main() File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 325, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=['wheel']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 295, in _get_build_requires self.run_setup() File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py"", line 311, in run_setup exec(code, locals()) File ""<string>"", line 258, in <module> File ""<string>"", line 195, in setup_package File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1154, in cythonize cythonize_one(*args) File ""C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py"", line 1321, in cythonize_one raise CompileError(None, pyx_file) Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error Getting requirements to build wheel did not run successfully. exit code: 1 See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. `",able install spacy235 version try install spacy235 resume analyser program  encounter pip subprocess install build dependency run successfully error  use python 3123 also give e053 config file error run program regard pyresparser    oserror   e053  could read config file c  smartresumeanalyserapp  mastervenvlibsite  packagespyresparserconfigcfg    venv  ps c  smartresumeanalyserapp  master  pip install spacy235 get requirement build wheel run successfully  exit code  1  267 line output  error compile cython file                                 lent  width int int nrlayer int batchsize   init    lent  width  int nrlayer  int batchsize  nogil                                 thincstructspxd1404  function definition pxd file must declare  cdef inline  error compile cython file                                 thisnrfeat   lent   calloc  batchsize  sizeof  lent   thisisvalid   int   calloc  batchsize  width  nrlayer1   sizeof  int   thiscost   weightt   calloc  batchsize  width  nrlayer1   sizeof  weightt   thissignature   uint64t   calloc  batchsize  sizeof  uint64t     dealloc     nogil                                 thincstructspxd1574  function definition pxd file must declare  cdef inline  error compile cython file                                 free  thisnrfeat  free  thisisvalid  free  thiscost  free  thissignature  void reset   nogil                                 thincstructspxd1724  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisi   free  thisfeat    thisfeat    null thisi  0 int nrin   nogil                                 thincstructspxd1894  function definition pxd file must declare  cdef inline  error compile cython file                                 thisi  0 int nrin   nogil  return thiswidth  0  int nrout   nogil                                 thincstructspxd1924  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiswidth  0  int nrout   nogil  return thiswidth  thisnrlayer  1  int pushback  const featurec  feat  int nrfeat                                 thincstructspxd1954  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisnrout     thisisvalid  thisi     1 thisi   1 return thisi   thisbatchsize featurec  feature  int  nogil                                 thincstructspxd2264  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisi   thisbatchsize featurec  feature  int  nogil  return thisfeat   int nrfeat  int  nogil                                 thincstructspxd2294  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfeat   int nrfeat  int  nogil  return thisnrfeat   weightt  fwd  int  int j  nogil                                 thincstructspxd2324  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisnrfeat   weightt  fwd  int  int j  nogil  return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil                                 thincstructspxd2354  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil  return thisbwd     j  thiswidth    weightt  score  int  nogil                                 thincstructspxd2384  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd     j  thiswidth    weightt  score  int  nogil  return thisfwd  thisnrlayer1   weightt  loss  int  nogil                                 thincstructspxd2414  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd  thisnrlayer1   weightt  loss  int  nogil  return thisbwd  thisnrlayer1   weightt  cost  int  nogil                                 thincstructspxd2444  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd  thisnrlayer1   weightt  cost  int  nogil  return thiscost    thisnrout    int  isvalid  int  nogil                                 thincstructspxd2474  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiscost    thisnrout    int  isvalid  int  nogil  return thisisvalid    thisnrout    int guess  int  nogil                                 thincstructspxd2504  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisisvalid    thisnrout    int guess  int  nogil  return vecvecargmaxiftrue  thisscore    thisisvalid    thisnrout    int good  int  nogil                                 thincstructspxd2534  function definition pxd file must declare  cdef inline  warning  thinclinalgpxd140   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd908   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd1748   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 compiling thinc  linalgpyx change  compile thinc  structspyx change  compile thinc  typedefspyx change  compile thinc  linear  avgtronpyx change  compile thinc  linear  featurespyx change  compile thinc  linear  serializepyx change  compile thinc  linear  sparsepyx change  compile thinc  linear  linearpyx change  compile thinc  neural  optimizerspyx change  compile thinc  neural  opspyx change  compile thinc  neuralalignedallocpyx change  compile thinc  extra  egpyx change  compile thinc  extra  mbpyx change  compile thinc  extra  searchpyx change  compile thinc  extra  cachepyx change   115  cythonize thinc  extra  cachepyx  215  cythonize thinc  extra  egpyx traceback  recent call last   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 353   module  main   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 335  main jsonout   returnval    hook    hookinput   kwarg     file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 118  getrequiresforbuildwheel return hook  configsetting   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 325  getrequiresforbuildwheel return selfgetbuildrequire  configsetting  requirements   wheel     file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 295   getbuildrequire selfrunsetup   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 311  runsetup exec  code  local    file    string    line 258   module  file    string    line 195  setuppackage file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1154  cythonize cythonizeone   args  file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1321  cythonizeone raise compileerror  none  pyxfile  cython  compiler  error  compileerror  thinc  extra  egpyx  end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error getting requirement build wheel run successfully  exit code  1 see output  note  error originate subprocess  likely problem pip   end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error  pip subprocess install build dependency run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip  ,try  pip install   githubcom  explosion  spacy  model  release  download  encorewebsm231  encorewebsm231targz pip install   githubcom  explosion  spacy  model  release  download  encorewebsm230  encorewebsm230targz,able install spacy235 version try install spacy235 resume analyser program  encounter pip subprocess install build dependency run successfully error  use python 3123 also give e053 config file error run program regard pyresparser    oserror   e053  could read config file c  smartresumeanalyserapp  mastervenvlibsite  packagespyresparserconfigcfg    venv  ps c  smartresumeanalyserapp  master  pip install spacy235 get requirement build wheel run successfully  exit code  1  267 line output  error compile cython file                                 lent  width int int nrlayer int batchsize   init    lent  width  int nrlayer  int batchsize  nogil                                 thincstructspxd1404  function definition pxd file must declare  cdef inline  error compile cython file                                 thisnrfeat   lent   calloc  batchsize  sizeof  lent   thisisvalid   int   calloc  batchsize  width  nrlayer1   sizeof  int   thiscost   weightt   calloc  batchsize  width  nrlayer1   sizeof  weightt   thissignature   uint64t   calloc  batchsize  sizeof  uint64t     dealloc     nogil                                 thincstructspxd1574  function definition pxd file must declare  cdef inline  error compile cython file                                 free  thisnrfeat  free  thisisvalid  free  thiscost  free  thissignature  void reset   nogil                                 thincstructspxd1724  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisi   free  thisfeat    thisfeat    null thisi  0 int nrin   nogil                                 thincstructspxd1894  function definition pxd file must declare  cdef inline  error compile cython file                                 thisi  0 int nrin   nogil  return thiswidth  0  int nrout   nogil                                 thincstructspxd1924  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiswidth  0  int nrout   nogil  return thiswidth  thisnrlayer  1  int pushback  const featurec  feat  int nrfeat                                 thincstructspxd1954  function definition pxd file must declare  cdef inline  error compile cython file                                 range  thisnrout     thisisvalid  thisi     1 thisi   1 return thisi   thisbatchsize featurec  feature  int  nogil                                 thincstructspxd2264  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisi   thisbatchsize featurec  feature  int  nogil  return thisfeat   int nrfeat  int  nogil                                 thincstructspxd2294  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfeat   int nrfeat  int  nogil  return thisnrfeat   weightt  fwd  int  int j  nogil                                 thincstructspxd2324  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisnrfeat   weightt  fwd  int  int j  nogil  return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil                                 thincstructspxd2354  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd     j  thiswidth    weightt  bwd  int  int j  nogil  return thisbwd     j  thiswidth    weightt  score  int  nogil                                 thincstructspxd2384  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd     j  thiswidth    weightt  score  int  nogil  return thisfwd  thisnrlayer1   weightt  loss  int  nogil                                 thincstructspxd2414  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisfwd  thisnrlayer1   weightt  loss  int  nogil  return thisbwd  thisnrlayer1   weightt  cost  int  nogil                                 thincstructspxd2444  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisbwd  thisnrlayer1   weightt  cost  int  nogil  return thiscost    thisnrout    int  isvalid  int  nogil                                 thincstructspxd2474  function definition pxd file must declare  cdef inline  error compile cython file                                 return thiscost    thisnrout    int  isvalid  int  nogil  return thisisvalid    thisnrout    int guess  int  nogil                                 thincstructspxd2504  function definition pxd file must declare  cdef inline  error compile cython file                                 return thisisvalid    thisnrout    int guess  int  nogil  return vecvecargmaxiftrue  thisscore    thisisvalid    thisnrout    int good  int  nogil                                 thincstructspxd2534  function definition pxd file must declare  cdef inline  warning  thinclinalgpxd140   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd908   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 warning  thinclinalgpxd1748   if  statement deprecate remove future cython version  consider use runtime condition c macros instead  see   githubcom  cython  cython  issues4310 compiling thinc  linalgpyx change  compile thinc  structspyx change  compile thinc  typedefspyx change  compile thinc  linear  avgtronpyx change  compile thinc  linear  featurespyx change  compile thinc  linear  serializepyx change  compile thinc  linear  sparsepyx change  compile thinc  linear  linearpyx change  compile thinc  neural  optimizerspyx change  compile thinc  neural  opspyx change  compile thinc  neuralalignedallocpyx change  compile thinc  extra  egpyx change  compile thinc  extra  mbpyx change  compile thinc  extra  searchpyx change  compile thinc  extra  cachepyx change   115  cythonize thinc  extra  cachepyx  215  cythonize thinc  extra  egpyx traceback  recent call last   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 353   module  main   file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 335  main jsonout   returnval    hook    hookinput   kwarg     file   c  smartresumeanalyserapp  mastervenvlibsite  packagespipvendorpyprojecthooksinprocessinprocesspy   line 118  getrequiresforbuildwheel return hook  configsetting   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 325  getrequiresforbuildwheel return selfgetbuildrequire  configsetting  requirements   wheel     file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 295   getbuildrequire selfrunsetup   file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagessetuptoolsbuildmetapy   line 311  runsetup exec  code  local    file    string    line 258   module  file    string    line 195  setuppackage file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1154  cythonize cythonizeone   args  file   c  usersvasudappdatalocaltemppip  build  env  iv7ops9soverlaylibsite  packagescythonbuilddependenciespy   line 1321  cythonizeone raise compileerror  none  pyxfile  cython  compiler  error  compileerror  thinc  extra  egpyx  end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error getting requirement build wheel run successfully  exit code  1 see output  note  error originate subprocess  likely problem pip   end output  note  error originate subprocess  likely problem pip  error  subprocess  exit  with  error  pip subprocess install build dependency run successfully   exit code  1    see output  note  error originate subprocess  likely problem pip   try  pip install   githubcom  explosion  spacy  model  release  download  encorewebsm231  encorewebsm231targz pip install   githubcom  explosion  spacy  model  release  download  encorewebsm230  encorewebsm230targz,Library/Tool-Based Queries
Filtering stop words out of a multiple text files (using a list of stop words),"I have a folder named cleaned_texts . The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format: ['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar'] . Example: a.txt contains ['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India'] and b.txt contains ['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada'] . I also have another folder named StopWords which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc). Example: currency.txt contains names of currencies (Eg: BAHT | Thailand, PESO | Mexico, YEN | Japan etc) . geographic.txt contains names of countries (Eg: Canada, China, India, Germany etc) . I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder. I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it. Here is my script: import glob import codecs import os #Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') clean_text_data = [] for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: clean_text_data.append(info.read()) #Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) for file in stopwords_files: with open(file, 'r') as w: stop_words = w.read() map_dict = {'|': ''} res = ''.join( idx if idx not in map_dict else map_dict[idx] for idx in stop_words) new_list = res.split() #new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada'] #Trying to save the filtered texts folder_name = ""new_texts"" Path(folder).mkdir(parents=True, exist_ok=True) filtered_sentence = [] for index, word in enumerate(clean_text_data): if word not in new_list: #print(filtered_sentence.append(word)) file_path = Path(folder_name, f""{index}.txt"") with pathlib.Path.open(file_path, ""w"", encoding=""utf-8"") as f: f.write(f""{filtered_sentence }"") Actual/Resulting Output: ""None"" is printing in all the text files. a.txt = None b.txt = None c.txt = None Expected Output: a.txt = ['Rise', 'of', 'e-health', 'and', 'its'] b.txt = ['Man', 'development', 'never','year', 'date']","['python', 'nlp']",1,"You are not correctly combining stopwords from different files. Also, you never assigned any value to filtered_sentence so you end up writing an empty list to your files, resulting in the unexpected output. Try the following instead: import glob import codecs import os from pathlib import Path # Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') # Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) # Combine all stop words into a single list stop_words = [] for file in stopwords_files: with open(file, 'r', encoding='utf-8') as w: stop_words_in_file = [word.strip() for word in w.read().split('|')] stop_words.extend(stop_words_in_file) # Remove duplicates and convert to set for faster lookup stop_words = set(stop_words) # Loop through cleaned texts and filter out stop words folder_name = ""new_texts"" Path(folder_name).mkdir(parents=True, exist_ok=True) for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: content = eval(info.read()) # Convert string to list filtered_content = [word for word in content if word not in stop_words] file_path = os.path.join(folder_name, os.path.basename(path)) with open(file_path, ""w"", encoding=""utf-8"") as f: f.write(str(filtered_content))",2024-05-23 22:15:12,2024-05-24 09:46:37,44,https://stackoverflow.com/questions/78525770/filtering-stop-words-out-of-a-multiple-text-files-using-a-list-of-stop-words,"Filtering stop words out of a multiple text files (using a list of stop words) I have a folder named cleaned_texts . The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format: ['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar'] . Example: a.txt contains ['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India'] and b.txt contains ['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada'] . I also have another folder named StopWords which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc). Example: currency.txt contains names of currencies (Eg: BAHT | Thailand, PESO | Mexico, YEN | Japan etc) . geographic.txt contains names of countries (Eg: Canada, China, India, Germany etc) . I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder. I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it. Here is my script: import glob import codecs import os #Cleaned texts os.getcwd() clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts') clean_text_data = [] for root, folders, files in os.walk(clean_texts_folder): for file in files: path = os.path.join(root, file) with codecs.open(path, encoding='utf-8', errors='ignore') as info: clean_text_data.append(info.read()) #Stop Words stopwords_folder_path = ""StopWords"" stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt')) for file in stopwords_files: with open(file, 'r') as w: stop_words = w.read() map_dict = {'|': ''} res = ''.join( idx if idx not in map_dict else map_dict[idx] for idx in stop_words) new_list = res.split() #new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada'] #Trying to save the filtered texts folder_name = ""new_texts"" Path(folder).mkdir(parents=True, exist_ok=True) filtered_sentence = [] for index, word in enumerate(clean_text_data): if word not in new_list: #print(filtered_sentence.append(word)) file_path = Path(folder_name, f""{index}.txt"") with pathlib.Path.open(file_path, ""w"", encoding=""utf-8"") as f: f.write(f""{filtered_sentence }"") Actual/Resulting Output: ""None"" is printing in all the text files. a.txt = None b.txt = None c.txt = None Expected Output: a.txt = ['Rise', 'of', 'e-health', 'and', 'its'] b.txt = ['Man', 'development', 'never','year', 'date']",filtering stop word multiple text file  use list stop word  folder name cleanedtexts  folder contain text file  atxt  btxt  ctxt etc  text file contain tokenized word format    rise    of    e  health    and    its    germany    dollar    example  atxt contain   rise    of    e  health    and    its    thailand    yen    india   btxt contain   peso    man    development    never    japan    year    date    canada    also another folder name stopwords also contain text file text file contain stop word  text file name format  currencytxt  namestxt  geographictxt etc   example  currencytxt contain name currency  eg  baht  thailand  peso  mexico  yen  japan etc   geographictxt contain name country  eg  canada  china  india  germany etc   want filter stop word contain text file inside stopwords folder  text file cleanedtexts folder  loop stop word folder  combine stop word convert list  challenge filter stop word cleanedtexts file  day could not figure  script  import glob import codec import os  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts   cleantextdata    root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  cleantextdataappend  inforead     stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt    file stopwordsfile  open  file   r   w  stopwords  wread   mapdict          re    join  idx idx mapdict else mapdict  idx  idx stopwords  newlist  ressplit    newlist output   smith    surname    from    1990    thailand    yen    india    peso    japan    canada    try save filter text foldername    newtexts  path  folder  mkdir  parent  true  existok  true  filteredsentence    index  word enumerate  cleantextdata   word newlist   print  filteredsentenceappend  word   filepath  path  foldername  f   index  txt   pathlibpathopen  filepath    w   encoding  utf8   f  fwrite  f   filteredsentence    actual  result output    none  print text file  atxt  none btxt  none ctxt  none expect output  atxt    rise    of    e  health    and    its   btxt    man    development    never    year    date  ,correctly combine stopword different file  also  never assign value filteredsentence end write empty list file  result unexpected output  try follow instead  import glob import codec import os pathlib import path  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts    stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt     combine stop word single list stopwords    file stopwordsfile  open  file   r   encodingutf8   w  stopwordsinfile   wordstrip   word wread   split       stopwordsextend  stopwordsinfile   remove duplicate convert set fast lookup stopwords  set  stopwords   loop clean text filter stop word foldername    newtexts  path  foldername  mkdir  parent  true  existok  true  root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  content  eval  inforead     convert string list filteredcontent   word word content word stopwords  filepath  ospathjoin  foldername  ospathbasename  path   open  filepath    w   encoding  utf8   f  fwrite  str  filteredcontent  ,filtering stop word multiple text file  use list stop word  folder name cleanedtexts  folder contain text file  atxt  btxt  ctxt etc  text file contain tokenized word format    rise    of    e  health    and    its    germany    dollar    example  atxt contain   rise    of    e  health    and    its    thailand    yen    india   btxt contain   peso    man    development    never    japan    year    date    canada    also another folder name stopwords also contain text file text file contain stop word  text file name format  currencytxt  namestxt  geographictxt etc   example  currencytxt contain name currency  eg  baht  thailand  peso  mexico  yen  japan etc   geographictxt contain name country  eg  canada  china  india  germany etc   want filter stop word contain text file inside stopwords folder  text file cleanedtexts folder  loop stop word folder  combine stop word convert list  challenge filter stop word cleanedtexts file  day could not figure  script  import glob import codec import os  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts   cleantextdata    root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  cleantextdataappend  inforead     stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt    file stopwordsfile  open  file   r   w  stopwords  wread   mapdict          re    join  idx idx mapdict else mapdict  idx  idx stopwords  newlist  ressplit    newlist output   smith    surname    from    1990    thailand    yen    india    peso    japan    canada    try save filter text foldername    newtexts  path  folder  mkdir  parent  true  existok  true  filteredsentence    index  word enumerate  cleantextdata   word newlist   print  filteredsentenceappend  word   filepath  path  foldername  f   index  txt   pathlibpathopen  filepath    w   encoding  utf8   f  fwrite  f   filteredsentence    actual  result output    none  print text file  atxt  none btxt  none ctxt  none expect output  atxt    rise    of    e  health    and    its   btxt    man    development    never    year    date   correctly combine stopword different file  also  never assign value filteredsentence end write empty list file  result unexpected output  try follow instead  import glob import codec import os pathlib import path  clean text osgetcwd   cleantextsfolder  ospathjoin  osgetcwd     cleanedtexts    stop word stopwordsfolderpath    stopwords  stopwordsfile  globglob  ospathjoin  stopwordsfolderpath    txt     combine stop word single list stopwords    file stopwordsfile  open  file   r   encodingutf8   w  stopwordsinfile   wordstrip   word wread   split       stopwordsextend  stopwordsinfile   remove duplicate convert set fast lookup stopwords  set  stopwords   loop clean text filter stop word foldername    newtexts  path  foldername  mkdir  parent  true  existok  true  root  folder  file oswalk  cleantextsfolder   file file  path  ospathjoin  root  file  codecsopen  path  encodingutf8   errorsignore   info  content  eval  inforead     convert string list filteredcontent   word word content word stopwords  filepath  ospathjoin  foldername  ospathbasename  path   open  filepath    w   encoding  utf8   f  fwrite  str  filteredcontent  ,Task-Specific Queries
How to lemmatize text column in pandas dataframes using stanza?,I read csv file into pandas dataframe. my text column is df['story']. how do I lemmatize this colummn ? should I tokenize before?,"['pandas', 'nlp', 'tokenize', 'lemmatization', 'stanza']",1,"No, you don't necessarily have to tokenize before lemmatizing. You can try the following code: import stanza import pandas as pd nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma') def lemmatize_text(text): doc = nlp(text) lemmas = [word.lemma for sent in doc.sentences for word in sent.words] return ' '.join(lemmas) df['lemmatized_story'] = df['story'].apply(lemmatize_text)",2024-05-16 12:34:50,2024-05-16 17:14:59,72,https://stackoverflow.com/questions/78489915/how-to-lemmatize-text-column-in-pandas-dataframes-using-stanza,How to lemmatize text column in pandas dataframes using stanza? I read csv file into pandas dataframe. my text column is df['story']. how do I lemmatize this colummn ? should I tokenize before?,lemmatize text column panda dataframe use stanza  read csv file panda dataframe  text column df   story    lemmatize colummn  tokenize , not necessarily tokenize lemmatizing  try follow code  import stanza import panda pd nlp  stanza  pipeline  langen   processorstokenize  mwt  pos  lemma   def lemmatizetext  text   doc  nlp  text  lemma   wordlemma send docsentence word sentword  return   join  lemmas  df   lemmatizedstory    df   story   apply  lemmatizetext ,lemmatize text column panda dataframe use stanza  read csv file panda dataframe  text column df   story    lemmatize colummn  tokenize   not necessarily tokenize lemmatizing  try follow code  import stanza import panda pd nlp  stanza  pipeline  langen   processorstokenize  mwt  pos  lemma   def lemmatizetext  text   doc  nlp  text  lemma   wordlemma send docsentence word sentword  return   join  lemmas  df   lemmatizedstory    df   story   apply  lemmatizetext ,Basic Understanding
Encode a list of sentences into embeddings using a HuggingFace model not in its hub,"I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's M2M100 model, I do not get the expected results. When using a model within SentenceTransformer() , my results look like this: from sentence_transformers import SentenceTransformer dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_1 = SentenceTransformer('all-distilroberta-v1') embeddings_1 = model_1.encode(dat) embeddings_1.shape > (2, 768) However, when I use the M2M100 model, my results do not look right at all, specifically I would expect 2 rows of results: from transformers import M2M100Tokenizer model_m2m = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"") model_m2m.src_lang = ""en"" embeddings_m2m = model_m2m.encode(dat, return_tensors=""pt"") embeddings_m2m.shape > torch.Size([1, 4]) How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding? (As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)","['nlp', 'huggingface-transformers', 'encode', 'embedding', 'huggingface']",1,"The code you provided only uses the tokenizer of the model, which maps the text to integer ids that don't represent any kind of (semantical) meaning. To retrieve sentence embeddings (i.e. a vector that represents the text) from facebook/m2m100_418M , which is an encoder-decoder model, you need to perform some kind of pooling over the last-hidden-state of the encoder. Common approaches, which are cls and mean pooling are shown in the example below: import torch from transformers import M2M100Tokenizer, M2M100Model def mean_pooling(last_hidden_state, attention_mask): non_pad_tokens = attention_mask.sum(1) sum_embeddings = torch.sum(attention_mask.unsqueeze(-1) * last_hidden_state, 1) return sum_embeddings/non_pad_tokens.unsqueeze(-1) def cls_pooling(last_hidden_state): return last_hidden_state[:,0] dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_id = ""facebook/m2m100_418M"" t_m2m = M2M100Tokenizer.from_pretrained(model_id) t_m2m.src_lang = ""en"" m_m2m = M2M100Model.from_pretrained(model_id) tokenized = t_m2m(dat, padding=True, return_tensors='pt') with torch.inference_mode(): encoder_o = m_m2m.encoder(**tokenized) encoder_last_hidden_state = encoder_o.last_hidden_state print(encoder_last_hidden_state.shape) mean_pooling_embeddings = mean_pooling(encoder_last_hidden_state, tokenized.attention_mask) print(mean_pooling_embeddings.shape) cls_pooling_embeddings = cls_pooling(encoder_last_hidden_state) print(cls_pooling_embeddings.shape) Output: torch.Size([2, 9, 1024]) torch.Size([2, 1024]) torch.Size([2, 1024]) Which of the two approaches works better for your downstream task, must be tested with your data. Please also note that even when you have the sentence embeddings now, it doesn't mean they are semantically meaningful (i.e. the embeddings are useless for your downstream task). Refer to this StackOverflow answer for further explanation.",2024-05-15 16:39:39,2024-05-19 10:57:24,771,https://stackoverflow.com/questions/78485347/encode-a-list-of-sentences-into-embeddings-using-a-huggingface-model-not-in-its,"Encode a list of sentences into embeddings using a HuggingFace model not in its hub I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's M2M100 model, I do not get the expected results. When using a model within SentenceTransformer() , my results look like this: from sentence_transformers import SentenceTransformer dat = ['Meteorite fell on the road ', 'I went in the wrong direction'] model_1 = SentenceTransformer('all-distilroberta-v1') embeddings_1 = model_1.encode(dat) embeddings_1.shape > (2, 768) However, when I use the M2M100 model, my results do not look right at all, specifically I would expect 2 rows of results: from transformers import M2M100Tokenizer model_m2m = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"") model_m2m.src_lang = ""en"" embeddings_m2m = model_m2m.encode(dat, return_tensors=""pt"") embeddings_m2m.shape > torch.Size([1, 4]) How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding? (As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)",encode list sentence embedding use huggingface model hub try encode list sentence list embedding  use model huggingface hub  work expect  use model hub  case facebook s m2m100 model  get expect result  use model within sentencetransformer    result look like  sentencetransformer import sentencetransformer dat    meteorite fall road    go wrong direction   model1  sentencetransformer   all  distilroberta  v1   embeddings1  model1encode  dat  embeddings1shape   2  768  however  use m2m100 model  result look right  specifically would expect 2 row result  transformer import m2m100tokenizer modelm2 m  m2m100tokenizerfrompretrained    facebook  m2m100418 m   modelm2msrclang    en  embeddingsm2 m  modelm2mencode  dat  returntensors  pt   embeddingsm2mshape  torch  size   1  4   format return n  dimensional list embedding  row correspond sentence number column equal dimensionality embed   note  eventually sentence language   m use multi  lingual model  ,code provide use tokenizer model  map text integer id not represent kind  semantical  meaning  retrieve sentence embedding  ie  vector represent text  facebook  m2m100418 m  encoder  decoder model  need perform kind pool last  hide  state encoder  common approach  cls mean pooling show example  import torch transformer import m2m100tokenizer  m2m100model def meanpooling  lasthiddenstate  attentionmask   nonpadtoken  attentionmasksum  1  sumembedding  torchsum  attentionmaskunsqueeze  1   lasthiddenstate  1  return sumembedding  nonpadtokensunsqueeze  1  def clspooling  lasthiddenstate   return lasthiddenstate    0  dat    meteorite fall road    go wrong direction   modelid    facebook  m2m100418 m  tm2 m  m2m100tokenizerfrompretrained  modelid  tm2msrclang    en  mm2 m  m2m100modelfrompretraine  modelid  tokenized  tm2 m  dat  pad  true  returntensorspt   torchinferencemode    encodero  mm2mencoder    tokenized  encoderlasthiddenstate  encoderolasthiddenstate print  encoderlasthiddenstateshape  meanpoolingembeddings  meanpooling  encoderlasthiddenstate  tokenizedattentionmask  print  meanpoolingembeddingsshape  clspoolingembedding  clspoole  encoderlasthiddenstate  print  clspoolingembeddingsshape  output  torch  size   2  9  1024   torch  size   2  1024   torch  size   2  1024   two approach work well downstream task  must test datum  please also note even sentence embedding  not mean semantically meaningful  ie  embedding useless downstream task   refer stackoverflow answer explanation ,encode list sentence embedding use huggingface model hub try encode list sentence list embedding  use model huggingface hub  work expect  use model hub  case facebook s m2m100 model  get expect result  use model within sentencetransformer    result look like  sentencetransformer import sentencetransformer dat    meteorite fall road    go wrong direction   model1  sentencetransformer   all  distilroberta  v1   embeddings1  model1encode  dat  embeddings1shape   2  768  however  use m2m100 model  result look right  specifically would expect 2 row result  transformer import m2m100tokenizer modelm2 m  m2m100tokenizerfrompretrained    facebook  m2m100418 m   modelm2msrclang    en  embeddingsm2 m  modelm2mencode  dat  returntensors  pt   embeddingsm2mshape  torch  size   1  4   format return n  dimensional list embedding  row correspond sentence number column equal dimensionality embed   note  eventually sentence language   m use multi  lingual model   code provide use tokenizer model  map text integer id not represent kind  semantical  meaning  retrieve sentence embedding  ie  vector represent text  facebook  m2m100418 m  encoder  decoder model  need perform kind pool last  hide  state encoder  common approach  cls mean pooling show example  import torch transformer import m2m100tokenizer  m2m100model def meanpooling  lasthiddenstate  attentionmask   nonpadtoken  attentionmasksum  1  sumembedding  torchsum  attentionmaskunsqueeze  1   lasthiddenstate  1  return sumembedding  nonpadtokensunsqueeze  1  def clspooling  lasthiddenstate   return lasthiddenstate    0  dat    meteorite fall road    go wrong direction   modelid    facebook  m2m100418 m  tm2 m  m2m100tokenizerfrompretrained  modelid  tm2msrclang    en  mm2 m  m2m100modelfrompretraine  modelid  tokenized  tm2 m  dat  pad  true  returntensorspt   torchinferencemode    encodero  mm2mencoder    tokenized  encoderlasthiddenstate  encoderolasthiddenstate print  encoderlasthiddenstateshape  meanpoolingembeddings  meanpooling  encoderlasthiddenstate  tokenizedattentionmask  print  meanpoolingembeddingsshape  clspoolingembedding  clspoole  encoderlasthiddenstate  print  clspoolingembeddingsshape  output  torch  size   2  9  1024   torch  size   2  1024   torch  size   2  1024   two approach work well downstream task  must test datum  please also note even sentence embedding  not mean semantically meaningful  ie  embedding useless downstream task   refer stackoverflow answer explanation ,Implementation Issues
How to optimize this function and improve running time?,"I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time? Here's my code: def pmi_count_phrase_create(pmi_tups,freq_list): import pandas as pd """"""pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)] freq_list is a result of running freq_list= finder.ngram_fd.items() -> df made up of columns for pmi list, count list, phrase list"""""" pmi3_list =[] count3_list =[] phrase3_list =[] for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..] for item in freq_list: quadgram,count = item if quadgram == phrase: pmi3_list.append(pmi) count3_list.append(count) phrase3_list.append(phrase) # create dataframe df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list}) return df Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.","['python', 'performance', 'optimization', 'nlp', 'nltk']",1,"Ended up changing my function to convert freq_list to a dictionary and list comprehensions instead of for loops and this code instantly returned a data-frame: def quicker_func(pmi_tups, freq_list): import pandas as pd freq_dict = dict(freq_list) # Create a dictionary for faster lookups pmi_list = [pmi for phrase, pmi in pmi_tups if phrase in freq_dict] count_list = [freq_dict[phrase] for phrase, pmi in pmi_tups if phrase in freq_dict] phrase_list = [phrase for phrase, pmi in pmi_tups if phrase in freq_dict] df = pd.DataFrame({'Phrase': phrase_list, 'PMI': pmi_list, 'Count': count_list}) return df",2024-05-10 15:18:00,2024-05-10 17:42:18,47,https://stackoverflow.com/questions/78461078/how-to-optimize-this-function-and-improve-running-time,"How to optimize this function and improve running time? I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time? Here's my code: def pmi_count_phrase_create(pmi_tups,freq_list): import pandas as pd """"""pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)] freq_list is a result of running freq_list= finder.ngram_fd.items() -> df made up of columns for pmi list, count list, phrase list"""""" pmi3_list =[] count3_list =[] phrase3_list =[] for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..] for item in freq_list: quadgram,count = item if quadgram == phrase: pmi3_list.append(pmi) count3_list.append(count) phrase3_list.append(phrase) # create dataframe df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list}) return df Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.",optimize function improve run time  function aim create data  frame three column  bigram  phrase  count  bigram  phrase   pmi score  bigram  phrase   since want run large dataset million phrase  compute time incredibly long  recognize nest for  loop matching condition contribute computation difficulty  alternative way thing cut run  time  s code  def pmicountphrasecreate  pmitup  freqlist   import panda pd     pmitup result run pmitup   finderscorengrams  bigrammeasurespmi   freqlist result run freqlist finderngramfditems     df make column pmi list  count list  phrase list    pmi3list    count3list    phrase3list    phrase  pmi pmitup   pmitup list tuple form     phrase   pmi     item freqlist  quadgram  count  item quadgram   phrase  pmi3listappend  pmi  count3listappend  count  phrase3listappend  phrase   create dataframe df  pd  dataframe    phrase   phrase3list   pmi   pmi3list   count   count3list   return df running code pmitup freqlist  still run s 1000 minute   m open also use different library evaluate bi  gram phrase  pmi s frequency ,end change function convert freqlist dictionary list comprehension instead loop code instantly return data  frame  def quickerfunc  pmitup  freqlist   import panda pd freqdict  dict  freqlist   create dictionary fast lookup pmilist   pmi phrase  pmi pmitup phrase freqdict  countlist   freqdict  phrase  phrase  pmi pmitup phrase freqdict  phraselist   phrase phrase  pmi pmitup phrase freqdict  df  pd  dataframe    phrase   phraselist   pmi   pmilist   count   countlist   return df,optimize function improve run time  function aim create data  frame three column  bigram  phrase  count  bigram  phrase   pmi score  bigram  phrase   since want run large dataset million phrase  compute time incredibly long  recognize nest for  loop matching condition contribute computation difficulty  alternative way thing cut run  time  s code  def pmicountphrasecreate  pmitup  freqlist   import panda pd     pmitup result run pmitup   finderscorengrams  bigrammeasurespmi   freqlist result run freqlist finderngramfditems     df make column pmi list  count list  phrase list    pmi3list    count3list    phrase3list    phrase  pmi pmitup   pmitup list tuple form     phrase   pmi     item freqlist  quadgram  count  item quadgram   phrase  pmi3listappend  pmi  count3listappend  count  phrase3listappend  phrase   create dataframe df  pd  dataframe    phrase   phrase3list   pmi   pmi3list   count   count3list   return df running code pmitup freqlist  still run s 1000 minute   m open also use different library evaluate bi  gram phrase  pmi s frequency  end change function convert freqlist dictionary list comprehension instead loop code instantly return data  frame  def quickerfunc  pmitup  freqlist   import panda pd freqdict  dict  freqlist   create dictionary fast lookup pmilist   pmi phrase  pmi pmitup phrase freqdict  countlist   freqdict  phrase  phrase  pmi pmitup phrase freqdict  phraselist   phrase phrase  pmi pmitup phrase freqdict  df  pd  dataframe    phrase   phraselist   pmi   pmilist   count   countlist   return df,Basic Understanding
Why doesn&#39;t fuzzywuzzy&#39;s process.extractBests give a 100% score when the tested string 100% contains the query string?,"I'm testing fuzzywuzzy 's process.extractBests() as follows: from fuzzywuzzy import process # Define the query string query = ""Apple"" # Define the list of choices choices = [""Apple"", ""Apple Inc."", ""Apple Computer"", ""Apple Records"", ""Apple TV""] # Call the process.extractBests function results = process.extractBests(query, choices) # Print the results for result in results: print(result) It outputs: ('Apple', 100) ('Apple Inc.', 90) ('Apple Computer', 90) ('Apple Records', 90) ('Apple TV', 90) Why didn't the scorer give 100 to all strings since they all 100% contain the query string (""Apple"")? I use fuzzywuzzy==0.18.0 with Python 3.11.7.","['python', 'nlp', 'string-matching', 'fuzzywuzzy']",1,"The fuzzywuzzy 's extractBests() function does not give 100% because it does not check for a match, it checks for similarity, such as length of string, contents of string compared to the query, positions of the query string, and a few other factors. In your case, it does not output 100% because ""Apple Inc."" is not an exact match of your query, ""Apple"". This is why only the ""Apple"" choice outputs 100%, because it 100% matches with the query, ""Apple"". I hoped this helped!",2024-05-09 14:24:36,2024-05-09 14:36:54,76,https://stackoverflow.com/questions/78455102/why-doesnt-fuzzywuzzys-process-extractbests-give-a-100-score-when-the-tested,"Why doesn&#39;t fuzzywuzzy&#39;s process.extractBests give a 100% score when the tested string 100% contains the query string? I'm testing fuzzywuzzy 's process.extractBests() as follows: from fuzzywuzzy import process # Define the query string query = ""Apple"" # Define the list of choices choices = [""Apple"", ""Apple Inc."", ""Apple Computer"", ""Apple Records"", ""Apple TV""] # Call the process.extractBests function results = process.extractBests(query, choices) # Print the results for result in results: print(result) It outputs: ('Apple', 100) ('Apple Inc.', 90) ('Apple Computer', 90) ('Apple Records', 90) ('Apple TV', 90) Why didn't the scorer give 100 to all strings since they all 100% contain the query string (""Apple"")? I use fuzzywuzzy==0.18.0 with Python 3.11.7.",  39  fuzzywuzzy   39  processextractbest give 100  score test string 100  contain query string   m test fuzzywuzzy s processextractbest   follow  fuzzywuzzy import process  define query string query    apple   define list choice choice     apple     apple inc     apple computer     apple records     apple tv    call processextractbest function result  processextractbest  query  choice   print result result result  print  result  output    apple   100    apple inc   90    apple computer   90    apple records   90    apple tv   90  not scorer give 100 string since 100  contain query string    apple    use fuzzywuzzy0180 python 3117 ,fuzzywuzzy s extractbest   function give 100  check match  check similarity  length string  content string compare query  position query string  factor  case  output 100    apple inc  exact match query    apple     apple  choice output 100   100  match query    apple   hoped help ,  39  fuzzywuzzy   39  processextractbest give 100  score test string 100  contain query string   m test fuzzywuzzy s processextractbest   follow  fuzzywuzzy import process  define query string query    apple   define list choice choice     apple     apple inc     apple computer     apple records     apple tv    call processextractbest function result  processextractbest  query  choice   print result result result  print  result  output    apple   100    apple inc   90    apple computer   90    apple records   90    apple tv   90  not scorer give 100 string since 100  contain query string    apple    use fuzzywuzzy0180 python 3117  fuzzywuzzy s extractbest   function give 100  check match  check similarity  length string  content string compare query  position query string  factor  case  output 100    apple inc  exact match query    apple     apple  choice output 100   100  match query    apple   hoped help ,Task-Specific Queries
FastText language_identification in R returns too many arguments - how to match to texts?,"FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document. There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts? Code: DF = data.frame(doc_id = seq(1, 5), speechtext = c(""Hello. Fake text entry 1."", ""Fake text entry 2"", ""more text"", ""Text in a different language"", ""Hola"")) library(fastText) # download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html file_ftz = system.file(""language_identification/lid.176.ftz"", package = ""fastText"") lang1 = language_identification(DF$speechtext, pre_trained_language_model_path = file_ftz, verbose = T) I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with. Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug. (I tried adding intern = T as an argument per R - fasttext how to load output into a dataframe from command line -- this is not recognized as an argument).","['r', 'nlp', 'fasttext', 'language-detection']",1,"The first argument to fastText::language_identification() is defined as: either a valid character string to a valid path where each line represents a different text extract or a vector of text extracts (emphasis mine) You have line breaks in your input data: DF$speechtext[4] [1] ""Text in a\ndifferent language"" As one prediction is generated per line, you'll get two predictions from this element. You have two options: Remove new lines in your input data. This makes sense in this case. Keep new lines and map document IDs to each line. This makes sense if new lines might actually be in different languages. Remove new lines If you replace new lines with spaces you will get the same number of predictions returned as input rows. In the regex below, I have used the PCRE \v which matches newlines and any character considered vertical whitespace. This now produces five rows, one relating to each input row. language_identification(gsub(""\\v"", "" "", DF$speechtext, perl = TRUE), file_ftz) # iso_lang_1 prob_1 # <char> <num> # 1: en 0.220767 # 2: en 0.388695 # 3: en 0.613707 # 4: en 0.757671 # 5: es 0.721487 \v includes several vertical space characters (such as form feed and line separator), so should cover all possible types of new line. For full details see the table here . Keep new lines and map document ID to each line Alternatively, if different lines of each input document might be in different languages, you may not want to remove new lines. In this case, you can predict each line separately and then map the document IDs to each line: # As before lang1 <- language_identification(DF$speechtext, file_ftz) # Add document IDs lang1$doc_id <- rep( DF$doc_id, lengths(strsplit(DF$speechtext, ""\\v"", perl = TRUE)) ) lang1 # iso_lang_1 prob_1 doc_id # <char> <num> <int> # 1: en 0.220767 1 # 2: en 0.388695 2 # 3: en 0.613707 3 # 4: en 0.932691 4 # 5: en 0.571937 4 # 6: es 0.721487 5",2024-05-07 16:46:21,2024-05-07 16:56:33,132,https://stackoverflow.com/questions/78443980/fasttext-language-identification-in-r-returns-too-many-arguments-how-to-match,"FastText language_identification in R returns too many arguments - how to match to texts? FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document. There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts? Code: DF = data.frame(doc_id = seq(1, 5), speechtext = c(""Hello. Fake text entry 1."", ""Fake text entry 2"", ""more text"", ""Text in a different language"", ""Hola"")) library(fastText) # download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html file_ftz = system.file(""language_identification/lid.176.ftz"", package = ""fastText"") lang1 = language_identification(DF$speechtext, pre_trained_language_model_path = file_ftz, verbose = T) I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with. Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug. (I tried adding intern = T as an argument per R - fasttext how to load output into a dataframe from command line -- this is not recognized as an argument).",fasttext languageidentification r return many argument  match text  fasttext languageidentification return multiple prediction per original text  also fails indicate belong original document  differ number prediction per original document  github forum close  anyone know match output original text  code  df  dataframe  docid  seq  1  5   speechtext  c    hello  fake text entry 1       fake text entry 2     text     text different language     hola    library  fasttext   download ftz pretraine model   fasttextcc  docs  en  language  identificationhtml fileftz  systemfile    languageidentification  lid176ftz   package    fasttext   lang1  languageidentification  df  speechtext  pretrainedlanguagemodelpath  fileftz  verbose   expect one prediction per original text  least consistent number  way mark document prediction align  really could guess base large number per series element output  not seem optimal  seem like bug   try add intern  argument per r  fasttext load output dataframe command line  recognize argument  ,first argument fasttext   languageidentification   define  either valid character string valid path line represent different text extract vector text extract  emphasis mine  line break input datum  df  speechtext  4   1    text andifferent language  one prediction generate per line  will get two prediction element  two option  remove new line input datum  make sense case  keep new line map document id line  make sense new line might actually different language  remove new line replace new line space get number prediction return input row  regex  use pcre v match newline character consider vertical whitespace  produce five row  one relate input row  languageidentification  gsub    v        df  speechtext  perl  true   fileftz   isolang1 prob1   char   num   1  en 0220767  2  en 0388695  3  en 0613707  4  en 0757671  5  es 0721487 v include several vertical space character  form feed line separator   cover possible type new line  full detail see table  keep new line map document id line alternatively  different line input document might different language  may want remove new line  case  predict line separately map document id line   lang1   languageidentification  df  speechtext  fileftz   add document id lang1  docid   rep  df  docid  length  strsplit  df  speechtext    v   perl  true    lang1  isolang1 prob1 docid   char   num   int   1  en 0220767 1  2  en 0388695 2  3  en 0613707 3  4  en 0932691 4  5  en 0571937 4  6  es 0721487 5,fasttext languageidentification r return many argument  match text  fasttext languageidentification return multiple prediction per original text  also fails indicate belong original document  differ number prediction per original document  github forum close  anyone know match output original text  code  df  dataframe  docid  seq  1  5   speechtext  c    hello  fake text entry 1       fake text entry 2     text     text different language     hola    library  fasttext   download ftz pretraine model   fasttextcc  docs  en  language  identificationhtml fileftz  systemfile    languageidentification  lid176ftz   package    fasttext   lang1  languageidentification  df  speechtext  pretrainedlanguagemodelpath  fileftz  verbose   expect one prediction per original text  least consistent number  way mark document prediction align  really could guess base large number per series element output  not seem optimal  seem like bug   try add intern  argument per r  fasttext load output dataframe command line  recognize argument   first argument fasttext   languageidentification   define  either valid character string valid path line represent different text extract vector text extract  emphasis mine  line break input datum  df  speechtext  4   1    text andifferent language  one prediction generate per line  will get two prediction element  two option  remove new line input datum  make sense case  keep new line map document id line  make sense new line might actually different language  remove new line replace new line space get number prediction return input row  regex  use pcre v match newline character consider vertical whitespace  produce five row  one relate input row  languageidentification  gsub    v        df  speechtext  perl  true   fileftz   isolang1 prob1   char   num   1  en 0220767  2  en 0388695  3  en 0613707  4  en 0757671  5  es 0721487 v include several vertical space character  form feed line separator   cover possible type new line  full detail see table  keep new line map document id line alternatively  different line input document might different language  may want remove new line  case  predict line separately map document id line   lang1   languageidentification  df  speechtext  fileftz   add document id lang1  docid   rep  df  docid  length  strsplit  df  speechtext    v   perl  true    lang1  isolang1 prob1 docid   char   num   int   1  en 0220767 1  2  en 0388695 2  3  en 0613707 3  4  en 0932691 4  5  en 0571937 4  6  es 0721487 5,Library/Tool-Based Queries
Determining contents of decoder_hidden_states from T5ForConditionalGeneration,"I'm using the Huggingface T5ForConditionalGeneration model without modification. I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the decoder_hidden_states contains what I'm looking for. I want to do something like this: # Prepare batch data sources = batch_df['Source'].tolist() tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda') input_ids = tokenized_input['input_ids'].to('cuda') attention_mask = tokenized_input['attention_mask'].to('cuda') input_batch = { 'input_ids': input_ids, 'attention_mask': attention_mask, 'do_sample': False, 'num_beams': 1, 'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id, 'max_length': self.max_output_length, 'output_scores': True, 'return_dict_in_generate': True, 'output_hidden_states': True, } outputs = self.model.generate(**input_batch) # Retrieve the decoder hidden states decoder_last_hidden_state = outputs.decoder_hidden_states[-1] # Last layer's hidden states # Compute the mean of the hidden states across the sequence length dimension mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False) This approach works for the encoder, but for the decoder, decoder_hidden_states[-1] is a tuple of tensors, not a tensor. When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors. When I inspected the dimensions, like this: for tuple_number in range(n): # Checking the tuples print(f""Tuple {layer_number}:"") for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]): print(f"" Tuple {i} in Layer {layer_number}: shape {tensor.shape}"") the outputs were all like this: Tuple 0: Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512]) Tuple 1: Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512]) . . . 512 is the max_length of my tokenizer, and 2 is my batch size. (I verified that 2 is the batch size because that number changed when I modified my batch size.) Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39. Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits. So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple. However, I don't understand exactly how the token length corresponds to the number of tuples. How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.","['pytorch', 'nlp', 'huggingface-transformers']",1,"I think whats happening is that T5 returns the hidden state per step of decoding. Therefore, the number of tuples should correspond to the longest generated sequence. You are most likely interested in the last decoding step and could take the last tuple. In that tuple you have a tuple of size num_layers + 1 (+1 for the final LayerNorm). The output of the last layer should be the last tuple entry.",2024-05-04 22:13:37,2024-05-08 10:14:16,121,https://stackoverflow.com/questions/78430524/determining-contents-of-decoder-hidden-states-from-t5forconditionalgeneration,"Determining contents of decoder_hidden_states from T5ForConditionalGeneration I'm using the Huggingface T5ForConditionalGeneration model without modification. I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the decoder_hidden_states contains what I'm looking for. I want to do something like this: # Prepare batch data sources = batch_df['Source'].tolist() tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda') input_ids = tokenized_input['input_ids'].to('cuda') attention_mask = tokenized_input['attention_mask'].to('cuda') input_batch = { 'input_ids': input_ids, 'attention_mask': attention_mask, 'do_sample': False, 'num_beams': 1, 'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id, 'max_length': self.max_output_length, 'output_scores': True, 'return_dict_in_generate': True, 'output_hidden_states': True, } outputs = self.model.generate(**input_batch) # Retrieve the decoder hidden states decoder_last_hidden_state = outputs.decoder_hidden_states[-1] # Last layer's hidden states # Compute the mean of the hidden states across the sequence length dimension mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False) This approach works for the encoder, but for the decoder, decoder_hidden_states[-1] is a tuple of tensors, not a tensor. When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors. When I inspected the dimensions, like this: for tuple_number in range(n): # Checking the tuples print(f""Tuple {layer_number}:"") for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]): print(f"" Tuple {i} in Layer {layer_number}: shape {tensor.shape}"") the outputs were all like this: Tuple 0: Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512]) Tuple 1: Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512]) Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512]) . . . 512 is the max_length of my tokenizer, and 2 is my batch size. (I verified that 2 is the batch size because that number changed when I modified my batch size.) Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39. Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits. So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple. However, I don't understand exactly how the token length corresponds to the number of tuples. How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.",determine content decoderhiddenstates t5forconditionalgeneration  m use huggingface t5forconditionalgeneration model without modification  want compute mean pool last hide state t5 decoder  can not determine part decoderhiddenstates contain  m look  want something like   prepare batch data source  batchdf   source   tolist   tokenizedinput  selftokenizer  source  returntensorspt   pad  true  truncation  true  maxlength  selfmaxlength  to   cuda   inputids  tokenizedinput   inputids   to   cuda   attentionmask  tokenizedinput   attentionmask   to   cuda   inputbatch    inputids   inputids   attentionmask   attentionmask   dosample   false   numbeams   1   eostokenid   selftokenizereostokenid   padtokenid   selftokenizerpadtokenid   maxlength   selfmaxoutputlength   outputscore   true   returndictingenerate   true   outputhiddenstate   true   output  selfmodelgenerate    inputbatch   retrieve decoder hide state decoderlasthiddenstate  outputsdecoderhiddenstate  1   last layer s hide state  compute mean hide state across sequence length dimension meanpooledoutput  torchmean  decoderlasthiddenstate  dim1  keepdim  false  approach work encoder  decoder  decoderhiddenstates  1  tuple tensor  tensor  first inspect tuple  10 tuple  tuple contain 7 tensor  inspect dimension  like  tuplenumber range  n    check tuple print  f  tuple  layernumber      tensor enumerate  outputsdecoderhiddenstate  layernumber    print  f  tuple   layer  layernumber   shape  tensorshape    output like  tuple 0  tensor 0 tuple 0  shape torch  size   2  1  512   tensor 1 tuple 0  shape torch  size   2  1  512   tensor 2 tuple 0  shape torch  size   2  1  512   tensor 3 tuple 0  shape torch  size   2  1  512   tensor 4 tuple 0  shape torch  size   2  1  512   tensor 5 tuple 0  shape torch  size   2  1  512   tensor 6 tuple 0  shape torch  size   2  1  512   tuple 1  tensor 0 tuple 1  shape torch  size   2  1  512   tensor 1 tuple 1  shape torch  size   2  1  512   tensor 2 tuple 1  shape torch  size   2  1  512   tensor 3 tuple 1  shape torch  size   2  1  512   tensor 4 tuple 1  shape torch  size   2  1  512   tensor 5 tuple 1  shape torch  size   2  1  512   tensor 6 tuple 1  shape torch  size   2  1  512      512 maxlength tokenizer  2 batch size   verify 2 batch size number change modify batch size    trim length input string 10 character  surprise  number tuple go 10 39  trim string 2 char per string  number tuple not increase beyond 39   double input string length instead  number tuple go 7   appear like number tuple correspond iteration decoder chunk size limit   wanted compute mean pool first token  seem like would compute mean last tensor first tuple  however  not understand exactly token length correspond number tuple  determine exactly represent tuple tensor  successful finding information go t5 source code ,think what s happen t5 return hide state per step decode  therefore  number tuple correspond long generate sequence  likely interested last decode step could take last tuple  tuple tuple size numlayer  1  1 final layernorm   output last layer last tuple entry ,determine content decoderhiddenstates t5forconditionalgeneration  m use huggingface t5forconditionalgeneration model without modification  want compute mean pool last hide state t5 decoder  can not determine part decoderhiddenstates contain  m look  want something like   prepare batch data source  batchdf   source   tolist   tokenizedinput  selftokenizer  source  returntensorspt   pad  true  truncation  true  maxlength  selfmaxlength  to   cuda   inputids  tokenizedinput   inputids   to   cuda   attentionmask  tokenizedinput   attentionmask   to   cuda   inputbatch    inputids   inputids   attentionmask   attentionmask   dosample   false   numbeams   1   eostokenid   selftokenizereostokenid   padtokenid   selftokenizerpadtokenid   maxlength   selfmaxoutputlength   outputscore   true   returndictingenerate   true   outputhiddenstate   true   output  selfmodelgenerate    inputbatch   retrieve decoder hide state decoderlasthiddenstate  outputsdecoderhiddenstate  1   last layer s hide state  compute mean hide state across sequence length dimension meanpooledoutput  torchmean  decoderlasthiddenstate  dim1  keepdim  false  approach work encoder  decoder  decoderhiddenstates  1  tuple tensor  tensor  first inspect tuple  10 tuple  tuple contain 7 tensor  inspect dimension  like  tuplenumber range  n    check tuple print  f  tuple  layernumber      tensor enumerate  outputsdecoderhiddenstate  layernumber    print  f  tuple   layer  layernumber   shape  tensorshape    output like  tuple 0  tensor 0 tuple 0  shape torch  size   2  1  512   tensor 1 tuple 0  shape torch  size   2  1  512   tensor 2 tuple 0  shape torch  size   2  1  512   tensor 3 tuple 0  shape torch  size   2  1  512   tensor 4 tuple 0  shape torch  size   2  1  512   tensor 5 tuple 0  shape torch  size   2  1  512   tensor 6 tuple 0  shape torch  size   2  1  512   tuple 1  tensor 0 tuple 1  shape torch  size   2  1  512   tensor 1 tuple 1  shape torch  size   2  1  512   tensor 2 tuple 1  shape torch  size   2  1  512   tensor 3 tuple 1  shape torch  size   2  1  512   tensor 4 tuple 1  shape torch  size   2  1  512   tensor 5 tuple 1  shape torch  size   2  1  512   tensor 6 tuple 1  shape torch  size   2  1  512      512 maxlength tokenizer  2 batch size   verify 2 batch size number change modify batch size    trim length input string 10 character  surprise  number tuple go 10 39  trim string 2 char per string  number tuple not increase beyond 39   double input string length instead  number tuple go 7   appear like number tuple correspond iteration decoder chunk size limit   wanted compute mean pool first token  seem like would compute mean last tensor first tuple  however  not understand exactly token length correspond number tuple  determine exactly represent tuple tensor  successful finding information go t5 source code  think what s happen t5 return hide state per step decode  therefore  number tuple correspond long generate sequence  likely interested last decode step could take last tuple  tuple tuple size numlayer  1  1 final layernorm   output last layer last tuple entry ,Implementation Issues
Extracting only technical keywords from a text using RAKE library in Python,"I want to use rake to extract technical keywords from a job description that I've found on Linkedin, which looks like this: input = ""In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch.Experience with front-end development using HTML, CSS, and JavaScript. Familiarity with database technologies such as SQL and NoSQL.Excellent problem-solving ability with solid communication and collaboration skills. Preferred Skills And QualificationsExperience with popular Python frameworks such as Django, Flask or Pyramid."" I run this code, as it's supposed to return the keywords. from rake_nltk import Rake r = Rake() r.extract_keywords_from_text(input) keywords = r.get_ranked_phrases_with_scores() for score, keyword in keywords: if len(keyword.split()) == 1: # Check if the keyword is one word print(f""{keyword}: {score}"") But the output is this: frameworks: 2.0 tools: 1.0 sql: 1.0 spacy: 1.0 scipy: 1.0 sci: 1.0 qualificationsexperience: 1.0 pytorch: 1.0 pyramid: 1.0 pandas: 1.0 numpy: 1.0 nosql: 1.0 nltk: 1.0 learn: 1.0 kit: 1.0 javascript: 1.0 front: 1.0 flask: 1.0 familiarity: 1.0 experience: 1.0 ecosystems: 1.0 django: 1.0 dask: 1.0 css: 1.0 Simply I just want the explicit name of tools, skills and frameworks. Such as ""Numpy"", ""Scipy"", ""HTML"", etc That are used in the text and NOT every single word that's found in it (such as ""experience"" or ""tools""). Is there any way to do so? Or should I just provide a list of all possible python frameworks and related skill and then filter the output of rake? If the latter one is the solution, How can I find/make a thorough list? Any help is appreciated.","['python', 'python-3.x', 'nlp', 'nltk', 'rake']",2,"You can utilize skill and knowledge token classification from Hugging Face's library from transformers import pipeline token_skill_classifier = pipeline(model=""jjzha/jobbert_skill_extraction"", aggregation_strategy=""first"") token_knowledge_classifier = pipeline(model=""jjzha/jobbert_knowledge_extraction"", aggregation_strategy=""first"") def aggregate_span(results): new_results = [] current_result = results[0] for result in results[1:]: if result[""start""] == current_result[""end""] + 1: current_result[""word""] += "" "" + result[""word""] current_result[""end""] = result[""end""] else: new_results.append(current_result) current_result = result new_results.append(current_result) return new_results def ner(text): output_skills = token_skill_classifier(text) for result in output_skills: if result.get(""entity_group""): result[""entity""] = ""Skill"" del result[""entity_group""] output_knowledge = token_knowledge_classifier(text) for result in output_knowledge: if result.get(""entity_group""): result[""entity""] = ""Knowledge"" del result[""entity_group""] if len(output_skills) > 0: output_skills = aggregate_span(output_skills) if len(output_knowledge) > 0: output_knowledge = aggregate_span(output_knowledge) return {""text"": text, ""entities"": output_skills}, {""text"": text, ""entities"": output_knowledge}",2024-04-28 05:52:10,2024-04-30 04:55:18,328,https://stackoverflow.com/questions/78397201/extracting-only-technical-keywords-from-a-text-using-rake-library-in-python,"Extracting only technical keywords from a text using RAKE library in Python I want to use rake to extract technical keywords from a job description that I've found on Linkedin, which looks like this: input = ""In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch.Experience with front-end development using HTML, CSS, and JavaScript. Familiarity with database technologies such as SQL and NoSQL.Excellent problem-solving ability with solid communication and collaboration skills. Preferred Skills And QualificationsExperience with popular Python frameworks such as Django, Flask or Pyramid."" I run this code, as it's supposed to return the keywords. from rake_nltk import Rake r = Rake() r.extract_keywords_from_text(input) keywords = r.get_ranked_phrases_with_scores() for score, keyword in keywords: if len(keyword.split()) == 1: # Check if the keyword is one word print(f""{keyword}: {score}"") But the output is this: frameworks: 2.0 tools: 1.0 sql: 1.0 spacy: 1.0 scipy: 1.0 sci: 1.0 qualificationsexperience: 1.0 pytorch: 1.0 pyramid: 1.0 pandas: 1.0 numpy: 1.0 nosql: 1.0 nltk: 1.0 learn: 1.0 kit: 1.0 javascript: 1.0 front: 1.0 flask: 1.0 familiarity: 1.0 experience: 1.0 ecosystems: 1.0 django: 1.0 dask: 1.0 css: 1.0 Simply I just want the explicit name of tools, skills and frameworks. Such as ""Numpy"", ""Scipy"", ""HTML"", etc That are used in the text and NOT every single word that's found in it (such as ""experience"" or ""tools""). Is there any way to do so? Or should I just provide a list of all possible python frameworks and related skill and then filter the output of rake? If the latter one is the solution, How can I find/make a thorough list? Any help is appreciated.",extract technical keyword text use rake library python want use rake extract technical keyword job description  ve find linkedin  look like  input    in  depth understand python software development stack  ecosystem  framework tool numpy  scipy  pandas  dask  spacy  nltk  sci  kit  learn pytorch  experience front  end development use html  css  javascript  familiarity database technologie sql nosqlexcellent problem  solve ability solid communication collaboration skill  preferred skills qualificationsexperience popular python framework django  flask pyramid   run code  s suppose return keyword  rakenltk import rake r  rake   rextractkeywordsfromtext  input  keyword  rgetrankedphraseswithscores   score  keyword keyword  len  keywordsplit      1   check keyword one word print  f   keyword    score    output  framework  20 tool  10 sql  10 spacy  10 scipy  10 sci  10 qualificationsexperience  10 pytorch  10 pyramid  10 panda  10 numpy  10 nosql  10 nltk  10 learn  10 kit  10 javascript  10 front  10 flask  10 familiarity  10 experience  10 ecosystem  10 django  10 dask  10 css  10 simply want explicit name tool  skill framework    numpy     scipy     html   etc use text every single word be find    experience    tool    way  provide list possible python framework relate skill filter output rake  latter one solution  find  make thorough list  help appreciate ,utilize skill knowledge token classification hugging face s library transformer import pipeline tokenskillclassifier  pipeline  model  jjzha  jobbertskillextraction   aggregationstrategy  first   tokenknowledgeclassifier  pipeline  model  jjzha  jobbertknowledgeextraction   aggregationstrategy  first   def aggregatespan  result   newresult    currentresult  result  0  result result  1    result    start     currentresult    end    1  currentresult    word          result    word   currentresult    end    result    end   else  newresultsappend  currentresult  currentresult  result newresultsappend  currentresult  return newresults def ner  text   outputskills  tokenskillclassifier  text  result outputskill  resultget    entitygroup    result    entity      skill  del result    entitygroup   outputknowledge  tokenknowledgeclassifier  text  result outputknowledge  resultget    entitygroup    result    entity      knowledge  del result    entitygroup   len  outputskills   0  outputskills  aggregatespan  outputskills  len  outputknowledge   0  outputknowledge  aggregatespan  outputknowledge  return    text   text    entity   outputskills      text   text    entity   outputknowledge ,extract technical keyword text use rake library python want use rake extract technical keyword job description  ve find linkedin  look like  input    in  depth understand python software development stack  ecosystem  framework tool numpy  scipy  pandas  dask  spacy  nltk  sci  kit  learn pytorch  experience front  end development use html  css  javascript  familiarity database technologie sql nosqlexcellent problem  solve ability solid communication collaboration skill  preferred skills qualificationsexperience popular python framework django  flask pyramid   run code  s suppose return keyword  rakenltk import rake r  rake   rextractkeywordsfromtext  input  keyword  rgetrankedphraseswithscores   score  keyword keyword  len  keywordsplit      1   check keyword one word print  f   keyword    score    output  framework  20 tool  10 sql  10 spacy  10 scipy  10 sci  10 qualificationsexperience  10 pytorch  10 pyramid  10 panda  10 numpy  10 nosql  10 nltk  10 learn  10 kit  10 javascript  10 front  10 flask  10 familiarity  10 experience  10 ecosystem  10 django  10 dask  10 css  10 simply want explicit name tool  skill framework    numpy     scipy     html   etc use text every single word be find    experience    tool    way  provide list possible python framework relate skill filter output rake  latter one solution  find  make thorough list  help appreciate  utilize skill knowledge token classification hugging face s library transformer import pipeline tokenskillclassifier  pipeline  model  jjzha  jobbertskillextraction   aggregationstrategy  first   tokenknowledgeclassifier  pipeline  model  jjzha  jobbertknowledgeextraction   aggregationstrategy  first   def aggregatespan  result   newresult    currentresult  result  0  result result  1    result    start     currentresult    end    1  currentresult    word          result    word   currentresult    end    result    end   else  newresultsappend  currentresult  currentresult  result newresultsappend  currentresult  return newresults def ner  text   outputskills  tokenskillclassifier  text  result outputskill  resultget    entitygroup    result    entity      skill  del result    entitygroup   outputknowledge  tokenknowledgeclassifier  text  result outputknowledge  resultget    entitygroup    result    entity      knowledge  del result    entitygroup   len  outputskills   0  outputskills  aggregatespan  outputskills  len  outputknowledge   0  outputknowledge  aggregatespan  outputknowledge  return    text   text    entity   outputskills      text   text    entity   outputknowledge ,Library/Tool-Based Queries
Langchain sql agent with context,"I am working on a langchain based SQL chat application and wanted my agent to understand context w.r.t the user session. For e.g. User - What is highest order placed in last placed? Bot - Order id : XYZ User - When was this placed? Here, bot should be able to deduce that 'this' refers to 'order id XYZ' from previous question. How can I incorporate this in my code? I am tried using ChatHistory but getting context from session history is where I am stuck.","['text', 'nlp', 'langchain']",1,"I had a similar issue and I solved by “Contextualizing” the questions. Here is an example (not for SQL) : https://python.langchain.com/docs/use_cases/question_answering/chat_history/ Read the part on Contextualizing the question to reformulate your question based on the history. In the context of a RAG system that uses a SQL agent, contextualizing questions involves modifying or framing these questions based on the specific data structure and contents of the SQL database. Here, the questions are crafted to match the structure of the database to efficiently retrieve relevant information. This means incorporating the correct table names, field names, and specific terms used within the database into the query. By doing so, the RAG system leverages the SQL agent to execute precise database queries that fetch data relevant to the ongoing conversation or task, thus enhancing the response accuracy and relevance. Hope this helps!",2024-04-27 07:49:59,2024-04-29 11:25:07,1083,https://stackoverflow.com/questions/78394225/langchain-sql-agent-with-context,"Langchain sql agent with context I am working on a langchain based SQL chat application and wanted my agent to understand context w.r.t the user session. For e.g. User - What is highest order placed in last placed? Bot - Order id : XYZ User - When was this placed? Here, bot should be able to deduce that 'this' refers to 'order id XYZ' from previous question. How can I incorporate this in my code? I am tried using ChatHistory but getting context from session history is where I am stuck.",langchain sql agent context work langchain base sql chat application want agent understand context wrt user session  eg  user  high order place last place  bot  order i d  xyz user  place   bot able deduce  this  refer  order i d xyz  previous question  incorporate code  try use chathistory get context session history stick ,similar issue solve  contextualizing  question  example  sql     pythonlangchaincom  docs  usecases  questionanswering  chathistory read part contextualizing question reformulate question base history  context rag system use sql agent  contextualizing question involve modify framing question base specific datum structure content sql database   question craft match structure database efficiently retrieve relevant information  mean incorporate correct table name  field name  specific term use within database query   rag system leverage sql agent execute precise database query fetch datum relevant ongoing conversation task  thus enhance response accuracy relevance  hope help ,langchain sql agent context work langchain base sql chat application want agent understand context wrt user session  eg  user  high order place last place  bot  order i d  xyz user  place   bot able deduce  this  refer  order i d xyz  previous question  incorporate code  try use chathistory get context session history stick  similar issue solve  contextualizing  question  example  sql     pythonlangchaincom  docs  usecases  questionanswering  chathistory read part contextualizing question reformulate question base history  context rag system use sql agent  contextualizing question involve modify framing question base specific datum structure content sql database   question craft match structure database efficiently retrieve relevant information  mean incorporate correct table name  field name  specific term use within database query   rag system leverage sql agent execute precise database query fetch datum relevant ongoing conversation task  thus enhance response accuracy relevance  hope help ,Basic Understanding
Performance of textSimilarity() from R&#39;s text library,"I have a large data.frame with about 4 million rows and 2 columns. The two columns contain long character strings, texts representing recipes. For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the text library in R. I like the textSimilarity() function, because it uses text embeddings that ""comprises values that represent the latent meaning of a word"". Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU? Example data - with way shorter texts: df <- data.frame( columnA= c(""tomato sauce is very tasty to use"", ""without garlic, this dish is not chinese"", ""British food is as tasteless as it can get""), columnB= c(""pizza is the source of life"", ""a nice xiaolongbao is steamed until it is soft"", ""braised pork can be very healthy if prepared well"") ) > df columnA columnB 1 tomato sauce is very tasty to use pizza is the source of life 2 without garlic, this dish is not chinese a nice xiaolongbao is steamed until it is soft 3 British food is as tasteless as it can get braised pork can be very healthy if prepared will To get the similarity, I use: df$sim <- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts) In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?","['r', 'performance', 'nlp', 'huggingface-transformers']",2,"Contextual embedding models are computationally expensive The default model used by the R text package is bert-base-uncased , which has 110m parameters, including a 12 layer feed-forward neural network. To compare similarity of sentences, each sentence is split into tokens (which are words or parts of words) and each token is represented as a 768-dimensional vector. Then the token vectors from each sentence are aggregated to create a single, 768-dimensional vector to represent that sentence in vector space. The idea is that with decent token (or word) embeddings and a sensible way of aggregating them this representation captures meaning, so similar sentences appear closer together in vector space. You can then calculate the distance between sentence vectors as a measure of their semantic similarity. The expensive step is creating the token vectors. Unlike older models such as Word2Vec, with BERT the vector representation of each token depends on the context. A canonical example of this is that the representation of the first word in Apple Inc. was founded by Steve Jobs will not be the same as the first word in Apple is my favourite fruit . This is a strength of this family of models. These sentences should not appear close together in vector space. But calculating the representation of each token based on the others around it, and working out which tokens affect the meaning of other tokens, is computationally expensive. Use a more lightweight context-dependent model A good way to speed up things a lot without sacrificing too much accuracy is by using a more lightweight model, e.g. distilbert which has 66m parameters and 6 rather than 12 feed-forward neural network layers. This is still a transformer model. You still get context-specific embeddings with an attention mechanism to appropriately weight each token based on the surrounding ones. With distilled models trained on larger models like BERT, the results tend to be fairly close to the model from which they're derived, and they're much faster: DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark. Benchmarking the default model We need a little more text to measure the performance. I'll use the Biomedical Semantic Similarity Estimation System (BIOSSES) dataset of 100 sentence pairs. # Read in the data biosses <- jsonlite::read_json(""https://datasets-server.huggingface.co/rows?dataset=biosses&config=default&split=train&offset=0&length=100"") df2 <- data.frame( sentence1 = sapply(biosses$rows, \(row) row$row$sentence1), sentence2 = sapply(biosses$rows, \(row) row$row$sentence2) ) # Define function to calculate similarity get_similarity <- function(model, dat = df2, col1 = ""sentence1"", col2 = ""sentence2"") { embeds_a <- text::textEmbed(dat[[col1]], model = model) embeds_b <- text::textEmbed(dat[[col2]], model = model) text::textSimilarity( embeds_a$texts$texts, embeds_b$texts$texts ) } No need to microbenchmark here as it takes so long (about 16 minutes): system.time( base_result <- get_similarity(model = ""bert-base-uncased"") ) # 956.136 seconds Speed of smaller BERT models Let's try three other BERT derivatives: DistilBERT . Bert tiny intel CPU optimized . DistilBERT distilled . models <- c( ""distilbert"" = ""distilbert-base-uncased"", ""tiny_distilbert"" = ""muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"", ""distilled_distilbert"" = ""distilbert/distilbert-base-uncased-distilled-squad"" ) l <- lapply(models, \(model) list( time = system.time(x <- get_similarity(model))[""elapsed""], result = x, diff_from_base = abs(base_result - x) ) ) To compare the results: sapply(l, \(x) x$time) # distilbert.elapsed tiny_distilbert.elapsed distilled_distilbert.elapsed # 622.071 61.956 569.000 So the tiny model is by far the fastest at just over a minute to run, with the other two much slower at around 9-10 mins. All are substantially faster than the base model. Accuracy of smaller BERT models Of course, speed is not everything. Let's compare the distribution of similarity scores to the bert-base-uncased scores: res <- cbind(sentence = seq(nrow(df2)), data.frame(l)) |> tidyr::pivot_longer( cols = !sentence, names_pattern = ""(.+)\\.(.+)"", names_to = c(""model"", ""var"") ) |> tidyr::pivot_wider( names_from = var ) library(ggplot2) ggplot(res) + geom_density(aes(x = diff_from_base, color = model, fill = model), alpha = 0.2) + theme(legend.position = ""bottom"") + labs(x = ""Absolute difference"", title = ""Distribution of differences from bert-base-uncased similarity score"") Interestingly, the fastest model also appears to have the closest results to the base model, at least on this task. You may want to do more robust checks with a subset of your actual data to find the optimal trade-off between speed and similarity to BERT (or whatever your gold standard of similarity is). Other approaches to speeding up this task Parallel processing You suggested using parallel processing. This should not speed things up, as under the hood, text::textEmbed() wraps the Python Hugging Face transformers library, which in turn calls the Python torch library, which already multi-threaded by default. When I run this code, CPU utilisation spikes for all cores. You might want to check the same happens on your machine but I suspect it will. GPU You suggested using a GPU. If you have one you should use it. This may speed things up significantly, though how much depends on the model. It looks like the way to do this is to call text::textEmbed(..., device = ""gpu"") . I would not feel completely confident this will work out of the box though. Getting torch to recognise a GPU generally requires specifying the Cuda version during installation. This cannot be done for you from R when you run text::textrpp_initialize() , as it will not know which hardware or drivers you have installed. You might want to enter the Python conda virtual environment created by the R text package and check whether torch.cuda.is_available() == True . If not, see this question for how to find out whether your GPU is compatible with Cuda and if so how to find the compatible torch version. A note on using R There is a cost to using R here. I replicated these results in Python and it was about twice as fast again. Each sentence is represented as a 768 dimensional vector. There's some work going on to take the Python list of Torch.tensor s representing each sentence, coerce to a numpy array, then pass this to R where ultimately it's stored as a row of a data frame with 768 columns. I suspect that as the size of the data increases, the time spent on translating the vectors from Python to R reduces as a proportion of the total time, but it would certainly be somewhat faster still to do all this in Python. Nevertheless, if muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu will do the job and there are benefits to keeping everything in R, hopefully a sixteen-fold speed up is enough to be able to do this.",2024-04-27 03:15:58,2024-04-30 08:30:55,246,https://stackoverflow.com/questions/78393709/performance-of-textsimilarity-from-rs-text-library,"Performance of textSimilarity() from R&#39;s text library I have a large data.frame with about 4 million rows and 2 columns. The two columns contain long character strings, texts representing recipes. For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the text library in R. I like the textSimilarity() function, because it uses text embeddings that ""comprises values that represent the latent meaning of a word"". Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU? Example data - with way shorter texts: df <- data.frame( columnA= c(""tomato sauce is very tasty to use"", ""without garlic, this dish is not chinese"", ""British food is as tasteless as it can get""), columnB= c(""pizza is the source of life"", ""a nice xiaolongbao is steamed until it is soft"", ""braised pork can be very healthy if prepared well"") ) > df columnA columnB 1 tomato sauce is very tasty to use pizza is the source of life 2 without garlic, this dish is not chinese a nice xiaolongbao is steamed until it is soft 3 British food is as tasteless as it can get braised pork can be very healthy if prepared will To get the similarity, I use: df$sim <- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts) In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?",performance textsimilarity   r   39  text library large dataframe 4 million row 2 column  two column contain long character string  text represent recipe  row  compare similarity recipe column column b  use textsimilarity   text library r like textsimilarity   function  use text embedding   comprise value represent latent meaning word   yet  performance slow  way speeding  code wrong  can  should set parallel  use gpu  example data  way short text  df   dataframe  columna c    tomato sauce tasty use     without garlic  dish chinese     british food tasteless get    columnb c    pizza source life     nice xiaolongbao steamed soft     braise pork healthy prepare well     df columna columnb 1 tomato sauce tasty use pizza source life 2 without garlic  dish chinese nice xiaolongbao steam soft 3 british food tasteless get braise pork healthy prepared get similarity  use  df  sim   textsimilarity  textembed  df  columna   text  text  textembed  df  columnb   text  text  current set  up  process take day rather hour  speed  parallelization  gpu  alternative ,contextual embed model computationally expensive default model use r text package bert  base  uncased  110 m parameter  include 12 layer feed  forward neural network  compare similarity sentence  sentence split token  word part word  token represent 768  dimensional vector  token vectors sentence aggregate create single  768  dimensional vector represent sentence vector space  idea decent token  word  embedding sensible way aggregate representation capture mean  similar sentence appear close together vector space  calculate distance sentence vector measure semantic similarity  expensive step create token vector  unlike old model word2vec  bert vector representation token depend context  canonical example representation first word apple inc found steve jobs first word apple favourite fruit  strength family model  sentence appear close together vector space  calculate representation token base other around  work token affect mean token  computationally expensive  use lightweight context  dependent model good way speed thing lot without sacrifice much accuracy use lightweight model  eg  distilbert 66 m parameter 6 rather 12 feed  forward neural network layer  still transformer model  still get context  specific embedding attention mechanism appropriately weight token base surround one  distilled model train large model like bert  result tend fairly close model be derive  be much fast  distilbert small  fast  cheap light transformer model train distilling bert base  40  less parameter google  bert  bert  base  uncased  run 60  fast preserve 95  bert  performance measure glue language understand benchmark  benchmarke default model need little text measure performance  will use biomedical semantic similarity estimation system  biosse  dataset 100 sentence pair   read data biosse   jsonlite   readjson      datasets  serverhuggingfaceco  row  dataset  biosse  config  default  split  train  offset0  length100   df2   dataframe  sentence1  sapply  biosse  row    row  row  row  sentence1   sentence2  sapply  biosse  row    row  row  row  sentence2    define function calculate similarity getsimilarity   function  model  dat  df2  col1    sentence1   col2    sentence2    embedsa   text   textembed  dat   col1    model  model  embedsb   text   textembed  dat   col2    model  model  text   textsimilarity  embedsa  text  text  embedsb  text  text   need microbenchmark take long  16 minute   systemtime  baseresult   getsimilarity  model    bert  base  uncased     956136 second speed small bert model let us try three bert derivative  distilbert  bert tiny intel cpu optimize  distilbert distil  model   c    distilbert     distilbert  base  uncased     tinydistilbert     muhtasham  bert  tiny  finetune  finer139  full  intel  cpu     distilleddistilbert     distilbert  distilbert  base  uncase  distil  squad   l   lapply  model    model  list  time  systemtime  x   getsimilarity  model      elapse    result  x  difffrombase  ab  baseresult  x    compare result  sapply  l    x  x  time   distilbertelapse tinydistilbertelapsed distilleddistilbertelapsed  622071 61956 569000 tiny model far fast minute run  two much slow around 9  10 min  substantially fast base model  accuracy small bert model course  speed everything  let us compare distribution similarity score bert  base  uncased score  re   cbind  sentence  seq  nrow  df2    dataframe  l     tidyr   pivotlonger  col   sentence  namespattern              namesto  c    model     var      tidyr   pivotwider  namesfrom  var  library  ggplot2  ggplot  re   geomdensity  aes  x  difffrombase  color  model  fill  model   alpha  02   theme  legendposition    bottom    lab  x    absolute difference   title    distribution difference bert  base  uncased similarity score   interestingly  fast model also appear close result base model  least task  may want robust check subset actual datum find optimal trade  off speed similarity bert  whatever gold standard similarity   approach speed task parallel processing suggest use parallel processing  speed thing  hood  text   textembed   wrap python hug face transformer library  turn call python torch library  already multi  thread default  run code  cpu utilisation spike core  might want check happen machine suspect  gpu suggest use gpu  one use  may speed thing significantly  though much depend model  look like way call text   textembed    device    gpu    would feel completely confident work box though  get torch recognise gpu generally require specify cuda version installation  do r run text   textrppinitialize    know hardware driver instal  might want enter python conda virtual environment create r text package check whether torchcudaisavailable     true   see question find whether gpu compatible cuda find compatible torch version  note use r cost use r  replicate result python twice fast  sentence represent 768 dimensional vector  s work go take python list torchtensor represent sentence  coerce numpy array  pass r ultimately be store row datum frame 768 column  suspect size data increase  time spend translate vector python r reduce proportion total time  would certainly somewhat fast still python  nevertheless  muhtasham  bert  tiny  finetune  finer139  full  intel  cpu job benefit keep everything r  hopefully sixteen  fold speed enough able ,performance textsimilarity   r   39  text library large dataframe 4 million row 2 column  two column contain long character string  text represent recipe  row  compare similarity recipe column column b  use textsimilarity   text library r like textsimilarity   function  use text embedding   comprise value represent latent meaning word   yet  performance slow  way speeding  code wrong  can  should set parallel  use gpu  example data  way short text  df   dataframe  columna c    tomato sauce tasty use     without garlic  dish chinese     british food tasteless get    columnb c    pizza source life     nice xiaolongbao steamed soft     braise pork healthy prepare well     df columna columnb 1 tomato sauce tasty use pizza source life 2 without garlic  dish chinese nice xiaolongbao steam soft 3 british food tasteless get braise pork healthy prepared get similarity  use  df  sim   textsimilarity  textembed  df  columna   text  text  textembed  df  columnb   text  text  current set  up  process take day rather hour  speed  parallelization  gpu  alternative  contextual embed model computationally expensive default model use r text package bert  base  uncased  110 m parameter  include 12 layer feed  forward neural network  compare similarity sentence  sentence split token  word part word  token represent 768  dimensional vector  token vectors sentence aggregate create single  768  dimensional vector represent sentence vector space  idea decent token  word  embedding sensible way aggregate representation capture mean  similar sentence appear close together vector space  calculate distance sentence vector measure semantic similarity  expensive step create token vector  unlike old model word2vec  bert vector representation token depend context  canonical example representation first word apple inc found steve jobs first word apple favourite fruit  strength family model  sentence appear close together vector space  calculate representation token base other around  work token affect mean token  computationally expensive  use lightweight context  dependent model good way speed thing lot without sacrifice much accuracy use lightweight model  eg  distilbert 66 m parameter 6 rather 12 feed  forward neural network layer  still transformer model  still get context  specific embedding attention mechanism appropriately weight token base surround one  distilled model train large model like bert  result tend fairly close model be derive  be much fast  distilbert small  fast  cheap light transformer model train distilling bert base  40  less parameter google  bert  bert  base  uncased  run 60  fast preserve 95  bert  performance measure glue language understand benchmark  benchmarke default model need little text measure performance  will use biomedical semantic similarity estimation system  biosse  dataset 100 sentence pair   read data biosse   jsonlite   readjson      datasets  serverhuggingfaceco  row  dataset  biosse  config  default  split  train  offset0  length100   df2   dataframe  sentence1  sapply  biosse  row    row  row  row  sentence1   sentence2  sapply  biosse  row    row  row  row  sentence2    define function calculate similarity getsimilarity   function  model  dat  df2  col1    sentence1   col2    sentence2    embedsa   text   textembed  dat   col1    model  model  embedsb   text   textembed  dat   col2    model  model  text   textsimilarity  embedsa  text  text  embedsb  text  text   need microbenchmark take long  16 minute   systemtime  baseresult   getsimilarity  model    bert  base  uncased     956136 second speed small bert model let us try three bert derivative  distilbert  bert tiny intel cpu optimize  distilbert distil  model   c    distilbert     distilbert  base  uncased     tinydistilbert     muhtasham  bert  tiny  finetune  finer139  full  intel  cpu     distilleddistilbert     distilbert  distilbert  base  uncase  distil  squad   l   lapply  model    model  list  time  systemtime  x   getsimilarity  model      elapse    result  x  difffrombase  ab  baseresult  x    compare result  sapply  l    x  x  time   distilbertelapse tinydistilbertelapsed distilleddistilbertelapsed  622071 61956 569000 tiny model far fast minute run  two much slow around 9  10 min  substantially fast base model  accuracy small bert model course  speed everything  let us compare distribution similarity score bert  base  uncased score  re   cbind  sentence  seq  nrow  df2    dataframe  l     tidyr   pivotlonger  col   sentence  namespattern              namesto  c    model     var      tidyr   pivotwider  namesfrom  var  library  ggplot2  ggplot  re   geomdensity  aes  x  difffrombase  color  model  fill  model   alpha  02   theme  legendposition    bottom    lab  x    absolute difference   title    distribution difference bert  base  uncased similarity score   interestingly  fast model also appear close result base model  least task  may want robust check subset actual datum find optimal trade  off speed similarity bert  whatever gold standard similarity   approach speed task parallel processing suggest use parallel processing  speed thing  hood  text   textembed   wrap python hug face transformer library  turn call python torch library  already multi  thread default  run code  cpu utilisation spike core  might want check happen machine suspect  gpu suggest use gpu  one use  may speed thing significantly  though much depend model  look like way call text   textembed    device    gpu    would feel completely confident work box though  get torch recognise gpu generally require specify cuda version installation  do r run text   textrppinitialize    know hardware driver instal  might want enter python conda virtual environment create r text package check whether torchcudaisavailable     true   see question find whether gpu compatible cuda find compatible torch version  note use r cost use r  replicate result python twice fast  sentence represent 768 dimensional vector  s work go take python list torchtensor represent sentence  coerce numpy array  pass r ultimately be store row datum frame 768 column  suspect size data increase  time spend translate vector python r reduce proportion total time  would certainly somewhat fast still python  nevertheless  muhtasham  bert  tiny  finetune  finer139  full  intel  cpu job benefit keep everything r  hopefully sixteen  fold speed enough able ,Basic Understanding
How to Generate Text with Specific Length Using AI API,"I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far: Setting Max Tokens : I've used the max_tokens parameter hoping to limit the output length, but then text is truncated. Explicit Prompt Requests : I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800). I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length? I need this because I inject the text in slides. Any alternative solution?","['python', 'nlp', 'openai-api', 'text-generation']",1,This won't work. LLMs are bad at counting. There are a lot of humorous examples over internet.,2024-04-25 09:30:56,2024-04-25 09:32:51,236,https://stackoverflow.com/questions/78383619/how-to-generate-text-with-specific-length-using-ai-api,"How to Generate Text with Specific Length Using AI API I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far: Setting Max Tokens : I've used the max_tokens parameter hoping to limit the output length, but then text is truncated. Explicit Prompt Requests : I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800). I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length? I need this because I inject the text in slides. Any alternative solution?",generate text specific length use ai api attempt generate text output exactly certain number character word long use ai api  openai gpt  claude  gemini     m face difficulty  s  ve try far  setting max tokens   ve use maxtokens parameter hope limit output length  text truncate  explicit prompt request   ve try include explicit instruction prompt desire length  however  approach produce precise output length need  ex  give 600 word instead 800   look suggestion configure api call promtp achieve exact output length  way well utilize openai  claude  parameter  method post  process text fit require length  need inject text slide  alternative solution ,will not work  llms bad counting  lot humorous example internet ,generate text specific length use ai api attempt generate text output exactly certain number character word long use ai api  openai gpt  claude  gemini     m face difficulty  s  ve try far  setting max tokens   ve use maxtokens parameter hope limit output length  text truncate  explicit prompt request   ve try include explicit instruction prompt desire length  however  approach produce precise output length need  ex  give 600 word instead 800   look suggestion configure api call promtp achieve exact output length  way well utilize openai  claude  parameter  method post  process text fit require length  need inject text slide  alternative solution  will not work  llms bad counting  lot humorous example internet ,Implementation Issues
ValueError: Cannot use a compiled regex as replacement pattern with regex=False,"I'm doing a project, on Google Colab, where I use the following version: !pip install ""gensim==4.2.0"" !pip install ""texthero==1.0.5"" Until recently, I received the following warning: FutureWarning: The default value of regex will change from True to False in a future version. return input.str.replace(r""^\d+\s|\s\d+\s|\s\d+$"", "" "") But the execution worked normally. Now, I'm getting the following error: How should I proceed? I tried different versions, but the problem persists.","['python', 'text', 'nlp']",1,"This is a texthero bug triggering a pandas error. Pandas str.replace now uses regex=False by default: Texthero's replace_digits function hasn't been updated in two years and doesn't explicitly pass regex=True : if only_blocks: pattern = r""\b\d+\b"" return s.str.replace(pattern, symbols) else: return s.str.replace(r""\d+"", symbols) You should fill a bug report to texthero, there are probably several other occurrences of str.replace to fix. In there meantime you can patch the library by changing the code to: if only_blocks: pattern = r""\b\d+\b"" return s.str.replace(pattern, symbols, regex=True) else: return s.str.replace(r""\d+"", symbols, regex=True) Or use a pandas version prior to 2 (e.g. 1.5.2 )",2024-04-24 01:52:07,2024-04-24 02:41:14,333,https://stackoverflow.com/questions/78375631/valueerror-cannot-use-a-compiled-regex-as-replacement-pattern-with-regex-false,"ValueError: Cannot use a compiled regex as replacement pattern with regex=False I'm doing a project, on Google Colab, where I use the following version: !pip install ""gensim==4.2.0"" !pip install ""texthero==1.0.5"" Until recently, I received the following warning: FutureWarning: The default value of regex will change from True to False in a future version. return input.str.replace(r""^\d+\s|\s\d+\s|\s\d+$"", "" "") But the execution worked normally. Now, I'm getting the following error: How should I proceed? I tried different versions, but the problem persists.",valueerror  use compile regex replacement pattern regex  false  m project  google colab  use follow version   pip install   gensim420   pip install   texthero105  recently  receive follow warning  futurewarning  default value regex change true false future version  return inputstrreplace  r  dssdssd         execution work normally    m get follow error  proceed  try different version  problem persist ,texthero bug trigger panda error  panda strreplace use regex  false default  texthero s replacedigits function not update two year not explicitly pass regex  true  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  else  return sstrreplace  r  d   symbol  fill bug report texthero  probably several occurrence strreplace fix  meantime patch library change code  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  regex  true  else  return sstrreplace  r  d   symbol  regex  true  use panda version prior 2  eg  152 ,valueerror  use compile regex replacement pattern regex  false  m project  google colab  use follow version   pip install   gensim420   pip install   texthero105  recently  receive follow warning  futurewarning  default value regex change true false future version  return inputstrreplace  r  dssdssd         execution work normally    m get follow error  proceed  try different version  problem persist  texthero bug trigger panda error  panda strreplace use regex  false default  texthero s replacedigits function not update two year not explicitly pass regex  true  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  else  return sstrreplace  r  d   symbol  fill bug report texthero  probably several occurrence strreplace fix  meantime patch library change code  onlyblock  pattern  r  bdb  return sstrreplace  pattern  symbol  regex  true  else  return sstrreplace  r  d   symbol  regex  true  use panda version prior 2  eg  152 ,Library/Tool-Based Queries
No Attention returned even when output_attentions= True,"I'm using a pretrained model based BERT (github link: DNABERT-2 ) It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base. I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem. output is of length 2 and each element is of shape: torch.Size([1, 7, 768]) and torch.Size([1, 768]) . When trying to get output.attentions I get None . I'm not sure where to search and what a solution would be. I'm providing my whole code: Defining model, trainer, data, tokenizer: from copy import deepcopy from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback # END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset @dataclass class ModelArguments: model_name_or_path: Optional[str] = field(default=""facebook/opt-125m"") use_lora: bool = field(default=False, metadata={""help"": ""whether to use LoRA""}) lora_r: int = field(default=8, metadata={""help"": ""hidden dimension for LoRA""}) lora_alpha: int = field(default=32, metadata={""help"": ""alpha for LoRA""}) lora_dropout: float = field(default=0.05, metadata={""help"": ""dropout rate for LoRA""}) lora_target_modules: str = field(default=""query,value"", metadata={""help"": ""where to perform LoRA""}) @dataclass class DataArguments: data_path: str = field(default=None, metadata={""help"": ""Path to the training data.""}) kmer: int = field(default=-1, metadata={""help"": ""k-mer for input sequence. -1 means not using k-mer.""}) @dataclass class TrainingArguments(transformers.TrainingArguments): cache_dir: Optional[str] = field(default=None) run_name: str = field(default=""run"") optim: str = field(default=""adamw_torch"") model_max_length: int = field(default=512, metadata={""help"": ""Maximum sequence length.""}) gradient_accumulation_steps: int = field(default=1) per_device_train_batch_size: int = field(default=1) per_device_eval_batch_size: int = field(default=1) num_train_epochs: int = field(default=1) logging_steps: int = field(default=100) save_steps: int = field(default=100) fp16: bool = field(default=False) # START NEW # eval_steps: int = field(default=100) eval_steps: int = field(default=0.1) # END NEW evaluation_strategy: str = field(default=""steps"") warmup_steps: int = field(default=50) weight_decay: float = field(default=0.01) learning_rate: float = field(default=1e-4) save_total_limit: int = field(default=3) load_best_model_at_end: bool = field(default=True) output_dir: str = field(default=""output"") find_unused_parameters: bool = field(default=False) checkpointing: bool = field(default=False) dataloader_pin_memory: bool = field(default=False) eval_and_save_results: bool = field(default=True) save_model: bool = field(default=False) seed: int = field(default=42) def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str): """"""Collects the state dict and dump to disk."""""" state_dict = trainer.model.state_dict() if trainer.args.should_save: cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()} del state_dict trainer._save(output_dir, state_dict=cpu_state_dict) # noqa """""" Get the reversed complement of the original DNA sequence. """""" def get_alter_of_dna_sequence(sequence: str): MAP = {""A"": ""T"", ""T"": ""A"", ""C"": ""G"", ""G"": ""C""} # return """".join([MAP[c] for c in reversed(sequence)]) return """".join([MAP[c] for c in sequence]) """""" Transform a dna sequence to k-mer string """""" def generate_kmer_str(sequence: str, k: int) -> str: """"""Generate k-mer string from DNA sequence."""""" return "" "".join([sequence[i:i + k] for i in range(len(sequence) - k + 1)]) """""" Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of ""_{k}mer"". """""" def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]: """"""Load or generate k-mer string for each DNA sequence."""""" kmer_path = data_path.tokenizerreplace("".csv"", f""_{k}mer.json"") if os.path.exists(kmer_path): logging.warning(f""Loading k-mer from {kmer_path}..."") with open(kmer_path, ""r"") as f: kmer = json.load(f) else: logging.warning(f""Generating k-mer..."") kmer = [generate_kmer_str(text, k) for text in texts] with open(kmer_path, ""w"") as f: logging.warning(f""Saving k-mer to {kmer_path}..."") json.dump(kmer, f) return kmer class SupervisedDataset(Dataset): """"""Dataset for supervised fine-tuning."""""" def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, kmer: int = -1): super(SupervisedDataset, self).__init__() # load data from the disk with open(data_path, ""r"") as f: data = list(csv.reader(f))[1:] if len(data[0]) == 2: # data is in the format of [text, label] logging.warning(""Perform single sequence classification..."") texts = [d[0] for d in data] labels = [int(d[1]) for d in data] # All genes sequences are concat: we don't work with the sequence-pair, # But we are tricking the model to think it is single sequence. elif len(data[0]) == 3: # data is in the format of [text1, text2, label] logging.warning(""Perform sequence-pair classification..."") texts = [[d[0], d[1]] for d in data] labels = [int(d[2]) for d in data] else: raise ValueError(""Data format not supported."") if kmer != -1: # only write file on the first process if torch.distributed.get_rank() not in [0, -1]: torch.distributed.barrier() logging.warning(f""Using {kmer}-mer as input..."") texts = load_or_generate_kmer(data_path, texts, kmer) if torch.distributed.get_rank() == 0: torch.distributed.barrier() output = tokenizer( texts, return_tensors=""pt"", padding=""longest"", max_length=tokenizer.model_max_length, truncation=True, ) self.input_ids = output[""input_ids""] # CHANGE self.input_ids[0][self.input_ids[0] == 0] = 2 # Change to which tokens we want to attend and to which we don't self.attention_mask = output[""attention_mask""] self.labels = labels self.num_labels = len(set(labels)) def __len__(self): return len(self.input_ids) def __getitem__(self, i) -> Dict[str, torch.Tensor]: return dict(input_ids=self.input_ids[i], labels=self.labels[i]) @dataclass class DataCollatorForSupervisedDataset(object): """"""Collate examples for supervised fine-tuning."""""" tokenizer: transformers.PreTrainedTokenizer def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]: input_ids, labels = tuple([instance[key] for instance in instances] for key in (""input_ids"", ""labels"")) input_ids = torch.nn.utils.rnn.pad_sequence( input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id ) labels = torch.Tensor(labels).long() return dict( input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ) """""" Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. """""" def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray): if logits.ndim == 3: # Reshape logits to 2D if needed logits = logits.reshape(-1, logits.shape[-1]) predictions = np.argmax(logits, axis=-1) valid_mask = labels != -100 # Exclude padding tokens (assuming -100 is the padding token ID) valid_predictions = predictions[valid_mask] valid_labels = labels[valid_mask] return { # START NEW ""sum prediction"": f'{sum(valid_predictions)}/{len(valid_predictions)}', # END NEW ""accuracy"": sklearn.metrics.accuracy_score(valid_labels, valid_predictions), ""f1"": sklearn.metrics.f1_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""matthews_correlation"": sklearn.metrics.matthews_corrcoef( valid_labels, valid_predictions ), ""precision"": sklearn.metrics.precision_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""recall"": sklearn.metrics.recall_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), } """""" Compute metrics used for huggingface trainer. """""" def compute_metrics(eval_pred): logits, labels = eval_pred if isinstance(logits, tuple): # Unpack logits if it's a tuple logits = logits[0] return calculate_metric_with_sklearn(logits, labels) class CustomTrainer(transformers.Trainer): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.epoch_predictions = [] self.epoch_labels = [] self.epoch_loss = [] def compute_loss(self, model, inputs, return_outputs=False): """""" MAX: Subclassed to compute training accuracy. How the loss is computed by Trainer. By default, all models return the loss in the first element. Subclass and override for custom behavior. """""" if self.label_smoother is not None and ""labels"" in inputs: labels = inputs.pop(""labels"") else: labels = None outputs = model(**inputs, output_attentions=True) # TEST try: print(f""Attention: {outputs.attentions}"") except Exception: print(""No Attention returned"") if ""labels"" in inputs: preds = outputs.logits.detach() # Log accuracy acc = ( (preds.argmax(axis=1) == inputs[""labels""]) .type(torch.float) .mean() .item() ) # Uncomment it if you want to plot the batch accuracy # wandb.log({""batch_accuracy"": acc}) # Log accuracy # Store predictions and labels for epoch-level metrics self.epoch_predictions.append(preds.cpu().numpy()) self.epoch_labels.append(inputs[""labels""].cpu().numpy()) # Save past state if it exists if self.args.past_index >= 0: self._past = outputs[self.args.past_index] if labels is not None: loss = self.label_smoother(outputs, labels) else: loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0] # Uncomment it if you want to plot the batch loss # wandb.log({""batch_loss"": loss}) self.epoch_loss.append(loss.item()) # Store loss for epoch-level metrics return (loss, outputs) if return_outputs else loss # Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback): def __init__(self, trainer) -> None: super().__init__() self._trainer = trainer def on_epoch_end(self, args, state, control, **kwargs): # Aggregate predictions and labels for the entire epoch epoch_predictions = np.concatenate(self._trainer.epoch_predictions) epoch_labels = np.concatenate(self._trainer.epoch_labels) # Compute accuracy accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels) # Compute mean loss mean_loss = np.mean(self._trainer.epoch_loss) # Compute precision, recall, and F1-score precision, recall, f1, _ = precision_recall_fscore_support( epoch_labels, epoch_predictions.argmax(axis=1), average=""weighted"" ) # Log epoch-level metrics wandb.log({""epoch_accuracy"": accuracy, ""epoch_loss"": mean_loss}) wandb.log({""precision"": precision, ""recall"": recall, ""f1"": f1}) # Clear stored predictions, labels, and loss for the next epoch self._trainer.epoch_predictions = [] self._trainer.epoch_labels = [] self._trainer.epoch_loss = [] return None # TODO: use this function to gather the prediction and labels and get the metrics #%% Instantiating and training: from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \ TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \ compute_metrics from copy import deepcopy from transformers import TrainerCallback # END NEW import os import json import torch import transformers from peft import ( LoraConfig, get_peft_model, get_peft_model_state_dict, ) import wandb run = wandb.init() assert run is wandb.run def train(device): parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments)) model_args, data_args, training_args = parser.parse_args_into_dataclasses() # load tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=""right"", use_fast=True, trust_remote_code=True, ) if ""InstaDeepAI"" in model_args.model_name_or_path: tokenizer.eos_token = tokenizer.pad_token # define datasets and data collator train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""train.csv""), kmer=data_args.kmer) val_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""dev.csv""), kmer=data_args.kmer) test_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""test.csv""), kmer=data_args.kmer) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) # load model model = transformers.AutoModelForSequenceClassification.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, num_labels=train_dataset.num_labels, trust_remote_code=True, output_attentions = True ).to(device) # configure LoRA if model_args.use_lora: lora_config = LoraConfig( r=model_args.lora_r, lora_alpha=model_args.lora_alpha, target_modules=list(model_args.lora_target_modules.split("","")), lora_dropout=model_args.lora_dropout, bias=""none"", task_type=""SEQ_CLS"", inference_mode=False, ) model = get_peft_model(model, lora_config) model.print_trainable_parameters() trainer = CustomTrainer(model=model, tokenizer=tokenizer, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator ) trainer.add_callback(CustomCallback(trainer)) trainer.train() # train_result = trainer.train() # loss = train_result[""loss""] # print(f""loss issss: {loss}"") # print(f""Train reusults: {train_result}"") # NEW: result: only returns metrics at the end of training if training_args.save_model: trainer.save_state() safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir) # get the evaluation results from trainer if training_args.eval_and_save_results: results_path = os.path.join(training_args.output_dir, ""results"", training_args.run_name) results = trainer.evaluate(eval_dataset=test_dataset) os.makedirs(results_path, exist_ok=True) with open(os.path.join(results_path, ""eval_results.json""), ""w"") as f: json.dump(results, f) if __name__ == ""__main__"": # Define device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device) # Call the train function with the device train(device) After training, I try to run it on an example: model_path = './finetune/output/dnabert2' tokenizer = AutoTokenizer.from_pretrained(model_path) # Load the model with output_attention=True model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True) model_input = tokenizer(""ACTGACGGGTAGTGACTG"", return_tensors=""pt"") with torch.inference_mode(): output = model(**model_input, output_attentions=True) My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.","['nlp', 'huggingface-transformers', 'bert-language-model', 'transformer-model', 'attention-model']",2,"The problem was deep in the structure: attention was discarded early in the model, I had therefore to go through the code to understand what is happening, and change it. https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted I had to extract attention_probs. Here are the changes I have done.",2024-04-23 10:48:50,2024-05-16 20:48:46,422,https://stackoverflow.com/questions/78371741/no-attention-returned-even-when-output-attentions-true,"No Attention returned even when output_attentions= True I'm using a pretrained model based BERT (github link: DNABERT-2 ) It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base. I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem. output is of length 2 and each element is of shape: torch.Size([1, 7, 768]) and torch.Size([1, 768]) . When trying to get output.attentions I get None . I'm not sure where to search and what a solution would be. I'm providing my whole code: Defining model, trainer, data, tokenizer: from copy import deepcopy from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback # END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset @dataclass class ModelArguments: model_name_or_path: Optional[str] = field(default=""facebook/opt-125m"") use_lora: bool = field(default=False, metadata={""help"": ""whether to use LoRA""}) lora_r: int = field(default=8, metadata={""help"": ""hidden dimension for LoRA""}) lora_alpha: int = field(default=32, metadata={""help"": ""alpha for LoRA""}) lora_dropout: float = field(default=0.05, metadata={""help"": ""dropout rate for LoRA""}) lora_target_modules: str = field(default=""query,value"", metadata={""help"": ""where to perform LoRA""}) @dataclass class DataArguments: data_path: str = field(default=None, metadata={""help"": ""Path to the training data.""}) kmer: int = field(default=-1, metadata={""help"": ""k-mer for input sequence. -1 means not using k-mer.""}) @dataclass class TrainingArguments(transformers.TrainingArguments): cache_dir: Optional[str] = field(default=None) run_name: str = field(default=""run"") optim: str = field(default=""adamw_torch"") model_max_length: int = field(default=512, metadata={""help"": ""Maximum sequence length.""}) gradient_accumulation_steps: int = field(default=1) per_device_train_batch_size: int = field(default=1) per_device_eval_batch_size: int = field(default=1) num_train_epochs: int = field(default=1) logging_steps: int = field(default=100) save_steps: int = field(default=100) fp16: bool = field(default=False) # START NEW # eval_steps: int = field(default=100) eval_steps: int = field(default=0.1) # END NEW evaluation_strategy: str = field(default=""steps"") warmup_steps: int = field(default=50) weight_decay: float = field(default=0.01) learning_rate: float = field(default=1e-4) save_total_limit: int = field(default=3) load_best_model_at_end: bool = field(default=True) output_dir: str = field(default=""output"") find_unused_parameters: bool = field(default=False) checkpointing: bool = field(default=False) dataloader_pin_memory: bool = field(default=False) eval_and_save_results: bool = field(default=True) save_model: bool = field(default=False) seed: int = field(default=42) def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str): """"""Collects the state dict and dump to disk."""""" state_dict = trainer.model.state_dict() if trainer.args.should_save: cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()} del state_dict trainer._save(output_dir, state_dict=cpu_state_dict) # noqa """""" Get the reversed complement of the original DNA sequence. """""" def get_alter_of_dna_sequence(sequence: str): MAP = {""A"": ""T"", ""T"": ""A"", ""C"": ""G"", ""G"": ""C""} # return """".join([MAP[c] for c in reversed(sequence)]) return """".join([MAP[c] for c in sequence]) """""" Transform a dna sequence to k-mer string """""" def generate_kmer_str(sequence: str, k: int) -> str: """"""Generate k-mer string from DNA sequence."""""" return "" "".join([sequence[i:i + k] for i in range(len(sequence) - k + 1)]) """""" Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved to the same directory as the original data with the same name but with a suffix of ""_{k}mer"". """""" def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -> List[str]: """"""Load or generate k-mer string for each DNA sequence."""""" kmer_path = data_path.tokenizerreplace("".csv"", f""_{k}mer.json"") if os.path.exists(kmer_path): logging.warning(f""Loading k-mer from {kmer_path}..."") with open(kmer_path, ""r"") as f: kmer = json.load(f) else: logging.warning(f""Generating k-mer..."") kmer = [generate_kmer_str(text, k) for text in texts] with open(kmer_path, ""w"") as f: logging.warning(f""Saving k-mer to {kmer_path}..."") json.dump(kmer, f) return kmer class SupervisedDataset(Dataset): """"""Dataset for supervised fine-tuning."""""" def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, kmer: int = -1): super(SupervisedDataset, self).__init__() # load data from the disk with open(data_path, ""r"") as f: data = list(csv.reader(f))[1:] if len(data[0]) == 2: # data is in the format of [text, label] logging.warning(""Perform single sequence classification..."") texts = [d[0] for d in data] labels = [int(d[1]) for d in data] # All genes sequences are concat: we don't work with the sequence-pair, # But we are tricking the model to think it is single sequence. elif len(data[0]) == 3: # data is in the format of [text1, text2, label] logging.warning(""Perform sequence-pair classification..."") texts = [[d[0], d[1]] for d in data] labels = [int(d[2]) for d in data] else: raise ValueError(""Data format not supported."") if kmer != -1: # only write file on the first process if torch.distributed.get_rank() not in [0, -1]: torch.distributed.barrier() logging.warning(f""Using {kmer}-mer as input..."") texts = load_or_generate_kmer(data_path, texts, kmer) if torch.distributed.get_rank() == 0: torch.distributed.barrier() output = tokenizer( texts, return_tensors=""pt"", padding=""longest"", max_length=tokenizer.model_max_length, truncation=True, ) self.input_ids = output[""input_ids""] # CHANGE self.input_ids[0][self.input_ids[0] == 0] = 2 # Change to which tokens we want to attend and to which we don't self.attention_mask = output[""attention_mask""] self.labels = labels self.num_labels = len(set(labels)) def __len__(self): return len(self.input_ids) def __getitem__(self, i) -> Dict[str, torch.Tensor]: return dict(input_ids=self.input_ids[i], labels=self.labels[i]) @dataclass class DataCollatorForSupervisedDataset(object): """"""Collate examples for supervised fine-tuning."""""" tokenizer: transformers.PreTrainedTokenizer def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]: input_ids, labels = tuple([instance[key] for instance in instances] for key in (""input_ids"", ""labels"")) input_ids = torch.nn.utils.rnn.pad_sequence( input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id ) labels = torch.Tensor(labels).long() return dict( input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ) """""" Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. """""" def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray): if logits.ndim == 3: # Reshape logits to 2D if needed logits = logits.reshape(-1, logits.shape[-1]) predictions = np.argmax(logits, axis=-1) valid_mask = labels != -100 # Exclude padding tokens (assuming -100 is the padding token ID) valid_predictions = predictions[valid_mask] valid_labels = labels[valid_mask] return { # START NEW ""sum prediction"": f'{sum(valid_predictions)}/{len(valid_predictions)}', # END NEW ""accuracy"": sklearn.metrics.accuracy_score(valid_labels, valid_predictions), ""f1"": sklearn.metrics.f1_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""matthews_correlation"": sklearn.metrics.matthews_corrcoef( valid_labels, valid_predictions ), ""precision"": sklearn.metrics.precision_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), ""recall"": sklearn.metrics.recall_score( valid_labels, valid_predictions, average=""macro"", zero_division=0 ), } """""" Compute metrics used for huggingface trainer. """""" def compute_metrics(eval_pred): logits, labels = eval_pred if isinstance(logits, tuple): # Unpack logits if it's a tuple logits = logits[0] return calculate_metric_with_sklearn(logits, labels) class CustomTrainer(transformers.Trainer): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.epoch_predictions = [] self.epoch_labels = [] self.epoch_loss = [] def compute_loss(self, model, inputs, return_outputs=False): """""" MAX: Subclassed to compute training accuracy. How the loss is computed by Trainer. By default, all models return the loss in the first element. Subclass and override for custom behavior. """""" if self.label_smoother is not None and ""labels"" in inputs: labels = inputs.pop(""labels"") else: labels = None outputs = model(**inputs, output_attentions=True) # TEST try: print(f""Attention: {outputs.attentions}"") except Exception: print(""No Attention returned"") if ""labels"" in inputs: preds = outputs.logits.detach() # Log accuracy acc = ( (preds.argmax(axis=1) == inputs[""labels""]) .type(torch.float) .mean() .item() ) # Uncomment it if you want to plot the batch accuracy # wandb.log({""batch_accuracy"": acc}) # Log accuracy # Store predictions and labels for epoch-level metrics self.epoch_predictions.append(preds.cpu().numpy()) self.epoch_labels.append(inputs[""labels""].cpu().numpy()) # Save past state if it exists if self.args.past_index >= 0: self._past = outputs[self.args.past_index] if labels is not None: loss = self.label_smoother(outputs, labels) else: loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0] # Uncomment it if you want to plot the batch loss # wandb.log({""batch_loss"": loss}) self.epoch_loss.append(loss.item()) # Store loss for epoch-level metrics return (loss, outputs) if return_outputs else loss # Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback): def __init__(self, trainer) -> None: super().__init__() self._trainer = trainer def on_epoch_end(self, args, state, control, **kwargs): # Aggregate predictions and labels for the entire epoch epoch_predictions = np.concatenate(self._trainer.epoch_predictions) epoch_labels = np.concatenate(self._trainer.epoch_labels) # Compute accuracy accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels) # Compute mean loss mean_loss = np.mean(self._trainer.epoch_loss) # Compute precision, recall, and F1-score precision, recall, f1, _ = precision_recall_fscore_support( epoch_labels, epoch_predictions.argmax(axis=1), average=""weighted"" ) # Log epoch-level metrics wandb.log({""epoch_accuracy"": accuracy, ""epoch_loss"": mean_loss}) wandb.log({""precision"": precision, ""recall"": recall, ""f1"": f1}) # Clear stored predictions, labels, and loss for the next epoch self._trainer.epoch_predictions = [] self._trainer.epoch_labels = [] self._trainer.epoch_loss = [] return None # TODO: use this function to gather the prediction and labels and get the metrics #%% Instantiating and training: from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \ TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \ compute_metrics from copy import deepcopy from transformers import TrainerCallback # END NEW import os import json import torch import transformers from peft import ( LoraConfig, get_peft_model, get_peft_model_state_dict, ) import wandb run = wandb.init() assert run is wandb.run def train(device): parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments)) model_args, data_args, training_args = parser.parse_args_into_dataclasses() # load tokenizer tokenizer = transformers.AutoTokenizer.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=""right"", use_fast=True, trust_remote_code=True, ) if ""InstaDeepAI"" in model_args.model_name_or_path: tokenizer.eos_token = tokenizer.pad_token # define datasets and data collator train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""train.csv""), kmer=data_args.kmer) val_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""dev.csv""), kmer=data_args.kmer) test_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=os.path.join(data_args.data_path, ""test.csv""), kmer=data_args.kmer) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) # load model model = transformers.AutoModelForSequenceClassification.from_pretrained( model_args.model_name_or_path, cache_dir=training_args.cache_dir, num_labels=train_dataset.num_labels, trust_remote_code=True, output_attentions = True ).to(device) # configure LoRA if model_args.use_lora: lora_config = LoraConfig( r=model_args.lora_r, lora_alpha=model_args.lora_alpha, target_modules=list(model_args.lora_target_modules.split("","")), lora_dropout=model_args.lora_dropout, bias=""none"", task_type=""SEQ_CLS"", inference_mode=False, ) model = get_peft_model(model, lora_config) model.print_trainable_parameters() trainer = CustomTrainer(model=model, tokenizer=tokenizer, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator ) trainer.add_callback(CustomCallback(trainer)) trainer.train() # train_result = trainer.train() # loss = train_result[""loss""] # print(f""loss issss: {loss}"") # print(f""Train reusults: {train_result}"") # NEW: result: only returns metrics at the end of training if training_args.save_model: trainer.save_state() safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir) # get the evaluation results from trainer if training_args.eval_and_save_results: results_path = os.path.join(training_args.output_dir, ""results"", training_args.run_name) results = trainer.evaluate(eval_dataset=test_dataset) os.makedirs(results_path, exist_ok=True) with open(os.path.join(results_path, ""eval_results.json""), ""w"") as f: json.dump(results, f) if __name__ == ""__main__"": # Define device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device) # Call the train function with the device train(device) After training, I try to run it on an example: model_path = './finetune/output/dnabert2' tokenizer = AutoTokenizer.from_pretrained(model_path) # Load the model with output_attention=True model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True) model_input = tokenizer(""ACTGACGGGTAGTGACTG"", return_tensors=""pt"") with torch.inference_mode(): output = model(**model_input, output_attentions=True) My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.",attention return even outputattentions true  m use pretraine model base bert  github link  dnabert2  use automodelforsequenceclassification mosaicml  mosaic  bert  base   m problem extract attention  read many post show way deal activate outputattention  true model  none post solve problem  output length 2 element shape  torch  size   1  7  768   torch  size   1  768    try get outputattention get none   m sure search solution would   m provide whole code  defining model  trainer  datum  tokenizer  copy import deepcopy sklearnmetrics import precisionrecallfscoresupport import wandb transformer import trainercallback  end new import os import csv import json import logging dataclasse import dataclass  field type import optional  dict  sequence  tuple  list import torch import transformer import sklearn import numpy np torchutilsdata import dataset  dataclass class modelarguments  modelnameorpath  optional  str   field  default  facebook  opt125 m   uselora  bool  field  default  false  metadata    help     whether use lora    lorar  int  field  default8  metadata    help     hidden dimension lora    loraalpha  int  field  default32  metadata    help     alpha lora    loradropout  float  field  default005  metadata    help     dropout rate lora    loratargetmodule  str  field  default  query  value   metadata    help     perform lora     dataclass class dataarguments  datapath  str  field  default  none  metadata    help     path training datum     kmer  int  field  default1  metadata    help     k  mer input sequence  1 mean use k  mer      dataclass class trainingarguments  transformer  trainingarguments   cachedir  optional  str   field  default  none  runname  str  field  default  run   optim  str  field  default  adamwtorch   modelmaxlength  int  field  default512  metadata    help     maximum sequence length     gradientaccumulationsteps  int  field  default1  perdevicetrainbatchsize  int  field  default1  perdeviceevalbatchsize  int  field  default1  numtrainepochs  int  field  default1  loggingstep  int  field  default100  savesteps  int  field  default100  fp16  bool  field  default  false   start new  evalstep  int  field  default100  evalstep  int  field  default01   end new evaluationstrategy  str  field  default  step   warmupsteps  int  field  default50  weightdecay  float  field  default001  learningrate  float  field  default1e4  savetotallimit  int  field  default3  loadbestmodelatend  bool  field  default  true  outputdir  str  field  default  output   findunusedparameter  bool  field  default  false  checkpointing  bool  field  default  false  dataloaderpinmemory  bool  field  default  false  evalandsaveresult  bool  field  default  true  savemodel  bool  field  default  false  seed  int  field  default42  def safesavemodelforhftrainer  trainer  transformer  trainer  outputdir  str       collect state dict dump disk     statedict  trainermodelstatedict   trainerargsshouldsave  cpustatedict   key  valuecpu   key  value statedictitems    del statedict trainersave  outputdir  statedict  cpustatedict   noqa     get reverse complement original dna sequence      def getalterofdnasequence  sequence  str   map                     c     g     g     c    return    join   map  c  c reverse  sequence    return    join   map  c  c sequence       transform dna sequence k  mer string     def generatekmerstr  sequence  str  k  int    str      generate k  mer string dna sequence     return     join   sequence    k  range  len  sequence   k  1        load generate k  mer string dna sequence  generate k  mer string save directory original datum name suffix     k  mer       def loadorgeneratekmer  datapath  str  text  list  str   k  int    list  str       load generate k  mer string dna sequence     kmerpath  datapathtokenizerreplace    csv   f    k  merjson   ospathexist  kmerpath   loggingwarne  f  loading k  mer  kmerpath     open  kmerpath    r   f  kmer  jsonload  f  else  loggingwarning  f  generate k  mer    kmer   generatekmerstr  text  k  text text  open  kmerpath    w   f  loggingwarning  f  saving k  mer  kmerpath     jsondump  kmer  f  return kmer class superviseddataset  dataset       dataset supervise fine  tuning     def   init    self  datapath  str  tokenizer  transformer  pretrainedtokenizer  kmer  int  1   super  superviseddataset  self  init      load data disk open  datapath    r   f  datum  list  csvreader  f    1   len  datum  0     2   datum format  text  label  loggingwarning    perform single sequence classification    text    0  datum  label   int   1   datum   gene sequence concat  not work sequence  pair   tricking model think single sequence  elif len  datum  0     3   datum format  text1  text2  label  loggingwarning    perform sequence  pair classification    text     0    1   datum  label   int   2   datum  else  raise valueerror    data format support    kmer   1   write file first process torchdistributedgetrank    0  1   torchdistributedbarrier   loggingwarning  f  use  kmer  mer input    text  loadorgeneratekmer  datapath  text  kmer  torchdistributedgetrank     0  torchdistributedbarri   output  tokenizer  text  returntensors  pt   padding  long   maxlength  tokenizermodelmaxlength  truncation  true   selfinputid  output    inputids    change selfinputid  0   selfinputid  0    0   2  change token want attend not selfattentionmask  output    attentionmask   selflabel  label selfnumlabel  len  set  label   def   len    self   return len  selfinputid  def   getitem    self     dict  str  torch  tensor   return dict  inputid  selfinputid    label  selflabel     dataclass class datacollatorforsuperviseddataset  object       collate example supervise fine  tuning     tokenizer  transformer  pretrainedtokenizer def   call    self  instance  sequence  dict     dict  str  torch  tensor   inputid  label  tuple   instance  key  instance instance  key    inputids     label    inputids  torchnnutilsrnnpadsequence  inputid  batchfirst  true  paddingvalue  selftokenizerpadtokenid  label  torch  tensor  label  long   return dict  inputid  inputids  label  label  attentionmask  inputidsne  selftokenizerpadtokenid        manually calculate accuracy  f1  matthewscorrelation  precision  recall sklearn      def calculatemetricwithsklearn  logit  npndarray  label  npndarray   logitsndim   3   reshape logit 2d need logit  logitsreshape  1  logitsshape  1   prediction  npargmax  logit  axis1  validmask  label   100  exclude padding token  assume 100 pad token id  validprediction  prediction  validmask  validlabel  label  validmask  return   start new   sum prediction   f   sum  validprediction     len  validprediction      end new   accuracy   sklearnmetricsaccuracyscore  validlabel  validprediction     f1   sklearnmetricsf1score  validlabel  validprediction  average  macro   zerodivision0     matthewscorrelation   sklearnmetricsmatthewscorrcoef  validlabel  validprediction     precision   sklearnmetricsprecisionscore  validlabel  validprediction  average  macro   zerodivision0     recall   sklearnmetricsrecallscore  validlabel  validprediction  average  macro   zerodivision0        compute metric use huggingface trainer      def computemetric  evalpred   logit  label  evalpred isinstance  logit  tuple    unpack logit s tuple logit  logit  0  return calculatemetricwithsklearn  logit  label  class customtrainer  transformer  trainer   def   init    self   args    kwargs   super   init     args    kwargs  selfepochprediction    selfepochlabel    selfepochloss    def computelos  self  model  input  returnoutput  false       max  subclassed compute training accuracy  loss compute trainer  default  model return loss first element  subclass override custom behavior      selflabelsmoother none   label  input  label  inputspop    label   else  label  none output  model    input  outputattention  true   test try  print  f  attention   outputsattention    except exception  print    attention return     label  input  pred  outputslogitsdetach    log accuracy acc    predsargmax  axis1    input    label    type  torchfloat  mean   item     uncomment want plot batch accuracy  wandblog     batchaccuracy   acc    log accuracy  store prediction label epoch  level metric selfepochpredictionsappend  predscpu   numpy    selfepochlabelsappend  input    label   cpu   numpy     save past state exist selfargspastindex   0  selfpast  output  selfargspastindex  label none  loss  selflabelsmoother  output  label  else  loss  output    loss   isinstance  output  dict  else output  0   uncomment want plot batch loss  wandblog     batchloss   loss   selfepochlossappend  lossitem     store loss epoch  level metric return  loss  output  returnoutput else loss  define custom callback calculate metric end epoch class customcallback  trainercallback   def   init    self  trainer    none  super   init     selftrainer  trainer def onepochend  self  args  state  control    kwarg    aggregate prediction label entire epoch epochprediction  npconcatenate  selftrainerepochprediction  epochlabels  npconcatenate  selftrainerepochlabel   compute accuracy accuracy  npmean  epochpredictionsargmax  axis1    epochlabel   compute mean loss meanloss  npmean  selftrainerepochloss   compute precision  recall  f1  score precision  recall  f1    precisionrecallfscoresupport  epochlabels  epochpredictionsargmax  axis1   average  weight    log epoch  level metric wandblog     epochaccuracy   accuracy    epochloss   meanloss   wandblog     precision   precision    recall   recall    f1   f1    clear store prediction  label  loss next epoch selftrainerepochprediction    selftrainerepochlabel    selftrainerepochloss    return none  todo  use function gather prediction label get metric    instantiating training  transformermodel import superviseddataset  datacollatorforsuperviseddataset  modelarguments   trainingarguments  dataarguments  safesavemodelforhftrainer  customtrainer  customcallback   computemetrics copy import deepcopy transformer import trainercallback  end new import os import json import torch import transformer peft import  loraconfig  getpeftmodel  getpeftmodelstatedict   import wandb run  wandbinit   assert run wandbrun def train  device   parser  transformer  hfargumentparser   modelarguments  dataarguments  trainingarguments   modelargs  dataargs  trainingargs  parserparseargsintodataclasse    load tokenizer tokenizer  transformer  autotokenizerfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  modelmaxlength  trainingargsmodelmaxlength  paddingside  right   usefast  true  trustremotecode  true     instadeepai  modelargsmodelnameorpath  tokenizereostoken  tokenizerpadtoken  define dataset datum collator traindataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    traincsv    kmer  dataargskmer  valdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    devcsv    kmer  dataargskmer  testdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    testcsv    kmer  dataargskmer  datacollator  datacollatorforsuperviseddataset  tokenizer  tokenizer   load model model  transformer  automodelforsequenceclassificationfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  numlabel  traindatasetnumlabel  trustremotecode  true  outputattention  true  to  device   configure lora modelargsuselora  loraconfig  loraconfig  r  modelargslorar  loraalpha  modelargsloraalpha  targetmodule  list  modelargsloratargetmodulessplit         loradropout  modelargsloradropout  bias  none   tasktype  seqcls   inferencemode  false   model  getpeftmodel  model  loraconfig  modelprinttrainableparameters   trainer  customtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  computemetric  computemetric  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  traineraddcallback  customcallback  trainer   trainertrain    trainresult  trainertrain    loss  trainresult    loss    print  f  loss issss   loss     print  f  train reusult   trainresult     new  result  return metric end training trainingargssavemodel  trainersavestate   safesavemodelforhftrainer  trainer  trainer  outputdir  trainingargsoutputdir   get evaluation result trainer trainingargsevalandsaveresults  resultspath  ospathjoin  trainingargsoutputdir    result   trainingargsrunname  result  trainerevaluate  evaldataset  testdataset  osmakedirs  resultspath  existok  true  open  ospathjoin  resultspath    evalresultsjson      w   f  jsondump  result  f    name         main      define device device  torchdevice   cuda  torchcudaisavailable   else  cpu   print   use device    device   call train function device train  device  training  try run example  modelpath   finetune  output  dnabert2  tokenizer  autotokenizerfrompretraine  modelpath   load model outputattention  true model  automodelfrompretrained  modelpath  trustremotecode  true  outputattention  true  modelinput  tokenizer    actgacgggtagtgactg   returntensors  pt   torchinferencemode    output  model    modelinput  outputattention  true  code might test print  let know anything missing  thank much help ,problem deep structure  attention discard early model  therefore go code understand happen  change    huggingfaceco  jaandoui  dnabert2  attentionextracted extract attentionprob  change do ,attention return even outputattentions true  m use pretraine model base bert  github link  dnabert2  use automodelforsequenceclassification mosaicml  mosaic  bert  base   m problem extract attention  read many post show way deal activate outputattention  true model  none post solve problem  output length 2 element shape  torch  size   1  7  768   torch  size   1  768    try get outputattention get none   m sure search solution would   m provide whole code  defining model  trainer  datum  tokenizer  copy import deepcopy sklearnmetrics import precisionrecallfscoresupport import wandb transformer import trainercallback  end new import os import csv import json import logging dataclasse import dataclass  field type import optional  dict  sequence  tuple  list import torch import transformer import sklearn import numpy np torchutilsdata import dataset  dataclass class modelarguments  modelnameorpath  optional  str   field  default  facebook  opt125 m   uselora  bool  field  default  false  metadata    help     whether use lora    lorar  int  field  default8  metadata    help     hidden dimension lora    loraalpha  int  field  default32  metadata    help     alpha lora    loradropout  float  field  default005  metadata    help     dropout rate lora    loratargetmodule  str  field  default  query  value   metadata    help     perform lora     dataclass class dataarguments  datapath  str  field  default  none  metadata    help     path training datum     kmer  int  field  default1  metadata    help     k  mer input sequence  1 mean use k  mer      dataclass class trainingarguments  transformer  trainingarguments   cachedir  optional  str   field  default  none  runname  str  field  default  run   optim  str  field  default  adamwtorch   modelmaxlength  int  field  default512  metadata    help     maximum sequence length     gradientaccumulationsteps  int  field  default1  perdevicetrainbatchsize  int  field  default1  perdeviceevalbatchsize  int  field  default1  numtrainepochs  int  field  default1  loggingstep  int  field  default100  savesteps  int  field  default100  fp16  bool  field  default  false   start new  evalstep  int  field  default100  evalstep  int  field  default01   end new evaluationstrategy  str  field  default  step   warmupsteps  int  field  default50  weightdecay  float  field  default001  learningrate  float  field  default1e4  savetotallimit  int  field  default3  loadbestmodelatend  bool  field  default  true  outputdir  str  field  default  output   findunusedparameter  bool  field  default  false  checkpointing  bool  field  default  false  dataloaderpinmemory  bool  field  default  false  evalandsaveresult  bool  field  default  true  savemodel  bool  field  default  false  seed  int  field  default42  def safesavemodelforhftrainer  trainer  transformer  trainer  outputdir  str       collect state dict dump disk     statedict  trainermodelstatedict   trainerargsshouldsave  cpustatedict   key  valuecpu   key  value statedictitems    del statedict trainersave  outputdir  statedict  cpustatedict   noqa     get reverse complement original dna sequence      def getalterofdnasequence  sequence  str   map                     c     g     g     c    return    join   map  c  c reverse  sequence    return    join   map  c  c sequence       transform dna sequence k  mer string     def generatekmerstr  sequence  str  k  int    str      generate k  mer string dna sequence     return     join   sequence    k  range  len  sequence   k  1        load generate k  mer string dna sequence  generate k  mer string save directory original datum name suffix     k  mer       def loadorgeneratekmer  datapath  str  text  list  str   k  int    list  str       load generate k  mer string dna sequence     kmerpath  datapathtokenizerreplace    csv   f    k  merjson   ospathexist  kmerpath   loggingwarne  f  loading k  mer  kmerpath     open  kmerpath    r   f  kmer  jsonload  f  else  loggingwarning  f  generate k  mer    kmer   generatekmerstr  text  k  text text  open  kmerpath    w   f  loggingwarning  f  saving k  mer  kmerpath     jsondump  kmer  f  return kmer class superviseddataset  dataset       dataset supervise fine  tuning     def   init    self  datapath  str  tokenizer  transformer  pretrainedtokenizer  kmer  int  1   super  superviseddataset  self  init      load data disk open  datapath    r   f  datum  list  csvreader  f    1   len  datum  0     2   datum format  text  label  loggingwarning    perform single sequence classification    text    0  datum  label   int   1   datum   gene sequence concat  not work sequence  pair   tricking model think single sequence  elif len  datum  0     3   datum format  text1  text2  label  loggingwarning    perform sequence  pair classification    text     0    1   datum  label   int   2   datum  else  raise valueerror    data format support    kmer   1   write file first process torchdistributedgetrank    0  1   torchdistributedbarrier   loggingwarning  f  use  kmer  mer input    text  loadorgeneratekmer  datapath  text  kmer  torchdistributedgetrank     0  torchdistributedbarri   output  tokenizer  text  returntensors  pt   padding  long   maxlength  tokenizermodelmaxlength  truncation  true   selfinputid  output    inputids    change selfinputid  0   selfinputid  0    0   2  change token want attend not selfattentionmask  output    attentionmask   selflabel  label selfnumlabel  len  set  label   def   len    self   return len  selfinputid  def   getitem    self     dict  str  torch  tensor   return dict  inputid  selfinputid    label  selflabel     dataclass class datacollatorforsuperviseddataset  object       collate example supervise fine  tuning     tokenizer  transformer  pretrainedtokenizer def   call    self  instance  sequence  dict     dict  str  torch  tensor   inputid  label  tuple   instance  key  instance instance  key    inputids     label    inputids  torchnnutilsrnnpadsequence  inputid  batchfirst  true  paddingvalue  selftokenizerpadtokenid  label  torch  tensor  label  long   return dict  inputid  inputids  label  label  attentionmask  inputidsne  selftokenizerpadtokenid        manually calculate accuracy  f1  matthewscorrelation  precision  recall sklearn      def calculatemetricwithsklearn  logit  npndarray  label  npndarray   logitsndim   3   reshape logit 2d need logit  logitsreshape  1  logitsshape  1   prediction  npargmax  logit  axis1  validmask  label   100  exclude padding token  assume 100 pad token id  validprediction  prediction  validmask  validlabel  label  validmask  return   start new   sum prediction   f   sum  validprediction     len  validprediction      end new   accuracy   sklearnmetricsaccuracyscore  validlabel  validprediction     f1   sklearnmetricsf1score  validlabel  validprediction  average  macro   zerodivision0     matthewscorrelation   sklearnmetricsmatthewscorrcoef  validlabel  validprediction     precision   sklearnmetricsprecisionscore  validlabel  validprediction  average  macro   zerodivision0     recall   sklearnmetricsrecallscore  validlabel  validprediction  average  macro   zerodivision0        compute metric use huggingface trainer      def computemetric  evalpred   logit  label  evalpred isinstance  logit  tuple    unpack logit s tuple logit  logit  0  return calculatemetricwithsklearn  logit  label  class customtrainer  transformer  trainer   def   init    self   args    kwargs   super   init     args    kwargs  selfepochprediction    selfepochlabel    selfepochloss    def computelos  self  model  input  returnoutput  false       max  subclassed compute training accuracy  loss compute trainer  default  model return loss first element  subclass override custom behavior      selflabelsmoother none   label  input  label  inputspop    label   else  label  none output  model    input  outputattention  true   test try  print  f  attention   outputsattention    except exception  print    attention return     label  input  pred  outputslogitsdetach    log accuracy acc    predsargmax  axis1    input    label    type  torchfloat  mean   item     uncomment want plot batch accuracy  wandblog     batchaccuracy   acc    log accuracy  store prediction label epoch  level metric selfepochpredictionsappend  predscpu   numpy    selfepochlabelsappend  input    label   cpu   numpy     save past state exist selfargspastindex   0  selfpast  output  selfargspastindex  label none  loss  selflabelsmoother  output  label  else  loss  output    loss   isinstance  output  dict  else output  0   uncomment want plot batch loss  wandblog     batchloss   loss   selfepochlossappend  lossitem     store loss epoch  level metric return  loss  output  returnoutput else loss  define custom callback calculate metric end epoch class customcallback  trainercallback   def   init    self  trainer    none  super   init     selftrainer  trainer def onepochend  self  args  state  control    kwarg    aggregate prediction label entire epoch epochprediction  npconcatenate  selftrainerepochprediction  epochlabels  npconcatenate  selftrainerepochlabel   compute accuracy accuracy  npmean  epochpredictionsargmax  axis1    epochlabel   compute mean loss meanloss  npmean  selftrainerepochloss   compute precision  recall  f1  score precision  recall  f1    precisionrecallfscoresupport  epochlabels  epochpredictionsargmax  axis1   average  weight    log epoch  level metric wandblog     epochaccuracy   accuracy    epochloss   meanloss   wandblog     precision   precision    recall   recall    f1   f1    clear store prediction  label  loss next epoch selftrainerepochprediction    selftrainerepochlabel    selftrainerepochloss    return none  todo  use function gather prediction label get metric    instantiating training  transformermodel import superviseddataset  datacollatorforsuperviseddataset  modelarguments   trainingarguments  dataarguments  safesavemodelforhftrainer  customtrainer  customcallback   computemetrics copy import deepcopy transformer import trainercallback  end new import os import json import torch import transformer peft import  loraconfig  getpeftmodel  getpeftmodelstatedict   import wandb run  wandbinit   assert run wandbrun def train  device   parser  transformer  hfargumentparser   modelarguments  dataarguments  trainingarguments   modelargs  dataargs  trainingargs  parserparseargsintodataclasse    load tokenizer tokenizer  transformer  autotokenizerfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  modelmaxlength  trainingargsmodelmaxlength  paddingside  right   usefast  true  trustremotecode  true     instadeepai  modelargsmodelnameorpath  tokenizereostoken  tokenizerpadtoken  define dataset datum collator traindataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    traincsv    kmer  dataargskmer  valdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    devcsv    kmer  dataargskmer  testdataset  superviseddataset  tokenizer  tokenizer  datapath  ospathjoin  dataargsdatapath    testcsv    kmer  dataargskmer  datacollator  datacollatorforsuperviseddataset  tokenizer  tokenizer   load model model  transformer  automodelforsequenceclassificationfrompretraine  modelargsmodelnameorpath  cachedir  trainingargscachedir  numlabel  traindatasetnumlabel  trustremotecode  true  outputattention  true  to  device   configure lora modelargsuselora  loraconfig  loraconfig  r  modelargslorar  loraalpha  modelargsloraalpha  targetmodule  list  modelargsloratargetmodulessplit         loradropout  modelargsloradropout  bias  none   tasktype  seqcls   inferencemode  false   model  getpeftmodel  model  loraconfig  modelprinttrainableparameters   trainer  customtrainer  model  model  tokenizer  tokenizer  arg  trainingargs  computemetric  computemetric  traindataset  traindataset  evaldataset  valdataset  datacollator  datacollator  traineraddcallback  customcallback  trainer   trainertrain    trainresult  trainertrain    loss  trainresult    loss    print  f  loss issss   loss     print  f  train reusult   trainresult     new  result  return metric end training trainingargssavemodel  trainersavestate   safesavemodelforhftrainer  trainer  trainer  outputdir  trainingargsoutputdir   get evaluation result trainer trainingargsevalandsaveresults  resultspath  ospathjoin  trainingargsoutputdir    result   trainingargsrunname  result  trainerevaluate  evaldataset  testdataset  osmakedirs  resultspath  existok  true  open  ospathjoin  resultspath    evalresultsjson      w   f  jsondump  result  f    name         main      define device device  torchdevice   cuda  torchcudaisavailable   else  cpu   print   use device    device   call train function device train  device  training  try run example  modelpath   finetune  output  dnabert2  tokenizer  autotokenizerfrompretraine  modelpath   load model outputattention  true model  automodelfrompretrained  modelpath  trustremotecode  true  outputattention  true  modelinput  tokenizer    actgacgggtagtgactg   returntensors  pt   torchinferencemode    output  model    modelinput  outputattention  true  code might test print  let know anything missing  thank much help  problem deep structure  attention discard early model  therefore go code understand happen  change    huggingfaceco  jaandoui  dnabert2  attentionextracted extract attentionprob  change do ,Implementation Issues
Fine-tuning BERT with deterministic masking instead of random masking,"I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons). To do so, I created a dataset that has two columns: text in which some tokens have been replaced with [MASK] (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and label where I have the whole text. Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's transformers library, but I do not want to use DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2) where the masking is done randomly and I only can control the probability. What can I do?","['nlp', 'huggingface-transformers', 'bert-language-model']",2,"This is what I did to solve my problem. I created a custom class and changed the tokenization in a way that I needed (mask one of the numerical spans in the input). class CustomDataCollator(DataCollatorForLanguageModeling): mlm: bool = True return_tensors: str = ""pt"" def __post_init__(self): if self.mlm and self.tokenizer.mask_token is None: raise ValueError( ""This tokenizer does not have a mask token which is necessary "" ""for masked language modeling. You should pass `mlm=False` to "" ""train on causal language modeling instead."" ) def torch_mask_tokens(self, inputs, special_tokens_mask): """""" Prepare masked tokens inputs/labels for masked language modeling. NOTE: keep `special_tokens_mask` as an argument for avoiding error """""" # labels is batch_size x length of the sequence tensor # with the original token id # the length of the sequence includes the special tokens (2) labels = inputs.clone() batch_size = inputs.size(0) # seq_len = inputs.size(1) # in each seq, find the indices of the tokens that represent digits dig_ids = [1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023] dig_idx = torch.zeros_like(labels) for dig_id in dig_ids: dig_idx += (labels == dig_id) dig_idx = dig_idx.bool() # in each seq, find the spans of Trues using `find_spans` function spans = [] for i in range(batch_size): spans.append(find_spans(dig_idx[i].tolist())) masked_indices = torch.zeros_like(labels) # spans is a list of lists of tuples # in each tuple, the first element is the start index # and the second element is the length # in each child list, choose a random tuple for i in range(batch_size): if len(spans[i]) > 0: idx = torch.randint(0, len(spans[i]), (1,)) start, length = spans[i][idx[0]] masked_indices[i, start:start + length] = 1 else: print(""No digit found in the sequence!"") masked_indices = masked_indices.bool() # We only compute loss on masked tokens labels[~masked_indices] = -100 # change the input's masked_indices to self.tokenizer.mask_token inputs[masked_indices] = self.tokenizer.mask_token_id return inputs, labels def find_spans(lst): spans = [] for k, g in groupby(enumerate(lst), key=itemgetter(1)): if k: glist = list(g) spans.append((glist[0][0], len(glist))) return spans",2024-04-22 10:39:05,2024-05-21 07:37:41,345,https://stackoverflow.com/questions/78365608/fine-tuning-bert-with-deterministic-masking-instead-of-random-masking,"Fine-tuning BERT with deterministic masking instead of random masking I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons). To do so, I created a dataset that has two columns: text in which some tokens have been replaced with [MASK] (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and label where I have the whole text. Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's transformers library, but I do not want to use DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2) where the masking is done randomly and I only can control the probability. What can I do?",fine  tuning bert deterministic masking instead random masking want fine  tune bert specific dataset  problem want mask tokens training dataset randomly  already choose tokens want mask  certain reason    create dataset two column  text token replace  mask   aware fact word could tokenise one token take care  label whole text  want fine  tune bert model  say  bert  base  uncased  use hugging face s transformer library  want use datacollatorforlanguagemodeling  tokenizer  tokenizer  mlmprobability02  mask do randomly control probability  ,solve problem  create custom class change tokenization way need  mask one numerical spans input   class customdatacollator  datacollatorforlanguagemodele   mlm  bool  true returntensor  str    pt  def   postinit    self   selfmlm selftokenizermasktoken none  raise valueerror    tokenizer mask token necessary     mask language modeling  pass  mlm  false      train causal language modeling instead    def torchmasktoken  self  input  specialtokensmask       prepare mask tokens input  label mask language modeling  note  keep  specialtokensmask  argument avoid error      label batchsize x length sequence tensor  original token i d  length sequence include special token  2  label  inputsclone   batchsize  inputssize  0   seqlen  inputssize  1   seq  find index token represent digit digid   1014  1015  1016  1017  1018  1019  1020  1021  1022  1023  digidx  torchzeroslike  label  digid digid  digidx    label   digid  digidx  digidxbool    seq  find span true use  findspans  function span    range  batchsize   spansappend  findspan  digidx   tolist     maskedindice  torchzeroslike  label   span list list tuple  tuple  first element start index  second element length  child list  choose random tuple range  batchsize   len  span     0  idx  torchrandint  0  len  span      1    start  length  span    idx  0   maskedindice   start  start  length   1 else  print    digit find sequence    maskedindice  maskedindicesbool    compute loss mask tokens label  maskedindices   100  change input s maskedindices selftokenizermasktoken input  maskedindice   selftokenizermasktokenid return input  label def findspans  lst   span    k  g groupby  enumerate  lst   key  itemgetter  1    k  glist  list  g  spansappend   glist  0   0   len  glist    return span,fine  tuning bert deterministic masking instead random masking want fine  tune bert specific dataset  problem want mask tokens training dataset randomly  already choose tokens want mask  certain reason    create dataset two column  text token replace  mask   aware fact word could tokenise one token take care  label whole text  want fine  tune bert model  say  bert  base  uncased  use hugging face s transformer library  want use datacollatorforlanguagemodeling  tokenizer  tokenizer  mlmprobability02  mask do randomly control probability   solve problem  create custom class change tokenization way need  mask one numerical spans input   class customdatacollator  datacollatorforlanguagemodele   mlm  bool  true returntensor  str    pt  def   postinit    self   selfmlm selftokenizermasktoken none  raise valueerror    tokenizer mask token necessary     mask language modeling  pass  mlm  false      train causal language modeling instead    def torchmasktoken  self  input  specialtokensmask       prepare mask tokens input  label mask language modeling  note  keep  specialtokensmask  argument avoid error      label batchsize x length sequence tensor  original token i d  length sequence include special token  2  label  inputsclone   batchsize  inputssize  0   seqlen  inputssize  1   seq  find index token represent digit digid   1014  1015  1016  1017  1018  1019  1020  1021  1022  1023  digidx  torchzeroslike  label  digid digid  digidx    label   digid  digidx  digidxbool    seq  find span true use  findspans  function span    range  batchsize   spansappend  findspan  digidx   tolist     maskedindice  torchzeroslike  label   span list list tuple  tuple  first element start index  second element length  child list  choose random tuple range  batchsize   len  span     0  idx  torchrandint  0  len  span      1    start  length  span    idx  0   maskedindice   start  start  length   1 else  print    digit find sequence    maskedindice  maskedindicesbool    compute loss mask tokens label  maskedindices   100  change input s maskedindices selftokenizermasktoken input  maskedindice   selftokenizermasktokenid return input  label def findspans  lst   span    k  g groupby  enumerate  lst   key  itemgetter  1    k  glist  list  g  spansappend   glist  0   0   len  glist    return span,Task-Specific Queries
Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java,"So we are using Stanford NLP to annotate input text, and these input texts are laughably small. Below is one example of the same. ""Can you give me details about Mohammad Siva John with identifiers 6745-3876-1354-8790 and 313-31-333"" Below is the Java code snippet to annotate. final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); Below is the Maven dependency. <dependency> <groupId>edu.stanford.nlp</groupId> <artifactId>stanford-corenlp</artifactId> <version>4.5.4</version> </dependency> This works fine, but after couple of days, the JVM crashes with a coredump. Coredump analysis shows that below line resulted into OutOfMemoryError pipeline.annotate(document); Any thoughts on how to resolve this? There are no field level variables in the class, and all of them are method level, and so should be 'freed' once the execution is done. So, there should be no OutOfMemoryError first of all to begin with. Quite perplexing. Any thoughts?","['java', 'nlp', 'out-of-memory']",1,"So found this in StanfordNLP Javadoc of void edu.stanford.nlp.pipeline.StanfordCoreNLP.clearAnnotatorPool() Call this if you are no longer using StanfordCoreNLP and want torelease the memory associated with the annotators. Calling this method itself almost has no impact, but the real game changer is calling 'System.gc()'. Below is the fix for this, just call clearAnnotatorPool and gc after annotate . final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); // Below two calls would fix memory issue. StanfordCoreNLP.clearAnnotatorPool(); System.gc(); This is how used memory is with only StanfordCoreNLP.clearAnnotatorPool() call. Note that this is without System.gc() call. Notice that for 1000 calls to 'annotate' in a loop the used memory crosses 2500 MB and then falls down below 500 MB. This is the case when we are letting JVM call gc. However, when I call both StanfordCoreNLP.clearAnnotatorPool(); and System.gc(); the results are drastically different. Note that used memory is within the range of 132 to 133 MB irrespective of whether the annotation pool has been cleared. One starts seeing the real of 'StanfordCoreNLP.clearAnnotatorPool()' around after 150 pipeline.annotate hits. Below is memory utilization observed over 1000 hits to pipeline.annotate. Observe that with 'StanfordCoreNLP.clearAnnotatorPool() and System.gc()', the memory utilization hovers just above 40 MB. If you rightly understand, this only signifies that somehow the JVM default gc execution is not releasing as much memory as when an explicit call is being made. I understand there would be timing difference and all, which only means that one needs to further research on the default gc and the JDK on being used (am on JDK 21 with IDE enforcing JDK 17 compliance level. The graphs are all almost the same when run on a JDK 17 directly too) and how this changes with various gc strategies. But then I'm happy with this and conclude! Hope this helps someone.",2024-04-18 05:13:30,2024-04-18 06:06:33,26,https://stackoverflow.com/questions/78344999/stanford-nlp-annotation-pipeline-annotate-resulting-into-outofmemoryerror-in-jav,"Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java So we are using Stanford NLP to annotate input text, and these input texts are laughably small. Below is one example of the same. ""Can you give me details about Mohammad Siva John with identifiers 6745-3876-1354-8790 and 313-31-333"" Below is the Java code snippet to annotate. final Properties properties = new Properties(); properties.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma""); final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); final Annotation document = new Annotation(text); pipeline.annotate(document); Below is the Maven dependency. <dependency> <groupId>edu.stanford.nlp</groupId> <artifactId>stanford-corenlp</artifactId> <version>4.5.4</version> </dependency> This works fine, but after couple of days, the JVM crashes with a coredump. Coredump analysis shows that below line resulted into OutOfMemoryError pipeline.annotate(document); Any thoughts on how to resolve this? There are no field level variables in the class, and all of them are method level, and so should be 'freed' once the execution is done. So, there should be no OutOfMemoryError first of all to begin with. Quite perplexing. Any thoughts?",stanford nlp annotation pipelineannotate result outofmemoryerror java use stanford nlp annotate input text  input text laughably small  one example    give detail mohammad siva john identifier 6745  3876  1354  8790 313  31  333  java code snippet annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document   maven dependency   dependency   groupid  edustanfordnlp  groupid   artifactid  stanford  corenlp  artifactid   version  454  version   dependency  work fine  couple day  jvm crash coredump  coredump analysis show line result outofmemoryerror pipelineannotate  document   thought resolve  field level variable class  method level   free  execution do   outofmemoryerror first begin  quite perplexing  thought ,find stanfordnlp javadoc void edustanfordnlppipeline  stanfordcorenlpclearannotatorpool   call long use stanfordcorenlp want torelease memory associate annotator  call method almost impact  real game changer call  systemgc     fix  call clearannotatorpool gc annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document    two call would fix memory issue  stanfordcorenlpclearannotatorpool    systemgc    use memory stanfordcorenlpclearannotatorpool   call  note without systemgc   call  notice 1000 call  annotate  loop use memory crosse 2500 mb fall 500 mb  case let jvm call gc  however  call stanfordcorenlpclearannotatorpool    systemgc    result drastically different  note use memory within range 132 133 mb irrespective whether annotation pool clear  one start see real  stanfordcorenlpclearannotatorpool    around 150 pipelineannotate hit  memory utilization observe 1000 hit pipelineannotate  observe  stanfordcorenlpclearannotatorpool   systemgc     memory utilization hover 40 mb  rightly understand  signify somehow jvm default gc execution release much memory explicit call make  understand would time difference  mean one need research default gc jdk use  jdk 21 ide enforce jdk 17 compliance level  graph almost run jdk 17 directly  change various gc strategy   m happy conclude  hope help someone ,stanford nlp annotation pipelineannotate result outofmemoryerror java use stanford nlp annotate input text  input text laughably small  one example    give detail mohammad siva john identifier 6745  3876  1354  8790 313  31  333  java code snippet annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document   maven dependency   dependency   groupid  edustanfordnlp  groupid   artifactid  stanford  corenlp  artifactid   version  454  version   dependency  work fine  couple day  jvm crash coredump  coredump analysis show line result outofmemoryerror pipelineannotate  document   thought resolve  field level variable class  method level   free  execution do   outofmemoryerror first begin  quite perplexing  thought  find stanfordnlp javadoc void edustanfordnlppipeline  stanfordcorenlpclearannotatorpool   call long use stanfordcorenlp want torelease memory associate annotator  call method almost impact  real game changer call  systemgc     fix  call clearannotatorpool gc annotate  final properties property  new properties    propertiessetproperty    annotator     tokenize  ssplit  pos  lemma    final stanfordcorenlp pipeline  new stanfordcorenlp  property   final annotation document  new annotation  text   pipelineannotate  document    two call would fix memory issue  stanfordcorenlpclearannotatorpool    systemgc    use memory stanfordcorenlpclearannotatorpool   call  note without systemgc   call  notice 1000 call  annotate  loop use memory crosse 2500 mb fall 500 mb  case let jvm call gc  however  call stanfordcorenlpclearannotatorpool    systemgc    result drastically different  note use memory within range 132 133 mb irrespective whether annotation pool clear  one start see real  stanfordcorenlpclearannotatorpool    around 150 pipelineannotate hit  memory utilization observe 1000 hit pipelineannotate  observe  stanfordcorenlpclearannotatorpool   systemgc     memory utilization hover 40 mb  rightly understand  signify somehow jvm default gc execution release much memory explicit call make  understand would time difference  mean one need research default gc jdk use  jdk 21 ide enforce jdk 17 compliance level  graph almost run jdk 17 directly  change various gc strategy   m happy conclude  hope help someone ,Implementation Issues
How to tag words that not include one specific symbol in Spacy?,"I'm trying to tag one word in Spacy using regex, but I want to add one condition: it can't contain symbol '/' in any place inside. My code looks like this: [{'lower': {""regex"": ""^.*(word).*?""}}] I tried using ^ to exclude this but It didn't work. So examples: 'subwordw' tagged: 'subword' 'subword/w' tagged nothing","['python', 'nlp', 'pattern-matching', 'spacy']",1,"try this: {'lower': {'REGEX': ""^([^\/]*word[^\/]*)$""}}",2024-04-18 04:21:10,2024-04-18 16:12:27,31,https://stackoverflow.com/questions/78344850/how-to-tag-words-that-not-include-one-specific-symbol-in-spacy,"How to tag words that not include one specific symbol in Spacy? I'm trying to tag one word in Spacy using regex, but I want to add one condition: it can't contain symbol '/' in any place inside. My code looks like this: [{'lower': {""regex"": ""^.*(word).*?""}}] I tried using ^ to exclude this but It didn't work. So examples: 'subwordw' tagged: 'subword' 'subword/w' tagged nothing",tag word include one specific symbol spacy   m try tag one word spacy use regex  want add one condition  can not contain symbol    place inside  code look like     low      regex        word         try use  exclude not work  example   subwordw  tagged   subword   subword  w  tagged nothing,try    low     regex           word         ,tag word include one specific symbol spacy   m try tag one word spacy use regex  want add one condition  can not contain symbol    place inside  code look like     low      regex        word         try use  exclude not work  example   subwordw  tagged   subword   subword  w  tagged nothing try    low     regex           word         ,Library/Tool-Based Queries
Sklearm FeatureHasher not working on a single column in a dataframe,"I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error: ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings. from sklearn.feature_extraction import FeatureHasher hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_df = pd.DataFrame(fh.transform(X_train[""Item_Identifier""]).toarray(), columns=['H'+str(i) for i in range (hash_vector_size)]) I was expecting a dataframe of 50 columns where the data would have been hashed in","['pandas', 'machine-learning', 'scikit-learn', 'nlp']",1,"You were almost there: from sklearn.feature_extraction import FeatureHasher import pandas as pd data = {""Item_Identifier"": [""ID1"", ""ID2"", ""ID3"", ""ID4"", ""ID5""]} X_train = pd.DataFrame(data) hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_features = fh.transform([[item] for item in X_train[""Item_Identifier""]]) hashed_df = pd.DataFrame(hashed_features.toarray(), columns=['H'+str(i) for i in range(hash_vector_size)]) print(hashed_df) which gives H0 H1 H2 H3 H4 H5 H6 H7 H8 H9 ... H40 H41 H42 H43 \ 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1.0 ... 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 H44 H45 H46 H47 H48 H49 0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 [5 rows x 50 columns]",2024-04-14 17:20:17,2024-04-14 17:48:30,111,https://stackoverflow.com/questions/78324647/sklearm-featurehasher-not-working-on-a-single-column-in-a-dataframe,"Sklearm FeatureHasher not working on a single column in a dataframe I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error: ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings. from sklearn.feature_extraction import FeatureHasher hash_vector_size = 50 fh = FeatureHasher(n_features=hash_vector_size, input_type='string') hashed_df = pd.DataFrame(fh.transform(X_train[""Item_Identifier""]).toarray(), columns=['H'+str(i) for i in range (hash_vector_size)]) I was expecting a dataframe of 50 columns where the data would have been hashed in",sklearm featurehasher work single column dataframe try perform feature hasher single column dataframe keep give error  valueerror  samples single string  input must iterable iterable string  sklearnfeatureextraction import featurehasher hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hasheddf  pd  dataframe  fhtransform  xtrain    itemidentifier    toarray    columns   hstr   range  hashvectorsize    expect dataframe 50 column datum would hash,almost  sklearnfeatureextraction import featurehasher import panda pd datum     itemidentifier      id1     id2     id3     id4     id5    xtrain  pd  dataframe  datum  hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hashedfeature  fhtransform    item  item xtrain    itemidentifier     hasheddf  pd  dataframe  hashedfeaturestoarray    columns   hstr   range  hashvectorsize    print  hasheddf  give h0 h1 h2 h3 h4 h5 h6 h7 h8 h9  h40 h41 h42 h43  0 00 00 00 00 00 00 00 00 00 10  00 00 00 00 1 00 00 00 00 00 00 00 00 00 00  00 00 00 00 2 00 00 00 00 00 00 00 00 00 00  00 00 00 00 3 00 00 00 00 00 00 00 00 00 00  00 00 00 00 4 00 00 00 00 00 00 00 00 00 00  00 00 00 00 h44 h45 h46 h47 h48 h49 0 00 00 00 00 00 00 1 00 00 00 00 00 00 2 00 00 00 00 00 00 3 00 00 00 00 00 00 4 00 00 00 00 00 00  5 row x 50 column ,sklearm featurehasher work single column dataframe try perform feature hasher single column dataframe keep give error  valueerror  samples single string  input must iterable iterable string  sklearnfeatureextraction import featurehasher hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hasheddf  pd  dataframe  fhtransform  xtrain    itemidentifier    toarray    columns   hstr   range  hashvectorsize    expect dataframe 50 column datum would hash almost  sklearnfeatureextraction import featurehasher import panda pd datum     itemidentifier      id1     id2     id3     id4     id5    xtrain  pd  dataframe  datum  hashvectorsize  50 fh  featurehasher  nfeature  hashvectorsize  inputtypestre   hashedfeature  fhtransform    item  item xtrain    itemidentifier     hasheddf  pd  dataframe  hashedfeaturestoarray    columns   hstr   range  hashvectorsize    print  hasheddf  give h0 h1 h2 h3 h4 h5 h6 h7 h8 h9  h40 h41 h42 h43  0 00 00 00 00 00 00 00 00 00 10  00 00 00 00 1 00 00 00 00 00 00 00 00 00 00  00 00 00 00 2 00 00 00 00 00 00 00 00 00 00  00 00 00 00 3 00 00 00 00 00 00 00 00 00 00  00 00 00 00 4 00 00 00 00 00 00 00 00 00 00  00 00 00 00 h44 h45 h46 h47 h48 h49 0 00 00 00 00 00 00 1 00 00 00 00 00 00 2 00 00 00 00 00 00 3 00 00 00 00 00 00 4 00 00 00 00 00 00  5 row x 50 column ,Basic Understanding
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data. However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model. This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?","['python', 'nlp', 'artificial-intelligence', 'word2vec', 'word-embedding']",2,"As has been pointed out before , there is no ""go-to"" way for fine-tuning Word2Vec type models. I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of sentence-transformers or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that. For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be faiss .",2024-04-12 17:57:56,2024-04-14 12:15:53,1349,https://stackoverflow.com/questions/78317989/is-it-possible-to-fine-tune-a-pretrained-word-embedding-model-like-vec2word,"Is it possible to fine-tune a pretrained word embedding model like vec2word? I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data. However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model. This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?",possible fine  tune pretraine word embed model like vec2word   m work semantic matching search engine system  see word embed use task  however  dataset limited small  not think training word embed model word2vec scratch yield good result   decide fine  tune pre  train model datum  however  can not find lot information  article documentation  fine  tuning  people even say s impossible fine  tune word embed model  raise question  fine  tune pre  train word embed model possible anyone try  currently   m stick look information  try train word embed model scratch approach ,point    go  to  way fine  tune word2vec type model  would suggest training model scratch  combine datum available datum similar domain  word2vec model fairly quick train would probably give good result  need static word  level embedding  would recommend consider contextualize embedding  example use sentence  transformer similar framework  wide selection already pre  train model choose  fine  tune type model specific datum rather easily  ton resource online  use case  embed document dense vector representation use abovementioned library  construct searchable index semantic space  order match query  embe query use model retrieve document high approximate inner product  often refer mips search  example library take look would faiss ,possible fine  tune pretraine word embed model like vec2word   m work semantic matching search engine system  see word embed use task  however  dataset limited small  not think training word embed model word2vec scratch yield good result   decide fine  tune pre  train model datum  however  can not find lot information  article documentation  fine  tuning  people even say s impossible fine  tune word embed model  raise question  fine  tune pre  train word embed model possible anyone try  currently   m stick look information  try train word embed model scratch approach  point    go  to  way fine  tune word2vec type model  would suggest training model scratch  combine datum available datum similar domain  word2vec model fairly quick train would probably give good result  need static word  level embedding  would recommend consider contextualize embedding  example use sentence  transformer similar framework  wide selection already pre  train model choose  fine  tune type model specific datum rather easily  ton resource online  use case  embed document dense vector representation use abovementioned library  construct searchable index semantic space  order match query  embe query use model retrieve document high approximate inner product  often refer mips search  example library take look would faiss ,Implementation Issues
R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens,"I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens. When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize(). However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option. Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()? A reprex: library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df%>%head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>%textrecipes:::get_tokens() Output: ""it"", ""be"", ""a"", ""day"", "","", ""Tuesday"", ""."", ""it"", ""be"", ""not"", ""Thursday"", ""!"" Desired output (Removal of ""!"", "","" and "".""): ""it"", ""be"", ""a"", ""day"", ""Tuesday"", ""it"", ""be"", ""not"", ""Thursday""","['r', 'nlp', 'spacy', 'tidymodels']",2,"You want to use the step_pos_filter() to filter the output of spacy by POS. It is a little annoying, because you have to specify the types to keep. Full list of tags found here https://github.com/explosion/spaCy/blob/master/spacy/glossary.py library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) pos <- c(""ADJ"", ""ADP"", ""ADV"", ""AUX"", ""CONJ"", ""CCONJ"", ""DET"", ""INTJ"", ""NOUN"", ""NUM"", ""PART"", ""PRON"", ""PROPN"", ""SCONJ"", ""SYM"", ""VERB"", ""X"", ""EOL"", ""SPACE"") lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df %>% head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_pos_filter(text, keep_tags = pos) %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>% textrecipes:::get_tokens() #> [[1]] #> [1] ""it"" ""be"" ""a"" ""day"" ""Tuesday"" ""it"" ""be"" #> [8] ""not"" ""Thursday""",2024-04-12 07:39:54,2024-04-12 15:00:44,90,https://stackoverflow.com/questions/78314842/r-tidymodels-textrecipes-tokenizing-with-spacyr-how-to-remove-punctuations-f,"R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens. When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize(). However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option. Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()? A reprex: library(tidyverse) library(tidymodels) library(textrecipes) library(spacyr) text = ""It was a day, Tuesday. It wasn't Thursday!"" df <- tibble(text) spacyr::spacy_initialize(entity = FALSE) lexicon_features_tokenized_lemmatised <- recipe(~ text, data = df%>%head(1)) %>% step_tokenize(text, engine = ""spacyr"") %>% step_lemma(text) %>% prep() %>% bake(new_data = NULL) lexicon_features_tokenized_lemmatised %>% pull(text) %>%textrecipes:::get_tokens() Output: ""it"", ""be"", ""a"", ""day"", "","", ""Tuesday"", ""."", ""it"", ""be"", ""not"", ""Thursday"", ""!"" Desired output (Removal of ""!"", "","" and "".""): ""it"", ""be"", ""a"", ""day"", ""Tuesday"", ""it"", ""be"", ""not"", ""Thursday""",r tidymodels textrecipe  tokenize spacyr  remove punctuation produce list token would like tokenize text use steptokenize spacyr engine proceeding lemmatisation use steplemma  follow  would like remove example punctuation list token  use default tokenizer   tokenizewords pass option list option steptokenize    however  understand steptokenize use spacyparse backend provide option  way remove eg  punctuations numeric tokens tokens produce lemmatisation use steplemma    reprex  library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  lexiconfeaturestokenizedlemmatised   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens   output                day          tuesday                       thursday       desire output  removal                                day     tuesday                 thursday ,want use stepposfilter   filter output spacy pos  little annoying  specify type keep  full list tag find   githubcom  explosion  spacy  blob  master  spacy  glossarypy library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  pos   c    adj     adp     adv     aux     conj     cconj     det     intj     noun     num     part     pron     propn     sconj     sym     verb     x     eol     space   lexiconfeaturestokenizedlemmatise   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      stepposfilter  text  keeptag  pos     steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens       1      1             day    tuesday           8       thursday ,r tidymodels textrecipe  tokenize spacyr  remove punctuation produce list token would like tokenize text use steptokenize spacyr engine proceeding lemmatisation use steplemma  follow  would like remove example punctuation list token  use default tokenizer   tokenizewords pass option list option steptokenize    however  understand steptokenize use spacyparse backend provide option  way remove eg  punctuations numeric tokens tokens produce lemmatisation use steplemma    reprex  library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  lexiconfeaturestokenizedlemmatised   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens   output                day          tuesday                       thursday       desire output  removal                                day     tuesday                 thursday  want use stepposfilter   filter output spacy pos  little annoying  specify type keep  full list tag find   githubcom  explosion  spacy  blob  master  spacy  glossarypy library  tidyverse  library  tidymodel  library  textrecipe  library  spacyr  text    day  tuesday  not thursday   df   tibble  text  spacyr   spacyinitialize  entity  false  pos   c    adj     adp     adv     aux     conj     cconj     det     intj     noun     num     part     pron     propn     sconj     sym     verb     x     eol     space   lexiconfeaturestokenizedlemmatise   recipe   text  datum  df    head  1      steptokenize  text  engine    spacyr      stepposfilter  text  keeptag  pos     steplemma  text     prep      bake  newdata  null  lexiconfeaturestokenizedlemmatise    pull  text     textrecipe    gettokens       1      1             day    tuesday           8       thursday ,Implementation Issues
"LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit","I am trying to ask GPT 4 to use Wikipedia for a prompt, using agents and tools via LangChain. The difficulty I'm running into is the book I've been using, Developing Apps with GPT-4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More , while published in 2023, already has code examples that are deprecated. For example, I am trying to do something similar to the code provided on page 114 of that book: from langchain.chat_models import ChatOpenAI from langchain.agents import load_tools, initialize_agent, AgentType llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) question = """"""What is the square root of the population of the capital of the Country where the Olympic Games were held in 2016?"""""" agent.run(question) I see much of this is deprecated (e.g., initialize_agent ), so I have looked around StackOverflow, GitHub, and the LangChain Python documents to come up with this: from langchain_openai import ChatOpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain.agents import ( load_tools, create_structured_chat_agent, AgentExecutor ) model = ChatOpenAI(model=""gpt-4"", temperature=0) tools = load_tools([""wikipedia""]) prompt = ChatPromptTemplate.from_template( """""" You are a research assistant, and your job is to retrieve information about movies and movie directors. Use the following tool: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question. You only need to give the number, no other information or explanation is necessary. Begin! Question: How many movies did the director of the {year} movie {name} direct before they made {name}? Thought: {agent_scratchpad} """""" ) agent = create_structured_chat_agent(model, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({""year"": ""1991"", ""name"": ""thelma and louise""}) I'm going to be running this through a loop of many movies, so I'd like it to only return one integer (in this case, 6). But it seems like I need to give it that full thought process prompt; I can't get it to run if I don't include {tools} , {tool_names} , and {agent_scratchpad} in the prompt ( per this GitHub post ). The frustrating thing is I eventually do get the correct answer, but note that it is throwing an error: ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: First, I need to find out who directed the movie ""Thelma and Louise"" in 1991. Action: wikipedia Action Input: {'query': 'Thelma and Louise'} Observation: ""Thelma & Louise"" is a 1991 American female buddy road film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip with unforeseen consequences. The film became a critical and commercial success, receiving six Academy Award nominations and winning one for Best Original Screenplay for Khouri. Scott was nominated for Best Director. Thought: Ridley Scott directed the movie ""Thelma and Louise"". Now I need to find out how many movies he directed before this one. Action: wikipedia Action Input: {'query': 'Ridley Scott filmography'} Observation: Ridley Scott is an English filmmaker. Following his commercial breakthrough with the science fiction horror film Alien (1979), his best known works are the neo-noir dystopian science fiction film Blade Runner (1982), historical drama Gladiator (2000), and science fiction film The Martian (2015). Scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. His films are also known for their strong female characters. Here is a list of his films before ""Thelma & Louise"": 1. The Duellists (1977) 2. Alien (1979) 3. Blade Runner (1982) 4. Legend (1985) 5. Someone to Watch Over Me (1987) 6. Black Rain (1989) Thought: Ridley Scott directed six movies before ""Thelma and Louise"". Final Answer: 6 This seems to be very common ( here , and here , and also here , and lastly here ). So, I do what it tells me ( see docs also ) and update my AgentExecutor to: agent_executor = AgentExecutor( agent=agent, tools=tools, handle_parsing_errors=True ) And that returns: {'year': '1991', 'name': 'thelma and louise', 'output': 'Agent stopped due to iteration limit or time limit.'} My question: How can I use LangChain to combine GPT 4 and Wikipedia to get an answer to a query, when all I want back is an integer?","['python', 'nlp', 'openai-api', 'langchain', 'large-language-model']",2,"Author of the book Developing Apps with GPT-4 and ChatGPT here, I already answered by mail, but just in case someone else stumbles upon this question... You can find updated code at https://github.com/malywut/gpt_examples . The updated code looks like this: from langchain_openai import ChatOpenAI from langchain.agents import load_tools, create_react_agent, AgentExecutor from langchain import hub llm = ChatOpenAI(model_name=""gpt-3.5-turbo"") tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = create_react_agent( tools=tools, llm=llm, prompt = hub.pull(""hwchase17/react""), ) question = ""..."" agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({""input"": question}) Hope this helps.",2024-04-10 20:52:34,2024-04-12 12:46:43,2638,https://stackoverflow.com/questions/78307073/langchain-agent-parsing-error-with-structured-chat-agent-and-wikipedia-tool-han,"LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit I am trying to ask GPT 4 to use Wikipedia for a prompt, using agents and tools via LangChain. The difficulty I'm running into is the book I've been using, Developing Apps with GPT-4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More , while published in 2023, already has code examples that are deprecated. For example, I am trying to do something similar to the code provided on page 114 of that book: from langchain.chat_models import ChatOpenAI from langchain.agents import load_tools, initialize_agent, AgentType llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) tools = load_tools([""wikipedia"", ""llm-math""], llm=llm) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) question = """"""What is the square root of the population of the capital of the Country where the Olympic Games were held in 2016?"""""" agent.run(question) I see much of this is deprecated (e.g., initialize_agent ), so I have looked around StackOverflow, GitHub, and the LangChain Python documents to come up with this: from langchain_openai import ChatOpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain.agents import ( load_tools, create_structured_chat_agent, AgentExecutor ) model = ChatOpenAI(model=""gpt-4"", temperature=0) tools = load_tools([""wikipedia""]) prompt = ChatPromptTemplate.from_template( """""" You are a research assistant, and your job is to retrieve information about movies and movie directors. Use the following tool: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question. You only need to give the number, no other information or explanation is necessary. Begin! Question: How many movies did the director of the {year} movie {name} direct before they made {name}? Thought: {agent_scratchpad} """""" ) agent = create_structured_chat_agent(model, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({""year"": ""1991"", ""name"": ""thelma and louise""}) I'm going to be running this through a loop of many movies, so I'd like it to only return one integer (in this case, 6). But it seems like I need to give it that full thought process prompt; I can't get it to run if I don't include {tools} , {tool_names} , and {agent_scratchpad} in the prompt ( per this GitHub post ). The frustrating thing is I eventually do get the correct answer, but note that it is throwing an error: ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: First, I need to find out who directed the movie ""Thelma and Louise"" in 1991. Action: wikipedia Action Input: {'query': 'Thelma and Louise'} Observation: ""Thelma & Louise"" is a 1991 American female buddy road film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip with unforeseen consequences. The film became a critical and commercial success, receiving six Academy Award nominations and winning one for Best Original Screenplay for Khouri. Scott was nominated for Best Director. Thought: Ridley Scott directed the movie ""Thelma and Louise"". Now I need to find out how many movies he directed before this one. Action: wikipedia Action Input: {'query': 'Ridley Scott filmography'} Observation: Ridley Scott is an English filmmaker. Following his commercial breakthrough with the science fiction horror film Alien (1979), his best known works are the neo-noir dystopian science fiction film Blade Runner (1982), historical drama Gladiator (2000), and science fiction film The Martian (2015). Scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. His films are also known for their strong female characters. Here is a list of his films before ""Thelma & Louise"": 1. The Duellists (1977) 2. Alien (1979) 3. Blade Runner (1982) 4. Legend (1985) 5. Someone to Watch Over Me (1987) 6. Black Rain (1989) Thought: Ridley Scott directed six movies before ""Thelma and Louise"". Final Answer: 6 This seems to be very common ( here , and here , and also here , and lastly here ). So, I do what it tells me ( see docs also ) and update my AgentExecutor to: agent_executor = AgentExecutor( agent=agent, tools=tools, handle_parsing_errors=True ) And that returns: {'year': '1991', 'name': 'thelma and louise', 'output': 'Agent stopped due to iteration limit or time limit.'} My question: How can I use LangChain to combine GPT 4 and Wikipedia to get an answer to a query, when all I want back is an integer?",langchain agent parse error structuredchatagent wikipedia tool  handleparsingerrors hit limit try ask gpt 4 use wikipedia prompt  use agent tool via langchain  difficulty  m run book  ve use  develop apps gpt4 chatgpt  build intelligent chatbots  content generators   publish 2023  already code example deprecate  example  try something similar code provide page 114 book  langchainchatmodel import chatopenai langchainagent import loadtools  initializeagent  agenttype llm  chatopenai  modelname  gpt35  turbo   temperature0  tool  loadtool     wikipedia     llm  math    llm  llm  agent  initializeagent  tool  llm  agent  agenttype  zeroshotreactdescription  verbose  true  question      square root population capital country olympic games hold 2016     agentrun  question  see much deprecate  eg  initializeagent   look around stackoverflow  github  langchain python document come  langchainopenai import chatopenai langchaincoreoutputparser import stroutputparser langchaincoreprompts import chatprompttemplate langchainagent import  loadtools  createstructuredchatagent  agentexecutor  model  chatopenai  model  gpt4   temperature0  tool  loadtool     wikipedia    prompt  chatprompttemplatefromtemplate      research assistant  job retrieve information movie movie director  use follow tool   tool  use follow format  question  input question must answer thought  always think action  action take  one   toolnames   action input  input action observation  result action   thought  action  action input  observation repeat n time  thought  know final answer final answer  final answer original input question  need give number  information explanation necessary  begin  question  many movie director  year  movie  name  direct make  name   thought   agentscratchpad       agent  createstructuredchatagent  model  tool  prompt  agentexecutor  agentexecutor  agent  agent  tool  tool  agentexecutorinvoke     year     1991     name     thelma louise     m go run loop many movie  would like return one integer  case  6   seem like need give full thought process prompt  can not get run not include  tool    toolnames    agentscratchpad  prompt  per github post   frustrating thing eventually get correct answer  note throw error  valueerror  output parsing error occur  order pass error back agent try  pass  handleparsingerror  true  agentexecutor  error  could parse llm output  first  need find direct movie   thelma louise  1991  action  wikipedia action input    query    thelma louise   observation    thelma  louise  1991 american female buddy road film direct ridley scott write callie khouri  star geena davis thelma susan sarandon louise  two friend embark road trip unforeseen consequence  film become critical commercial success  receive six academy award nomination win one best original screenplay khouri  scott nominate best director  thought  ridley scott direct movie   thelma louise   need find many movie direct one  action  wikipedia action input    query    ridley scott filmography   observation  ridley scott english filmmaker  follow commercial breakthrough science fiction horror film alien  1979   well know work neo  noir dystopian science fiction film blade runner  1982   historical drama gladiator  2000   science fiction film martian  2015   scott direct 25 film know atmospheric  highly concentrated visual style  film also know strong female character  list film   thelma  louise   1  duellist  1977  2  alien  1979  3  blade runner  1982  4  legend  1985  5  someone watch  1987  6  black rain  1989  thought  ridley scott direct six movie   thelma louise   final answer  6 seem common    also  lastly    tell  see doc also  update agentexecutor  agentexecutor  agentexecutor  agent  agent  tool  tool  handleparsingerror  true  return    year    1991    name    thelma louise    output    agent stop due iteration limit time limit    question  use langchain combine gpt 4 wikipedia get answer query  want back integer ,author book develop apps gpt4 chatgpt  already answer mail  case someone else stumble upon question  find update code   githubcom  malywut  gptexample  update code look like  langchainopenai import chatopenai langchainagent import loadtools  createreactagent  agentexecutor langchain import hub llm  chatopenai  modelname  gpt35  turbo   tool  loadtool     wikipedia     llm  math    llm  llm  agent  createreactagent  tool  tool  llm  llm  prompt  hubpull    hwchase17  react     question      agentexecutor  agentexecutor  agent  agent  tool  tool  verbose  true  agentexecutorinvoke     input   question   hope help ,langchain agent parse error structuredchatagent wikipedia tool  handleparsingerrors hit limit try ask gpt 4 use wikipedia prompt  use agent tool via langchain  difficulty  m run book  ve use  develop apps gpt4 chatgpt  build intelligent chatbots  content generators   publish 2023  already code example deprecate  example  try something similar code provide page 114 book  langchainchatmodel import chatopenai langchainagent import loadtools  initializeagent  agenttype llm  chatopenai  modelname  gpt35  turbo   temperature0  tool  loadtool     wikipedia     llm  math    llm  llm  agent  initializeagent  tool  llm  agent  agenttype  zeroshotreactdescription  verbose  true  question      square root population capital country olympic games hold 2016     agentrun  question  see much deprecate  eg  initializeagent   look around stackoverflow  github  langchain python document come  langchainopenai import chatopenai langchaincoreoutputparser import stroutputparser langchaincoreprompts import chatprompttemplate langchainagent import  loadtools  createstructuredchatagent  agentexecutor  model  chatopenai  model  gpt4   temperature0  tool  loadtool     wikipedia    prompt  chatprompttemplatefromtemplate      research assistant  job retrieve information movie movie director  use follow tool   tool  use follow format  question  input question must answer thought  always think action  action take  one   toolnames   action input  input action observation  result action   thought  action  action input  observation repeat n time  thought  know final answer final answer  final answer original input question  need give number  information explanation necessary  begin  question  many movie director  year  movie  name  direct make  name   thought   agentscratchpad       agent  createstructuredchatagent  model  tool  prompt  agentexecutor  agentexecutor  agent  agent  tool  tool  agentexecutorinvoke     year     1991     name     thelma louise     m go run loop many movie  would like return one integer  case  6   seem like need give full thought process prompt  can not get run not include  tool    toolnames    agentscratchpad  prompt  per github post   frustrating thing eventually get correct answer  note throw error  valueerror  output parsing error occur  order pass error back agent try  pass  handleparsingerror  true  agentexecutor  error  could parse llm output  first  need find direct movie   thelma louise  1991  action  wikipedia action input    query    thelma louise   observation    thelma  louise  1991 american female buddy road film direct ridley scott write callie khouri  star geena davis thelma susan sarandon louise  two friend embark road trip unforeseen consequence  film become critical commercial success  receive six academy award nomination win one best original screenplay khouri  scott nominate best director  thought  ridley scott direct movie   thelma louise   need find many movie direct one  action  wikipedia action input    query    ridley scott filmography   observation  ridley scott english filmmaker  follow commercial breakthrough science fiction horror film alien  1979   well know work neo  noir dystopian science fiction film blade runner  1982   historical drama gladiator  2000   science fiction film martian  2015   scott direct 25 film know atmospheric  highly concentrated visual style  film also know strong female character  list film   thelma  louise   1  duellist  1977  2  alien  1979  3  blade runner  1982  4  legend  1985  5  someone watch  1987  6  black rain  1989  thought  ridley scott direct six movie   thelma louise   final answer  6 seem common    also  lastly    tell  see doc also  update agentexecutor  agentexecutor  agentexecutor  agent  agent  tool  tool  handleparsingerror  true  return    year    1991    name    thelma louise    output    agent stop due iteration limit time limit    question  use langchain combine gpt 4 wikipedia get answer query  want back integer  author book develop apps gpt4 chatgpt  already answer mail  case someone else stumble upon question  find update code   githubcom  malywut  gptexample  update code look like  langchainopenai import chatopenai langchainagent import loadtools  createreactagent  agentexecutor langchain import hub llm  chatopenai  modelname  gpt35  turbo   tool  loadtool     wikipedia     llm  math    llm  llm  agent  createreactagent  tool  tool  llm  llm  prompt  hubpull    hwchase17  react     question      agentexecutor  agentexecutor  agent  agent  tool  tool  verbose  true  agentexecutorinvoke     input   question   hope help ,Basic Understanding
"Error when calling Hugging Face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)","I'm following the huggingface tutorial here and it's giving me a strange error. When I run the following code: from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding from torch.utils.data import DataLoader raw_datasets = load_dataset(""glue"", ""mrpc"") Here is what I see: Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00<00:00, 3.35MB/s] Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00<00:00, 6.63MB/s] Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.89s/it] Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 127.92it/s] Traceback (most recent call last): File ""/Users/ameenizhac/Downloads/transformers_playground.py"", line 5, in <module> raw_datasets = load_dataset(""glue"", ""mrpc"") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py"", line 1782, in load_dataset builder_instance.download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 872, in download_and_prepare self._download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 967, in _download_and_prepare self._prepare_split(split_generator, **prepare_split_kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 1709, in _prepare_split split_info = self.info.splits[split_generator.name] ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py"", line 530, in __getitem__ instructions = make_file_instructions( ^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 112, in make_file_instructions name2filenames = { ^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 113, in <dictcomp> info.name: filenames_for_dataset_split( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 70, in filenames_for_dataset_split prefix = filename_prefix_for_split(dataset_name, split) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 54, in filename_prefix_for_split if os.path.basename(name) != name: ^^^^^^^^^^^^^^^^^^^^^^ File ""<frozen posixpath>"", line 142, in basename TypeError: expected str, bytes or os.PathLike object, not NoneType I don't know where to start because I don't understand where the error is coming from.","['python', 'nlp', 'huggingface', 'huggingface-datasets']",1,"I tried on my PC and on Google Colab. The strange thing is that on Colab it works, on my PC it does not. Anyway, a possible workaround is the following: raw_datasets = load_dataset(""SetFit/mrpc"") If you print it, you will see that the dataset is the same, it just has a different name: DatasetDict({ train: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 3668 }) test: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 1725 }) validation: Dataset({ features: ['text1', 'text2', 'label', 'idx', 'label_text'], num_rows: 408 }) })",2024-04-08 19:21:13,2024-04-11 13:25:21,456,https://stackoverflow.com/questions/78294720/error-when-calling-hugging-face-load-datasetglue-mrpc,"Error when calling Hugging Face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;) I'm following the huggingface tutorial here and it's giving me a strange error. When I run the following code: from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding from torch.utils.data import DataLoader raw_datasets = load_dataset(""glue"", ""mrpc"") Here is what I see: Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00<00:00, 3.35MB/s] Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00<00:00, 6.63MB/s] Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.89s/it] Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 127.92it/s] Traceback (most recent call last): File ""/Users/ameenizhac/Downloads/transformers_playground.py"", line 5, in <module> raw_datasets = load_dataset(""glue"", ""mrpc"") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py"", line 1782, in load_dataset builder_instance.download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 872, in download_and_prepare self._download_and_prepare( File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 967, in _download_and_prepare self._prepare_split(split_generator, **prepare_split_kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py"", line 1709, in _prepare_split split_info = self.info.splits[split_generator.name] ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py"", line 530, in __getitem__ instructions = make_file_instructions( ^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 112, in make_file_instructions name2filenames = { ^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py"", line 113, in <dictcomp> info.name: filenames_for_dataset_split( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 70, in filenames_for_dataset_split prefix = filename_prefix_for_split(dataset_name, split) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py"", line 54, in filename_prefix_for_split if os.path.basename(name) != name: ^^^^^^^^^^^^^^^^^^^^^^ File ""<frozen posixpath>"", line 142, in basename TypeError: expected str, bytes or os.PathLike object, not NoneType I don't know where to start because I don't understand where the error is coming from.",error call hugging face loaddataset   quot  glue  quot    quot  mrpc  quot    m follow huggingface tutorial s give strange error  run follow code  dataset import loaddataset transformer import autotokenizer  datacollatorwithpadde torchutilsdata import dataloader rawdataset  loaddataset    glue     mrpc   see  download datum  100                                                                                                                                           151k151k  0000  0000  335mb  s  download datum  100                                                                                                                                         111k111k  0000  0000  663mb  s  download datum file  100                                                                                                                                          33  0032  0000  1089s  it  extract data file  100                                                                                                                                          33  0000  0000  12792it  s  traceback  recent call last   file   users  ameenizhac  download  transformersplaygroundpy   line 5   module  rawdataset  loaddataset    glue     mrpc    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  loadpy   line 1782  loaddataset builderinstancedownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 872  downloadandprepare selfdownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 967   downloadandprepare selfpreparesplit  splitgenerator    preparesplitkwargs  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 1709   preparesplit splitinfo  selfinfosplit  splitgeneratorname   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  splitspy   line 530    getitem   instruction  makefileinstructions   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 112  makefileinstructions name2filename    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 113   dictcomp  infoname  filenamesfordatasetsplit   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 70  filenamesfordatasetsplit prefix  filenameprefixforsplit  datasetname  split   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 54  filenameprefixforsplit ospathbasename  name    name   file    frozen posixpath    line 142  basename typeerror  expect str  byte os  pathlike object  nonetype not know start not understand error come ,try pc google colab  strange thing colab work  pc  anyway  possible workaround following  rawdataset  loaddataset    setfit  mrpc   print  see dataset  different name  datasetdict   train  dataset   feature    text1    text2    label    idx    labeltext    numrows  3668   test  dataset   feature    text1    text2    label    idx    labeltext    numrows  1725   validation  dataset   feature    text1    text2    label    idx    labeltext    numrows  408    ,error call hugging face loaddataset   quot  glue  quot    quot  mrpc  quot    m follow huggingface tutorial s give strange error  run follow code  dataset import loaddataset transformer import autotokenizer  datacollatorwithpadde torchutilsdata import dataloader rawdataset  loaddataset    glue     mrpc   see  download datum  100                                                                                                                                           151k151k  0000  0000  335mb  s  download datum  100                                                                                                                                         111k111k  0000  0000  663mb  s  download datum file  100                                                                                                                                          33  0032  0000  1089s  it  extract data file  100                                                                                                                                          33  0000  0000  12792it  s  traceback  recent call last   file   users  ameenizhac  download  transformersplaygroundpy   line 5   module  rawdataset  loaddataset    glue     mrpc    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  loadpy   line 1782  loaddataset builderinstancedownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 872  downloadandprepare selfdownloadandprepare  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 967   downloadandprepare selfpreparesplit  splitgenerator    preparesplitkwargs  file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  builderpy   line 1709   preparesplit splitinfo  selfinfosplit  splitgeneratorname   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  splitspy   line 530    getitem   instruction  makefileinstructions   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 112  makefileinstructions name2filename    file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  arrowreaderpy   line 113   dictcomp  infoname  filenamesfordatasetsplit   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 70  filenamesfordatasetsplit prefix  filenameprefixforsplit  datasetname  split   file   library  frameworks  pythonframework  versions311  lib  python311  site  package  dataset  namingpy   line 54  filenameprefixforsplit ospathbasename  name    name   file    frozen posixpath    line 142  basename typeerror  expect str  byte os  pathlike object  nonetype not know start not understand error come  try pc google colab  strange thing colab work  pc  anyway  possible workaround following  rawdataset  loaddataset    setfit  mrpc   print  see dataset  different name  datasetdict   train  dataset   feature    text1    text2    label    idx    labeltext    numrows  3668   test  dataset   feature    text1    text2    label    idx    labeltext    numrows  1725   validation  dataset   feature    text1    text2    label    idx    labeltext    numrows  408    ,Library/Tool-Based Queries
What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax?,"I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?","['machine-learning', 'nlp', 'word2vec', 'hierarchical', 'softmax']",1,"While there are many slightly-different ways to think about it, it may help to consider the values in the (trained, frozen) neural network as associated with edges moreso than vertexes (nodes). The network ""projection weights"" leading from an (abstract) one-hot encoding of each known word into the network are essentially the actual per-word word-vectors. Assuming a common case of 300d vectors, the 300 edges from the single-word node to the inner nodes are that words. Then, there's another set of edge-weights from the internal activation to the ""output"" layer, which is offering the network's training goal of in-context word-projection. In the (more common & usual default) negative-sampling approach, each output node corresponds to a single predictable word. That's a very easy output-shape to visualize. And the value of negative-sampling is that you only check the activations of the desired word, and n more randomly chosen negative words, to perform your training updates. That's way less calculation than if you checked the output values at all V (size of vocabulary) output nodes, and still works pretty well, and doesn't get more expensive with larger vocabularies (unless you choose for other reasons to also increase your choice of n negative samples). In hierarchical softmax, the interpretation of the output nodes is more complicated. Rather than the activation at a single node indicating the prediction of a single word, a (varying) set of nodes must have the right on/off activations to communicate the variable-length huffman-code of a word. So in HS, to backprop the network more towards predicting your desired ""positive"" word (from one input context), the algorithm considers just those nodes involved in that words nique coding – a smaller set of nodes for the most-ommon words, but a larger set of nodes for rarer words – and nudges each of them more towards the pattern that predicts the desired word. Again, you get the sparse training efficiency of updating only a tiny subset, far smaller than all V nodes, each training-step. But, the cost will vary based on the target word, and grow with the log of V as vocabulary-size grows. Further, as the original/naive assignment of codes is based strictly on word-frequency, quite-dissimilar words may have very-similar codings, perhaps causing more word-to-word interference. (There were hints in the original word2vec.c release of refining the HS word-codings over time to ensure similar words share similar Huffman codings, but I've seen litle followup on that idea, perhaps because of the dominance of negative-sampling.) So, in an HS network, the weights from the inner-activations, to the output nodes, are tuned to indicate, by Huffman code, which word is the preferred prediction from a context. In the word2vec implementation I'm most familiar with, Python Gensim, these ""hidden to output"" weights are not even randomly initialized at the beginning, instead left as 0.0 – and I think this was directly copied from the initialization of Google's word2vec.c release. But, as soon as training begins, the explict random initialization of those ""input weights"" (initial random input word-vectors) means those weights are immediately perturbed in a way that at 1st is nearly all random but becomes more helpful over the SGD training. So: those inner weights start 0.0 but quickly start reflecting the influence of the (initially-random) word vectors and training examples they're usually not harvested from the network after training, like the final word-vectors are – but would be kept around if you wanted to continue training later, and perhaps provide a running start to future training runs (In the negative-sampling case, some research suggested those hidden-to-output weights could also be interpreted as same embedding_size per-word vectors, with some usefulness: see paper by Mitra et al at Microsoft about ""Dual Word Embeddings"". But given the varying-length codings of output words in the HS case, extracting/interpretating those output-weights would be trickier.) In the implementation I'm most familiar with, the It may help to think instead of the shallow neural network's ""projection layer"" (effectively the word-vectors themselves, as each virtual ""single node"" 1-hot word has its embedding_size out-weights) and ""hidden layer",2024-04-06 18:15:41,2024-04-11 21:30:15,148,https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax,"What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax? I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?",  39  inside inner vertex word2vec hierarchical softmax  question hierarchical softmax  actually  quite understand store inner vertex  leaf vertex   clearly understand main idea algorithm  step calculate dot product input word embed word embed inner vertice  vector inside inner vertex  randomly initialize vector size equal embeddingsize coordinate change due backpropagation step stop ,many slightly  different way think  may help consider value  train  frozen  neural network associate edge moreso vertex  node   network   projection weight  lead  abstract  one  hot encoding know word network essentially actual per  word word  vector  assume common case 300d vector  300 edge single  word node inner node word   be another set edge  weight internal activation   output  layer  offer network s training goal in  context word  projection   common  usual default  negative  sample approach  output node correspond single predictable word  s easy output  shape visualize  value negative  sample check activation desire word  n randomly choose negative word  perform training update  s way less calculation check output value v  size vocabulary  output node  still work pretty well  not get expensive large vocabulary  unless choose reason also increase choice n negative sample   hierarchical softmax  interpretation output node complicated  rather activation single node indicate prediction single word   vary  set node must right on  off activation communicate variable  length huffman  code word  hs  backprop network towards predict desire   positive  word  one input context   algorithm consider node involve word nique coding  small set node most  ommon word  large set node rare word  nudge towards pattern predict desire word   get sparse training efficiency update tiny subset  far small v node  training  step   cost vary base target word  grow log v vocabulary  size grow   original  naive assignment code base strictly word  frequency  quite  dissimilar word may very  similar coding  perhaps cause word  to  word interference   hint original word2vecc release refine hs word  coding time ensure similar word share similar huffman coding   ve see litle followup idea  perhaps dominance negative  sampling    hs network  weight inner  activation  output node  tune indicate  huffman code  word prefer prediction context  word2vec implementation  m familiar  python gensim    hidden output  weight even randomly initialize beginning  instead leave 00  think directly copy initialization google s word2vecc release   soon training begin  explict random initialization   input weight   initial random input word  vector  mean weight immediately perturb way 1st nearly random become helpful sgd training   inner weight start 00 quickly start reflect influence  initially  random  word vector training example be usually harvest network training  like final word  vector  would keep around want continue training later  perhaps provide run start future training run  negative  sampling case  research suggest hide  to  output weight could also interpret embeddingsize per  word vector  usefulness  see paper mitra et al microsoft   dual word embeddings   give vary  length coding output word hs case  extracting  interpretate output  weight would tricky   implementation  m familiar  may help think instead shallow neural network s   projection layer   effectively word  vector  virtual   single node  1  hot word embeddingsize out  weight    hidden layer,  39  inside inner vertex word2vec hierarchical softmax  question hierarchical softmax  actually  quite understand store inner vertex  leaf vertex   clearly understand main idea algorithm  step calculate dot product input word embed word embed inner vertice  vector inside inner vertex  randomly initialize vector size equal embeddingsize coordinate change due backpropagation step stop  many slightly  different way think  may help consider value  train  frozen  neural network associate edge moreso vertex  node   network   projection weight  lead  abstract  one  hot encoding know word network essentially actual per  word word  vector  assume common case 300d vector  300 edge single  word node inner node word   be another set edge  weight internal activation   output  layer  offer network s training goal in  context word  projection   common  usual default  negative  sample approach  output node correspond single predictable word  s easy output  shape visualize  value negative  sample check activation desire word  n randomly choose negative word  perform training update  s way less calculation check output value v  size vocabulary  output node  still work pretty well  not get expensive large vocabulary  unless choose reason also increase choice n negative sample   hierarchical softmax  interpretation output node complicated  rather activation single node indicate prediction single word   vary  set node must right on  off activation communicate variable  length huffman  code word  hs  backprop network towards predict desire   positive  word  one input context   algorithm consider node involve word nique coding  small set node most  ommon word  large set node rare word  nudge towards pattern predict desire word   get sparse training efficiency update tiny subset  far small v node  training  step   cost vary base target word  grow log v vocabulary  size grow   original  naive assignment code base strictly word  frequency  quite  dissimilar word may very  similar coding  perhaps cause word  to  word interference   hint original word2vecc release refine hs word  coding time ensure similar word share similar huffman coding   ve see litle followup idea  perhaps dominance negative  sampling    hs network  weight inner  activation  output node  tune indicate  huffman code  word prefer prediction context  word2vec implementation  m familiar  python gensim    hidden output  weight even randomly initialize beginning  instead leave 00  think directly copy initialization google s word2vecc release   soon training begin  explict random initialization   input weight   initial random input word  vector  mean weight immediately perturb way 1st nearly random become helpful sgd training   inner weight start 00 quickly start reflect influence  initially  random  word vector training example be usually harvest network training  like final word  vector  would keep around want continue training later  perhaps provide run start future training run  negative  sampling case  research suggest hide  to  output weight could also interpret embeddingsize per  word vector  usefulness  see paper mitra et al microsoft   dual word embeddings   give vary  length coding output word hs case  extracting  interpretate output  weight would tricky   implementation  m familiar  may help think instead shallow neural network s   projection layer   effectively word  vector  virtual   single node  1  hot word embeddingsize out  weight    hidden layer,Library/Tool-Based Queries
Extracting dates from a sentence in spaCy,"I have a string like so: ""The dates are from 30 June 2019 to 1 January 2022 inclusive"" I want to extract the dates from this string using spaCy. Here is my function so far: def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": dates_with_year.append(ent.text) return dates_with_year This returns the following output: ['30 June 2019 to 1 January 2022'] However, I want output like: ['30 June 2019', '1 January 2022']","['python', 'regex', 'nlp', 'spacy', 'named-entity-recognition']",1,"The issue is that ""to"" is considered part of the date. So when you do for ent in doc.ents , your loop only has one iteration, as ""30 June 2019 to 1 January 2022"" is considered one entity. As you don't want this behaviour, you can amend your function to split on ""to"" : def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": for ent_txt in ent.text.split(""to""): dates_with_year.append(ent_txt.strip()) return dates_with_year This will correctly handle dates like these, as well as single dates, and strings with multiple dates: txt = """""" The dates are from 30 June 2019 to 1 January 2022 inclusive. And oddly also 5 January 2024. And exclude 21 July 2019 until 23 July 2019. """""" extract_dates_with_year(txt) # Output: [ '30 June 2019', '1 January 2022', '5 January 2024', '21 July 2019', '23 July 2019' ]",2024-04-06 17:12:39,2024-04-06 19:15:08,78,https://stackoverflow.com/questions/78285241/extracting-dates-from-a-sentence-in-spacy,"Extracting dates from a sentence in spaCy I have a string like so: ""The dates are from 30 June 2019 to 1 January 2022 inclusive"" I want to extract the dates from this string using spaCy. Here is my function so far: def extract_dates_with_year(text): doc = nlp(text) dates_with_year = [] for ent in doc.ents: if ent.label_ == ""DATE"": dates_with_year.append(ent.text) return dates_with_year This returns the following output: ['30 June 2019 to 1 January 2022'] However, I want output like: ['30 June 2019', '1 January 2022']",extract date sentence spacy string like    date 30 june 2019 1 january 2022 inclusive  want extract date string use spacy  function far  def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   dateswithyearappend  enttext  return dateswithyear return follow output    30 june 2019 1 january 2022   however  want output like    30 june 2019    1 january 2022  ,issue    consider part date  ent docent  loop one iteration    30 june 2019 1 january 2022  consider one entity  not want behaviour  amend function split     def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   enttxt enttextsplit       dateswithyearappend  enttxtstrip    return dateswithyear correctly handle date like  well single date  string multiple date  txt      date 30 june 2019 1 january 2022 inclusive  oddly also 5 january 2024  exclude 21 july 2019 23 july 2019      extractdateswithyear  txt   output    30 june 2019    1 january 2022    5 january 2024    21 july 2019    23 july 2019  ,extract date sentence spacy string like    date 30 june 2019 1 january 2022 inclusive  want extract date string use spacy  function far  def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   dateswithyearappend  enttext  return dateswithyear return follow output    30 june 2019 1 january 2022   however  want output like    30 june 2019    1 january 2022   issue    consider part date  ent docent  loop one iteration    30 june 2019 1 january 2022  consider one entity  not want behaviour  amend function split     def extractdateswithyear  text   doc  nlp  text  dateswithyear    ent docent  entlabel      date   enttxt enttextsplit       dateswithyearappend  enttxtstrip    return dateswithyear correctly handle date like  well single date  string multiple date  txt      date 30 june 2019 1 january 2022 inclusive  oddly also 5 january 2024  exclude 21 july 2019 23 july 2019      extractdateswithyear  txt   output    30 june 2019    1 january 2022    5 january 2024    21 july 2019    23 july 2019  ,Implementation Issues
What is the best function/stage to use tokenizer in Pytorch&#39;s data processing?,"I am subclassing torch.utils.data.Dataset and writing a collate function to be passed to Dataloader's dataset and collate_fn argument respectively. Between Dataset's __getitem__ or collate_fn I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?) collate_fn seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.","['pytorch', 'nlp']",2,"It is more efficient applying it on the collate_fn as the tokenizer accepts a list of texts (the batch), specially if the sequences need to be padded at a fixed length, you would be processing batch-wise",2024-04-06 15:14:45,2024-04-14 18:37:04,1650,https://stackoverflow.com/questions/78284866/what-is-the-best-function-stage-to-use-tokenizer-in-pytorchs-data-processing,"What is the best function/stage to use tokenizer in Pytorch&#39;s data processing? I am subclassing torch.utils.data.Dataset and writing a collate function to be passed to Dataloader's dataset and collate_fn argument respectively. Between Dataset's __getitem__ or collate_fn I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?) collate_fn seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.",good function  stage use tokenizer pytorch   39  datum processing  subclasse torchutilsdata  dataset write collate function pass dataloader s dataset collatefn argument respectively  dataset s   getitem   collatefn wondering would well function use tokenizer  huggingface s fasttokenizer   another well option two not know   collatefn seem good option since use tokenizer tokenize add padding full batch together  also help tokenization speed  sure cause problem  future teammate start use  customize code ,efficient apply collatefn tokenizer accept list text  batch   specially sequence need pad fix length  would process batch  wise,good function  stage use tokenizer pytorch   39  datum processing  subclasse torchutilsdata  dataset write collate function pass dataloader s dataset collatefn argument respectively  dataset s   getitem   collatefn wondering would well function use tokenizer  huggingface s fasttokenizer   another well option two not know   collatefn seem good option since use tokenizer tokenize add padding full batch together  also help tokenization speed  sure cause problem  future teammate start use  customize code  efficient apply collatefn tokenizer accept list text  batch   specially sequence need pad fix length  would process batch  wise,Task-Specific Queries
Python Spacy Pattern- How to tag a word based on another word?,"I'm trying to write a pattern that would tag the whole word as unit based on one substring. Here's example: terms = [{'ent': ""UNIT"", 'patterns':[ [{'lemma':'liter'}]]}] text = ""There were 46 kiloliters of juice available"" I wanna tag 'kiloliters' as Unit based on this pattern. I tried using 'lemma"" but it won't work in this case.","['python', 'nlp', 'spacy']",1,"You haven't said which model you're using so I'll use en_web_core_sm : import spacy from spacy.matcher import Matcher nlp = spacy.load(""en_core_web_sm"") matcher = Matcher(nlp.vocab) doc = nlp(""There were 46 kiloliters of juice available"") The first thing is that none of these have an ent_type of UNIT : for tok in doc: print(f""'{tok}': ent_type: '{tok.ent_type_}', lemma: '{tok.lemma_}'"") 'There': ent_type: '', lemma: 'there' 'were': ent_type: '', lemma: 'be' '46': ent_type: 'CARDINAL', lemma: '46' 'kiloliters': ent_type: '', lemma: 'kiloliter' 'of': ent_type: '', lemma: 'of' 'juice': ent_type: '', lemma: 'juice' 'available': ent_type: '', lemma: 'available' Also, as you can see, the lemma of kiloliters is kiloliter . This is a bit annoying as you don't want to have to specify milliliters, liters etc. separately. One alternative is to look for a CARDINAL token (which also includes words e.g. ""two liters"" ) followed a regex: doc = nlp("""""" There were 46 kiloliters of juice available. I could not drink more than two liters a day. I would only give a child 500 milliliters. """""" ) pattern = [{'ENT_TYPE': 'CARDINAL'}, {""TEXT"": {""REGEX"": ""^.*(liter)s?$""}}] matcher.add(""unit"", [pattern]) matches = matcher(doc, as_spans=True) for span in matches: print(span[-1].text) Output: kiloliters liters milliliters",2024-04-03 08:07:09,2024-04-03 08:54:15,47,https://stackoverflow.com/questions/78266192/python-spacy-pattern-how-to-tag-a-word-based-on-another-word,"Python Spacy Pattern- How to tag a word based on another word? I'm trying to write a pattern that would tag the whole word as unit based on one substring. Here's example: terms = [{'ent': ""UNIT"", 'patterns':[ [{'lemma':'liter'}]]}] text = ""There were 46 kiloliters of juice available"" I wanna tag 'kiloliters' as Unit based on this pattern. I tried using 'lemma"" but it won't work in this case.",python spacy pattern tag word base another word   m try write pattern would tag whole word unit base one substring  s example  term     ent     unit    pattern       lemma    liter       text    46 kiloliter juice available  wan na tag  kiloliter  unit base pattern  try use  lemma  will not work case ,not say model be use will use enwebcoresm  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   matcher  matcher  nlpvocab  doc  nlp    46 kiloliter juice available   first thing none enttype unit  tok doc  print  f    tok    enttype    tokenttype     lemma    toklemma       there   enttype     lemma   there   be   enttype     lemma   be   46   enttype   cardinal   lemma   46   kiloliter   enttype     lemma   kiloliter   of   enttype     lemma   of   juice   enttype     lemma   juice   available   enttype     lemma   available  also  see  lemma kiloliter kiloliter  bit annoying not want specify milliliter  liter etc  separately  one alternative look cardinal token  also include word eg    two liter   follow regex  doc  nlp      46 kiloliter juice available  could drink two liter day  would give child 500 milliliter       pattern     enttype    cardinal       text      regex         liter        matcheradd    unit    pattern   match  matcher  doc  asspan  true  span match  print  span  1  text  output  kiloliter liter milliliter,python spacy pattern tag word base another word   m try write pattern would tag whole word unit base one substring  s example  term     ent     unit    pattern       lemma    liter       text    46 kiloliter juice available  wan na tag  kiloliter  unit base pattern  try use  lemma  will not work case  not say model be use will use enwebcoresm  import spacy spacymatcher import matcher nlp  spacyload    encorewebsm   matcher  matcher  nlpvocab  doc  nlp    46 kiloliter juice available   first thing none enttype unit  tok doc  print  f    tok    enttype    tokenttype     lemma    toklemma       there   enttype     lemma   there   be   enttype     lemma   be   46   enttype   cardinal   lemma   46   kiloliter   enttype     lemma   kiloliter   of   enttype     lemma   of   juice   enttype     lemma   juice   available   enttype     lemma   available  also  see  lemma kiloliter kiloliter  bit annoying not want specify milliliter  liter etc  separately  one alternative look cardinal token  also include word eg    two liter   follow regex  doc  nlp      46 kiloliter juice available  could drink two liter day  would give child 500 milliliter       pattern     enttype    cardinal       text      regex         liter        matcheradd    unit    pattern   match  matcher  doc  asspan  true  span match  print  span  1  text  output  kiloliter liter milliliter,Library/Tool-Based Queries
Gensim&#39;s Doc2Vec with documents in multiple languages,"I'm building a content based recommender system using similarities on vector representations of documents. My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents. Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9. Why is this happening? import pandas as pd from pathlib import Path from gensim.models.doc2vec import TaggedDocument, Doc2Vec from gensim.utils import simple_preprocess import numpy as np from tqdm import tqdm from sklearn.metrics.pairwise import cosine_similarity data_folder = Path.cwd().parent / 'data' transformed_folder = data_folder / 'transformed' BOOKS_PATH = Path(transformed_folder, ""books_final.csv"") import seaborn as sns book_df = pd.read_csv(BOOKS_PATH) languages=book_df.language.unique() training_corpus = np.empty(len(book_df), dtype=object) for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))): training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)]) model=Doc2Vec(vector_size=50, min_count=2, epochs=40) model.build_vocab(training_corpus) #train the model model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs) #get the keys of the trained model model_keys=list(model.dv.key_to_index.keys()) matrix = model.dv.vectors similarity_matrix = cosine_similarity(matrix) indices_for_language = {} for language, group_df in book_df.groupby('language'): indices_for_language[language] = group_df.index.to_list() sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']] #make a heatmap sns.heatmap(sim_japanese_italian) Heatmap of similarities between Italian and Japanese documents","['python', 'nlp', 'gensim', 'recommendation-engine', 'doc2vec']",1,"You corpus is a bit small compared to published results for the Doc2Vec (""Paragraph Vectors"") algorithm, which is usually demonstrated on set of tens-of-thousands to millions of documents. In particular, if you only have (4742-4655=) 93 non-English description texts, and those are split over 16 non-English languages, it seems unlikely to me that any of the words from those other languages will have enough in-context usage examples to either (1) survive the min_count cutoff (which generally shouldn't be as low as 2!); or (2) obtain meaningful/generalizable implications in the model. That is: all those very-rare words will have fairly idiosyncratic/arbitrary meanings, and description texts filled with all rare/weakly-understood words will tend to have weak, almost random doc-vectors. So, spurious similarities aren't that much of a surprise. If those ""non-English"" description texts do have a few English-looking or omnilingual tokens in them – stray words or false cognates or punctuation or glitches – those might also dominate the description -to- description comparison. You should check, on your suspiciously-too-similar pairs, how many of the words in each document are actually in the trained model – keeping in mind words not in the trained model are essentially ignored in training or post-rtraining inference. (A document of 100 words, in a language where there are only a few other 100-word documents, might in fact be dominated by single-appearance words, that don't even meet the min_count=2 , and so when slimmed to the surviving words be short docs filled with weakly-understood words.) If your similarity scores between English documents are generally sensible, then the model is doing what it can, where it has enough data. You really want this family of algorithms – word2vec, Doc2Vec , FastText, etc – to have many, varied, representative examples of any word's usage in sensible contrasting contexts for it to be able to do any meaningful later modeling of those words (and the documents that contain them). So: if you can't manage to use a min_count of at least the usual default ( 5 ), or ideally even higher, you likely don't have sufficient data for these algorithms to show their strengths. Extending your corpus – even with other texts not of direct interest, but of sufficient variety to better represent the languages used in your texts of interest – might help. (That is: even if your universe-of-recommendations is <5000 descriptions , mixing another 200,000 similar short blurbs from other sources, so that words and langguages of interest have enough coverage, could be worthwhile.)",2024-04-02 15:47:30,2024-04-12 18:55:31,141,https://stackoverflow.com/questions/78262529/gensims-doc2vec-with-documents-in-multiple-languages,"Gensim&#39;s Doc2Vec with documents in multiple languages I'm building a content based recommender system using similarities on vector representations of documents. My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents. Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9. Why is this happening? import pandas as pd from pathlib import Path from gensim.models.doc2vec import TaggedDocument, Doc2Vec from gensim.utils import simple_preprocess import numpy as np from tqdm import tqdm from sklearn.metrics.pairwise import cosine_similarity data_folder = Path.cwd().parent / 'data' transformed_folder = data_folder / 'transformed' BOOKS_PATH = Path(transformed_folder, ""books_final.csv"") import seaborn as sns book_df = pd.read_csv(BOOKS_PATH) languages=book_df.language.unique() training_corpus = np.empty(len(book_df), dtype=object) for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))): training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)]) model=Doc2Vec(vector_size=50, min_count=2, epochs=40) model.build_vocab(training_corpus) #train the model model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs) #get the keys of the trained model model_keys=list(model.dv.key_to_index.keys()) matrix = model.dv.vectors similarity_matrix = cosine_similarity(matrix) indices_for_language = {} for language, group_df in book_df.groupby('language'): indices_for_language[language] = group_df.index.to_list() sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']] #make a heatmap sns.heatmap(sim_japanese_italian) Heatmap of similarities between Italian and Japanese documents",gensim   39  doc2vec document multiple language  m build content base recommender system use similarity vector representation document  document description book  english  language  use gensim s doc2vec building vector representation document  base understanding model  document different language low similarity  word even overlap  s happen  document different language dataset high similarity 09  happen  import panda pd pathlib import path gensimmodelsdoc2vec import taggeddocument  doc2vec gensimutil import simplepreprocess import numpy np tqdm import tqdm sklearnmetricspairwise import cosinesimilarity datafolder  pathcwd   parent   datum  transformedfolder  datafold   transform  bookspath  path  transformedfolder    booksfinalcsv   import seaborn sns bookdf  pdreadcsv  bookspath  language  bookdflanguageunique   trainingcorpus  npempty  len  bookdf   dtype  object    isbn  desc  tqdm  enumerate  zip  bookdf   isbn    bookdf   description       trainingcorpus    taggeddocument  simplepreprocess  desc    str     model  doc2vec  vectorsize50  mincount2  epochs40  modelbuildvocab  trainingcorpus   train model modeltrain  trainingcorpus  totalexample  modelcorpuscount  epoch  modelepoch   get key train model modelkey  list  modeldvkeytoindexkeys    matrix  modeldvvectors similaritymatrix  cosinesimilarity  matrix  indicesforlanguage    language  groupdf bookdfgroupby   language    indicesforlanguage  language   groupdfindextolist   simjapaneseitalian  similaritymatrix  indicesforlanguage   japanese       indicesforlanguage   italian     make heatmap snsheatmap  simjapaneseitalian  heatmap similaritie italian japanese document,corpus bit small compare publish result doc2vec    paragraph vectors   algorithm  usually demonstrate set ten  of  thousand million document  particular   4742  4655  93 non  english description text  split 16 non  english language  seem unlikely word language enough in  context usage example either  1  survive mincount cutoff  generally not low 2     2  obtain meaningful  generalizable implication model   very  rare word fairly idiosyncratic  arbitrary meaning  description text fill rare  weakly  understand word tend weak  almost random doc  vector   spurious similarity not much surprise    non  english  description text english  looking omnilingual token  stray word false cognate punctuation glitch  might also dominate description to description comparison  check  suspiciously  too  similar pair  many word document actually train model  keep mind word train model essentially ignore training post  rtraining inference   document 100 word  language 100  word document  might fact dominate single  appearance word  not even meet mincount2  slimme surviving word short doc fill weakly  understand word   similarity score english document generally sensible  model  enough datum  really want family algorithm  word2vec  doc2vec  fasttext  etc  many  varied  representative example word s usage sensible contrasting context able meaningful later modeling word  document contain    can not manage use mincount least usual default  5   ideally even higher  likely not sufficient data algorithm show strength  extend corpus  even text direct interest  sufficient variety well represent language use text interest  might help    even universe  of  recommendation  5000 description  mix another 200000 similar short blurb source  word langguage interest enough coverage  could worthwhile  ,gensim   39  doc2vec document multiple language  m build content base recommender system use similarity vector representation document  document description book  english  language  use gensim s doc2vec building vector representation document  base understanding model  document different language low similarity  word even overlap  s happen  document different language dataset high similarity 09  happen  import panda pd pathlib import path gensimmodelsdoc2vec import taggeddocument  doc2vec gensimutil import simplepreprocess import numpy np tqdm import tqdm sklearnmetricspairwise import cosinesimilarity datafolder  pathcwd   parent   datum  transformedfolder  datafold   transform  bookspath  path  transformedfolder    booksfinalcsv   import seaborn sns bookdf  pdreadcsv  bookspath  language  bookdflanguageunique   trainingcorpus  npempty  len  bookdf   dtype  object    isbn  desc  tqdm  enumerate  zip  bookdf   isbn    bookdf   description       trainingcorpus    taggeddocument  simplepreprocess  desc    str     model  doc2vec  vectorsize50  mincount2  epochs40  modelbuildvocab  trainingcorpus   train model modeltrain  trainingcorpus  totalexample  modelcorpuscount  epoch  modelepoch   get key train model modelkey  list  modeldvkeytoindexkeys    matrix  modeldvvectors similaritymatrix  cosinesimilarity  matrix  indicesforlanguage    language  groupdf bookdfgroupby   language    indicesforlanguage  language   groupdfindextolist   simjapaneseitalian  similaritymatrix  indicesforlanguage   japanese       indicesforlanguage   italian     make heatmap snsheatmap  simjapaneseitalian  heatmap similaritie italian japanese document corpus bit small compare publish result doc2vec    paragraph vectors   algorithm  usually demonstrate set ten  of  thousand million document  particular   4742  4655  93 non  english description text  split 16 non  english language  seem unlikely word language enough in  context usage example either  1  survive mincount cutoff  generally not low 2     2  obtain meaningful  generalizable implication model   very  rare word fairly idiosyncratic  arbitrary meaning  description text fill rare  weakly  understand word tend weak  almost random doc  vector   spurious similarity not much surprise    non  english  description text english  looking omnilingual token  stray word false cognate punctuation glitch  might also dominate description to description comparison  check  suspiciously  too  similar pair  many word document actually train model  keep mind word train model essentially ignore training post  rtraining inference   document 100 word  language 100  word document  might fact dominate single  appearance word  not even meet mincount2  slimme surviving word short doc fill weakly  understand word   similarity score english document generally sensible  model  enough datum  really want family algorithm  word2vec  doc2vec  fasttext  etc  many  varied  representative example word s usage sensible contrasting context able meaningful later modeling word  document contain    can not manage use mincount least usual default  5   ideally even higher  likely not sufficient data algorithm show strength  extend corpus  even text direct interest  sufficient variety well represent language use text interest  might help    even universe  of  recommendation  5000 description  mix another 200000 similar short blurb source  word langguage interest enough coverage  could worthwhile  ,Implementation Issues
Get previous sentence while using SpaCy matcher,"I am running a SpaCy Matcher line-by-line on a text file. My file has each text entry on a separate line. I am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. I am able to get the first two, but I am having trouble getting the previous sentence , given that there isn't a sentence index (from this post ). Here is my code: with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: string_id = nlp.vocab.strings[match_id] span = doc[start:end] for sent in doc.sents: if matcher(sent): instances.append(pd.Series({""instance"":str(span.text), ""sentence"":str(sent.text), ""previous_sentence"":str(sent[-1].text)})) I understand that the bolded part is giving me the previous token, not sentence (I tried to get around this with the list, but it doesn't work). Any advice for retrieving the previous sentence would be greatly appreciated. Thank you!","['python', 'nlp', 'spacy']",1,"Adjustments: Track Previous Sentence: We now maintain a prev_sent variable that tracks the previous sentence as we iterate through all sentences in a document. Matcher Usage: We only need to create the Matcher instance once, outside the loop through lines in the file, and then apply it to each sentence within the loop. This is more efficient than recreating it for every line. Check for Previous Sentence: We handle cases where there might not be a previous sentence (e.g., the match is found in the first sentence of the document) by checking if prev_sent is None. If it is, we set the ""previous_sentence"" field to ""N/A"" or any placeholder text you find suitable. import spacy from spacy.matcher import Matcher import pandas as pd # Load the SpaCy model nlp = spacy.load(""en_core_web_sm"") # Adjust model as necessary # Define your pattern here pattern = [{""LOWER"": ""example""}] # Example pattern # Initialize matcher with the vocab matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) instances = [] # List to hold match details with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) prev_sent = None # Variable to keep track of the previous sentence for sent in doc.sents: matches = matcher(sent) if matches: for match_id, start, end in matches: instance_text = sent[start:end].text # The matched instance current_sentence = sent.text previous_sentence = prev_sent.text if prev_sent else ""N/A"" # Handle the case where there's no previous sentence # Append the extracted information to your instances list instances.append(pd.Series({""instance"": instance_text, ""sentence"": current_sentence, ""previous_sentence"": previous_sentence})) prev_sent = sent # Update the previous sentence for the next iteration # Convert instances list to DataFrame df_instances = pd.DataFrame(instances) print(df_instances)",2024-04-02 00:14:51,2024-04-02 05:16:42,50,https://stackoverflow.com/questions/78258373/get-previous-sentence-while-using-spacy-matcher,"Get previous sentence while using SpaCy matcher I am running a SpaCy Matcher line-by-line on a text file. My file has each text entry on a separate line. I am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. I am able to get the first two, but I am having trouble getting the previous sentence , given that there isn't a sentence index (from this post ). Here is my code: with open('file.txt', 'r') as f: for line in iter(f.readline, ''): doc = nlp(line) matcher = Matcher(nlp.vocab) matcher.add(""pattern_of_interest"", [pattern]) matches = matcher(doc) for match_id, start, end in matches: string_id = nlp.vocab.strings[match_id] span = doc[start:end] for sent in doc.sents: if matcher(sent): instances.append(pd.Series({""instance"":str(span.text), ""sentence"":str(sent.text), ""previous_sentence"":str(sent[-1].text)})) I understand that the bolded part is giving me the previous token, not sentence (I tried to get around this with the list, but it doesn't work). Any advice for retrieving the previous sentence would be greatly appreciated. Thank you!",get previous sentence use spacy matcher run spacy matcher line  by  line text file  file text entry separate line  try extract 1  match instance  2  full sentence  3  previous sentence  able get first two  trouble get previous sentence  give not sentence index  post   code  open   filetxt    r   f  line iter  freadline      doc  nlp  line  matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   match  matcher  doc  matchid  start  end match  stringid  nlpvocabstrings  matchid  span  doc  start  end  send docsent  matcher  send   instancesappend  pd  series     instance   str  spantext     sentence   str  senttext     previoussentence   str  send  1  text     understand bolde part give previous token  sentence  try get around list  not work   advice retrieve previous sentence would greatly appreciate  thank ,adjustment  track previous sentence  maintain prevsent variable track previous sentence iterate sentence document  matcher usage  need create matcher instance  outside loop line file  apply sentence within loop  efficient recreate every line  check previous sentence  handle case might previous sentence  eg  match find first sentence document  check prevsent none   set   previoussentence  field   n  a  placeholder text find suitable  import spacy spacymatcher import matcher import panda pd  load spacy model nlp  spacyload    encorewebsm    adjust model necessary  define pattern pattern      lower     example     example pattern  initialize matcher vocab matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   instance     list hold match detail open   filetxt    r   f  line iter  freadline      doc  nlp  line  prevsent  none  variable keep track previous sentence send docsent  match  matcher  send  match  matchid  start  end match  instancetext  send  start  end  text  match instance currentsentence  senttext previoussentence  prevsenttext prevsent else   n  a   handle case s previous sentence  append extract information instance list instancesappend  pd  series     instance   instancetext    sentence   currentsentence    previoussentence   previoussentence    prevsent  send  update previous sentence next iteration  convert instance list dataframe dfinstance  pd  dataframe  instance  print  dfinstance ,get previous sentence use spacy matcher run spacy matcher line  by  line text file  file text entry separate line  try extract 1  match instance  2  full sentence  3  previous sentence  able get first two  trouble get previous sentence  give not sentence index  post   code  open   filetxt    r   f  line iter  freadline      doc  nlp  line  matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   match  matcher  doc  matchid  start  end match  stringid  nlpvocabstrings  matchid  span  doc  start  end  send docsent  matcher  send   instancesappend  pd  series     instance   str  spantext     sentence   str  senttext     previoussentence   str  send  1  text     understand bolde part give previous token  sentence  try get around list  not work   advice retrieve previous sentence would greatly appreciate  thank  adjustment  track previous sentence  maintain prevsent variable track previous sentence iterate sentence document  matcher usage  need create matcher instance  outside loop line file  apply sentence within loop  efficient recreate every line  check previous sentence  handle case might previous sentence  eg  match find first sentence document  check prevsent none   set   previoussentence  field   n  a  placeholder text find suitable  import spacy spacymatcher import matcher import panda pd  load spacy model nlp  spacyload    encorewebsm    adjust model necessary  define pattern pattern      lower     example     example pattern  initialize matcher vocab matcher  matcher  nlpvocab  matcheradd    patternofinterest    pattern   instance     list hold match detail open   filetxt    r   f  line iter  freadline      doc  nlp  line  prevsent  none  variable keep track previous sentence send docsent  match  matcher  send  match  matchid  start  end match  instancetext  send  start  end  text  match instance currentsentence  senttext previoussentence  prevsenttext prevsent else   n  a   handle case s previous sentence  append extract information instance list instancesappend  pd  series     instance   instancetext    sentence   currentsentence    previoussentence   previoussentence    prevsent  send  update previous sentence next iteration  convert instance list dataframe dfinstance  pd  dataframe  instance  print  dfinstance ,Library/Tool-Based Queries
"How to detect if two sentences are simmilar, not in meaning, but in syllables/words?","Here are some examples of the types of sentences that need to be considered ""similar"" there was a most extraordinary noise going on shrinking rapidly she soon made out there was a most extraordinary noise going on shrinking rapid that will be a very little alice knew it was just possible it had thou wilt be very little alice i knew it was possible to add however at last it sat down and looked very anxiously into her face and however that lives in sadtown and look very anxiously into him facing it she went in search of her or of anything to say she simply bowed she went in the search of her own or of anything to say and she squeezed herself up on tiptoe and peeped over the wig he did and she squeezed herself up on the tiptoe and peeped over her wig he did she had not noticed before and behind it was very glad to find that she had not noticed before and behind it it was very glad to find that as soon as the soldiers had to fall a long hookah and taking not soon as the soldiers have to fall along huka and taking knots And here are some examples of more difficult edge cases I would be able to like to catch, but are not as necessary so she tucked it under her arm with its head it would not join she tucked it under her arm with its head let me see four times five is twelve and four times five is twelve let me see times is and times is let me see four times seven is oh dear run home this moment and times is o dear run home this moment and in a minute or two she walked sadly down the middle being held up and then well see you sidely down the middle in health often Sentences that are somewhat different and have no such similarities need to be marked as dissimilar. If there is an algorithm that exists that outputs a ""score"" versus just a boolean similar or not, I could determine what threshold would be necessary through my own testing. The top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. If there is some syllabic comparison method that would be much more accurate given that I have the initial source text as well as the audio, I could also employ that instead of this word comparison technique. My current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. If at least 10 words match in either indexing order, I count the sentences as similar. However, all of the presented examples are cases where this strategy does not work.","['search', 'nlp', 'full-text-search', 'similarity', 'sentence-similarity']",2,"This is exactly similar to my answer above, but this is in nodejs. Apart from the language difference, code works exactly the same. First you need to install the natural module using npm. npm install natural const natural = require('natural'); function dotProduct(vector1, vector2) { return vector1.reduce((acc, val, index) => acc + val * vector2[index], 0); } function magnitude(vector) { return Math.sqrt(vector.reduce((acc, val) => acc + val * val, 0)); } function cosineSimilarity(vector1, vector2) { const dotProd = dotProduct(vector1, vector2); const mag1 = magnitude(vector1); const mag2 = magnitude(vector2); if (mag1 === 0 || mag2 === 0) { return 0; // Avoid division by zero } return dotProd / (mag1 * mag2); } function sentenceSimilarity(sentence1, sentence2) { // Tokenizing sentences const tokenizer = new natural.WordTokenizer(); const sentence1Tokens = tokenizer.tokenize(sentence1); const sentence2Tokens = tokenizer.tokenize(sentence2); // Creating a bade of words from tokens const bagOfWords = new Set([...sentence1Tokens, ...sentence2Tokens]); // Convert tokens to vectors const vector1 = Array.from(bagOfWords).map(word => sentence1Tokens.includes(word) ? 1 : 0); const vector2 = Array.from(bagOfWords).map(word => sentence2Tokens.includes(word) ? 1 : 0); // Calculate cosine similarity const similarity = cosineSimilarity(vector1, vector2); return similarity; } // Example usage const sentence1 = ""This is a sentence.""; const sentence2 = ""This is another sentence.""; const similarityScore = sentenceSimilarity(sentence1, sentence2); console.log(""Similarity score:"", similarityScore); Function dotProduct(), magnitude(), and cosineSimilarity() were needed to be defined since I was not able to find a library that provides these in node unlike in python. Apart from that, all the other logic are similar to the python code above.",2024-03-29 02:08:38,2024-04-02 04:54:45,126,https://stackoverflow.com/questions/78241665/how-to-detect-if-two-sentences-are-simmilar-not-in-meaning-but-in-syllables-wo,"How to detect if two sentences are simmilar, not in meaning, but in syllables/words? Here are some examples of the types of sentences that need to be considered ""similar"" there was a most extraordinary noise going on shrinking rapidly she soon made out there was a most extraordinary noise going on shrinking rapid that will be a very little alice knew it was just possible it had thou wilt be very little alice i knew it was possible to add however at last it sat down and looked very anxiously into her face and however that lives in sadtown and look very anxiously into him facing it she went in search of her or of anything to say she simply bowed she went in the search of her own or of anything to say and she squeezed herself up on tiptoe and peeped over the wig he did and she squeezed herself up on the tiptoe and peeped over her wig he did she had not noticed before and behind it was very glad to find that she had not noticed before and behind it it was very glad to find that as soon as the soldiers had to fall a long hookah and taking not soon as the soldiers have to fall along huka and taking knots And here are some examples of more difficult edge cases I would be able to like to catch, but are not as necessary so she tucked it under her arm with its head it would not join she tucked it under her arm with its head let me see four times five is twelve and four times five is twelve let me see times is and times is let me see four times seven is oh dear run home this moment and times is o dear run home this moment and in a minute or two she walked sadly down the middle being held up and then well see you sidely down the middle in health often Sentences that are somewhat different and have no such similarities need to be marked as dissimilar. If there is an algorithm that exists that outputs a ""score"" versus just a boolean similar or not, I could determine what threshold would be necessary through my own testing. The top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. If there is some syllabic comparison method that would be much more accurate given that I have the initial source text as well as the audio, I could also employ that instead of this word comparison technique. My current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. If at least 10 words match in either indexing order, I count the sentences as similar. However, all of the presented examples are cases where this strategy does not work.",detect two sentence simmilar  meaning  syllable  word  example type sentence need consider   similar  extraordinary noise go shrink rapidly soon make extraordinary noise go shrink rapid little alice know possible thou wilt little alice know possible add however last sat look anxiously face however life sadtown look anxiously face go search anything say simply bow go search anything say squeeze tiptoe peep wig squeeze tiptoe peep wig notice behind glad find notice behind glad find soon soldier fall long hookah take soon soldier fall along huka take knot example difficult edge case would able like catch  necessary tuck arm head would join tuck arm head let see four time five twelve four time five twelve let see time time let see four time seven oh dear run home moment time dear run home moment minute two walk sadly middle hold well see sidely middle health often sentence somewhat different similarity need mark dissimilar  algorithm exist output   score  versus boolean similar  could determine threshold would necessary testing  top sentence example randomly generate  bottom sentence output speech  to  text neural network  audio file someone read top line  syllabic comparison method would much accurate give initial source text well audio  could also employ instead word comparison technique  current method involve indexing word  forwards  reverse  check many word line  least 10 word match either indexing order  count sentence similar  however  present example case strategy work ,exactly similar answer  nodejs  apart language difference  code work exactly  first need install natural module use npm  npm install natural const natural  require   natural    function dotproduct  vector1  vector2   return vector1reduce   acc  val  index    acc  val  vector2  index   0    function magnitude  vector   return mathsqrt  vectorreduce   acc  val    acc  val  val  0     function cosinesimilarity  vector1  vector2   const dotprod  dotproduct  vector1  vector2   const mag1  magnitude  vector1   const mag2  magnitude  vector2    mag1    0  mag2    0   return 0   avoid division zero  return dotprod   mag1  mag2    function sentencesimilarity  sentence1  sentence2    tokenize sentence const tokenizer  new natural  wordtokenizer    const sentence1token  tokenizertokenize  sentence1   const sentence2token  tokenizertokenize  sentence2    create bade word token const bagofword  new set    sentence1tokens   sentence2token     convert token vector const vector1  arrayfrom  bagofword  map  word   sentence1tokensincludes  word   1  0   const vector2  arrayfrom  bagofword  map  word   sentence2tokensincludes  word   1  0    calculate cosine similarity const similarity  cosinesimilarity  vector1  vector2   return similarity    example usage const sentence1    sentence     const sentence2    another sentence     const similarityscore  sentencesimilarity  sentence1  sentence2   consolelog    similarity score    similarityscore   function dotproduct    magnitude    cosinesimilarity   needed define since able find library provide node unlike python  apart  logic similar python code ,detect two sentence simmilar  meaning  syllable  word  example type sentence need consider   similar  extraordinary noise go shrink rapidly soon make extraordinary noise go shrink rapid little alice know possible thou wilt little alice know possible add however last sat look anxiously face however life sadtown look anxiously face go search anything say simply bow go search anything say squeeze tiptoe peep wig squeeze tiptoe peep wig notice behind glad find notice behind glad find soon soldier fall long hookah take soon soldier fall along huka take knot example difficult edge case would able like catch  necessary tuck arm head would join tuck arm head let see four time five twelve four time five twelve let see time time let see four time seven oh dear run home moment time dear run home moment minute two walk sadly middle hold well see sidely middle health often sentence somewhat different similarity need mark dissimilar  algorithm exist output   score  versus boolean similar  could determine threshold would necessary testing  top sentence example randomly generate  bottom sentence output speech  to  text neural network  audio file someone read top line  syllabic comparison method would much accurate give initial source text well audio  could also employ instead word comparison technique  current method involve indexing word  forwards  reverse  check many word line  least 10 word match either indexing order  count sentence similar  however  present example case strategy work  exactly similar answer  nodejs  apart language difference  code work exactly  first need install natural module use npm  npm install natural const natural  require   natural    function dotproduct  vector1  vector2   return vector1reduce   acc  val  index    acc  val  vector2  index   0    function magnitude  vector   return mathsqrt  vectorreduce   acc  val    acc  val  val  0     function cosinesimilarity  vector1  vector2   const dotprod  dotproduct  vector1  vector2   const mag1  magnitude  vector1   const mag2  magnitude  vector2    mag1    0  mag2    0   return 0   avoid division zero  return dotprod   mag1  mag2    function sentencesimilarity  sentence1  sentence2    tokenize sentence const tokenizer  new natural  wordtokenizer    const sentence1token  tokenizertokenize  sentence1   const sentence2token  tokenizertokenize  sentence2    create bade word token const bagofword  new set    sentence1tokens   sentence2token     convert token vector const vector1  arrayfrom  bagofword  map  word   sentence1tokensincludes  word   1  0   const vector2  arrayfrom  bagofword  map  word   sentence2tokensincludes  word   1  0    calculate cosine similarity const similarity  cosinesimilarity  vector1  vector2   return similarity    example usage const sentence1    sentence     const sentence2    another sentence     const similarityscore  sentencesimilarity  sentence1  sentence2   consolelog    similarity score    similarityscore   function dotproduct    magnitude    cosinesimilarity   needed define since able find library provide node unlike python  apart  logic similar python code ,Implementation Issues
Is BertForSequenceClassification using the CLS vector?,"In the hugging face source code , pooled_output = outputs[1] is used. outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) pooled_output = outputs[1] Shouldn't it be pooled_output = outputs[0] ? (This answer mentioning BertPooler seems to be outdated) Based on this answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that BertForSequenceClassification freezes the Bert model and only trains the classification head, but maybe that's not the case) Would a sentence embedding be equivalent or even better than the [CLS] token embedding?","['nlp', 'huggingface-transformers', 'bert-language-model']",1,"Would a sentence embedding be equivalent or even better than the [CLS] token embedding? A sentence embedding is everything that represents the input sequence as a numerical vector. The question is whether this embedding is semantical meaningful (e.g. can we use it with similarity metrics). This is for example not the case for the pretrained Bert weights released by google (refer to this answer for more information). Is the CLS token a sentence embedding? Yes. Is some kind of pooling a sentence embedding? Yes. Are they semantically meaningful with the Bert weights release by google? No. Shouldn't it be pooled_output = outputs[0]? No, because when you check the code , you will see that the first element of the tuple is the last_hidden_state sequence_output = encoder_outputs[0] pooled_output = self.pooler(sequence_output) if self.pooler is not None else None if not return_dict: return (sequence_output, pooled_output) + encoder_outputs[1:] I am confused as to why/how masked language modeling would lead to the start token learning a sentence level representation. Because it is included in every training sequence and the [CLS] ""absorbs"" the other tokens. You can also see this in the attention mechanism (compare Revealing the Dark Secrets of BERT paper ). As mentioned above, the questions is if they are semantically meaningful without any further finetuning. No (compare this StackOverflow answer ).",2024-03-28 20:54:17,2024-05-11 14:04:25,216,https://stackoverflow.com/questions/78240828/is-bertforsequenceclassification-using-the-cls-vector,"Is BertForSequenceClassification using the CLS vector? In the hugging face source code , pooled_output = outputs[1] is used. outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) pooled_output = outputs[1] Shouldn't it be pooled_output = outputs[0] ? (This answer mentioning BertPooler seems to be outdated) Based on this answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that BertForSequenceClassification freezes the Bert model and only trains the classification head, but maybe that's not the case) Would a sentence embedding be equivalent or even better than the [CLS] token embedding?",bertforsequenceclassification use cls vector  hug face source code  pooledoutput  output  1  use  output  selfbert  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  inputsembed  inputsembeds  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   pooledoutput  output  1  not pooledoutput  output  0    answer mention bertpooler seem outdated  base answer  seem cls token learn sentence level representation  confuse why  how mask language modelling would lead start token learn sentence level representation   think bertforsequenceclassification freeze bert model train classification head  maybe s case  would sentence embed equivalent even well  cls  token embed ,would sentence embed equivalent even well  cls  token embed  sentence embed everything represent input sequence numerical vector  question whether embed semantical meaningful  eg  use similarity metric   example case pretraine bert weight release google  refer answer information   cls token sentence embed  yes  kind pool sentence embed  yes  semantically meaningful bert weight release google   not pooledoutput  output  0    check code  see first element tuple lasthiddenstate sequenceoutput  encoderoutput  0  pooledoutput  selfpooler  sequenceoutput  selfpool none else none returndict  return  sequenceoutput  pooledoutput   encoderoutput  1   confuse why  how mask language modeling would lead start token learn sentence level representation  include every training sequence  cls    absorb  token  also see attention mechanism  compare revealing dark secrets bert paper   mention  question semantically meaningful without finetune   compare stackoverflow answer  ,bertforsequenceclassification use cls vector  hug face source code  pooledoutput  output  1  use  output  selfbert  inputids  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  inputsembed  inputsembeds  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   pooledoutput  output  1  not pooledoutput  output  0    answer mention bertpooler seem outdated  base answer  seem cls token learn sentence level representation  confuse why  how mask language modelling would lead start token learn sentence level representation   think bertforsequenceclassification freeze bert model train classification head  maybe s case  would sentence embed equivalent even well  cls  token embed  would sentence embed equivalent even well  cls  token embed  sentence embed everything represent input sequence numerical vector  question whether embed semantical meaningful  eg  use similarity metric   example case pretraine bert weight release google  refer answer information   cls token sentence embed  yes  kind pool sentence embed  yes  semantically meaningful bert weight release google   not pooledoutput  output  0    check code  see first element tuple lasthiddenstate sequenceoutput  encoderoutput  0  pooledoutput  selfpooler  sequenceoutput  selfpool none else none returndict  return  sequenceoutput  pooledoutput   encoderoutput  1   confuse why  how mask language modeling would lead start token learn sentence level representation  include every training sequence  cls    absorb  token  also see attention mechanism  compare revealing dark secrets bert paper   mention  question semantically meaningful without finetune   compare stackoverflow answer  ,Implementation Issues
How to remove layers in Huggingface&#39;s transformers GPT2 pre-trained models?,"My code: from transformers import GPT2Config, GPT2Model from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""openai-community/gpt2"") print(decoder) Here is the output of the console, listing the model architecture: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) I want to remove the first layer: (wte): Embedding(50257, 768) I've tried the following way: def deleteEncodingLayers(model, num_layers_to_keep): # must pass in the full bert model oldModuleList = model.bert.encoder.layer newModuleList = nn.ModuleList() # Now iterate over all layers, only keepign only the relevant layers. for i in range(0, len(num_layers_to_keep)): newModuleList.append(oldModuleList[i]) # create a copy of the model, modify it with the new list, and return copyOfModel = copy.deepcopy(model) copyOfModel.bert.encoder.layer = newModuleList return copyOfModel But it didn't work. Who knows how to fix it?","['python', 'machine-learning', 'deep-learning', 'nlp', 'transformer-model']",1,"Try these parameters to bypass the embedding layer : class GPT2WithoutWTE(GPT2Model): def __init__(self, config): super().__init__(config) # Remove the word token embedding layer del self.wte def forward( self, inputs_embeds, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, ): # here you will bypass the embedding layer and use inputs_embeds directly return super().forward( inputs_embeds=inputs_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) for the input embeddings you can use the following inputs_embeds = torch.rand(1, 10, config.n_embd) Load the config of GPT2, send it to the class and then use the inputs_embeds for the new model.",2024-03-25 12:28:15,2024-03-25 13:22:28,343,https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models,"How to remove layers in Huggingface&#39;s transformers GPT2 pre-trained models? My code: from transformers import GPT2Config, GPT2Model from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(""openai-community/gpt2"") print(decoder) Here is the output of the console, listing the model architecture: GPT2LMHeadModel( (transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) ) I want to remove the first layer: (wte): Embedding(50257, 768) I've tried the following way: def deleteEncodingLayers(model, num_layers_to_keep): # must pass in the full bert model oldModuleList = model.bert.encoder.layer newModuleList = nn.ModuleList() # Now iterate over all layers, only keepign only the relevant layers. for i in range(0, len(num_layers_to_keep)): newModuleList.append(oldModuleList[i]) # create a copy of the model, modify it with the new list, and return copyOfModel = copy.deepcopy(model) copyOfModel.bert.encoder.layer = newModuleList return copyOfModel But it didn't work. Who knows how to fix it?",remove layer huggingface   39  transformer gpt2 pre  train model  code  transformer import gpt2config  gpt2model transformer import autotokenizer  automodelformaskedlm  automodelforcausallm model  automodelforcausallmfrompretraine    openai  community  gpt2   print  decoder  output console  list model architecture  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   want remove first layer   wte   embed  50257  768   ve try follow way  def deleteencodinglayer  model  numlayerstokeep    must pass full bert model oldmodulelist  modelbertencoderlayer newmodulelist  nn  modulelist    iterate layer  keepign relevant layer  range  0  len  numlayerstokeep    newmodulelistappend  oldmodulelist     create copy model  modify new list  return copyofmodel  copydeepcopy  model  copyofmodelbertencoderlayer  newmodulelist return copyofmodel not work  know fix ,try parameter bypass embed layer  class gpt2withoutwte  gpt2model   def   init    self  config   super   init    config   remove word token embed layer del selfwte def forward  self  inputsembed  attentionmask  none  tokentypeid  none  positionid  none  headmask  none  input  none  encoderhiddenstate  none  encoderattentionmask  none  pastkeyvalue  none  usecache  none  outputattention  none  outputhiddenstate  none  returndict  none     bypass embed layer use inputsembed directly return super   forward  inputsembed  inputsembeds  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  encoderhiddenstate  encoderhiddenstates  encoderattentionmask  encoderattentionmask  pastkeyvalue  pastkeyvalues  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   input embedding use follow inputsembed  torchrand  1  10  confignembd  load config gpt2  send class use inputsembed new model ,remove layer huggingface   39  transformer gpt2 pre  train model  code  transformer import gpt2config  gpt2model transformer import autotokenizer  automodelformaskedlm  automodelforcausallm model  automodelforcausallmfrompretraine    openai  community  gpt2   print  decoder  output console  list model architecture  gpt2lmheadmodel   transformer   gpt2model   wte   embed  50257  768   wpe   embed  1024  768   drop   dropout  p01  inplace  false   h   modulelist   0  11   12 x gpt2block   ln1   layernorm   768    eps1e05  elementwiseaffine  true   attn   gpt2attention   cattn   conv1d    cproj   conv1d    attndropout   dropout  p01  inplace  false   residdropout   dropout  p01  inplace  false    ln2   layernorm   768    eps1e05  elementwiseaffine  true   mlp   gpt2mlp   cfc   conv1d    cproj   conv1d    act   newgeluactivation    dropout   dropout  p01  inplace  false      lnf   layernorm   768    eps1e05  elementwiseaffine  true    lmhead   linear  infeatures768  outfeatures50257  bias  false   want remove first layer   wte   embed  50257  768   ve try follow way  def deleteencodinglayer  model  numlayerstokeep    must pass full bert model oldmodulelist  modelbertencoderlayer newmodulelist  nn  modulelist    iterate layer  keepign relevant layer  range  0  len  numlayerstokeep    newmodulelistappend  oldmodulelist     create copy model  modify new list  return copyofmodel  copydeepcopy  model  copyofmodelbertencoderlayer  newmodulelist return copyofmodel not work  know fix  try parameter bypass embed layer  class gpt2withoutwte  gpt2model   def   init    self  config   super   init    config   remove word token embed layer del selfwte def forward  self  inputsembed  attentionmask  none  tokentypeid  none  positionid  none  headmask  none  input  none  encoderhiddenstate  none  encoderattentionmask  none  pastkeyvalue  none  usecache  none  outputattention  none  outputhiddenstate  none  returndict  none     bypass embed layer use inputsembed directly return super   forward  inputsembed  inputsembeds  attentionmask  attentionmask  tokentypeid  tokentypeid  positionid  positionid  headmask  headmask  encoderhiddenstate  encoderhiddenstates  encoderattentionmask  encoderattentionmask  pastkeyvalue  pastkeyvalues  usecache  usecache  outputattention  outputattention  outputhiddenstate  outputhiddenstates  returndict  returndict   input embedding use follow inputsembed  torchrand  1  10  confignembd  load config gpt2  send class use inputsembed new model ,Basic Understanding
How can i get the first content of a python synsets list?,"enter image description here I have a scrapped text stored under the variable ""message"". I have removed the StopWords and stored the result with the variable ""without_stop_words"". I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS. Currently i am trying to get the MEANINGS but I'm getting an error: ""IndexError: list index out of range"" enter image description here for writeup in writeups: message = writeup.text #Stop Words stop_words = set(stopwords.words('english')) #print(stop_words) tokenized_words = word_tokenize(message) #Filtering Stop Words without_stop_words = [] for word in tokenized_words: if word not in stop_words: without_stop_words.append(word) #Word Meanings word_meanings = [] for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) meaning = sync_words[0].definition() print(meaning) I want to get the MEANING of each word in the ""without_stop_words"".","['python', 'nlp', 'nltk', 'sentiment-analysis', 'synset']",1,"The error comes from this line meaning = sync_words[0].definition() And it indicates that sync_words is empty for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) if sync_words: meaning = sync_words[0].definition() word_meanings.append(meaning) else: # Whatever you want to do if it's empty This will stop the error, but you should try to find out why sync_words is empty in the first place.",2024-03-23 14:53:18,2024-03-23 15:00:56,43,https://stackoverflow.com/questions/78211318/how-can-i-get-the-first-content-of-a-python-synsets-list,"How can i get the first content of a python synsets list? enter image description here I have a scrapped text stored under the variable ""message"". I have removed the StopWords and stored the result with the variable ""without_stop_words"". I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS. Currently i am trying to get the MEANINGS but I'm getting an error: ""IndexError: list index out of range"" enter image description here for writeup in writeups: message = writeup.text #Stop Words stop_words = set(stopwords.words('english')) #print(stop_words) tokenized_words = word_tokenize(message) #Filtering Stop Words without_stop_words = [] for word in tokenized_words: if word not in stop_words: without_stop_words.append(word) #Word Meanings word_meanings = [] for each_word in without_stop_words: sync_words = wordnet.synsets(each_word) meaning = sync_words[0].definition() print(meaning) I want to get the MEANING of each word in the ""without_stop_words"".",get first content python synset list  enter image description scrap text store variable   message   remove stopwords store result variable   withoutstopwords   want loop word  withoutstopwords  get meaning pronouns  currently try get meanings  m get error    indexerror  list index range  enter image description writeup writeup  message  writeuptext  stop word stopwords  set  stopwordsword   english     print  stopwords  tokenizedwords  wordtokenize  message   filtering stop word withoutstopwords    word tokenizedwords  word stopwords  withoutstopwordsappend  word   word meanings wordmeaning    eachword withoutstopwords  syncword  wordnetsynset  eachword  mean  syncword  0  definition   print  meaning  want get meaning word   withoutstopwords  ,error come line meaning  syncword  0  definition   indicate syncwords empty eachword withoutstopwords  syncword  wordnetsynset  eachword  syncword  mean  syncword  0  definition   wordmeaningsappend  mean  else   whatever want s empty stop error  try find syncword empty first place ,get first content python synset list  enter image description scrap text store variable   message   remove stopwords store result variable   withoutstopwords   want loop word  withoutstopwords  get meaning pronouns  currently try get meanings  m get error    indexerror  list index range  enter image description writeup writeup  message  writeuptext  stop word stopwords  set  stopwordsword   english     print  stopwords  tokenizedwords  wordtokenize  message   filtering stop word withoutstopwords    word tokenizedwords  word stopwords  withoutstopwordsappend  word   word meanings wordmeaning    eachword withoutstopwords  syncword  wordnetsynset  eachword  mean  syncword  0  definition   print  meaning  want get meaning word   withoutstopwords   error come line meaning  syncword  0  definition   indicate syncwords empty eachword withoutstopwords  syncword  wordnetsynset  eachword  syncword  mean  syncword  0  definition   wordmeaningsappend  mean  else   whatever want s empty stop error  try find syncword empty first place ,Basic Understanding
Solution to solve problem different results when run Doc2vec gensim?,"I try to find information about problem that Doc2vec returns different results when it runs. I saw many previous questions about this and I know It happens because vector is randomly initialize. However, I am creating a website which uses this result to display in frontend. The difference in results makes reliability of systems reduce. I know my dataset is really small. But infer_vector() can't return same vectors with same documents and results most_similar() are different in each run. How do I prevent this problem or having alternative way to apply doc2vec model in my application to avoid difference of results? This is some code: model = gensim.models.doc2vec.Doc2Vec(vector_size=50, dm=1, window=5, min_count=2, epochs=100, negative=0, workers=5) But I received warning: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. I try to set negative=-1 but I see explain from gensim : negative must be integer.","['nlp', 'doc2vec']",1,"These are potentially, two different issues. With regard to the warning you're seeing: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. The warning is complete and truthful, it already describes what you're doing wrong and how to solve it. You must set either hs or negative to be positive or else no training will happen in your model . negative=-1 is an illegal setting, and not positive. If you want to use Doc2Vec , you need to either have the negative parameter as a positive integer (as with its default value negative=5 ), or if you want to set negative=0 then you need to enable the alternative ""hierarchical softmax"" mode with hs=1 . The algorithm will do nothing but error or given nonsense untrained results if you give it illegal configurations. As is explained in the Q12 of the Gensim Project FAQ & other StackOverflow answers , the operation of the Doc2Vec algorithm naturally allows for variance in the vectors returned by infer_vector() from run to run. And, if that ""jitter"" between inferences is s making a big difference in results, there are probably other serious problems in your use of Doc2Vec , such as insufficient data or bad parameters, that you should fix, rather than trying to force a false determinism onto your calculations. In particular, if the model whose changing infer_vector() results was ""trained"" – not really – with the shown parameters ( negative=0 without enabled hs ), ignoring the warning that won't work, that is the first big problem to solve. It will make all inferred vetor random and meaninglfess (as opposed to just ""a little noisy""). But, if after fixing the total failure of training you then insistently want to do the incorrect thing, you can force inference determinism as is described in another answer at: removing randomization of vector initialization for doc2vec",2024-03-19 03:20:17,2024-03-19 12:50:10,43,https://stackoverflow.com/questions/78184077/solution-to-solve-problem-different-results-when-run-doc2vec-gensim,"Solution to solve problem different results when run Doc2vec gensim? I try to find information about problem that Doc2vec returns different results when it runs. I saw many previous questions about this and I know It happens because vector is randomly initialize. However, I am creating a website which uses this result to display in frontend. The difference in results makes reliability of systems reduce. I know my dataset is really small. But infer_vector() can't return same vectors with same documents and results most_similar() are different in each run. How do I prevent this problem or having alternative way to apply doc2vec model in my application to avoid difference of results? This is some code: model = gensim.models.doc2vec.Doc2Vec(vector_size=50, dm=1, window=5, min_count=2, epochs=100, negative=0, workers=5) But I received warning: You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training. I try to set negative=-1 but I see explain from gensim : negative must be integer.",solution solve problem different result run doc2vec gensim  try find information problem doc2vec return different result run  see many previous question know happen vector randomly initialize  however  create website use result display frontend  difference result make reliability system reduce  know dataset really small  infervector   can not return vector document result mostsimilar   different run  prevent problem alternative way apply doc2vec model application avoid difference result  code  model  gensimmodelsdoc2vec  doc2vec  vectorsize50  dm1  window5  mincount2  epochs100  negative0  workers5  receive warning  must set either  hs   negative  positive proper training   hs0   negative0   training  try set negative1 see explain gensim  negative must integer ,potentially  two different issue  regard warning be see  must set either  hs   negative  positive proper training   hs0   negative0   training  warn complete truthful  already describe be wrong solve  must set either hs negative positive else training happen model  negative1 illegal setting  positive  want use doc2vec  need either negative parameter positive integer  default value negative5   want set negative0 need enable alternative   hierarchical softmax  mode hs1  algorithm nothing error give nonsense untraine result give illegal configuration  explain q12 gensim project faq  stackoverflow answer  operation doc2vec algorithm naturally allow variance vector return infervector   run run     jitter  inference make big difference result  probably serious problem use doc2vec  insufficient datum bad parameter  fix  rather try force false determinism onto calculation  particular  model whose change infervector   result   train   really  show parameter  negative0 without enable hs   ignore warning will not work  first big problem solve  make inferred vetor random meaninglfess  oppose   little noisy     fix total failure training insistently want incorrect thing  force inference determinism describe another answer  remove randomization vector initialization doc2vec,solution solve problem different result run doc2vec gensim  try find information problem doc2vec return different result run  see many previous question know happen vector randomly initialize  however  create website use result display frontend  difference result make reliability system reduce  know dataset really small  infervector   can not return vector document result mostsimilar   different run  prevent problem alternative way apply doc2vec model application avoid difference result  code  model  gensimmodelsdoc2vec  doc2vec  vectorsize50  dm1  window5  mincount2  epochs100  negative0  workers5  receive warning  must set either  hs   negative  positive proper training   hs0   negative0   training  try set negative1 see explain gensim  negative must integer  potentially  two different issue  regard warning be see  must set either  hs   negative  positive proper training   hs0   negative0   training  warn complete truthful  already describe be wrong solve  must set either hs negative positive else training happen model  negative1 illegal setting  positive  want use doc2vec  need either negative parameter positive integer  default value negative5   want set negative0 need enable alternative   hierarchical softmax  mode hs1  algorithm nothing error give nonsense untraine result give illegal configuration  explain q12 gensim project faq  stackoverflow answer  operation doc2vec algorithm naturally allow variance vector return infervector   run run     jitter  inference make big difference result  probably serious problem use doc2vec  insufficient datum bad parameter  fix  rather try force false determinism onto calculation  particular  model whose change infervector   result   train   really  show parameter  negative0 without enable hs   ignore warning will not work  first big problem solve  make inferred vetor random meaninglfess  oppose   little noisy     fix total failure training insistently want incorrect thing  force inference determinism describe another answer  remove randomization vector initialization doc2vec,Implementation Issues
How to optimize the function which uses looping on lists on pandas dataframe?,"I am using a function on a pandas dataframe as : import spacy from collections import Counter # Load English language model nlp = spacy.load(""en_core_web_sm"") # Function to filter out only nouns from a list of words def filter_nouns(words): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' filtered_nouns = [] # Preprocess the text by removing symbols and splitting into words words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp("" "".join(words)) if token.pos_ == ""NOUN""] return filtered_nouns # Apply filtering logic to all rows in the 'NOTE' column df['filtered_nouns'] = sf['NOTE'].apply(lambda x: filter_nouns(x)) I have a dataset containing 6400 rows and df['NOTE'] is a very long paragraph converted from the Oracle CLOB datatype. This function is working quickly for 5-10 rows but for 6400 rows, it is taking a very long time. Any ways to optimize this.","['python', 'python-3.x', 'pandas', 'list', 'nlp']",2,"The first thing you should do is remove all the repetition in your function. In this line: words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] You are building the translation dictionary every time you translate a word, and calling translate for each word in the text. It is far more efficient to do each of those once: tr = str.maketrans('', '', SYMBOLS) words = words.strip().translate(tr).split() This makes about a 50x speed-up on a 1000-word string on my computer. In the next line you are then joining all the words for every call to nlp . You should do that once: text = ' '.join(words) filtered_nouns = [token.text for token in nlp(text) if token.pos_ == ""NOUN""] But note that you just split on spaces, so you might as well skip that step completely. In total: def filter_nouns(text): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' tr = str.maketrans('', '', SYMBOLS) # Preprocess the text by removing symbols words = text.strip().translate(tr) # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp(words) if token.pos_ == ""NOUN""] return filtered_nouns Finally, note that .apply(lambda x: filter_nouns(x)) is the same as .apply(filter_nouns) .",2024-03-14 02:56:32,2024-03-14 05:40:23,94,https://stackoverflow.com/questions/78157864/how-to-optimize-the-function-which-uses-looping-on-lists-on-pandas-dataframe,"How to optimize the function which uses looping on lists on pandas dataframe? I am using a function on a pandas dataframe as : import spacy from collections import Counter # Load English language model nlp = spacy.load(""en_core_web_sm"") # Function to filter out only nouns from a list of words def filter_nouns(words): SYMBOLS = '{}()[].,:;+-*/&|<>=~$1234567890#_%' filtered_nouns = [] # Preprocess the text by removing symbols and splitting into words words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()] # Process each word and filter only nouns filtered_nouns = [token.text for token in nlp("" "".join(words)) if token.pos_ == ""NOUN""] return filtered_nouns # Apply filtering logic to all rows in the 'NOTE' column df['filtered_nouns'] = sf['NOTE'].apply(lambda x: filter_nouns(x)) I have a dataset containing 6400 rows and df['NOTE'] is a very long paragraph converted from the Oracle CLOB datatype. This function is working quickly for 5-10 rows but for 6400 rows, it is taking a very long time. Any ways to optimize this.",optimize function use loop list panda dataframe  use function panda dataframe  import spacy collection import counter  load english language model nlp  spacyload    encorewebsm    function filter noun list word def filternoun  word   symbols                        1234567890     filterednoun     preprocess text remove symbol split word word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit     process word filter noun filterednouns   tokentext token nlp      join  word   tokenpos      noun   return filterednouns  apply filter logic row  note  column df   filterednoun    sf   note   apply  lambda x  filternouns  x   dataset contain 6400 row df   note   long paragraph convert oracle clob datatype  function work quickly 5  10 row 6400 row  take long time  way optimize ,first thing remove repetition function  line  word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit    build translation dictionary every time translate word  call translate word text  far efficient  tr  strmaketrans        symbols  word  wordsstrip   translate  tr  split   make 50x speed  up 1000  word string computer  next line join word every call nlp   text    join  word  filterednoun   tokentext token nlp  text  tokenpos      noun   note split space  might well skip step completely  total  def filternoun  text   symbols                        1234567890     tr  strmaketrans        symbols   preprocess text remove symbol word  textstrip   translate  tr   process word filter noun filterednouns   tokentext token nlp  word  tokenpos      noun   return filterednouns finally  note apply  lambda x  filternouns  x   apply  filternouns  ,optimize function use loop list panda dataframe  use function panda dataframe  import spacy collection import counter  load english language model nlp  spacyload    encorewebsm    function filter noun list word def filternoun  word   symbols                        1234567890     filterednoun     preprocess text remove symbol split word word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit     process word filter noun filterednouns   tokentext token nlp      join  word   tokenpos      noun   return filterednouns  apply filter logic row  note  column df   filterednoun    sf   note   apply  lambda x  filternouns  x   dataset contain 6400 row df   note   long paragraph convert oracle clob datatype  function work quickly 5  10 row 6400 row  take long time  way optimize  first thing remove repetition function  line  word   wordtranslate   ord  sym   none sym symbols   strip   word wordssplit    build translation dictionary every time translate word  call translate word text  far efficient  tr  strmaketrans        symbols  word  wordsstrip   translate  tr  split   make 50x speed  up 1000  word string computer  next line join word every call nlp   text    join  word  filterednoun   tokentext token nlp  text  tokenpos      noun   note split space  might well skip step completely  total  def filternoun  text   symbols                        1234567890     tr  strmaketrans        symbols   preprocess text remove symbol word  textstrip   translate  tr   process word filter noun filterednouns   tokentext token nlp  word  tokenpos      noun   return filterednouns finally  note apply  lambda x  filternouns  x   apply  filternouns  ,Library/Tool-Based Queries
SpaCy: Regex pattern does not work in rule-based matcher,"I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model. The aim is to add tokens with ""COMP"" label whenever it finds words structured like this: XXX-Ynnn XXX Ynnn Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination. To do so, I use the following method def add_component_patterns_re(input_references, model_ruler): ruler = model_ruler ref_patterns = [] letters = ['V', 'B', 'F', 'K', 'S'] print(""Adding component patterns"") for ref in input_references.iloc[:, 0]: # print(f""Adding references for system: {ref}"") for letter in letters: pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}' pattern = {""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}} ref_patterns.append({""label"":""COMP"", ""pattern"":pattern}) ruler.add_patterns(ref_patterns) return ref_patterns Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler. For information, i've also tried to change the pattern variable as a list entry, like this: pattern = [{""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}}] But the result is the same, it can't seem to get any match. Does someone have any suggestion? Thanks in advance!","['python', 'nlp', 'spacy', 'named-entity-recognition']",2,"In the end I got print(f""Adding references for system: {ref}"") for letter in letters: for nnn in range(1000): pattern = f""{ref}-{letter}{nnn:03d}"" ref_patterns.append({""label"": ""COMP"", ""pattern"": pattern}) pattern = f""{ref} {letter}{nnn:03d}"" ref_patterns.append({""label"": ""COMP"", ""pattern"": pattern}) For each pattern. The code is lengthier and a tad slower but it does the job just fine!",2024-03-11 13:19:59,2024-03-15 10:02:03,97,https://stackoverflow.com/questions/78140912/spacy-regex-pattern-does-not-work-in-rule-based-matcher,"SpaCy: Regex pattern does not work in rule-based matcher I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model. The aim is to add tokens with ""COMP"" label whenever it finds words structured like this: XXX-Ynnn XXX Ynnn Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination. To do so, I use the following method def add_component_patterns_re(input_references, model_ruler): ruler = model_ruler ref_patterns = [] letters = ['V', 'B', 'F', 'K', 'S'] print(""Adding component patterns"") for ref in input_references.iloc[:, 0]: # print(f""Adding references for system: {ref}"") for letter in letters: pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}' pattern = {""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}} ref_patterns.append({""label"":""COMP"", ""pattern"":pattern}) ruler.add_patterns(ref_patterns) return ref_patterns Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler. For information, i've also tried to change the pattern variable as a list entry, like this: pattern = [{""TEXT"": {""REGEX"": fr'{ref}(-| ){letter}[0-9]{{3}}'}}] But the result is the same, it can't seem to get any match. Does someone have any suggestion? Thanks in advance!",spacy  regex pattern work rule  base matcher try define regular expression use text pattern entity ruler component spacy model  aim add tokens   comp  label whenever find word structure like  xxx  ynnn xxx ynnn  xxx  trigram list    letter  nnn  digit combination   use follow method def addcomponentpatternsre  inputreferences  modelruler   ruler  modelruler refpatterns    letter    v    b    f    k   s   print    add component pattern   ref inputreferencesiloc    0    print  f  add reference system   ref    letter letter  patterntext  fr   ref      letter   0  9    3    pattern     text      regex   fr   ref      letter   0  9    3      refpatternsappend     label    comp     pattern   pattern   ruleraddpattern  refpattern  return refpattern printing add pattern  seem output list correct  guess something wrong defining pattern add ruler  information   ve also try change pattern variable list entry  like  pattern      text      regex   fr   ref      letter   0  9    3       result  can not seem get match  someone suggestion  thank advance ,end get print  f  add reference system   ref    letter letter  nnn range  1000   pattern  f   ref    letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  f   ref   letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  code lengthy tad slow job fine ,spacy  regex pattern work rule  base matcher try define regular expression use text pattern entity ruler component spacy model  aim add tokens   comp  label whenever find word structure like  xxx  ynnn xxx ynnn  xxx  trigram list    letter  nnn  digit combination   use follow method def addcomponentpatternsre  inputreferences  modelruler   ruler  modelruler refpatterns    letter    v    b    f    k   s   print    add component pattern   ref inputreferencesiloc    0    print  f  add reference system   ref    letter letter  patterntext  fr   ref      letter   0  9    3    pattern     text      regex   fr   ref      letter   0  9    3      refpatternsappend     label    comp     pattern   pattern   ruleraddpattern  refpattern  return refpattern printing add pattern  seem output list correct  guess something wrong defining pattern add ruler  information   ve also try change pattern variable list entry  like  pattern      text      regex   fr   ref      letter   0  9    3       result  can not seem get match  someone suggestion  thank advance  end get print  f  add reference system   ref    letter letter  nnn range  1000   pattern  f   ref    letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  f   ref   letter   nnn03d   refpatternsappend     label     comp     pattern   pattern   pattern  code lengthy tad slow job fine ,Library/Tool-Based Queries
